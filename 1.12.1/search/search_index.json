{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the AutoML-Toolkit framework docs.</p> <p>Tip</p> <p>See the navigation links in the header or side-bars. Click the  button (top left) on mobile.</p> <p>For a quick-start, check out examples for copy-pastable snippets to start from. For a more guided tour through what AutoML-Toolkit can offer, please check out our guides. If you've used AutoML-Toolkit before but need some refreshers, you can look through our reference pages or the API docs.</p>"},{"location":"#what-is-automl-toolkit","title":"What is AutoML-Toolkit?","text":"<p>AutoML-Toolkit is a highly-flexible set of modules and components, allowing you to define, search and build machine learning systems.</p> <ul> <li> <p> Python</p> <p>Use the programming language that defines modern machine learning research. We use mypy internally and for external API so you can identify and fix errors before a single line of code runs.</p> </li> </ul> <ul> <li> <p> Minimal Dependencies</p> <p>AutoML-Toolkit was designed to not introduce dependencies on your code. We support some tool integrations but only if they are optionally installed!.</p> </li> </ul> <ul> <li> <p> Plug-and-play</p> <p>We can't support all frameworks, and thankfully we don't have to. AutoML-Toolkit was designed to be plug-and-play. Integrate in your own optimizers, search spaces, execution backends, builders and more.</p> <p>We've worked hard to make sure that how we integrate tools can be done for your own tools we don't cover.</p> </li> </ul> <ul> <li> <p> Event Driven</p> <p>AutoML-Toolkit is event driven, meaning you write code that reacts to events as they happen. You can ignore, extend and create new events that have meaning to the systems you build.</p> <p>This enables tools built from AutoML-Toolkit to support greater forms of interaction, automation and deployment.</p> </li> </ul> <ul> <li> <p> Task Agnostic</p> <p>AutoML-Toolkit is task agnostic, meaning you can use it for any machine learning task. We provide a base Task which you can extend with events and functionality specific to the tasks you care about.</p> </li> </ul>"},{"location":"changelog/","title":"What's New?","text":""},{"location":"changelog/#1121-2024-08-13","title":"1.12.1 (2024-08-13)","text":""},{"location":"changelog/#fix","title":"Fix","text":"<ul> <li>pipeline: <code>request</code> now fails without default (#284)</li> </ul>"},{"location":"changelog/#1120-2024-04-24","title":"1.12.0 (2024-04-24)","text":""},{"location":"changelog/#feat","title":"Feat","text":"<ul> <li>PyTorch: Add functionality to construct a PyTorch Model from a pipeline (#276)</li> </ul>"},{"location":"changelog/#fix_1","title":"Fix","text":"<ul> <li>Pass in sampler to <code>create_study</code> (#282)</li> <li>Pytorch: Fix builders.py (#280)</li> <li>precommit issues from #276 (#277)</li> </ul>"},{"location":"changelog/#1110-2024-02-29","title":"1.11.0 (2024-02-29)","text":""},{"location":"changelog/#feat_1","title":"Feat","text":"<ul> <li>CVEvaluator: Add feature for post_split and post_processing (#260)</li> <li>sklearn: <code>X_test</code>, <code>y_test</code> to CVEvaluator (#258)</li> <li>CVEarlyStopping (#254)</li> <li>sklearn: CVEvaluator allows <code>configure</code> and <code>build</code> params (#250)</li> <li>sklearn: Provide a standard CVEvaluator (#244)</li> </ul>"},{"location":"changelog/#fix_2","title":"Fix","text":"<ul> <li>trial: Don't record metric values for deserialized NaN's or None (#263)</li> <li>pipeline: Ensure optimizer is updated with report (#261)</li> <li>scheduling: Safe termination of processes, avoiding lifetime race condition (#256)</li> <li>metalearning: Portfolio Check for Dataframe as Input (#253)</li> <li>CVEvaluator: <code>clone</code> the estimator before use (#249)</li> <li>Node: Ensure that parent name does not conflict with children (#248)</li> <li>CVEvaluator: When on_error=\"raise\", inform of which trial failed (#247)</li> <li>Trial: Give trials a created_at stamp (#246)</li> </ul>"},{"location":"changelog/#refactor","title":"Refactor","text":"<ul> <li>pipeline: <code>optimize</code> now requires one of <code>timeout</code> or (#252)</li> <li>Metric, Trial: Cleanup of metrics and <code>Trial</code> (#242)</li> </ul>"},{"location":"changelog/#1101-2024-01-28","title":"1.10.1 (2024-01-28)","text":""},{"location":"changelog/#fix_3","title":"Fix","text":"<ul> <li>dask-jobqueue: Make sure to close client</li> </ul>"},{"location":"changelog/#refactor_1","title":"Refactor","text":"<ul> <li>Make things more context manager</li> <li>trial: Remove <code>begin()</code> (#238)</li> </ul>"},{"location":"changelog/#1100-2024-01-26","title":"1.10.0 (2024-01-26)","text":""},{"location":"changelog/#feat_2","title":"Feat","text":"<ul> <li>Pipeline: Optimize pipelines directly with <code>optimize()</code> (#230)</li> </ul>"},{"location":"changelog/#190-2024-01-26","title":"1.9.0 (2024-01-26)","text":""},{"location":"changelog/#feat_3","title":"Feat","text":"<ul> <li>Optimizer: Allow for batch ask requests (#224)</li> </ul>"},{"location":"changelog/#fix_4","title":"Fix","text":"<ul> <li>Pynisher: Ensure system supports limit (#223)</li> </ul>"},{"location":"changelog/#180-2024-01-22","title":"1.8.0 (2024-01-22)","text":""},{"location":"changelog/#feat_4","title":"Feat","text":"<ul> <li>Pynisher: Detect tasks with <code>Trial</code> to report <code>FAIL</code> (#220)</li> <li>Pipeline: <code>factorize()</code> a pipeline into its possibilities (#217)</li> </ul>"},{"location":"changelog/#170-2024-01-16","title":"1.7.0 (2024-01-16)","text":""},{"location":"changelog/#feat_5","title":"Feat","text":"<ul> <li>Scheduler: Respond to cancelled futures (#214)</li> <li>scheduler: Handled errors with specific method (#213)</li> </ul>"},{"location":"changelog/#fix_5","title":"Fix","text":"<ul> <li>History: Explicitly check type in add() (#210)</li> </ul>"},{"location":"changelog/#160-2024-01-10","title":"1.6.0 (2024-01-10)","text":""},{"location":"changelog/#feat_6","title":"Feat","text":"<ul> <li>history: Get <code>best()</code> from History (#209)</li> </ul>"},{"location":"changelog/#150-2024-01-09","title":"1.5.0 (2024-01-09)","text":""},{"location":"changelog/#feat_7","title":"Feat","text":"<ul> <li>add EmissionsTrackerPlugin for codecarbon (#196)</li> </ul>"},{"location":"changelog/#140-2023-12-12","title":"1.4.0 (2023-12-12)","text":""},{"location":"changelog/#feat_8","title":"Feat","text":"<ul> <li>Scheduler: Monitor to view efficiency (#197)</li> </ul>"},{"location":"changelog/#fix_6","title":"Fix","text":"<ul> <li>data: <code>reduce_int_span</code> with nullable dtypes (#200)</li> </ul>"},{"location":"changelog/#refactor_2","title":"Refactor","text":"<ul> <li>Scheduling: <code>limit</code> -&gt; <code>max_calls</code> (#201)</li> </ul>"},{"location":"changelog/#134-2023-12-07","title":"1.3.4 (2023-12-07)","text":""},{"location":"changelog/#fix_7","title":"Fix","text":"<ul> <li>pipeline: configure only operates on chosen choice (#195)</li> </ul>"},{"location":"changelog/#133-2023-12-06","title":"1.3.3 (2023-12-06)","text":""},{"location":"changelog/#fix_8","title":"Fix","text":"<ul> <li>rich: Move import into scoped block (#193)</li> </ul>"},{"location":"changelog/#132-2023-12-06","title":"1.3.2 (2023-12-06)","text":""},{"location":"changelog/#fix_9","title":"Fix","text":"<ul> <li>_doc: Catch missing else statement for <code>doc_print</code> (#190)</li> </ul>"},{"location":"changelog/#131-2023-12-05","title":"1.3.1 (2023-12-05)","text":""},{"location":"changelog/#fix_10","title":"Fix","text":"<ul> <li>pyproject: Change to recognized classifier for PyPI (#189)</li> </ul>"},{"location":"changelog/#130-2023-12-05","title":"1.3.0 (2023-12-05)","text":""},{"location":"changelog/#feat_9","title":"Feat","text":"<ul> <li>History: Default to normalizing time of history output</li> </ul>"},{"location":"changelog/#fix_11","title":"Fix","text":"<ul> <li>Provide more information if <code>built_item</code> fails (#187)</li> <li>_doc: Use <code>isinstance</code> on types</li> <li>Optimizers: Default to optimizer name #174</li> </ul>"},{"location":"changelog/#refactor_3","title":"Refactor","text":"<ul> <li>History: History provides mutator methods</li> <li>Move <code>StoredValue</code> to own file in <code>.store</code></li> </ul>"},{"location":"changelog/#124-2023-11-25","title":"1.2.4 (2023-11-25)","text":""},{"location":"changelog/#fix_12","title":"Fix","text":"<ul> <li>docs: Optimizer inline examples (#172)</li> </ul>"},{"location":"changelog/#123-2023-11-24","title":"1.2.3 (2023-11-24)","text":""},{"location":"changelog/#fix_13","title":"Fix","text":"<ul> <li>Trial: Add table to rich renderables (#170)</li> <li>dask-jobqueue: Default to <code>scale()</code> for predictable behaviour (#168)</li> </ul>"},{"location":"changelog/#122-2023-11-23","title":"1.2.2 (2023-11-23)","text":""},{"location":"changelog/#fix_14","title":"Fix","text":"<ul> <li>test: remove stray output from test/docs</li> </ul>"},{"location":"changelog/#121-2023-11-22","title":"1.2.1 (2023-11-22)","text":""},{"location":"changelog/#fix_15","title":"Fix","text":"<ul> <li>pipeline: <code>request()</code> correctly sets config key</li> <li>Scheduler: Make sure it's displayable in notebook</li> </ul>"},{"location":"changelog/#120-2023-11-20","title":"1.2.0 (2023-11-20)","text":""},{"location":"changelog/#feat_10","title":"Feat","text":"<ul> <li>sklearn: Special keywords for splits</li> </ul>"},{"location":"changelog/#fix_16","title":"Fix","text":"<ul> <li>trials: Always use a <code>PathBucket</code> with optimizers</li> <li>Trial: Trial should now always have a bucket</li> </ul>"},{"location":"changelog/#111-2023-11-19","title":"1.1.1 (2023-11-19)","text":""},{"location":"changelog/#fix_17","title":"Fix","text":"<ul> <li>doc: Add classifiers to pypi for bades (#155)</li> </ul>"},{"location":"changelog/#110-2023-11-19","title":"1.1.0 (2023-11-19)","text":""},{"location":"changelog/#feat_11","title":"Feat","text":"<ul> <li>scheduler: method <code>call_later</code> (#145)</li> </ul>"},{"location":"changelog/#refactor_4","title":"Refactor","text":"<ul> <li>optimization: Add concept of <code>Metric</code> (#150)</li> </ul>"},{"location":"changelog/#101-2023-11-15","title":"1.0.1 (2023-11-15)","text":""},{"location":"changelog/#fix_18","title":"Fix","text":"<ul> <li>CHANGELOG: Fresh changelog</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thanks for checking out the contribution page! <code>amltk</code> is open to all open-source contributions, whether it be fixes, features, integrations or even just simple doc fixes.</p> <p>This guide will walk through some simple guidelines on the workflow of <code>amltk</code> but also some design principles that are core to its development. Some of these principles may be new for first time contributors and examples will be given where necessary.</p>"},{"location":"contributing/#clone-the-repo","title":"Clone the repo","text":"<p>Clone the repo manually or with the below <code>hub</code> cli from GitHub.</p> <pre><code>hub repo fork automl/amltk\n</code></pre>"},{"location":"contributing/#quickstart","title":"Quickstart","text":"<p>Below is a quickstart guide for those familiar with open-source development. You do not need to use <code>just</code>, however we provide it as a convenient workflow tool. Please refer to the <code>justfile</code> for the commands ran if you wish to use your own workflow.</p> <p>NOTE: If you are using Windows, please go to the Windows installation section.</p> <pre><code># Install just, the Makefile-like tool for this repo\n# https://github.com/casey/just#installation\nsudo apt install just\n\n# Make virtual env (however you like)\npython -m venv .my-virtual-env\nsource ./.my-virtual-env/bin/activate\n\n# Install the library with dev dependancies\njust install\n\n# ... make a new branch\njust pr-feat my-new-feature\n\n# ... make changes\n# ... commit changes\n\n# Run tests\njust test\n\n# Run the documentation, fix any warnings\njust docs\n# just docs-code  # Run code and display output (slower)\n# just docs-full  # Run examples and code (slowest)\n\n# Run pre-commit checks\njust check\n\n# ... fix anything that needs fixing\n\n# Push to your fork\ngit push\n\n# Create a PR (opening the browser too)\nhub pull-request --browse\n</code></pre> <p>Below we will go into more detail on each of these steps.</p>"},{"location":"contributing/#setting-up","title":"Setting up","text":"<p>The core workflows of <code>amltk</code> are accessed through the <code>justfile</code> It is recommended to have this installed with their simple one-liners on their repo. All of these were developed with bash in mind and your usage with other platforms may vary, please use the <code>justfile</code> as reference if this is the case.</p>"},{"location":"contributing/#forking","title":"Forking","text":"<p>If you are contributing from outside the <code>automl</code> org and under your own github profile, you'll have to create your own fork.</p> <p>If you use the <code>hub</code> tool from github, you can do this locally with:</p> <pre><code># Clones this repo\nhub clone automl/amltk\n\n# Forks the repo to your own user account and sets up tracking\n# to match your repo, not the automl/amltk version.\nhub fork\n</code></pre>"},{"location":"contributing/#installation","title":"Installation","text":"<p>To install <code>amltk</code> for development, we rely on specific dependencies that are not required for the actual library to run. These are listed in the <code>pyproject.toml</code> under the <code>[project.optional-dependencies]</code> header.</p> <p>You can install all of these by doing the following:</p> <pre><code># Create a virtual environment in your preferred way.\npython -m venv .my-virtual-env\nsource ./.my-virtual-env/bin/activate\n\n# Install all required dependencies\njust install\n</code></pre>"},{"location":"contributing/#setting-up-code-quality-tools","title":"Setting up code quality tools","text":"<p>When you ran <code>just install</code>, the tool <code>pre-commit</code> was installed.</p> <p>This is a framework that the repo has set up to run a set of code quality tools upon each commit, fixing up easy to fix issues, run some automatic formatting and run a static type checker on the code in the repository. The configuration for <code>pre-commit</code> can be found in <code>.pre-commit-config.yaml</code>.</p> <p>To run these checks at any time, use the command <code>just fix</code>, followed by <code>just check</code>. Any list of errors will be presented to you, and we recommend fixing these before committing.</p> <p>While these can certainly be skipped, these checks will be run using github actions, a Continuous Integration (CI) service. If there are problems you are not sure how to fix, please feel free to discuss them in the Pull Request and we will help you solve them!</p> <p>To see a list of tools used and their purposes, please see the section on Code Quality.</p>"},{"location":"contributing/#creating-a-new-branch","title":"Creating a new branch","text":"<p>We follow a Pull Request into <code>main</code> workflow, which is essentially that any contributions to <code>amltk</code> should be done in a branch with a pull request to the <code>main</code> branch. We prefer a branch name that describes the kind of pull request that it is. We have provided some default options but please feel free to use your own if you are familiar with these workflows:</p> <pre><code># These utilities will pull the most recent `main` branch,\n# create a new branch with your `branchname` and and push\n# the new branch back to github\njust pr-feat branchname  # Creates a branch feat-branchname\njust pr-doc branchname   # Creates a branch doc-branchname\njust pr-fix branchname   # Creates a branch fix-branchname\njust pr-other branchname # Creates a branch other-branchname\n</code></pre>"},{"location":"contributing/#submitting-a-pr","title":"Submitting a PR","text":"<p>If you are unfamiliar with creating a PR on github, please check out this guide.</p> <p>Please provide a short summary of your changes to help give context to any reviewers who may be looking at your code. If submitting a more complex PR that changes behaviours, please consider more context when describing not only what you changed but why and how.</p> <p>We may ask you to break up your changes into smaller atomic units that are easier to verify and review, but we will describe this process to you if required.</p>"},{"location":"contributing/#reviews","title":"Reviews","text":"<p>Once your PR is submitted, we will likely have comments and changes that are required. Please be patient as we may not be able to respond immediately. If there are only minor comments, we will simply annotate your code where these changes are required and upon fixing them, we will happily merge these into the <code>main</code> branch and thank you for your open-source contributions!</p> <p>Good practice is to actually review your own PR after submitting it. You'll often find small issues such as out-of-sync doc strings or even small logical issues. In general, if you can't understand your own PR, it's likely we won't either.</p>"},{"location":"contributing/#granting-access-to-your-fork","title":"Granting access to your fork","text":"<p>If the PR requires larger structural changes or more discussion, there will likely be a few back-and-forth discussion points which we will actively respond to help get your contribution in.</p> <p>If you do not wish to actively monitor the PR for whatever reason, granting us access to modify your PR will substantially help integration. To do so, please follow the instructions here.</p>"},{"location":"contributing/#commits","title":"Commits","text":"<p>This library uses conventional commits as a way to write commits. This makes commit messages simpler and easier to read as well as allows for an easier time managing the repo, such as changelogs and versioning. This is important enough that we even enforce this through <code>pre-commit</code> to fail the commit if the message does not follow the format. Please follow the link above to find out more but for reference, here are some short examples:</p> <pre><code>fix(scheduler): Use X instead of Y\nfeat(pipeline): Allow for Z\nrefactor(Optuna): Move integrations to seperate file\ndoc(Example): Integrating custom space parser\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Our testing for the library is done using <code>pytest</code>, with some additional utilities from <code>pytest-coverage</code> for code coverage and <code>pytest-cases</code> for test structure.</p> <p>In general, writing a test and running <code>just test</code> to test the whole library should be sufficient. If you need more fine-grained control, such as only testing a particular test, please refer to this cheatsheet.</p> <pre><code>pytest                              # Test whole library and examples\npytest \"tests/path/to/testfile.py\"  # Test a particular file\npytest -k \"test_name_of_my_test\"    # Test a particular test\n</code></pre> <p> In general, you should prefer to run <code>just test</code> over <code>pytest</code> if new to testing. This will run all test until it hits its first failure which allows for better incremental testing. It will also avoid running the examples which are often longer and saved for CI.</p>"},{"location":"contributing/#testing-examples","title":"Testing examples","text":"<p>If testing any added examples, please use the <code>just test-examples</code> command, which is a shortcut for <code>pytest \"tests/test_examples.py\" -x --lf</code>. There is unfortunately no way to sub-select one.</p> <p>If you are not sure how to test your contribution or need some pointers to get started, please reach out in the PR comments and we will be happy to assist!</p>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<p>To ensure a consistent code quality and to reduce noise in the PR, there are a selection of code quality tools that run.</p> <p>These will be run automatically before a commit can be done with <code>pre-commit</code>. The configuration for this can be found in <code>.pre-commit-config.yaml</code>. All of these can be manually triggered using <code>just check</code>.</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This will automatically run the below tools.</p>"},{"location":"contributing/#ruff-code-linting-and-formatting","title":"Ruff - Code Linting and formatting","text":"<p>The primary linter we use is <code>ruff</code>. The fixes and formatting can be done manually as so:</p> <pre><code>ruff --fix src\nruff format src\n\n# Or this which does both\njust fix\n</code></pre>"},{"location":"contributing/#mypy-static-type-checking","title":"Mypy - Static Type Checking","text":"<p>This codebase also relies heavily on pythons <code>typing</code> and <code>mypy</code> to ensure correctness across modules. Running this standalone on all files can take some time, so we don't require you to run this, our automated testers will. If you wish to do so manually, then use <code>just check-types</code>.</p> <p>If any of the typing concepts are confusing, now is a good chance to learn, and we would be happy to assist in helping properly type your PR if things do not work. If all else fails, please feel free to introduce a <code># type: ignore</code> to tell <code>mypy</code> to shut up along with a description to why it is there. This will help future contributors and maintainers understand the reasons behind these ignores. You can find a cheatsheet for basic mypy type hinting here.</p>"},{"location":"contributing/#git-workflow","title":"Git workflow","text":"<p>We follow a PR into trunk development flow (whatever that's meant to be called), in which all development is done in feature branches and the merged into <code>main</code>. The reason for feature branches is to allow multiple maintainers to actively work on <code>amltk</code> without interfering. The <code>main</code> branch is locked down, meaning commits can not be made directly to main, and features actions to trigger releases.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The documentation is handled by <code>mkdocs-material</code> with some additional plugins. This is a markdown based documentation generator which allows custom documentation and renders API documentation written in the code itself.</p> <p>Document features where the code lives. For example, if you introduce a new function, prefer to add documentation with an example in the docstring of the function itself. There is also some module level documentation which is used as a reference for the API. Most likely updating this is only required for larger changes.</p> <p>You can find a collection of features for custom documentation here as well as code reference documentation here</p> <p>You can find the entry point for the documentation infrastructure of <code>mkdocs</code> in <code>mkdocs.yml</code>.</p>"},{"location":"contributing/#viewing-documentation","title":"Viewing Documentation","text":"<p>You can live view documentation changes by running <code>just docs</code>, which will open your webbrowser and run <code>mkdocs --serve</code> to watch all files and live update any changes that occur.</p> <p>Please do not ignore warnings. The CI will fail if there are any warnings and we will not merge any PR's that have warnings.</p>"},{"location":"contributing/#example-syntax","title":"Example Syntax","text":"<p>If creating an example, there is a custom format used to render <code>.py</code> files and convert them to markdown that we can host.</p> <p>An example is just a python file, using the triple quote <code>\"\"\"</code> comments to switch between commentary and code blocks.</p> <p>The first <code>\"\"\"</code> block is special, in that the first line, in this case My Example Name is the name of the example, with anything following it being simple commentary.</p> <pre><code>\"\"\"My Example Name\n\nHere's a short description.\n\"\"\"\nfrom x import a\nfrom y import b\n\n\"\"\"\nThis is a commentary section. To see what can go in here,\ntake a look at https://squidfunk.github.io/mkdocs-material/reference/\n\nBelow we set some variables to some values.\n\n!!! note \"Special note\"\n\n    We use the variables p, q for fun.\n\"\"\"\np = 2  # (1)!\np = 3  # &lt;!&gt; (2)!\n\nprint(p)  # (3)!\n\n# 1. You can add annotations to lines, where the text to annotate goes at\n    the bottom of the code block, before the next commentary section.\n    https://squidfunk.github.io/mkdocs-material/reference/annotations/\n# 2. You can use &lt;!&gt; to highlight specific lines\n# 3. Anything printed out using `print` will be rendered\n\"\"\"\nThis concludes this example, check out ./examples for examples on how\nto create ... examples.\n\"\"\"\n</code></pre>"},{"location":"contributing/#maintainer-guide","title":"Maintainer Guide","text":"<p>This section serves as a guide for active maintainers of <code>amltk</code> to keep the ship running smoothly and help foster a growing user-base. All maintainers must be familiar with the rest of the <code>CONTRIBUTING.md</code>.</p>"},{"location":"contributing/#ethos","title":"Ethos","text":"<p>We appreciate all open-source contributions, whether that be a question, issue or PR. This also pertains to potentially first-time contributors and people new to Python and open-source in general. This includes objective non-personal criticisms. We will try to be as helpful and communicative as possible with respect to our availability, and encourage open discussion.</p> <p>To foster growth and contribution, we will guide users through the library as required and encourage any and all contributions. If more work is required on a PR, please encourage users to grant access to their fork such that we can actively contribute to their contribution and utilize a collaborative approach. This will also help prevent staling contributions.</p> <p>In the event of any individual who makes personal attacks or derogative comments, please maintain decorum, respond nicely, and if issues persist, then inform the user they will be blocked.</p> <p>Please check the <code>CODE_OF_CONDUCT.md</code> for more details.</p>"},{"location":"contributing/#merging","title":"Merging","text":"<p>We use <code>squash-merge</code> from feature branches to keep the commit log to the <code>convential-commits</code> standard. This helps automate systems.</p> <p>Please familiarize yourself with conventional commits and ensure that the PR is up-to-date with the <code>main</code> branch before merging.</p> <p>There should be no manual versioning, as this will take place during releases automatically.</p> <p>In the case of staling PR's, these will likely need a forceful rebase from the <code>main</code> branch. This often has a negative impact on the commit history of a pull request but this will be removed by <code>squash-merge</code>.</p>"},{"location":"contributing/#workflows","title":"Workflows","text":"<p>To keep things relatively uniform, we try support recommended workflows through the <code>justfile</code>. If there is a workflow that you prefer and is not covered, please add your own.</p>"},{"location":"contributing/#automation","title":"Automation","text":"<p>Maintaining repositories is time-consuming work, whether that be benchmarking, experimenting, testing, versioning, issues, pull requests, documentation and anything else tangential to code features. Any and all automation to the repository is greatly appreciated but should be documented in the <code>Maintainers</code> section.</p>"},{"location":"contributing/#dependencies","title":"Dependencies","text":"<p>One of the hardest parts of maintenance for a mature library, especially one that supports integrations from both mature and research code is managing dependencies. Where possible, prefer not adding an explicit dependency. This mainly holds for the required dependencies which all users must install. For developer dependencies, please feel free to add one with good justification. When integrating some machine learning ecosystem like <code>scikit-learn</code> or <code>pytorch</code>, please try to bundle these dependencies as optional and reflect so accordingly in the code.</p> <p>There is some utility to work with optional dependencies in <code>amltk.types</code>, such as <code>safe_isinstance</code> and <code>safe_issubclass</code>, to not rely on the library being installed for runtime type checking. For static compile time type checking, please use mypy's <code>if TYPE_CHECKING:</code> idiom. This will prevent runtime errors for users who do not have these dependencies installed. For example:</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from ConfigSpace import ConfigurationSpace\n\n\ndef draw_configspace(self, space: ConfigurationSpace) -&gt; None:\n    ...\n</code></pre> <p>The exception to this rule is any modules a user must explicitly import for the integration. In this case, it is fine to assume the user has the required dependencies and any error generated is considered user error and if possible guide them to the <code>pip install \"amltk[optional_dep]\"</code> that they require for the integration.</p>"},{"location":"contributing/#dependency-updates","title":"Dependency updates","text":"<p>We have <code>dependabot</code> enabled in the repository using the <code>.github/dependabot.yml</code>. This bot will periodically make pull requests to the repository that update dependencies. Do not accept these blindly but rather wait for any CI to finish and ensure all tests still pass.</p>"},{"location":"contributing/#windows-installation","title":"Windows Installation","text":"<p>If you are not using Windows, feel free to skip this section.</p>"},{"location":"contributing/#installing-wsl-windows-subsystem-for-linux","title":"Installing WSL (Windows Subsystem for Linux)","text":"<ol> <li>Install WSL and Ubuntu by following the steps outlined in    the official Ubuntu installation guide.</li> </ol>"},{"location":"contributing/#setting-up-pycharm-with-wsl","title":"Setting up PyCharm with WSL","text":"<ol> <li>Open the cloned repo in PyCharm and navigate to \"Add new Interpreter\" -&gt; \"On WSL...\"</li> <li>Choose WSL and specify the directory of the virtual env.</li> <li>Open a PyCharm terminal and click \"New predefined session\" and select Ubuntu.</li> </ol>"},{"location":"contributing/#installing-dependencies","title":"Installing Dependencies","text":"<p>Since WSL is a Linux environment, you need to install Python separately, even if you have it on your Windows machine.</p> <p>In the terminal, run the following commands to set up the project dependencies:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install python3.10\nsudo apt install python3-pip\npip install --upgrade pip setuptools\nsudo apt install python3.10-venv\npython3 -m venv .venv\n</code></pre> <p>Then create a virtual environment and activate it:</p> <pre><code>source .venv/bin/activate\njust install\n</code></pre>"},{"location":"api/","title":"Index","text":""},{"location":"api/#api","title":"API","text":"<p>This houses all the documentation for the various modules in the project. Use the navigation bar to the left to view more.</p>"},{"location":"api/amltk/distances/","title":"Distances","text":""},{"location":"api/amltk/distances/#amltk.distances","title":"amltk.distances","text":"<p>Distance functions.</p> <p>This module contains functions for calculating the distance between two vectors.</p>"},{"location":"api/amltk/distances/#amltk.distances.DistanceMetric","title":"DistanceMetric  <code>module-attribute</code>","text":"<pre><code>DistanceMetric: TypeAlias = Callable[\n    [ArrayLike, ArrayLike], float\n]\n</code></pre> <p>A metric used for calculating distances.</p> <p>Takes two arrays-like objects and returns a float.</p>"},{"location":"api/amltk/distances/#amltk.distances.NamedDistance","title":"NamedDistance  <code>module-attribute</code>","text":"<pre><code>NamedDistance: TypeAlias = Literal[\n    \"l1\", \"l2\", \"euclidean\", \"cosine\", \"max\"\n]\n</code></pre> <p>Predefined distance metrics.</p> <p>Possible values are:</p> <ul> <li><code>\"l1\"</code>: <code>l1_distance()</code></li> <li><code>\"l2\"</code>: <code>l2_distance()</code></li> <li><code>\"euclidean\"</code>: <code>euclidean_distance()</code></li> <li><code>\"cosine\"</code>: <code>cosine_distance()</code></li> <li><code>\"max\"</code>: <code>linf_distance()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.euclidean_distance","title":"euclidean_distance  <code>module-attribute</code>","text":"<pre><code>euclidean_distance = l2_distance\n</code></pre> <p>Calculates the euclidean distance between each column in x and y.</p> <p>Same as <code>l2_distance()</code>.</p>"},{"location":"api/amltk/distances/#amltk.distances.l1_distance","title":"l1_distance  <code>module-attribute</code>","text":"<pre><code>l1_distance = partial(pnorm, p=1)\n</code></pre> <p>Calculates the l1 distance between each column in x and y.</p> <p>The l1 distance is defined as:</p> <pre><code>`||x - y||_1 = sum_i(|x_i - y_i|)`\n</code></pre> <p>This is the sum of the absolute differences between each element in x and y.</p> See Also <ul> <li><code>pnorm()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.l2_distance","title":"l2_distance  <code>module-attribute</code>","text":"<pre><code>l2_distance = partial(pnorm, p=2)\n</code></pre> <p>Calculates the l2 distance between each column in x and y.</p> <p>The l2 distance is defined as:</p> <pre><code>`||x - y||_2 = sqrt(sum_i(|x_i - y_i|^2))`\n</code></pre> <p>This is the square root of the sum of the squared differences between each element in x and y.</p> See Also <ul> <li><code>pnorm()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.linf_distance","title":"linf_distance  <code>module-attribute</code>","text":"<pre><code>linf_distance = partial(pnorm, p=inf)\n</code></pre> <p>Calculates the linf distance between each column in x and y.</p> <p>The linf distance is defined as:</p> <pre><code>`||x - y||_inf = max_i(|x_i - y_i|)`\n</code></pre> <p>This is the maximum absolute difference between each element in x and y.</p> See Also <ul> <li><code>pnorm()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.NearestNeighborsDistance","title":"NearestNeighborsDistance","text":"<pre><code>NearestNeighborsDistance(**nn_kwargs: Any)\n</code></pre> <p>Uses sklearn.neighbors.NearestNeighbors to calculate the distance.</p> PARAMETER DESCRIPTION <code>**nn_kwargs</code> <p>Keyword arguments to pass to sklearn.neighbors.NearestNeighbors.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/amltk/distances.py</code> <pre><code>def __init__(self, **nn_kwargs: Any):\n    \"\"\"Creates a new NearestNeighborsDistance.\n\n    Args:\n        **nn_kwargs: Keyword arguments to pass to\n            [sklearn.neighbors.NearestNeighbors][].\n    \"\"\"\n    super().__init__()\n    self.nn_kwargs = nn_kwargs\n</code></pre>"},{"location":"api/amltk/distances/#amltk.distances.NearestNeighborsDistance.__call__","title":"__call__","text":"<pre><code>__call__(x: ArrayLike, y: ArrayLike) -&gt; NDArray[floating]\n</code></pre> <p>Calculates the distance between each column in x and y.</p> PARAMETER DESCRIPTION <code>x</code> <p>An array-like with columns being the features and rows being the samples.</p> <p> TYPE: <code>ArrayLike</code> </p> <code>y</code> <p>A array with the same index as x.</p> <p> TYPE: <code>ArrayLike</code> </p> RETURNS DESCRIPTION <code>NDArray[floating]</code> <p>An array with the same index as x.</p> Source code in <code>src/amltk/distances.py</code> <pre><code>def __call__(\n    self,\n    x: npt.ArrayLike,\n    y: npt.ArrayLike,\n) -&gt; npt.NDArray[np.floating]:\n    \"\"\"Calculates the distance between each column in x and y.\n\n    Args:\n        x: An array-like with columns being the features and rows being the samples.\n        y: A array with the same index as x.\n\n    Returns:\n        An array with the same index as x.\n    \"\"\"\n    from sklearn.neighbors import NearestNeighbors\n\n    self.nn = NearestNeighbors(**self.nn_kwargs)\n\n    _x = np.asarray(x)\n    _y = np.asarray(y)\n\n    if _y.ndim != 1:\n        raise ValueError(f\"y must be a 1-dimensional array. Got shape {_y.shape}\")\n\n    _y = _y.reshape(1, -1)\n    _x = _x.T\n\n    if _x.ndim == 1:\n        _x = np.asarray([_x])\n\n    self.nn.fit(_x)\n    distances, _ = self.nn.kneighbors(\n        _y,\n        n_neighbors=len(_x),\n        return_distance=True,\n    )\n    return np.asarray(distances.reshape(-1), dtype=float)\n</code></pre>"},{"location":"api/amltk/distances/#amltk.distances.cosine_distance","title":"cosine_distance","text":"<pre><code>cosine_distance(x: ArrayLike, y: ArrayLike) -&gt; float\n</code></pre> <p>Calculates the cosine distance between each column in x and y.</p> <p>The cosine distance is defined as 1 - cosine_similarity. This means the distance is 0 when the vectors are identical, 1 when orthogonal and 2 when they are opposite.</p> PARAMETER DESCRIPTION <code>x</code> <p>A dataframe with columns being the features and rows being the samples.</p> <p> TYPE: <code>ArrayLike</code> </p> <code>y</code> <p>A series with the same index as x.</p> <p> TYPE: <code>ArrayLike</code> </p> RETURNS DESCRIPTION <code>float</code> <p>A series with the same index as x.</p> Source code in <code>src/amltk/distances.py</code> <pre><code>def cosine_distance(x: npt.ArrayLike, y: npt.ArrayLike) -&gt; float:\n    \"\"\"Calculates the cosine distance between each column in x and y.\n\n    The cosine distance is defined as 1 - cosine_similarity. This means\n    the distance is 0 when the vectors are identical, 1 when orthogonal\n    and 2 when they are opposite.\n\n    Args:\n        x: A dataframe with columns being the features and rows being the samples.\n        y: A series with the same index as x.\n\n    Returns:\n        A series with the same index as x.\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    cosine_similarity = np.dot(x, y) / (_norm(x) * _norm(y))\n    return float(1 - cosine_similarity)\n</code></pre>"},{"location":"api/amltk/distances/#amltk.distances.pnorm","title":"pnorm","text":"<pre><code>pnorm(\n    x: ArrayLike, y: ArrayLike, p: int | float = 2\n) -&gt; float\n</code></pre> <p>Calculates the p-norm between each column in x and y.</p> <p>The p-norm is defined as:</p> <pre><code>`||x - y||_p = (sum_i(|x_i - y_i|^p))^(1/p)`\n</code></pre> <p>The common values for p are 1, 2 and infinity.</p> <ul> <li><code>l1_distance()</code></li> <li><code>l2_distance()</code></li> <li><code>linf_distance()</code></li> </ul> <p>Using a <code>partial</code></p> <p>To use this function with <code>dataset_distance()</code>, you can wrap this in <code>functools.partial()</code>.</p> <pre><code>from functools import partial\nfrom amltk.metalearning import dataset_distance\nfrom amltk.distances import pnorm\n\ndataset_distance(\n    target,\n    dataset_metafeatures,\n    method=partial(pnorm, p=3), # (1)!\n)\n</code></pre> <ol> <li><code>partial()</code> creates a new function with the <code>p</code> argument set to 3.</li> </ol> PARAMETER DESCRIPTION <code>x</code> <p>The vector to compare.</p> <p> TYPE: <code>ArrayLike</code> </p> <code>y</code> <p>The vector to compute the distance to</p> <p> TYPE: <code>ArrayLike</code> </p> <code>p</code> <p>The p in p-norm.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>2</code> </p> RETURNS DESCRIPTION <code>float</code> <p>A series with the same index as x.</p> Source code in <code>src/amltk/distances.py</code> <pre><code>def pnorm(\n    x: npt.ArrayLike,\n    y: npt.ArrayLike,\n    p: int | float = 2,\n) -&gt; float:\n    \"\"\"Calculates the p-norm between each column in x and y.\n\n    The p-norm is defined as:\n\n        `||x - y||_p = (sum_i(|x_i - y_i|^p))^(1/p)`\n\n    The common values for p are 1, 2 and infinity.\n\n    * [`l1_distance()`][amltk.distances.l1_distance]\n    * [`l2_distance()`][amltk.distances.l2_distance]\n    * [`linf_distance()`][amltk.distances.linf_distance]\n\n    !!! tip \"Using a `partial`\"\n\n        To use this function with\n        [`dataset_distance()`][amltk.metalearning.dataset_distance],\n        you can wrap this in [`functools.partial()`][functools.partial].\n\n        ```python\n        from functools import partial\n        from amltk.metalearning import dataset_distance\n        from amltk.distances import pnorm\n\n        dataset_distance(\n            target,\n            dataset_metafeatures,\n            method=partial(pnorm, p=3), # (1)!\n        )\n        ```\n\n        1. [`partial()`][functools.partial] creates a new function with the\n        `p` argument set to 3.\n\n    Args:\n        x: The vector to compare.\n        y: The vector to compute the distance to\n        p: The p in p-norm.\n\n    Returns:\n        A series with the same index as x.\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    if p is np.inf:\n        return float(np.max(np.abs(x - y)))\n\n    return float(np.linalg.norm(x - y, ord=p))\n</code></pre>"},{"location":"api/amltk/exceptions/","title":"Exceptions","text":""},{"location":"api/amltk/exceptions/#amltk.exceptions","title":"amltk.exceptions","text":"<p>A module holding a decorator to wrap a function to add a traceback to any exception raised.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.AutomaticParameterWarning","title":"AutomaticParameterWarning","text":"<p>               Bases: <code>UserWarning</code></p> <p>Raised when an \"auto\" parameter of a function is used and triggers some behaviour which would be better explicitly set.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.AutomaticTaskTypeInferredWarning","title":"AutomaticTaskTypeInferredWarning","text":"<p>               Bases: <code>TaskTypeWarning</code>, <code>AutomaticParameterWarning</code></p> <p>A warning raised when the task type is inferred from the target data.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.AutomaticThreadPoolCTLWarning","title":"AutomaticThreadPoolCTLWarning","text":"<p>               Bases: <code>AutomaticParameterWarning</code></p> <p>Raised when automatic threadpoolctl is enabled.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.CVEarlyStoppedError","title":"CVEarlyStoppedError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>An exception raised when a CV evaluation is early stopped.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.ComponentBuildError","title":"ComponentBuildError","text":"<p>               Bases: <code>TypeError</code></p> <p>Raised when failing to build a component.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.DuplicateNamesError","title":"DuplicateNamesError","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when duplicate names are found.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.EventNotKnownError","title":"EventNotKnownError","text":"<p>               Bases: <code>ValueError</code></p> <p>The event is not a known one.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.ImplicitMetricConversionWarning","title":"ImplicitMetricConversionWarning","text":"<p>               Bases: <code>UserWarning</code></p> <p>A warning raised when a metric is implicitly converted to an sklearn scorer.</p> <p>This is raised when a metric is provided with a custom function and is implicitly converted to an sklearn scorer. This may fail in some cases and it is recommended to explicitly convert the metric to an sklearn scorer with <code>make_scorer</code> and then pass it to the metric with <code>Metric(fn=...)</code>.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.IntegrationNotFoundError","title":"IntegrationNotFoundError","text":"<pre><code>IntegrationNotFoundError(name: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>An exception raised when no integration is found.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the integration that was not found.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        name: The name of the integration that was not found.\n    \"\"\"\n    super().__init__(f\"No integration found for {name}.\")\n</code></pre>"},{"location":"api/amltk/exceptions/#amltk.exceptions.MatchChosenDimensionsError","title":"MatchChosenDimensionsError","text":"<pre><code>MatchChosenDimensionsError(\n    choice_name: str,\n    chosen_node_name: str | None = None,\n    *args: Any\n)\n</code></pre> <p>               Bases: <code>KeyError</code></p> <p>An exception raised related to matching dimensions for chosen nodes.</p> PARAMETER DESCRIPTION <code>choice_name</code> <p>The name of the choice that caused the error.</p> <p> TYPE: <code>str</code> </p> <code>chosen_node_name</code> <p>The name of the chosen node if available.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>*args</code> <p>Additional arguments to pass to the exception.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def __init__(\n    self,\n    choice_name: str,\n    chosen_node_name: str | None = None,\n    *args: Any,\n) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        choice_name: The name of the choice that caused the error.\n        chosen_node_name: The name of the chosen node if available.\n        *args: Additional arguments to pass to the exception.\n    \"\"\"\n    if chosen_node_name:\n        message = (\n            f\"Error in matching dimensions for chosen node '{chosen_node_name}' \"\n            f\"of Choice '{choice_name}'. Make sure that the names for \"\n            f\"Choice and MatchChosenDimensions 'choices' parameters match.\"\n        )\n    else:\n        message = (\n            f\"Choice name '{choice_name}' is not found in the chosen nodes.\"\n            f\"Make sure that the names for Choice and \"\n            f\"MatchChosenDimensions 'choice_name' parameters match.\"\n        )\n    super().__init__(message, *args)\n</code></pre>"},{"location":"api/amltk/exceptions/#amltk.exceptions.MatchDimensionsError","title":"MatchDimensionsError","text":"<pre><code>MatchDimensionsError(\n    layer_name: str, param: str | None, *args: Any\n)\n</code></pre> <p>               Bases: <code>KeyError</code></p> <p>An exception raised for errors related to matching dimensions in a pipeline.</p> PARAMETER DESCRIPTION <code>layer_name</code> <p>The name of the layer.</p> <p> TYPE: <code>str</code> </p> <code>param</code> <p>The parameter causing the error, if any.</p> <p> TYPE: <code>str | None</code> </p> <code>*args</code> <p>Additional arguments to pass to the exception.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def __init__(self, layer_name: str, param: str | None, *args: Any) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        layer_name: The name of the layer.\n        param: The parameter causing the error, if any.\n        *args: Additional arguments to pass to the exception.\n    \"\"\"\n    if param:\n        super().__init__(\n            f\"Error in matching dimensions for layer '{layer_name}'. \"\n            f\"Parameter '{param}' not found in the configuration.\",\n            *args,\n        )\n    else:\n        super().__init__(\n            f\"Error in matching dimensions for layer '{layer_name}'.\"\n            f\" Configuration not found.\",\n            *args,\n        )\n</code></pre>"},{"location":"api/amltk/exceptions/#amltk.exceptions.MismatchedTaskTypeWarning","title":"MismatchedTaskTypeWarning","text":"<p>               Bases: <code>TaskTypeWarning</code></p> <p>A warning raised when inferred task type with <code>task_hint</code> does not match the inferred task type from the target data.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.NoChoiceMadeError","title":"NoChoiceMadeError","text":"<p>               Bases: <code>ValueError</code></p> <p>No choice was made.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.NodeNotFoundError","title":"NodeNotFoundError","text":"<p>               Bases: <code>ValueError</code></p> <p>The node was not found.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.RequestNotMetError","title":"RequestNotMetError","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when a request is not met.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.SchedulerNotRunningError","title":"SchedulerNotRunningError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>The scheduler is not running.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.TaskTypeWarning","title":"TaskTypeWarning","text":"<p>               Bases: <code>UserWarning</code></p> <p>A warning raised about the task type.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.TrialError","title":"TrialError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>An exception raised from a trial and it is meant to be raised directly to the user.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.safe_map","title":"safe_map","text":"<pre><code>safe_map(\n    f: Callable[..., R], args: Iterable[Any]\n) -&gt; Iterator[R | tuple[Exception, str]]\n</code></pre> <p>Map a function over an iterable, catching any exceptions.</p> PARAMETER DESCRIPTION <code>f</code> <p>The function to map.</p> <p> TYPE: <code>Callable[..., R]</code> </p> <code>args</code> <p>The iterable to map over.</p> <p> TYPE: <code>Iterable[Any]</code> </p> YIELDS DESCRIPTION <code>R | tuple[Exception, str]</code> <p>The return value of the function, or the exception raised.</p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def safe_map(\n    f: Callable[..., R],\n    args: Iterable[Any],\n) -&gt; Iterator[R | tuple[Exception, str]]:\n    \"\"\"Map a function over an iterable, catching any exceptions.\n\n    Args:\n        f: The function to map.\n        args: The iterable to map over.\n\n    Yields:\n        The return value of the function, or the exception raised.\n    \"\"\"\n    for arg in args:\n        try:\n            yield f(arg)\n        except Exception as e:  # noqa: BLE001\n            yield e, traceback.format_exc()\n</code></pre>"},{"location":"api/amltk/exceptions/#amltk.exceptions.safe_starmap","title":"safe_starmap","text":"<pre><code>safe_starmap(\n    f: Callable[..., R], args: Iterable[Iterable[Any]]\n) -&gt; Iterator[R | tuple[Exception, str]]\n</code></pre> <p>Map a function over an iterable, catching any exceptions.</p> PARAMETER DESCRIPTION <code>f</code> <p>The function to map.</p> <p> TYPE: <code>Callable[..., R]</code> </p> <code>args</code> <p>The iterable to map over.</p> <p> TYPE: <code>Iterable[Iterable[Any]]</code> </p> YIELDS DESCRIPTION <code>R | tuple[Exception, str]</code> <p>The return value of the function, or the exception raised.</p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def safe_starmap(\n    f: Callable[..., R],\n    args: Iterable[Iterable[Any]],\n) -&gt; Iterator[R | tuple[Exception, str]]:\n    \"\"\"Map a function over an iterable, catching any exceptions.\n\n    Args:\n        f: The function to map.\n        args: The iterable to map over.\n\n    Yields:\n        The return value of the function, or the exception raised.\n    \"\"\"\n    for arg in args:\n        try:\n            yield f(*arg)\n        except Exception as e:  # noqa: BLE001\n            yield e, traceback.format_exc()\n</code></pre>"},{"location":"api/amltk/options/","title":"Options","text":""},{"location":"api/amltk/options/#amltk.options","title":"amltk.options","text":"<p>Options for the AMTLK package.</p> <p>In general, these options are not intended for functional differences but more for any output generated by the package, such as rich or logging.</p>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions","title":"AMLTKOptions","text":"<p>               Bases: <code>TypedDict</code></p> <p>The options available for AMTLK.</p> <pre><code>from amltk import options\n\nprint(options)\n</code></pre> <pre><code>&lt;module 'amltk.options' from '/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/site-packages/amltk/options.py'&gt;\n</code></pre>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions.links","title":"links  <code>instance-attribute</code>","text":"<pre><code>links: dict[str, str | Callable[[str], str]]\n</code></pre> <p>The links to use in rich output.</p> <p>The keys are the names of the packages, and the values are either the direct link to use or a callable that takes the fully qualified name of the object and returns the link to use.</p>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions.rich_link","title":"rich_link  <code>instance-attribute</code>","text":"<pre><code>rich_link: Literal['auto', False]\n</code></pre> <p>Whether to use links in rich output.</p>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions.rich_signatures","title":"rich_signatures  <code>instance-attribute</code>","text":"<pre><code>rich_signatures: bool\n</code></pre> <p>Whether to display full signatures in rich output.</p>"},{"location":"api/amltk/options/#amltk.options.get_option","title":"get_option","text":"<pre><code>get_option(\n    name: str, default: T | None = None\n) -&gt; Any | T | None\n</code></pre> <p>Get an option.</p> <pre><code>from amltk import options\n\nprint(options.get_option(\"rich_signatures\"))\n</code></pre> <pre><code>True\n</code></pre> Source code in <code>src/amltk/options.py</code> <pre><code>def get_option(name: str, default: T | None = None) -&gt; Any | T | None:\n    \"\"\"Get an option.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\"\n    from amltk import options\n\n    print(options.get_option(\"rich_signatures\"))\n    ```\n    \"\"\"\n    return _amltk_options.get(name, default)\n</code></pre>"},{"location":"api/amltk/randomness/","title":"Randomness","text":""},{"location":"api/amltk/randomness/#amltk.randomness","title":"amltk.randomness","text":"<p>Utilities for dealing with randomness.</p>"},{"location":"api/amltk/randomness/#amltk.randomness.as_int","title":"as_int","text":"<pre><code>as_int(seed: Seed | None = None) -&gt; int\n</code></pre> <p>Converts a valid seed arg into an integer.</p> PARAMETER DESCRIPTION <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>A valid integer to use as a seed</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def as_int(seed: Seed | None = None) -&gt; int:\n    \"\"\"Converts a valid seed arg into an integer.\n\n    Args:\n        seed: The seed to use\n\n    Returns:\n        A valid integer to use as a seed\n    \"\"\"\n    match seed:\n        case None:\n            return int(np.random.default_rng().integers(0, MAX_INT))\n        case np.integer() | int():\n            return int(seed)\n        case np.random.Generator():\n            return int(seed.integers(0, MAX_INT))\n        case np.random.RandomState():\n            return int(seed.randint(0, MAX_INT))\n\n    raise ValueError(f\"Can't {seed=} ({type(seed)}) to create int\")\n</code></pre>"},{"location":"api/amltk/randomness/#amltk.randomness.as_randomstate","title":"as_randomstate","text":"<pre><code>as_randomstate(seed: Seed | None = None) -&gt; RandomState\n</code></pre> <p>Converts a valid seed arg into a numpy.random.RandomState instance.</p> PARAMETER DESCRIPTION <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>RandomState</code> <p>A valid np.random.RandomSTate object to use</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def as_randomstate(seed: Seed | None = None) -&gt; np.random.RandomState:\n    \"\"\"Converts a valid seed arg into a numpy.random.RandomState instance.\n\n    Args:\n        seed: The seed to use\n\n    Returns:\n        A valid np.random.RandomSTate object to use\n    \"\"\"\n    match seed:\n        case None | int() | np.integer():\n            return np.random.RandomState(seed)\n        case np.random.RandomState():\n            return seed\n        case np.random.Generator():\n            _seed = seed.integers(0, MAX_INT)\n            return np.random.RandomState(_seed)\n\n    raise ValueError(f\"Can't {seed=} ({type(seed)}) to create numpy.random.RandomState\")\n</code></pre>"},{"location":"api/amltk/randomness/#amltk.randomness.as_rng","title":"as_rng","text":"<pre><code>as_rng(seed: Seed | None = None) -&gt; Generator\n</code></pre> <p>Converts a valid seed arg into a numpy.random.Generator instance.</p> PARAMETER DESCRIPTION <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Generator</code> <p>A valid np.random.Generator object to use</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def as_rng(seed: Seed | None = None) -&gt; np.random.Generator:\n    \"\"\"Converts a valid seed arg into a numpy.random.Generator instance.\n\n    Args:\n        seed: The seed to use\n\n    Returns:\n        A valid np.random.Generator object to use\n    \"\"\"\n    match seed:\n        case None | int() | np.integer():\n            return np.random.default_rng(seed)\n        case np.random.Generator():\n            return seed\n        case np.random.RandomState():\n            _seed = seed.randint(0, MAX_INT)\n            return np.random.default_rng(_seed)\n\n    raise ValueError(f\"Can't {seed=} ({type(seed)}) to create numpy.random.Generator\")\n</code></pre>"},{"location":"api/amltk/randomness/#amltk.randomness.randuid","title":"randuid","text":"<pre><code>randuid(\n    k: int = 8,\n    *,\n    charset: Sequence[str] = ALPHABET,\n    seed: Seed | None = None\n) -&gt; str\n</code></pre> <p>Generate a random alpha-numeric uuid of a specified length.</p> <p>See: stackoverflow.com/a/56398787/5332072</p> PARAMETER DESCRIPTION <code>k</code> <p>The length of the uuid to generate</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>charset</code> <p>The charset to use</p> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>ALPHABET</code> </p> <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A random uid</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def randuid(\n    k: int = 8,\n    *,\n    charset: Sequence[str] = ALPHABET,\n    seed: Seed | None = None,\n) -&gt; str:\n    \"\"\"Generate a random alpha-numeric uuid of a specified length.\n\n    See: https://stackoverflow.com/a/56398787/5332072\n\n    Args:\n        k: The length of the uuid to generate\n        charset: The charset to use\n        seed: The seed to use\n\n    Returns:\n        A random uid\n    \"\"\"\n    rng = as_rng(seed)\n    return \"\".join(rng.choice(np.asarray(charset), size=k))\n</code></pre>"},{"location":"api/amltk/types/","title":"Types","text":""},{"location":"api/amltk/types/#amltk.types","title":"amltk.types","text":"<p>Stores low-level types used through the library.</p>"},{"location":"api/amltk/types/#amltk.types.Config","title":"Config  <code>module-attribute</code>","text":"<pre><code>Config: TypeAlias = Mapping[str, Any]\n</code></pre> <p>An object representing a configuration of a pipeline.</p>"},{"location":"api/amltk/types/#amltk.types.FidT","title":"FidT  <code>module-attribute</code>","text":"<pre><code>FidT: TypeAlias = (\n    tuple[int, int] | tuple[float, float] | list[Any]\n)\n</code></pre> <p>Type alias for a fidelity bound.</p>"},{"location":"api/amltk/types/#amltk.types.Item","title":"Item  <code>module-attribute</code>","text":"<pre><code>Item = TypeVar('Item')\n</code></pre> <p>The type associated with components, splits and choices</p>"},{"location":"api/amltk/types/#amltk.types.Seed","title":"Seed  <code>module-attribute</code>","text":"<pre><code>Seed: TypeAlias = int | integer | RandomState | Generator\n</code></pre> <p>Type alias for kinds of Seeded objects.</p>"},{"location":"api/amltk/types/#amltk.types.SortedIterable","title":"SortedIterable  <code>module-attribute</code>","text":"<pre><code>SortedIterable: TypeAlias = Iterable[T]\n</code></pre> <p>An iterable that is sorted. Only useful for typing</p>"},{"location":"api/amltk/types/#amltk.types.SortedSequence","title":"SortedSequence  <code>module-attribute</code>","text":"<pre><code>SortedSequence: TypeAlias = Sequence[T]\n</code></pre> <p>A sequence that is sorted. Only useful for typing</p>"},{"location":"api/amltk/types/#amltk.types.Space","title":"Space  <code>module-attribute</code>","text":"<pre><code>Space = TypeVar('Space')\n</code></pre> <p>Generic for objects that are aware of a space but not the specific kind</p>"},{"location":"api/amltk/types/#amltk.types.Comparable","title":"Comparable","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for annotating comparable types.</p>"},{"location":"api/amltk/types/#amltk.types.Requeue","title":"Requeue","text":"<pre><code>Requeue(generator: Iterable[T])\n</code></pre> <p>               Bases: <code>Iterator[T]</code></p> <p>A queue that can have items requeued.</p> Requeue<pre><code>import random\nfrom amltk.types import Requeue\n\nname_generator = iter([\"Alice\", \"Bob\", \"Charlie\"])\nqueue: Requeue[str] = Requeue(name_generator)\n\nrng = random.Random(1)\n\ndef process_name(name: str) -&gt; bool:\n    return rng.choice([True, False])\n\nfor name in queue:\n    print(f\"Processing {name}\")\n    processed = process_name(name)\n    if not processed:\n        print(f\"Failed to process {name}, requeuing\")\n        queue.requeue(name)\n</code></pre> <pre><code>Processing Alice\nProcessing Bob\nProcessing Charlie\nFailed to process Charlie, requeuing\nProcessing Charlie\n</code></pre> See Also <ul> <li> <p><code>Requeue.from_func(f)</code></p> <p>If you have a function which will generate items, you can use this to create a requeue from it.</p> </li> <li> <p><code>.append(item)</code></p> <p>Append an item to the end of the queue</p> </li> <li> <p><code>.requeue(item)</code></p> <p>Requeue an item to the start of the queue</p> </li> </ul> Source code in <code>src/amltk/types.py</code> <pre><code>def __init__(self, generator: Iterable[T]) -&gt; None:\n    \"\"\"Create a requeue from an iterable.\"\"\"\n    super().__init__()\n    self.generator = iter(generator)\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.Requeue.append","title":"append","text":"<pre><code>append(item: T) -&gt; None\n</code></pre> <p>Append an item to the queue.</p> Source code in <code>src/amltk/types.py</code> <pre><code>def append(self, item: T) -&gt; None:\n    \"\"\"Append an item to the queue.\"\"\"\n    self.generator = chain(self.generator, [item])\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.Requeue.from_func","title":"from_func  <code>classmethod</code>","text":"<pre><code>from_func(\n    func: Callable[[], T], n: int | None = None\n) -&gt; Requeue[T]\n</code></pre> <p>Create a Requeue from a function.</p> Source code in <code>src/amltk/types.py</code> <pre><code>@classmethod\ndef from_func(cls, func: Callable[[], T], n: int | None = None) -&gt; Requeue[T]:\n    \"\"\"Create a Requeue from a function.\"\"\"\n    repeater = repeat(None) if n is None else repeat(None, times=n)\n    return cls(func() for _ in repeater)\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.Requeue.requeue","title":"requeue","text":"<pre><code>requeue(item: T) -&gt; None\n</code></pre> <p>Requeue an item.</p> Source code in <code>src/amltk/types.py</code> <pre><code>def requeue(self, item: T) -&gt; None:\n    \"\"\"Requeue an item.\"\"\"\n    self.generator = chain([item], self.generator)\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.assert_never","title":"assert_never","text":"<pre><code>assert_never(value: NoReturn) -&gt; NoReturn\n</code></pre> <p>Utility function for asserting that a value is never reached.</p> Source code in <code>src/amltk/types.py</code> <pre><code>def assert_never(value: NoReturn) -&gt; NoReturn:\n    \"\"\"Utility function for asserting that a value is never reached.\"\"\"\n    # This also works in runtime as well:\n    raise AssertionError(f\"This code should never be reached, got: {value}\")\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.safe_isinstance","title":"safe_isinstance","text":"<pre><code>safe_isinstance(obj: Any, t: str | tuple[str, ...]) -&gt; bool\n</code></pre> <p>Check if an object is of a given type.</p> <p>This is a safe version of isinstance that relies on strings, which is useful for when the type is not importable.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to check</p> <p> TYPE: <code>Any</code> </p> <code>t</code> <p>The type to check for.</p> <p> TYPE: <code>str | tuple[str, ...]</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>bool</p> Source code in <code>src/amltk/types.py</code> <pre><code>def safe_isinstance(obj: Any, t: str | tuple[str, ...]) -&gt; bool:\n    \"\"\"Check if an object is of a given type.\n\n    This is a safe version of isinstance that relies on strings,\n    which is useful for when the type is not importable.\n\n    Args:\n        obj: The object to check\n        t: The type to check for.\n\n    Returns:\n        bool\n    \"\"\"\n    return safe_issubclass(type(obj), t)\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.safe_issubclass","title":"safe_issubclass","text":"<pre><code>safe_issubclass(\n    cls: type, classes: str | tuple[str, ...]\n) -&gt; bool\n</code></pre> <p>Check if a class is a subclass of a given type.</p> <p>This is a safe version of issubclass that relies on strings, which is useful for when the type is not importable.</p> PARAMETER DESCRIPTION <code>cls</code> <p>The class to check</p> <p> TYPE: <code>type</code> </p> <code>classes</code> <p>The type to check for.</p> <p> TYPE: <code>str | tuple[str, ...]</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>bool</p> Source code in <code>src/amltk/types.py</code> <pre><code>def safe_issubclass(cls: type, classes: str | tuple[str, ...]) -&gt; bool:\n    \"\"\"Check if a class is a subclass of a given type.\n\n    This is a safe version of issubclass that relies on strings,\n    which is useful for when the type is not importable.\n\n    Args:\n        cls: The class to check\n        classes: The type to check for.\n\n    Returns:\n        bool\n    \"\"\"\n\n    def type_names(o: type) -&gt; Iterator[str]:\n        yield o.__qualname__\n        for parent in o.__bases__:\n            yield from type_names(parent)\n\n    allowable_names = {classes} if isinstance(classes, str) else set(classes)\n    return any(name in allowable_names for name in type_names(cls))\n</code></pre>"},{"location":"api/amltk/data/conversions/","title":"Conversions","text":""},{"location":"api/amltk/data/conversions/#amltk.data.conversions","title":"amltk.data.conversions","text":"<p>Conversions between different data repesentations and formats.</p>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.as_str_dtype_if_str_object","title":"as_str_dtype_if_str_object","text":"<pre><code>as_str_dtype_if_str_object(x: ndarray) -&gt; ndarray\n</code></pre> <p>Convert to string dtype if object dtype and string values.</p> PARAMETER DESCRIPTION <code>x</code> <p>The data to convert</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The converted data if it can be done</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def as_str_dtype_if_str_object(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert to string dtype if object dtype and string values.\n\n    Args:\n        x: The data to convert\n\n    Returns:\n        The converted data if it can be done\n    \"\"\"\n    if is_str_object_dtype(x):\n        return x.astype(str)\n    return x\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.flatten_if_1d","title":"flatten_if_1d","text":"<pre><code>flatten_if_1d(\n    x: ndarray | DataFrame | Series,\n) -&gt; ndarray | DataFrame | Series\n</code></pre> <p>Flatten if 1d.</p> <p>Retains the type of the input, i.e. pandas stays pandas and numpy stays numpy.</p> PARAMETER DESCRIPTION <code>x</code> <p>The data to flatten</p> <p> TYPE: <code>ndarray | DataFrame | Series</code> </p> RETURNS DESCRIPTION <code>ndarray | DataFrame | Series</code> <p>The flattened data</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def flatten_if_1d(\n    x: np.ndarray | pd.DataFrame | pd.Series,\n) -&gt; np.ndarray | pd.DataFrame | pd.Series:\n    \"\"\"Flatten if 1d.\n\n    Retains the type of the input, i.e. pandas stays pandas and numpy stays numpy.\n\n    Args:\n        x: The data to flatten\n\n    Returns:\n        The flattened data\n    \"\"\"\n    if isinstance(x, np.ndarray) and x.ndim == 2 and x.shape[1] == 1:  # noqa: PLR2004\n        x = np.ravel(x)\n    elif (\n        isinstance(x, pd.DataFrame) and x.ndim == 2 and x.shape[1] == 1  # noqa: PLR2004\n    ):\n        x = x.iloc[:, 0]\n\n    return x\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.is_str_object_dtype","title":"is_str_object_dtype","text":"<pre><code>is_str_object_dtype(x: ndarray) -&gt; bool\n</code></pre> <p>Check if object dtype and string values.</p> PARAMETER DESCRIPTION <code>x</code> <p>The data to check</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether it is object dtype and string values</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def is_str_object_dtype(x: np.ndarray) -&gt; bool:\n    \"\"\"Check if object dtype and string values.\n\n    Args:\n        x: The data to check\n\n    Returns:\n        Whether it is object dtype and string values\n    \"\"\"\n    return x.dtype == object and isinstance(x[0], str)\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.probabilities_to_classes","title":"probabilities_to_classes","text":"<pre><code>probabilities_to_classes(\n    probabilities: NDArray[floating],\n    classes: ndarray | ArrayLike | list,\n) -&gt; ndarray\n</code></pre> <p>Convert probabilities to classes.</p> <p>Note</p> <p>Converts using the logic of <code>predict()</code> of <code>RandomForestClassifier</code>.</p> PARAMETER DESCRIPTION <code>probabilities</code> <p>The probabilities to convert</p> <p> TYPE: <code>NDArray[floating]</code> </p> <code>classes</code> <p>The classes to use.</p> <p> TYPE: <code>ndarray | ArrayLike | list</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The classes corresponding to the probabilities</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def probabilities_to_classes(\n    probabilities: npt.NDArray[np.floating],\n    classes: np.ndarray | npt.ArrayLike | list,\n) -&gt; np.ndarray:\n    \"\"\"Convert probabilities to classes.\n\n    !!! note\n\n        Converts using the logic of `predict()` of `RandomForestClassifier`.\n\n    Args:\n        probabilities: The probabilities to convert\n        classes: The classes to use.\n\n    Returns:\n        The classes corresponding to the probabilities\n    \"\"\"\n    # Taken from `predict()` of RandomForestclassifier\n    classes = np.asarray(classes)\n    n_outputs = 1 if classes.ndim == 1 else classes.shape[1]\n    if n_outputs == 1:\n        return classes.take(np.argmax(probabilities, axis=1), axis=0)  # type: ignore\n\n    n_samples = probabilities[0].shape[0]\n    # all dtypes should be the same, so just take the first\n    class_type = classes[0].dtype\n    predictions = np.empty((n_samples, n_outputs), dtype=class_type)\n\n    for k in range(n_outputs):\n        predictions[:, k] = classes[k].take(np.argmax(probabilities[k], axis=1), axis=0)\n\n    return predictions\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy(\n    x: ndarray | DataFrame | Series,\n    *,\n    flatten_if_1d: bool = False\n) -&gt; ndarray\n</code></pre> <p>Convert to numpy array.</p> PARAMETER DESCRIPTION <code>x</code> <p>The data to convert</p> <p> TYPE: <code>ndarray | DataFrame | Series</code> </p> <code>flatten_if_1d</code> <p>Whether to flatten the array if it is 1d</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The converted data</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def to_numpy(\n    x: np.ndarray | pd.DataFrame | pd.Series,\n    *,\n    flatten_if_1d: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Convert to numpy array.\n\n    Args:\n        x: The data to convert\n        flatten_if_1d: Whether to flatten the array if it is 1d\n\n    Returns:\n        The converted data\n    \"\"\"\n    _x = x.to_numpy() if isinstance(x, pd.DataFrame | pd.Series) else np.asarray(x)\n\n    if (\n        flatten_if_1d\n        and x.ndim == 2  # noqa: PLR2004 # type: ignore\n        and x.shape[1] == 1  # type: ignore\n    ):\n        _x = np.ravel(_x)\n\n    assert isinstance(_x, np.ndarray)\n    return _x\n</code></pre>"},{"location":"api/amltk/data/dtype_reduction/","title":"Dtype reduction","text":""},{"location":"api/amltk/data/dtype_reduction/#amltk.data.dtype_reduction","title":"amltk.data.dtype_reduction","text":"<p>Reduce the dtypes of data.</p>"},{"location":"api/amltk/data/dtype_reduction/#amltk.data.dtype_reduction.reduce_dtypes","title":"reduce_dtypes","text":"<pre><code>reduce_dtypes(\n    x: D,\n    *,\n    reduce_int: bool = True,\n    reduce_float: bool = True\n) -&gt; D\n</code></pre> <p>Reduce the dtypes of data.</p> <p>When a dataframe, will reduce the dtypes of all columns. When applied to an iterable, will apply to all elements of the iterable.</p> <p>For an int array, will reduce to the smallest dtype that can hold the minimum and maximum values of the array. Otherwise for floats, will reduce by one step, i.e. float32 -&gt; float16, float64 -&gt; float32.</p> PARAMETER DESCRIPTION <code>x</code> <p>The data to reduce.</p> <p> TYPE: <code>D</code> </p> <code>reduce_int</code> <p>Whether to reduce integer dtypes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>reduce_float</code> <p>Whether to reduce floating point dtypes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/data/dtype_reduction.py</code> <pre><code>def reduce_dtypes(x: D, *, reduce_int: bool = True, reduce_float: bool = True) -&gt; D:\n    \"\"\"Reduce the dtypes of data.\n\n    When a dataframe, will reduce the dtypes of all columns.\n    When applied to an iterable, will apply to all elements of the iterable.\n\n    For an int array, will reduce to the smallest dtype that can hold the\n    minimum and maximum values of the array. Otherwise for floats, will reduce\n    by one step, i.e. float32 -&gt; float16, float64 -&gt; float32.\n\n    Args:\n        x: The data to reduce.\n        reduce_int: Whether to reduce integer dtypes.\n        reduce_float: Whether to reduce floating point dtypes.\n    \"\"\"\n    if not isinstance(x, pd.DataFrame | pd.Series | np.ndarray):\n        raise TypeError(f\"Cannot reduce data of type {type(x)}.\")\n\n    if isinstance(x, pd.Series | pd.DataFrame):\n        x = x.convert_dtypes()\n\n    if reduce_int:\n        x = reduce_int_span(x)\n    if reduce_float:\n        x = reduce_floating_precision(x)\n\n    return x\n</code></pre>"},{"location":"api/amltk/data/dtype_reduction/#amltk.data.dtype_reduction.reduce_floating_precision","title":"reduce_floating_precision","text":"<pre><code>reduce_floating_precision(x: D) -&gt; D\n</code></pre> <p>Reduce the floating point precision of the data.</p> <p>For a float array, will reduce by one step, i.e. float32 -&gt; float16, float64 -&gt; float32.</p> PARAMETER DESCRIPTION <code>x</code> <p>The data to reduce.</p> <p> TYPE: <code>D</code> </p> RETURNS DESCRIPTION <code>D</code> <p>The reduced data.</p> Source code in <code>src/amltk/data/dtype_reduction.py</code> <pre><code>def reduce_floating_precision(x: D) -&gt; D:\n    \"\"\"Reduce the floating point precision of the data.\n\n    For a float array, will reduce by one step, i.e. float32 -&gt; float16, float64\n    -&gt; float32.\n\n    Args:\n        x: The data to reduce.\n\n    Returns:\n        The reduced data.\n    \"\"\"\n    # For a dataframe, we recurse over all columns\n    if isinstance(x, pd.DataFrame):\n        # Using `apply` doesn't work\n        for col in x.columns:\n            x[col] = reduce_floating_precision(x[col])\n        return x  # type: ignore\n\n    if x.dtype.kind != \"f\":\n        return x\n\n    _reduction_map = {\n        # Base numpy dtypes\n        \"float128\": \"float64\",\n        \"float96\": \"float64\",\n        \"float64\": \"float32\",\n        \"float32\": \"float16\",\n        # Nullable pandas dtypes (only supports 64 and 32 bit)\n        \"Float64\": \"Float32\",\n    }\n\n    if (dtype := _reduction_map.get(x.dtype.name)) is not None:\n        return x.astype(dtype)  # type: ignore\n\n    return x\n</code></pre>"},{"location":"api/amltk/data/dtype_reduction/#amltk.data.dtype_reduction.reduce_int_span","title":"reduce_int_span","text":"<pre><code>reduce_int_span(x: D) -&gt; D\n</code></pre> <p>Reduce the integer span of the data.</p> <p>For an int array, will reduce to the smallest dtype that can hold the minimum and maximum values of the array.</p> PARAMETER DESCRIPTION <code>x</code> <p>The data to reduce.</p> <p> TYPE: <code>D</code> </p> RETURNS DESCRIPTION <code>D</code> <p>The reduced data.</p> Source code in <code>src/amltk/data/dtype_reduction.py</code> <pre><code>def reduce_int_span(x: D) -&gt; D:\n    \"\"\"Reduce the integer span of the data.\n\n    For an int array, will reduce to the smallest dtype that can hold the\n    minimum and maximum values of the array.\n\n    Args:\n        x: The data to reduce.\n\n    Returns:\n        The reduced data.\n    \"\"\"\n    # For a dataframe, we recurse over all columns\n    if isinstance(x, pd.DataFrame):\n        # Using `apply` doesn't work\n        for col in x.columns:\n            x[col] = reduce_int_span(x[col])\n        return x  # type: ignore\n\n    if x.dtype.kind not in \"iu\":\n        return x\n\n    min_dtype = np.min_scalar_type(x.min())  # type: ignore\n    max_dtype = np.min_scalar_type(x.max())  # type: ignore\n    dtype = np.result_type(min_dtype, max_dtype)\n\n    # The above dtype is a numpy dtype and may not allow for nullable values,\n    # which are permissible in pandas. `to_numeric` will convert to appropriate\n    # pandas nullable dtypes.\n    if isinstance(x, pd.Series):\n        dc = \"unsigned\" if \"uint\" in dtype.name else \"integer\"\n        return pd.to_numeric(x, downcast=dc)\n\n    return x.astype(dtype)\n</code></pre>"},{"location":"api/amltk/data/measure/","title":"Measure","text":""},{"location":"api/amltk/data/measure/#amltk.data.measure","title":"amltk.data.measure","text":"<p>Measure things about data.</p>"},{"location":"api/amltk/data/measure/#amltk.data.measure.byte_size","title":"byte_size","text":"<pre><code>byte_size(data: Any | Iterable[Any]) -&gt; int\n</code></pre> <p>Measure the size of data.</p> <p>Works for numpy-arrays, pandas DataFrames and Series, and iterables of any of these.</p> PARAMETER DESCRIPTION <code>data</code> <p>The data to measure.</p> <p> TYPE: <code>Any | Iterable[Any]</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The size of the data.</p> Source code in <code>src/amltk/data/measure.py</code> <pre><code>def byte_size(data: Any | Iterable[Any]) -&gt; int:\n    \"\"\"Measure the size of data.\n\n    Works for numpy-arrays, pandas DataFrames and Series, and iterables of any of\n    these.\n\n    Args:\n        data: The data to measure.\n\n    Returns:\n        The size of the data.\n    \"\"\"\n    if isinstance(data, np.ndarray):\n        return data.nbytes\n    if isinstance(data, pd.DataFrame):\n        return int(data.memory_usage(deep=True).sum())\n    if isinstance(data, pd.Series):\n        return int(data.memory_usage(deep=True))\n    if isinstance(data, str):\n        return sys.getsizeof(data)\n    if isinstance(data, Iterable):\n        return sum(byte_size(d) for d in data)\n\n    return sys.getsizeof(data)\n</code></pre>"},{"location":"api/amltk/ensembling/weighted_ensemble_caruana/","title":"Weighted ensemble caruana","text":""},{"location":"api/amltk/ensembling/weighted_ensemble_caruana/#amltk.ensembling.weighted_ensemble_caruana","title":"amltk.ensembling.weighted_ensemble_caruana","text":"<p>Implementation of the weighted ensemble procedure from Caruana et al. 2004.</p> Reference <p>Ensemble selection from libraries of models</p> <p>Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew and Alex Ksikes</p> <p>ICML 2004</p> <p>dl.acm.org/doi/10.1145/1015330.1015432</p> <p>www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf</p>"},{"location":"api/amltk/ensembling/weighted_ensemble_caruana/#amltk.ensembling.weighted_ensemble_caruana.weighted_ensemble_caruana","title":"weighted_ensemble_caruana","text":"<pre><code>weighted_ensemble_caruana(\n    *,\n    model_predictions: Mapping[K, ndarray],\n    targets: ndarray,\n    size: int,\n    metric: Callable[[ndarray, ndarray], T],\n    select: Callable[[Iterable[T]], T],\n    seed: Seed | None = None\n) -&gt; tuple[dict[K, float], list[tuple[K, T]], ndarray]\n</code></pre> <p>Calculate a weighted ensemble of <code>n</code> models.</p> PARAMETER DESCRIPTION <code>model_predictions</code> <p>Mapping from model id to predictions</p> <p> TYPE: <code>Mapping[K, ndarray]</code> </p> <code>targets</code> <p>The targets</p> <p> TYPE: <code>ndarray</code> </p> <code>size</code> <p>The size of the ensemble to create</p> <p> TYPE: <code>int</code> </p> <code>metric</code> <p>The metric to use in calculating which models to add to the ensemble.</p> <p> TYPE: <code>Callable[[ndarray, ndarray], T]</code> </p> <code>select</code> <p>Selects a models from the list based on the values of the metric on their predictions. Can return a single ID or a list of them, in which case a random selection will be made.</p> <p> TYPE: <code>Callable[[Iterable[T]], T]</code> </p> <code>seed</code> <p>The seed to use for breaking ties</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[dict[K, float], list[tuple[K, T]], ndarray]</code> <p>A mapping from id's to it's weight in the ensemble and the trajectory.</p> Source code in <code>src/amltk/ensembling/weighted_ensemble_caruana.py</code> <pre><code>def weighted_ensemble_caruana(\n    *,\n    model_predictions: Mapping[K, np.ndarray],\n    targets: np.ndarray,\n    size: int,\n    metric: Callable[[np.ndarray, np.ndarray], T],\n    select: Callable[[Iterable[T]], T],\n    seed: Seed | None = None,\n) -&gt; tuple[dict[K, float], list[tuple[K, T]], np.ndarray]:\n    \"\"\"Calculate a weighted ensemble of `n` models.\n\n    Args:\n        model_predictions: Mapping from model id to predictions\n        targets: The targets\n        size: The size of the ensemble to create\n        metric: The metric to use in calculating which models to add to the ensemble.\n        select: Selects a models from the list based on the values of the metric on\n            their predictions. Can return a single ID or a list of them, in which\n            case a random selection will be made.\n        seed: The seed to use for breaking ties\n\n    Returns:\n        A mapping from id's to it's weight in the ensemble and the trajectory.\n    \"\"\"\n    if not size &gt; 0:\n        raise ValueError(\"`size` must be positive\")\n\n    if len(model_predictions) == 0:\n        raise ValueError(\"`model_predictions` is empty\")\n\n    rng = as_rng(seed)\n    predictions = list(model_predictions.values())\n\n    dtype = predictions[0].dtype\n    if np.issubdtype(dtype, np.integer):\n        logger.warning(\n            f\"Predictions were {dtype=}, converting to np.float64 to\"\n            \" allow for weighted ensemble procedure.\",\n        )\n        dtype = np.float64\n\n    # Current sum of predictions in the ensemble\n    current = np.zeros_like(predictions[0], dtype=dtype)\n\n    # Buffer where new models predictions are added to current to try them\n    buffer = np.empty_like(predictions[0], dtype=dtype)\n\n    ensemble: list[K] = []\n    trajectory: list[tuple[K, T]] = []\n\n    def value_if_added(_pred: np.ndarray) -&gt; T:\n        # Get the value if the model was added to the current set of predicitons\n        np.add(current, _pred, out=buffer)\n        np.multiply(buffer, (1.0 / float(len(ensemble) + 1)), out=buffer)\n\n        return metric(targets, buffer)\n\n    for _ in range(size):\n        # Get the value if added for each model\n        scores = {_id: value_if_added(pred) for _id, pred in model_predictions.items()}\n\n        # Get the choices that produce the best value\n        chosen_val = select(scores.values())\n\n        choices = [_id for _id, score in scores.items() if score == chosen_val]\n        choice = rng.choice(np.asarray(choices))\n\n        # Add the predictions of the chosen model\n        np.add(current, model_predictions[choice], out=current)\n\n        # Record it's addition and the score of the ensemble with this\n        # choice added\n        ensemble.append(choice)\n        trajectory.append((choice, chosen_val))\n\n        # In the case of only one model, have calculated it's loss\n        # and it's the only available model to add to the ensemble\n        if len(model_predictions) == 1:\n            ensemble *= size\n            trajectory *= size\n            break\n\n    final = np.multiply(current, (1.0 / float(len(ensemble))))\n\n    return (\n        {_id: count / size for _id, count in Counter(ensemble).items()},\n        trajectory,\n        final,\n    )\n</code></pre>"},{"location":"api/amltk/metalearning/dataset_distances/","title":"Dataset distances","text":""},{"location":"api/amltk/metalearning/dataset_distances/#amltk.metalearning.dataset_distances","title":"amltk.metalearning.dataset_distances","text":"<p>Calculating metadata distances.</p>"},{"location":"api/amltk/metalearning/dataset_distances/#amltk.metalearning.dataset_distances.dataset_distance","title":"dataset_distance","text":"<pre><code>dataset_distance(\n    target: Series,\n    dataset_metafeatures: Mapping[str, Series],\n    *,\n    distance_metric: (\n        DistanceMetric\n        | NearestNeighborsDistance\n        | NamedDistance\n    ) = \"l2\",\n    scaler: (\n        TransformerMixin\n        | Callable[[DataFrame], DataFrame]\n        | Literal[\"minmax\"]\n        | None\n    ) = None,\n    closest_n: int | None = None\n) -&gt; Series\n</code></pre> <p>Calculates the distance between a target dataset and a set of datasets.</p> <p>This uses the metafeatures of the datasets to calculate the distance.</p> PARAMETER DESCRIPTION <code>target</code> <p>The target dataset's metafeatures.</p> <p> TYPE: <code>Series</code> </p> <code>dataset_metafeatures</code> <p>A dictionary of dataset names to their metafeatures.</p> <p> TYPE: <code>Mapping[str, Series]</code> </p> <code>distance_metric</code> <p>The method to use to calculate the distance. Takes in the target dataset's metafeatures and a dataset's metafeatures Should return the distance between the two.</p> <p> TYPE: <code>DistanceMetric | NearestNeighborsDistance | NamedDistance</code> DEFAULT: <code>'l2'</code> </p> <code>scaler</code> <p>A scaler to use to scale the metafeatures.</p> <p> TYPE: <code>TransformerMixin | Callable[[DataFrame], DataFrame] | Literal['minmax'] | None</code> DEFAULT: <code>None</code> </p> <code>closest_n</code> <p>The number of closest datasets to return. If None, all datasets are returned.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>Series with the index being the dataset name and the values being the distance.</p> Source code in <code>src/amltk/metalearning/dataset_distances.py</code> <pre><code>def dataset_distance(  # noqa: C901, PLR0912\n    target: pd.Series,\n    dataset_metafeatures: Mapping[str, pd.Series],\n    *,\n    distance_metric: (DistanceMetric | NearestNeighborsDistance | NamedDistance) = \"l2\",\n    scaler: TransformerMixin\n    | Callable[[pd.DataFrame], pd.DataFrame]\n    | Literal[\"minmax\"]\n    | None = None,\n    closest_n: int | None = None,\n) -&gt; pd.Series:\n    \"\"\"Calculates the distance between a target dataset and a set of datasets.\n\n    This uses the metafeatures of the datasets to calculate the distance.\n\n    Args:\n        target: The target dataset's metafeatures.\n        dataset_metafeatures: A dictionary of dataset names to their metafeatures.\n        distance_metric: The method to use to calculate the distance.\n            Takes in the target dataset's metafeatures and a dataset's metafeatures\n            Should return the distance between the two.\n        scaler: A scaler to use to scale the metafeatures.\n        closest_n: The number of closest datasets to return. If None, all datasets\n            are returned.\n\n    Returns:\n        Series with the index being the dataset name and the values being the distance.\n    \"\"\"\n    outname: str\n    if isinstance(distance_metric, str):\n        outname = distance_metric\n    else:\n        outname = funcname(distance_metric)\n\n    if target.name is None:\n        target = target.copy()\n        target.name = \"target-dataset\"\n\n    _method = (\n        distance_metrics[distance_metric]\n        if isinstance(distance_metric, str)\n        else distance_metric\n    )\n\n    if not isinstance(_method, NearestNeighborsDistance):\n        _method = _metric_for_frame(_method)\n\n    metafeatures = {\n        name: ds_metafeatures.rename(name)\n        for name, ds_metafeatures in dataset_metafeatures.items()\n    }\n\n    # Index is dataset name with columns being the values\n    #      | mf1 | mf2\n    # d1\n    # d2\n    # d3\n    combined = pd.concat([target, *metafeatures.values()], axis=1).T\n\n    if scaler is None:\n        pass\n    elif scaler == \"minmax\":\n        min_maxs = combined.agg([\"min\", \"max\"], axis=0).T\n\n        mins = min_maxs[\"min\"]\n        maxs = min_maxs[\"max\"]\n        normalizer = maxs - mins\n        normalizer[normalizer == 0] = 1\n        mins[normalizer == 0] = 0\n\n        norm = lambda col: (col - mins) / normalizer\n        combined = combined.apply(norm, axis=1)\n    elif safe_isinstance(scaler, \"TransformerMixin\"):\n        combined = scaler.set_output(transform=\"pandas\").fit_transform(  # type: ignore\n            combined,\n        )\n    elif callable(scaler):\n        combined = scaler(combined)\n    else:\n        raise ValueError(f\"Unsure how to handle {scaler=}\")\n\n    # We now transpose the dataframe so that the index is the metafeature name\n    # while the columns are the dataset names\n    #   x   | d1 | d2 | d3          y | dy\n    #  mf1                      mf1\n    #  mf2                      mf2\n    x = combined.T.drop(columns=target.name)\n    y = combined.loc[target.name]\n\n    # Should return a series with index being dataset names and values being the\n    #     | distance\n    # d1\n    # d2\n    dataset_distances = _method(x, y)\n\n    if not isinstance(dataset_distances, pd.Series):\n        dataset_distances = pd.Series(\n            dataset_distances,\n            dtype=float,\n            index=list(dataset_metafeatures.keys()),\n            name=outname,\n        )\n    else:\n        dataset_distances = dataset_distances.astype(float).rename(outname)\n\n    dataset_distances = dataset_distances.sort_values()\n\n    if closest_n is not None:\n        if closest_n &gt; len(dataset_distances):\n            warnings.warn(\n                f\"Cannot get {closest_n} closest datasets when there are\"\n                f\" only {len(dataset_distances)} datasets. Returning all.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n        dataset_distances = dataset_distances.iloc[:closest_n]\n\n    return dataset_distances\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/","title":"Metafeatures","text":""},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures","title":"amltk.metalearning.metafeatures","text":"<p>Metafeatures access.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalColumns","title":"CategoricalColumns","text":"<p>               Bases: <code>DatasetStatistic[DataFrame]</code></p> <p>The categorical columns in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalColumns.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalColumns.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalColumns.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalImbalanceRatios","title":"CategoricalImbalanceRatios","text":"<p>               Bases: <code>DatasetStatistic[dict[str, tuple[Series, float]]]</code></p> <p>Imbalance ratios of each class in the dataset.</p> <p>Will return the ratios of each class, the ratio expected if perfectly balanced,</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalImbalanceRatios.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalImbalanceRatios.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalImbalanceRatios.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassCounts","title":"ClassCounts","text":"<p>               Bases: <code>DatasetStatistic[Series]</code></p> <p>Number of instances per class.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassCounts.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassCounts.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassCounts.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalance","title":"ClassImbalance","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Mean Target Imbalance of the classes in general.</p> <p>0 =&gt; Balanced. 1 Imbalanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalance.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalance.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalance.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalance.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalance.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalanceRatios","title":"ClassImbalanceRatios","text":"<p>               Bases: <code>DatasetStatistic[tuple[Series, float]]</code></p> <p>Imbalance ratios of each class in the dataset.</p> <p>Will return the ratios of each class, the ratio expected if perfectly balanced,</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalanceRatios.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalanceRatios.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalanceRatios.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic","title":"DatasetStatistic","text":"<p>               Bases: <code>ABC</code>, <code>Generic[S]</code></p> <p>Base class for a dataset statistic.</p> <p>A dataset statistic is a function that takes a dataset and returns some value(s) that describe the dataset.</p> <p>If looking to create meta-features, see the <code>MetaFeature</code> class which restricts the statistic to be a single number.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.compute","title":"compute  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>compute(\n    x: DataFrame,\n    y: Series | DataFrame,\n    dependancy_values: DSdict,\n) -&gt; S\n</code></pre> <p>Compute the value of this statistic.</p> PARAMETER DESCRIPTION <code>x</code> <p>The features of the dataset.</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The labels of the dataset.</p> <p> TYPE: <code>Series | DataFrame</code> </p> <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>DSdict</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\n@abstractmethod\ndef compute(\n    cls,\n    x: pd.DataFrame,\n    y: pd.Series | pd.DataFrame,\n    dependancy_values: DSdict,\n) -&gt; S:\n    \"\"\"Compute the value of this statistic.\n\n    Args:\n        x: The features of the dataset.\n        y: The labels of the dataset.\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ImbalancePerCategory","title":"ImbalancePerCategory","text":"<p>               Bases: <code>DatasetStatistic[dict[str, float]]</code></p> <p>Imbalance of each categorical feature. 0 =&gt; Balanced. 1 most imbalanced.</p> <p>No categories implies perfectly balanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ImbalancePerCategory.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ImbalancePerCategory.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ImbalancePerCategory.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.InstanceCount","title":"InstanceCount","text":"<p>               Bases: <code>MetaFeature[int]</code></p> <p>Number of instances in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.InstanceCount.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.InstanceCount.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.InstanceCount.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.InstanceCount.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.InstanceCount.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMax","title":"KurtosisMax","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The max kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMax.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMax.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMax.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMax.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMax.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMean","title":"KurtosisMean","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The mean kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMean.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMean.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMean.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMean.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMean.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMin","title":"KurtosisMin","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The min kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMin.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMin.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMin.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMin.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMin.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisPerNumericalColumn","title":"KurtosisPerNumericalColumn","text":"<p>               Bases: <code>DatasetStatistic[dict[str, float]]</code></p> <p>Kurtosis of each numerical feature.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisPerNumericalColumn.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisPerNumericalColumn.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisPerNumericalColumn.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisStd","title":"KurtosisStd","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The std kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisStd.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisStd.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisStd.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisStd.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisStd.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogInstanceCount","title":"LogInstanceCount","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Logarithm of the number of instances in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogInstanceCount.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogInstanceCount.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogInstanceCount.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogInstanceCount.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogInstanceCount.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogNumberOfFeatures","title":"LogNumberOfFeatures","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Logarithm of the number of features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogNumberOfFeatures.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogNumberOfFeatures.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogNumberOfFeatures.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogNumberOfFeatures.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogNumberOfFeatures.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MajorityClassImbalance","title":"MajorityClassImbalance","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Imbalance of the majority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MajorityClassImbalance.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MajorityClassImbalance.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MajorityClassImbalance.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MajorityClassImbalance.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MajorityClassImbalance.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MeanCategoricalImbalance","title":"MeanCategoricalImbalance","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The mean imbalance of categorical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MeanCategoricalImbalance.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MeanCategoricalImbalance.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MeanCategoricalImbalance.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MeanCategoricalImbalance.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MeanCategoricalImbalance.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature","title":"MetaFeature","text":"<p>               Bases: <code>DatasetStatistic[M]</code></p> <p>Used to indicate a metafeature to include.</p> <p>This differs from DatasetStatistic in that it must return a single value.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.compute","title":"compute  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>compute(\n    x: DataFrame,\n    y: Series | DataFrame,\n    dependancy_values: DSdict,\n) -&gt; S\n</code></pre> <p>Compute the value of this statistic.</p> PARAMETER DESCRIPTION <code>x</code> <p>The features of the dataset.</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The labels of the dataset.</p> <p> TYPE: <code>Series | DataFrame</code> </p> <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>DSdict</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\n@abstractmethod\ndef compute(\n    cls,\n    x: pd.DataFrame,\n    y: pd.Series | pd.DataFrame,\n    dependancy_values: DSdict,\n) -&gt; S:\n    \"\"\"Compute the value of this statistic.\n\n    Args:\n        x: The features of the dataset.\n        y: The labels of the dataset.\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MinorityClassImbalance","title":"MinorityClassImbalance","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Imbalance of the minority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MinorityClassImbalance.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MinorityClassImbalance.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MinorityClassImbalance.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MinorityClassImbalance.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MinorityClassImbalance.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NAValues","title":"NAValues","text":"<p>               Bases: <code>DatasetStatistic[DataFrame]</code></p> <p>Mask of missing values in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NAValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NAValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NAValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfCategoricalFeatures","title":"NumberOfCategoricalFeatures","text":"<p>               Bases: <code>MetaFeature[int]</code></p> <p>Number of categorical features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfCategoricalFeatures.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfCategoricalFeatures.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfCategoricalFeatures.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfCategoricalFeatures.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfCategoricalFeatures.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfClasses","title":"NumberOfClasses","text":"<p>               Bases: <code>MetaFeature[int]</code></p> <p>Number of classes in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfClasses.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfClasses.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfClasses.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfClasses.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfClasses.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfFeatures","title":"NumberOfFeatures","text":"<p>               Bases: <code>MetaFeature[int]</code></p> <p>Number of features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfFeatures.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfFeatures.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfFeatures.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfFeatures.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfFeatures.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfNumericFeatures","title":"NumberOfNumericFeatures","text":"<p>               Bases: <code>MetaFeature[int]</code></p> <p>Number of numeric features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfNumericFeatures.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfNumericFeatures.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfNumericFeatures.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfNumericFeatures.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfNumericFeatures.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumericalColumns","title":"NumericalColumns","text":"<p>               Bases: <code>DatasetStatistic[DataFrame]</code></p> <p>The numerical columns in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumericalColumns.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumericalColumns.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumericalColumns.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageMissingValues","title":"PercentageMissingValues","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Percentage of missing values in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageMissingValues.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageMissingValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageMissingValues.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageMissingValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageMissingValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalColumnsWithMissingValues","title":"PercentageOfCategoricalColumnsWithMissingValues","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Percentage of categorical columns with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalColumnsWithMissingValues.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalColumnsWithMissingValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalColumnsWithMissingValues.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalColumnsWithMissingValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalColumnsWithMissingValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalValuesWithMissingValues","title":"PercentageOfCategoricalValuesWithMissingValues","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Percentage of categorical values with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalValuesWithMissingValues.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalValuesWithMissingValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalValuesWithMissingValues.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalValuesWithMissingValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalValuesWithMissingValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfFeaturesWithMissingValues","title":"PercentageOfFeaturesWithMissingValues","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Percentage of features with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfFeaturesWithMissingValues.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfFeaturesWithMissingValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfFeaturesWithMissingValues.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfFeaturesWithMissingValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfFeaturesWithMissingValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfInstancesWithMissingValues","title":"PercentageOfInstancesWithMissingValues","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Percentage of instances with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfInstancesWithMissingValues.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfInstancesWithMissingValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfInstancesWithMissingValues.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfInstancesWithMissingValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfInstancesWithMissingValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericColumnsWithMissingValues","title":"PercentageOfNumericColumnsWithMissingValues","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Percentage of numeric columns with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericColumnsWithMissingValues.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericColumnsWithMissingValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericColumnsWithMissingValues.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericColumnsWithMissingValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericColumnsWithMissingValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericValuesWithMissingValues","title":"PercentageOfNumericValuesWithMissingValues","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Percentage of numeric values with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericValuesWithMissingValues.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericValuesWithMissingValues.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericValuesWithMissingValues.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericValuesWithMissingValues.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericValuesWithMissingValues.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioCategoricalFeatures","title":"RatioCategoricalFeatures","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Ratio of categoricals features to total features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioCategoricalFeatures.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioCategoricalFeatures.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioCategoricalFeatures.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioCategoricalFeatures.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioCategoricalFeatures.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioFeaturesToInstances","title":"RatioFeaturesToInstances","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Ratio of features to instances in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioFeaturesToInstances.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioFeaturesToInstances.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioFeaturesToInstances.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioFeaturesToInstances.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioFeaturesToInstances.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioNumericalFeatures","title":"RatioNumericalFeatures","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>Ratio of numerical features to total features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioNumericalFeatures.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioNumericalFeatures.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioNumericalFeatures.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioNumericalFeatures.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioNumericalFeatures.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMax","title":"SkewnessMax","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The max skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMax.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMax.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMax.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMax.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMax.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMean","title":"SkewnessMean","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The mean skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMean.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMean.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMean.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMean.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMean.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMin","title":"SkewnessMin","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The min skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMin.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMin.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMin.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMin.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMin.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessPerNumericalColumn","title":"SkewnessPerNumericalColumn","text":"<p>               Bases: <code>DatasetStatistic[dict[str, float]]</code></p> <p>Skewness of each numerical feature.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessPerNumericalColumn.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessPerNumericalColumn.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessPerNumericalColumn.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessStd","title":"SkewnessStd","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The std skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessStd.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessStd.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessStd.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessStd.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessStd.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.StdCategoricalImbalance","title":"StdCategoricalImbalance","text":"<p>               Bases: <code>MetaFeature[float]</code></p> <p>The std imbalance of categorical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.StdCategoricalImbalance.skip","title":"skip  <code>class-attribute</code>","text":"<pre><code>skip: bool = False\n</code></pre> <p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.StdCategoricalImbalance.description","title":"description  <code>classmethod</code>","text":"<pre><code>description() -&gt; str\n</code></pre> <p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.StdCategoricalImbalance.iter","title":"iter  <code>classmethod</code>","text":"<pre><code>iter() -&gt; Iterator[type[MetaFeature]]\n</code></pre> <p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.StdCategoricalImbalance.name","title":"name  <code>classmethod</code>","text":"<pre><code>name() -&gt; str\n</code></pre> <p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.StdCategoricalImbalance.retrieve","title":"retrieve  <code>classmethod</code>","text":"<pre><code>retrieve(\n    dependancy_values: Mapping[\n        type[DatasetStatistic[T]], T\n    ]\n) -&gt; S\n</code></pre> <p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.column_imbalance","title":"column_imbalance","text":"<pre><code>column_imbalance(\n    ratios: Series, balanced_ratio: float\n) -&gt; float\n</code></pre> <p>Compute the imbalance of a column.</p> <p>This is done by computing the distance of each item's ratio to what a perfectly balanced ratio would be. We then sum up the distances, dividing by the worst case to normalize between 0 and 1. 0 indicates a perfectly balanced column, 1 indicates a column where all items are of the same type.</p> PARAMETER DESCRIPTION <code>ratios</code> <p>The ratios of each item in the column.</p> <p> TYPE: <code>Series</code> </p> <code>balanced_ratio</code> <p>The ratio of a column if perfectly balanced.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The imbalance of the column.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def column_imbalance(ratios: pd.Series, balanced_ratio: float) -&gt; float:\n    \"\"\"Compute the imbalance of a column.\n\n    This is done by computing the distance of each item's ratio to what\n    a perfectly balanced ratio would be. We then sum up the distances,\n    dividing by the worst case to normalize between 0 and 1. 0 indicates\n    a perfectly balanced column, 1 indicates a column where all items\n    are of the same type.\n\n    Args:\n        ratios: The ratios of each item in the column.\n        balanced_ratio: The ratio of a column if perfectly balanced.\n\n    Returns:\n        The imbalance of the column.\n    \"\"\"\n    item_ratios_distance_from_balanced_ratio = np.abs(ratios - balanced_ratio)\n\n    # The most imbalanced dataset would be one where we somehow have 0\n    # items of each type **except** 1 type, which has all the instances.\n\n    # In the case of a symbol group with 0 instance, their distance to the balanced\n    # ratio is just the balanced ratio itself.\n    zero_instance_ratio_distance = balanced_ratio\n    dominant_ratio_distance = np.abs(1 - balanced_ratio)\n    n_items = len(ratios)\n\n    worst = (n_items - 1) * zero_instance_ratio_distance + dominant_ratio_distance\n    normalizer = 1 / worst\n\n    return float(normalizer * np.sum(item_ratios_distance_from_balanced_ratio))\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.compute_metafeatures","title":"compute_metafeatures","text":"<pre><code>compute_metafeatures(\n    X: DataFrame,\n    y: Series | DataFrame,\n    *,\n    features: Iterable[type[MetaFeature]] | None = None\n) -&gt; Series\n</code></pre> <p>Compute metafeatures for a dataset.</p> PARAMETER DESCRIPTION <code>X</code> <p>The features of the dataset.</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The labels of the dataset.</p> <p> TYPE: <code>Series | DataFrame</code> </p> <code>features</code> <p>The metafeatures to compute. If None, all metafeatures subclasses of <code>MetaFeature</code> will be computed.</p> <p> TYPE: <code>Iterable[type[MetaFeature]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A series of metafeatures.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def compute_metafeatures(\n    X: pd.DataFrame,  # noqa: N803\n    y: pd.Series | pd.DataFrame,\n    *,\n    features: Iterable[type[MetaFeature]] | None = None,\n) -&gt; pd.Series:\n    \"\"\"Compute metafeatures for a dataset.\n\n    Args:\n        X: The features of the dataset.\n        y: The labels of the dataset.\n        features: The metafeatures to compute. If None, all metafeatures subclasses\n            of [`MetaFeature`][amltk.metalearning.MetaFeature] will be computed.\n\n    Returns:\n        A series of metafeatures.\n    \"\"\"\n    if features is None:\n        features = MetaFeature.iter()\n\n    def _calc(\n        _x: pd.DataFrame,\n        _y: pd.Series | pd.DataFrame,\n        _metafeature: type[DatasetStatistic],\n        _values: dict[type[DatasetStatistic], Any],\n    ) -&gt; dict[type[DatasetStatistic], Any]:\n        for dep in _metafeature.dependencies:\n            _values = _calc(_x, _y, dep, _values)\n\n        if _metafeature not in _values:\n            _values[_metafeature] = _metafeature.compute(_x, _y, _values)\n\n        return _values\n\n    values: dict[type[DatasetStatistic], Any] = {}\n    for mf in features:\n        values = _calc(X, y, mf, values)\n\n    return pd.Series(\n        {\n            key.name(): value\n            for key, value in values.items()\n            if issubclass(key, MetaFeature)\n        },\n    )\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.imbalance_ratios","title":"imbalance_ratios","text":"<pre><code>imbalance_ratios(\n    col: Series | DataFrame,\n) -&gt; tuple[Series, float]\n</code></pre> <p>Compute the imbalance ratio of a categorical column.</p> <p>This is done by computing the distance of each item's ratio to what a perfectly balanced ratio would be. We then sum up the distances, dividing by the worst case to normalize between 0 and 1.</p> PARAMETER DESCRIPTION <code>col</code> <p>A column of values. If a DataFrame, the values from the subset of columns will be used.</p> <p> TYPE: <code>Series | DataFrame</code> </p> RETURNS DESCRIPTION <code>tuple[Series, float]</code> <p>A tuple of the imbalance ratios, sorted from lowest (0) to highest (1) and the expected ratio if perfectly balanced.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def imbalance_ratios(col: pd.Series | pd.DataFrame) -&gt; tuple[pd.Series, float]:\n    \"\"\"Compute the imbalance ratio of a categorical column.\n\n    This is done by computing the distance of each item's ratio to what\n    a perfectly balanced ratio would be. We then sum up the distances,\n    dividing by the worst case to normalize between 0 and 1.\n\n    Args:\n        col: A column of values. If a DataFrame, the values from the subset of columns\n            will be used.\n\n    Returns:\n        A tuple of the imbalance ratios, sorted from lowest (0) to highest (1)\n        and the expected ratio if perfectly balanced.\n    \"\"\"\n    ratios = col.value_counts(dropna=True, normalize=True, ascending=True)\n    if len(ratios) == 1:\n        return ratios, 1.0\n\n    n_uniq = len(ratios)\n\n    # A balanced ratio is one where all items are equally distributed\n    balanced_ratio = float(1 / n_uniq)\n    return ratios, balanced_ratio\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.metafeature_descriptions","title":"metafeature_descriptions","text":"<pre><code>metafeature_descriptions(\n    features: (\n        Iterable[type[DatasetStatistic]] | None\n    ) = None,\n) -&gt; dict[str, str]\n</code></pre> <p>Get the descriptions of meatfeatures available.</p> PARAMETER DESCRIPTION <code>features</code> <p>The metafeatures. If None, all metafeatures subclasses of <code>MetaFeature</code> will be returned.</p> <p> TYPE: <code>Iterable[type[DatasetStatistic]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, str]</code> <p>The descriptions of the metafeatures.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def metafeature_descriptions(\n    features: Iterable[type[DatasetStatistic]] | None = None,\n) -&gt; dict[str, str]:\n    \"\"\"Get the descriptions of meatfeatures available.\n\n    Args:\n        features: The metafeatures. If None, all metafeatures subclasses\n            of [`MetaFeature`][amltk.metalearning.MetaFeature] will be returned.\n\n    Returns:\n        The descriptions of the metafeatures.\n    \"\"\"\n    if features is None:\n        features = MetaFeature.iter()\n\n    return {mf.name(): mf.description() for mf in features}\n</code></pre>"},{"location":"api/amltk/metalearning/portfolio/","title":"Portfolio","text":""},{"location":"api/amltk/metalearning/portfolio/#amltk.metalearning.portfolio","title":"amltk.metalearning.portfolio","text":"<p>Portfolio selection.</p>"},{"location":"api/amltk/metalearning/portfolio/#amltk.metalearning.portfolio.portfolio_selection","title":"portfolio_selection","text":"<pre><code>portfolio_selection(\n    items: dict[K, Series] | DataFrame,\n    k: int,\n    *,\n    row_reducer: Callable[[Series], float] = np.max,\n    aggregator: Callable[[Series], float] = np.mean,\n    portfolio_value: (\n        Callable[[DataFrame], float] | None\n    ) = None,\n    maximize: bool = True,\n    scaler: (\n        TransformerMixin | Literal[\"minmax\"] | None\n    ) = \"minmax\",\n    with_replacement: bool = False,\n    stop_if_worse: bool = False,\n    seed: Seed | None = None\n) -&gt; tuple[DataFrame, Series]\n</code></pre> <p>Selects a portfolio of <code>k</code> items from <code>items</code>.</p> <p>A portfolio is a subset of the items, and is selected by maximizing the <code>portfolio_value</code> function in a greedy selection approach.</p> <p>At each iteration <code>0 &lt;= i &lt; k</code>, the <code>portfolio_value</code> function is calculated for the portfolio obtained by adding the <code>i</code>th item to the portfolio. The item that maximizes the <code>portfolio_value</code> function is then added to the portfolio for the next iteration.</p> <p>The <code>portfolio_function</code> can often be define by a row wise reduction (<code>row_reducer=</code>) followed by some aggregation over these reductions (<code>aggregator=</code>). You can also supply your own value function if desired (<code>portfolio_value=</code>).</p> <p>A Single Iteration</p> <p>This uses the <code>row_reducer=np.max</code> and <code>aggregator=np.mean</code> to calculate the value of a portfolio.</p> <p>In this case, we have 4 datasets and our current portfolio consists of <code>config_1</code> and <code>config_2</code>. We are going to calculate the value of adding <code>config_try</code> to the current best portfolio.</p> <pre><code>            | config_1 | config_2 | config_try\ndataset_1   |    1     |    0     |    0\ndataset_2   |    0     |   0.5    |    1\ndataset_3   |    0     |   0.5    |   0.5\ndataset_4   |    1     |    1     |    0\n</code></pre> <p>Apply <code>row_reducer</code> to each row, in this case <code>np.max</code></p> <pre><code>            |   max\ndataset_1   |    1\ndataset_2   |    1\ndataset_3   |   0.5\ndataset_4   |    1\n</code></pre> <p>Apply <code>aggregator</code> to the reduced rows, in this case <code>np.mean</code></p> <pre><code>portfolio_value = np.mean([1, 1, 0.5, 1]) # 0.875\n</code></pre> PARAMETER DESCRIPTION <code>items</code> <p>A dictionary of items to select from.</p> <p> TYPE: <code>dict[K, Series] | DataFrame</code> </p> <code>k</code> <p>The number of items to select.</p> <p> TYPE: <code>int</code> </p> <code>row_reducer</code> <p>A function to aggregate the rows of the portfolio. This is applied to a potential portfolio, for example to calculate the max score of all configs, for a given dataset (row).</p> <p> TYPE: <code>Callable[[Series], float]</code> DEFAULT: <code>max</code> </p> <code>aggregator</code> <p>A function to take all the single values reduced by <code>row_reducer</code>, and aggregate them into a final value for the portfolio.</p> <p> TYPE: <code>Callable[[Series], float]</code> DEFAULT: <code>mean</code> </p> <code>portfolio_value</code> <p>A custom function to calculate the value of a portfolio. This will take precedence over <code>row_reducer</code> and <code>aggregator</code>.</p> <p> TYPE: <code>Callable[[DataFrame], float] | None</code> DEFAULT: <code>None</code> </p> <code>maximize</code> <p>Whether to maximize or minimize the portfolio value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>scaler</code> <p>A scaler to use to scale the portfolio values. Is applied across the rows.</p> <p> TYPE: <code>TransformerMixin | Literal['minmax'] | None</code> DEFAULT: <code>'minmax'</code> </p> <code>with_replacement</code> <p>Whether to select items with replacement.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stop_if_worse</code> <p>Whether to stop if the portfolio value is worse than the current best.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for breaking ties.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[DataFrame, Series]</code> <p>The final portfolio The trajectory, where the entry is the value once added to the portfolio.</p> Source code in <code>src/amltk/metalearning/portfolio.py</code> <pre><code>def portfolio_selection(\n    items: dict[K, pd.Series] | pd.DataFrame,\n    k: int,\n    *,\n    row_reducer: Callable[[pd.Series], float] = np.max,\n    aggregator: Callable[[pd.Series], float] = np.mean,\n    portfolio_value: Callable[[pd.DataFrame], float] | None = None,\n    maximize: bool = True,\n    scaler: TransformerMixin | Literal[\"minmax\"] | None = \"minmax\",\n    with_replacement: bool = False,\n    stop_if_worse: bool = False,\n    seed: Seed | None = None,\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Selects a portfolio of `k` items from `items`.\n\n    A portfolio is a subset of the items, and is selected by maximizing the\n    `portfolio_value` function in a greedy selection approach.\n\n    At each iteration `0 &lt;= i &lt; k`, the `portfolio_value` function is calculated\n    for the portfolio obtained by adding the `i`th item to the portfolio. The item\n    that maximizes the `portfolio_value` function is then added to the portfolio for\n    the next iteration.\n\n    The `portfolio_function` can often be define by a row wise reduction\n    (`row_reducer=`) followed by some aggregation over these reductions (`aggregator=`).\n    You can also supply your own value function if desired (`portfolio_value=`).\n\n    !!! example \"A Single Iteration\"\n\n        This uses the `row_reducer=np.max` and `aggregator=np.mean` to calculate the\n        value of a portfolio.\n\n        In this case, we have 4 datasets and our current portfolio\n        consists of `config_1` and `config_2`. We are going to calculate the value of\n        adding `config_try` to the current best portfolio.\n\n        ```python\n                    | config_1 | config_2 | config_try\n        dataset_1   |    1     |    0     |    0\n        dataset_2   |    0     |   0.5    |    1\n        dataset_3   |    0     |   0.5    |   0.5\n        dataset_4   |    1     |    1     |    0\n        ```\n\n        Apply `row_reducer` to each row, in this case `np.max`\n\n        ```python\n                    |   max\n        dataset_1   |    1\n        dataset_2   |    1\n        dataset_3   |   0.5\n        dataset_4   |    1\n        ```\n\n        Apply `aggregator` to the reduced rows, in this case `np.mean`\n\n        ```python\n        portfolio_value = np.mean([1, 1, 0.5, 1]) # 0.875\n        ```\n\n    Args:\n        items: A dictionary of items to select from.\n        k: The number of items to select.\n        row_reducer: A function to aggregate the rows of the portfolio.\n            This is applied to a potential portfolio, for example to calculate\n            the max score of all configs, for a given dataset (row).\n        aggregator: A function to take all the single values reduced by `row_reducer`,\n            and aggregate them into a final value for the portfolio.\n        portfolio_value: A custom function to calculate the value of a portfolio.\n            This will take precedence over `row_reducer` and `aggregator`.\n        maximize: Whether to maximize or minimize the portfolio value.\n        scaler: A scaler to use to scale the portfolio values. Is applied across\n            the rows.\n        with_replacement: Whether to select items with replacement.\n        stop_if_worse: Whether to stop if the portfolio value is worse than the\n            current best.\n        seed: The seed to use for breaking ties.\n\n    Returns:\n        The final portfolio\n        The trajectory, where the entry is the value once added to the portfolio.\n    \"\"\"\n    n_items = len(items) if isinstance(items, dict) else items.shape[1]\n    if not (1 &lt;= k &lt; n_items):\n        raise ValueError(f\"k must be in [1, {n_items=})\")\n\n    all_portfolio = pd.DataFrame(items)\n\n    # Normalize if needed\n    if scaler is None:\n        pass\n    elif scaler == \"minmax\":\n        min_maxs = all_portfolio.agg([\"min\", \"max\"], axis=1)\n\n        mins = min_maxs[\"min\"]\n        maxs = min_maxs[\"max\"]\n        normalizer = maxs - mins\n\n        # If everything is equal, we need to make sure the normalizing\n        # doesn't do anything\n        normalizer[normalizer == 0] = 1\n        mins[normalizer == 0] = 0\n\n        norm = lambda col: (col - mins) / normalizer\n        all_portfolio: pd.DataFrame = all_portfolio.apply(norm, axis=0)  # type: ignore\n    elif safe_isinstance(scaler, \"TransformerMixin\"):\n        assert not isinstance(scaler, str)\n        all_portfolio = scaler.fit_transform(all_portfolio.T).T\n    else:\n        raise ValueError(f\"Invalid scaler: {scaler}\")\n\n    # Set up the portfolio value function\n    if portfolio_value is None:\n        portfolio_value = lambda _portfolio: float(\n            aggregator(_portfolio.apply(row_reducer, axis=1)),\n        )\n\n    # Make a copy as we will del from it\n    items = dict(items)\n    rng = as_rng(seed)\n    best = max if maximize else min\n\n    # Running counters during the algorithm loop\n    added_items: list[K] = []\n    values: list[float] = []\n    current_best: float = -np.inf if maximize else np.inf\n\n    for _ in range(k):\n        possible_portfolios = [(k, all_portfolio[[*added_items, k]]) for k in items]\n        values_possible = {\n            k: portfolio_value(possible_portfolio)\n            for k, possible_portfolio in possible_portfolios\n        }\n\n        # This is the highest value we can get from a portfolio of the current size\n        best_possible = best(values_possible.values())\n\n        # If the best possible value of what we can do does not improve over the current\n        #    best portfolio, stop (if enabled)\n        if stop_if_worse and current_best == best(best_possible, current_best):\n            break\n\n        current_best = best_possible\n\n        # Possible get multiple best choices, we choose one at random if so\n        best_keys = [k for k, v in values_possible.items() if v == best_possible]\n        best_key = (\n            best_keys[0] if len(best_keys) == 1 else rng.choice(best_keys)  # type: ignore\n        )\n\n        # We found something better, add it in\n        added_items.append(best_key)\n        values.append(best_possible)\n\n        if not with_replacement:\n            del items[best_key]\n\n    # Rename the columns of the portfolio to be the keys\n    return all_portfolio[added_items], pd.Series(values, index=added_items)\n</code></pre>"},{"location":"api/amltk/optimization/evaluation/","title":"Evaluation","text":""},{"location":"api/amltk/optimization/evaluation/#amltk.optimization.evaluation","title":"amltk.optimization.evaluation","text":"<p>Evaluation protocols for how a trial and a pipeline should be evaluated.</p> <p>TODO: Sorry</p>"},{"location":"api/amltk/optimization/history/","title":"History","text":""},{"location":"api/amltk/optimization/history/#amltk.optimization.history","title":"amltk.optimization.history","text":"<p>The <code>History</code> is used to keep a structured record of what occured with <code>Trial</code>s and their associated <code>Report</code>s.</p> Usage <pre><code>from amltk.optimization import Trial, History, Metric\nfrom amltk.store import PathBucket\n\nloss = Metric(\"loss\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n    trial.store({\"config.json\": trial.config})\n\n    loss = x**2 - y\n    return trial.success(loss=loss)\n\n# ... usually obtained from an optimizer\nbucket = PathBucket(\"all-trial-results\")\nhistory = History()\n\nfor x, y in zip([1, 2, 3], [4, 5, 6]):\n    name = f\"trial_{x}_{y}\"\n    trial = Trial.create(name=name, config={\"x\": x, \"y\": y}, bucket=bucket / name, metrics=[loss])\n    report = target_function(trial)\n    history.add(report)\n\nprint(history.df())\nbucket.rmdir()  # markdon-exec: hide\n</code></pre>              status  trial_seed  ... config:x config:y name                            ...                   trial_1_4  success          ...        1        4 trial_2_5  success          ...        2        5 trial_3_6  success          ...        3        6  [3 rows x 10 columns]   <p>You'll often need to perform some operations on a <code>History</code> so we provide some utility functions here:</p> <ul> <li><code>filter(key=...)</code> - Filters the history by some     predicate, e.g. <code>history.filter(lambda report: report.status == \"success\")</code></li> <li><code>groupby(key=...)</code> - Groups the history by some     key, e.g. <code>history.groupby(lambda report: report.config[\"x\"] &lt; 5)</code></li> <li><code>sortby(key=...)</code> - Sorts the history by some     key, e.g. <code>history.sortby(lambda report: report.profiles[\"trial\"].time.end)</code></li> </ul> <p>There is also some serialization capabilities built in, to allow you to store your reports and load them back in later:</p> <ul> <li><code>df(...)</code> - Output a <code>pd.DataFrame</code> of all  the information available.</li> <li><code>from_df(...)</code> - Create a <code>History</code> from     a <code>pd.DataFrame</code>.</li> </ul> <p>You can also retrieve individual reports from the history by using their name, e.g. <code>history.reports[\"some-unique-name\"]</code> or iterate through the history with <code>for report in history: ...</code>.</p>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History","title":"History  <code>dataclass</code>","text":"<pre><code>History(\n    reports: list[Report] = list(),\n    metrics: dict[str, Metric] = dict(),\n    _lookup: dict[str, int] = dict(),\n)\n</code></pre> <p>               Bases: <code>RichRenderable</code></p> <p>A history of trials.</p> <p>This is a collections of reports from trials, where you can access the reports by their trial name. It is unsorted in general, but by using <code>sortby()</code> you can sort the history.</p> History<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [\n    Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric])\n    for i in range(10)\n]\nhistory = History()\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    report = trial.success(cost=x**2 - x*2 + 4)\n    history.add(report)\n\nfor report in history:\n    print(f\"{report.name=}, {report}\")\n\nprint(history.metrics)\nprint(history.df())\n\nprint(history.best())\n</code></pre> <pre><code>report.name='trial_0', Trial.Report(trial=Trial(name='trial_0', config={'x': 0}, bucket=PathBucket(PosixPath('trial-trial_0-2024-08-13T07:34:20.272248')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272237), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272943), exception=None, values={'cost': 4})\nreport.name='trial_1', Trial.Report(trial=Trial(name='trial_1', config={'x': 1}, bucket=PathBucket(PosixPath('trial-trial_1-2024-08-13T07:34:20.272372')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272370), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273067), exception=None, values={'cost': 3})\nreport.name='trial_2', Trial.Report(trial=Trial(name='trial_2', config={'x': 2}, bucket=PathBucket(PosixPath('trial-trial_2-2024-08-13T07:34:20.272433')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272432), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273154), exception=None, values={'cost': 4})\nreport.name='trial_3', Trial.Report(trial=Trial(name='trial_3', config={'x': 3}, bucket=PathBucket(PosixPath('trial-trial_3-2024-08-13T07:34:20.272488')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272487), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273233), exception=None, values={'cost': 7})\nreport.name='trial_4', Trial.Report(trial=Trial(name='trial_4', config={'x': 4}, bucket=PathBucket(PosixPath('trial-trial_4-2024-08-13T07:34:20.272539')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272538), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273304), exception=None, values={'cost': 12})\nreport.name='trial_5', Trial.Report(trial=Trial(name='trial_5', config={'x': 5}, bucket=PathBucket(PosixPath('trial-trial_5-2024-08-13T07:34:20.272590')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272589), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273368), exception=None, values={'cost': 19})\nreport.name='trial_6', Trial.Report(trial=Trial(name='trial_6', config={'x': 6}, bucket=PathBucket(PosixPath('trial-trial_6-2024-08-13T07:34:20.272642')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272641), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273432), exception=None, values={'cost': 28})\nreport.name='trial_7', Trial.Report(trial=Trial(name='trial_7', config={'x': 7}, bucket=PathBucket(PosixPath('trial-trial_7-2024-08-13T07:34:20.272735')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272731), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273499), exception=None, values={'cost': 39})\nreport.name='trial_8', Trial.Report(trial=Trial(name='trial_8', config={'x': 8}, bucket=PathBucket(PosixPath('trial-trial_8-2024-08-13T07:34:20.272803')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272802), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273568), exception=None, values={'cost': 52})\nreport.name='trial_9', Trial.Report(trial=Trial(name='trial_9', config={'x': 9}, bucket=PathBucket(PosixPath('trial-trial_9-2024-08-13T07:34:20.272867')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272866), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273633), exception=None, values={'cost': 67})\n{'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}\n          status  trial_seed  ... metric:cost (minimize) config:x\nname                          ...                                \ntrial_0  success        &lt;NA&gt;  ...                      4        0\ntrial_1  success        &lt;NA&gt;  ...                      3        1\ntrial_2  success        &lt;NA&gt;  ...                      4        2\ntrial_3  success        &lt;NA&gt;  ...                      7        3\ntrial_4  success        &lt;NA&gt;  ...                     12        4\ntrial_5  success        &lt;NA&gt;  ...                     19        5\ntrial_6  success        &lt;NA&gt;  ...                     28        6\ntrial_7  success        &lt;NA&gt;  ...                     39        7\ntrial_8  success        &lt;NA&gt;  ...                     52        8\ntrial_9  success        &lt;NA&gt;  ...                     67        9\n\n[10 rows x 9 columns]\nTrial.Report(trial=Trial(name='trial_1', config={'x': 1}, bucket=PathBucket(PosixPath('trial-trial_1-2024-08-13T07:34:20.272372')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 272370), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 273067), exception=None, values={'cost': 3})\n</code></pre> ATTRIBUTE DESCRIPTION <code>reports</code> <p>A mapping of trial names to reports.</p> <p> TYPE: <code>list[Report]</code> </p>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.add","title":"add","text":"<pre><code>add(report: Report | Iterable[Report]) -&gt; None\n</code></pre> <p>Adds a report or reports to the history.</p> PARAMETER DESCRIPTION <code>report</code> <p>A report or reports to add.</p> <p> TYPE: <code>Report | Iterable[Report]</code> </p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def add(self, report: Trial.Report | Iterable[Trial.Report]) -&gt; None:\n    \"\"\"Adds a report or reports to the history.\n\n    Args:\n        report: A report or reports to add.\n    \"\"\"\n    match report:\n        case Trial.Report():\n            self.metrics.update(report.metrics)\n            self.reports.append(report)\n            self._lookup[report.name] = len(self.reports) - 1\n        case Iterable():\n            for _report in report:\n                # Could endless loop\n                if not isinstance(_report, Trial.Report):\n                    raise TypeError(\n                        f\"Expected an iterable of Trial.Report\"\n                        f\" but got {type(_report)} ({_report})\"\n                        f\" in add({report=})\",\n                    )\n                self.add(_report)\n        case _:\n            raise TypeError(\n                f\"Expected a Trial.Report or an iterable of Trial.Report\"\n                f\" but got {type(report)} ({report})\",\n            )\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.best","title":"best","text":"<pre><code>best(metric: str | Metric | None = None) -&gt; Report\n</code></pre> <p>Returns the best report in the history.</p> PARAMETER DESCRIPTION <code>metric</code> <p>The metric to sort by. If <code>None</code>, it will use the first metric in the history. If there are multiple metrics and non are specified, it will raise an error.</p> <p> TYPE: <code>str | Metric | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Report</code> <p>The best report.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def best(self, metric: str | Metric | None = None) -&gt; Trial.Report:\n    \"\"\"Returns the best report in the history.\n\n    Args:\n        metric: The metric to sort by. If `None`, it will use the\n            first metric in the history. If there are multiple metrics\n            and non are specified, it will raise an error.\n\n    Returns:\n        The best report.\n    \"\"\"\n    match metric:\n        case None:\n            if len(self.metrics) &gt; 1:\n                raise ValueError(\n                    \"There are multiple metrics in the history, \"\n                    \"please specify which metric to sort by for best.\",\n                )\n\n            _metric_def = next(iter(self.metrics.values()))\n            _metric_name = _metric_def.name\n        case str():\n            if metric not in self.metrics:\n                raise ValueError(\n                    f\"Metric {metric} not found in history. \"\n                    f\"Available metrics: {list(self.metrics.keys())}\",\n                )\n            _metric_def = self.metrics[metric]\n            _metric_name = metric\n        case Metric():\n            _metric_def = metric\n            _metric_name = metric.name\n\n    _by = min if _metric_def.minimize else max\n    return _by(\n        (r for r in self.reports if _metric_name in r.values),\n        key=lambda r: r.values[_metric_name],\n    )\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.df","title":"df","text":"<pre><code>df(\n    *,\n    profiles: bool = True,\n    configs: bool = True,\n    summary: bool = True,\n    metrics: bool = True,\n    normalize_time: bool | float = True\n) -&gt; DataFrame\n</code></pre> <p>Returns a pandas DataFrame of the history.</p> <p>Each individual trial will be a row in the dataframe.</p> <p>Prefixes</p> <ul> <li><code>summary</code>: Entries will be prefixed with <code>\"summary:\"</code></li> <li><code>config</code>: Entries will be prefixed with <code>\"config:\"</code></li> <li><code>metrics</code>: Entries will be prefixed with <code>\"metrics:\"</code></li> </ul> df<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    report = trial.success(cost=x**2 - x*2 + 4)\n    history.add(report)\n\nprint(history.df())\n</code></pre> <pre><code>          status  trial_seed  ... metric:cost (minimize) config:x\nname                          ...                                \ntrial_0  success        &lt;NA&gt;  ...                      4        0\ntrial_1  success        &lt;NA&gt;  ...                      3        1\ntrial_2  success        &lt;NA&gt;  ...                      4        2\ntrial_3  success        &lt;NA&gt;  ...                      7        3\ntrial_4  success        &lt;NA&gt;  ...                     12        4\ntrial_5  success        &lt;NA&gt;  ...                     19        5\ntrial_6  success        &lt;NA&gt;  ...                     28        6\ntrial_7  success        &lt;NA&gt;  ...                     39        7\ntrial_8  success        &lt;NA&gt;  ...                     52        8\ntrial_9  success        &lt;NA&gt;  ...                     67        9\n\n[10 rows x 9 columns]\n</code></pre> PARAMETER DESCRIPTION <code>profiles</code> <p>Whether to include the profiles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>configs</code> <p>Whether to include the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>summary</code> <p>Whether to include the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>metrics</code> <p>Whether to include the metrics.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize_time</code> <p>Whether to normalize the time to the first report. If given a <code>float</code>, it will normalize to that value.</p> <p>Will normalize all columns with <code>\"time:end\"</code>. and <code>\"time:start\"</code> in their name. It will use the time of the earliest report as the offset.</p> <p> TYPE: <code>bool | float</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas DataFrame of the history.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def df(\n    self,\n    *,\n    profiles: bool = True,\n    configs: bool = True,\n    summary: bool = True,\n    metrics: bool = True,\n    normalize_time: bool | float = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Returns a pandas DataFrame of the history.\n\n    Each individual trial will be a row in the dataframe.\n\n    !!! note \"Prefixes\"\n\n        * `summary`: Entries will be prefixed with `#!python \"summary:\"`\n        * `config`: Entries will be prefixed with `#!python \"config:\"`\n        * `metrics`: Entries will be prefixed with `#!python \"metrics:\"`\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"df\" hl_lines=\"12\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n        trial.bucket.rmdir() # markdown-exec: hide\n\n    print(history.df())\n    ```\n\n    Args:\n        profiles: Whether to include the profiles.\n        configs: Whether to include the configs.\n        summary: Whether to include the summary.\n        metrics: Whether to include the metrics.\n        normalize_time: Whether to normalize the time to the first\n            report. If given a `#!python float`, it will normalize\n            to that value.\n\n            Will normalize all columns with `#!python \"time:end\"`.\n            and `#!python \"time:start\"` in their name. It will use\n            the time of the earliest report as the offset.\n\n    Returns:\n        A pandas DataFrame of the history.\n    \"\"\"  # noqa: E501\n    if len(self) == 0:\n        return pd.DataFrame()\n\n    _df = pd.concat(\n        [\n            report.df(\n                profiles=profiles,\n                configs=configs,\n                summary=summary,\n                metrics=metrics,\n            )\n            for report in self.reports\n        ],\n    )\n    _df = _df.convert_dtypes()\n\n    match normalize_time:\n        case True if \"time:start\" in _df.columns:\n            time_columns = (\"time:start\", \"time:end\")\n            cols = [c for c in _df.columns if c.endswith(time_columns)]\n            _df[cols] -= _df[\"time:start\"].min()\n        case float():\n            time_columns = (\"time:start\", \"time:end\")\n            cols = [c for c in _df.columns if c.endswith(time_columns)]\n            _df[cols] -= normalize_time\n        case _:\n            pass\n\n    return _df\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.filter","title":"filter","text":"<pre><code>filter(key: Callable[[Report], bool]) -&gt; History\n</code></pre> <p>Filters the history by a predicate.</p> filter<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    report = trial.success(cost=x**2 - x*2 + 4)\n    history.add(report)\n\nfiltered_history = history.filter(lambda report: report.values[\"cost\"] &lt; 10)\nfor report in filtered_history:\n    cost = report.values[\"cost\"]\n    print(f\"{report.name}, {cost=}, {report}\")\n</code></pre> <pre><code>trial_0, cost=4, Trial.Report(trial=Trial(name='trial_0', config={'x': 0}, bucket=PathBucket(PosixPath('trial-trial_0-2024-08-13T07:34:20.422597')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 422592), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 423200), exception=None, values={'cost': 4})\ntrial_1, cost=3, Trial.Report(trial=Trial(name='trial_1', config={'x': 1}, bucket=PathBucket(PosixPath('trial-trial_1-2024-08-13T07:34:20.422710')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 422709), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 423312), exception=None, values={'cost': 3})\ntrial_2, cost=4, Trial.Report(trial=Trial(name='trial_2', config={'x': 2}, bucket=PathBucket(PosixPath('trial-trial_2-2024-08-13T07:34:20.422770')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 422769), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 423382), exception=None, values={'cost': 4})\ntrial_3, cost=7, Trial.Report(trial=Trial(name='trial_3', config={'x': 3}, bucket=PathBucket(PosixPath('trial-trial_3-2024-08-13T07:34:20.422824')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 422823), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 423448), exception=None, values={'cost': 7})\n</code></pre> PARAMETER DESCRIPTION <code>key</code> <p>A predicate to filter by.</p> <p> TYPE: <code>Callable[[Report], bool]</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A new history with the filtered reports.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def filter(self, key: Callable[[Trial.Report], bool]) -&gt; History:\n    \"\"\"Filters the history by a predicate.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"filter\" hl_lines=\"12\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        trial.bucket.rmdir() # markdown-exec: hide\n        history.add(report)\n\n    filtered_history = history.filter(lambda report: report.values[\"cost\"] &lt; 10)\n    for report in filtered_history:\n        cost = report.values[\"cost\"]\n        print(f\"{report.name}, {cost=}, {report}\")\n    ```\n\n    Args:\n        key: A predicate to filter by.\n\n    Returns:\n        A new history with the filtered reports.\n    \"\"\"  # noqa: E501\n    return History.from_reports([report for report in self.reports if key(report)])\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.find","title":"find","text":"<pre><code>find(name: str) -&gt; Report\n</code></pre> <p>Finds a report by trial name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the trial.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Report</code> <p>The report.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def find(self, name: str) -&gt; Trial.Report:\n    \"\"\"Finds a report by trial name.\n\n    Args:\n        name: The name of the trial.\n\n    Returns:\n        The report.\n    \"\"\"\n    return self.reports[self._lookup[name]]\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.from_df","title":"from_df  <code>classmethod</code>","text":"<pre><code>from_df(df: DataFrame) -&gt; History\n</code></pre> <p>Loads a history from a pandas DataFrame.</p> PARAMETER DESCRIPTION <code>df</code> <p>The DataFrame to load the history from.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A History.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>@classmethod\ndef from_df(cls, df: pd.DataFrame) -&gt; History:\n    \"\"\"Loads a history from a pandas DataFrame.\n\n    Args:\n        df: The DataFrame to load the history from.\n\n    Returns:\n        A History.\n    \"\"\"\n    if len(df) == 0:\n        return cls()\n    return History.from_reports(Trial.Report.from_df(s) for _, s in df.iterrows())\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.from_reports","title":"from_reports  <code>classmethod</code>","text":"<pre><code>from_reports(reports: Iterable[Report]) -&gt; History\n</code></pre> <p>Creates a history from reports.</p> PARAMETER DESCRIPTION <code>reports</code> <p>An iterable of reports.</p> <p> TYPE: <code>Iterable[Report]</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A history.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>@classmethod\ndef from_reports(cls, reports: Iterable[Trial.Report]) -&gt; History:\n    \"\"\"Creates a history from reports.\n\n    Args:\n        reports: An iterable of reports.\n\n    Returns:\n        A history.\n    \"\"\"\n    history = cls()\n    history.add(reports)\n    return history\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.groupby","title":"groupby","text":"<pre><code>groupby(\n    key: Literal[\"status\"] | Callable[[Report], Hashable]\n) -&gt; dict[Hashable, History]\n</code></pre> <p>Groups the history by the values of a key.</p> groupby<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    if x % 2 == 0:\n        report = trial.fail(cost=1_000)\n    else:\n        report = trial.success(cost=x**2 - x*2 + 4)\n    history.add(report)\n\nfor status, history in history.groupby(\"status\").items():\n    print(f\"{status=}, {len(history)=}\")\n</code></pre> <pre><code>status=&lt;Status.FAIL: 'fail'&gt;, len(history)=5\nstatus=&lt;Status.SUCCESS: 'success'&gt;, len(history)=5\n</code></pre> <p>You can pass a <code>Callable</code> to group by any key you like:</p> <pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    report = trial.fail(cost=x)\n    history.add(report)\n\nfor below_5, history in history.groupby(lambda r: r.values[\"cost\"] &lt; 5).items():\n    print(f\"{below_5=}, {len(history)=}\")\n</code></pre> <pre><code>below_5=True, len(history)=5\nbelow_5=False, len(history)=5\n</code></pre> PARAMETER DESCRIPTION <code>key</code> <p>A key to group by. If <code>\"status\"</code> is passed, the history will be grouped by the status of the reports.</p> <p> TYPE: <code>Literal['status'] | Callable[[Report], Hashable]</code> </p> RETURNS DESCRIPTION <code>dict[Hashable, History]</code> <p>A mapping of keys to histories.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def groupby(\n    self,\n    key: Literal[\"status\"] | Callable[[Trial.Report], Hashable],\n) -&gt; dict[Hashable, History]:\n    \"\"\"Groups the history by the values of a key.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"groupby\" hl_lines=\"15\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        x = trial.config[\"x\"]\n        if x % 2 == 0:\n            report = trial.fail(cost=1_000)\n        else:\n            report = trial.success(cost=x**2 - x*2 + 4)\n        trial.bucket.rmdir() # markdown-exec: hide\n        history.add(report)\n\n    for status, history in history.groupby(\"status\").items():\n        print(f\"{status=}, {len(history)=}\")\n    ```\n\n    You can pass a `#!python Callable` to group by any key you like:\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        x = trial.config[\"x\"]\n        report = trial.fail(cost=x)\n        history.add(report)\n        trial.bucket.rmdir() # markdown-exec: hide\n\n    for below_5, history in history.groupby(lambda r: r.values[\"cost\"] &lt; 5).items():\n        print(f\"{below_5=}, {len(history)=}\")\n    ```\n\n    Args:\n        key: A key to group by. If `\"status\"` is passed, the history will be\n            grouped by the status of the reports.\n\n    Returns:\n        A mapping of keys to histories.\n    \"\"\"  # noqa: E501\n    d = defaultdict(list)\n\n    if key == \"status\":\n        key = operator.attrgetter(\"status\")\n\n    for report in self.reports:\n        d[key(report)].append(report)\n\n    return {k: History.from_reports(v) for k, v in d.items()}\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.incumbents","title":"incumbents","text":"<pre><code>incumbents(\n    key: Callable[[Report, Report], bool] | str,\n    *,\n    sortby: (\n        Callable[[Report], Comparable] | str\n    ) = lambda r: r.reported_at,\n    reverse: bool | None = None,\n    ffill: bool = False\n) -&gt; History\n</code></pre> <p>Returns a trace of the incumbents, where only the report that is better than the previous best report is kept.</p> incumbents<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    report = trial.success(cost=x**2 - x*2 + 4)\n    history.add(report)\n\nincumbents = (\n    history\n    .incumbents(\"cost\", sortby=lambda r: r.reported_at)\n)\nfor report in incumbents:\n    print(f\"{report.values=}, {report.config=}\")\n</code></pre> <pre><code>report.values={'cost': 4}, report.config={'x': 0}\nreport.values={'cost': 3}, report.config={'x': 1}\n</code></pre> PARAMETER DESCRIPTION <code>key</code> <p>The key to use. If given a str, it will use that as the key to use in the metrics, defining if one report is better than another. If given a <code>Callable</code>, it should return a <code>bool</code>, indicating if the first argument report is better than the second argument report.</p> <p> TYPE: <code>Callable[[Report, Report], bool] | str</code> </p> <code>sortby</code> <p>The key to sort by. If given a str, it will sort by the value of that key in the <code>.metrics</code> and also filter out anything that does not contain this key. By default, it will sort by the end time of the report.</p> <p> TYPE: <code>Callable[[Report], Comparable] | str</code> DEFAULT: <code>lambda r: reported_at</code> </p> <code>reverse</code> <p>Whether to sort in some given order. By default (<code>None</code>), if given a metric key, the reports with the best metric values will be sorted first. If given a <code>Callable</code>, the reports with the smallest values will be sorted first. Using <code>reverse=True</code> will always reverse this order, while <code>reverse=False</code> will always preserve it.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>ffill</code> <p>Whether to forward fill the incumbents. This means that if a report is not an incumbent, it will be replaced with the current best. This is useful if you want to visualize the incumbents over some x axis, where the you have a point at every place along the axis.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>History</code> <p>The history of incumbents.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def incumbents(\n    self,\n    key: Callable[[Trial.Report, Trial.Report], bool] | str,\n    *,\n    sortby: Callable[[Trial.Report], Comparable] | str = lambda r: r.reported_at,\n    reverse: bool | None = None,\n    ffill: bool = False,\n) -&gt; History:\n    \"\"\"Returns a trace of the incumbents, where only the report that is better than the previous\n    best report is kept.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"incumbents\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n        trial.bucket.rmdir() # markdown-exec: hide\n\n    incumbents = (\n        history\n        .incumbents(\"cost\", sortby=lambda r: r.reported_at)\n    )\n    for report in incumbents:\n        print(f\"{report.values=}, {report.config=}\")\n    ```\n\n    Args:\n        key: The key to use. If given a str, it will use that as the\n            key to use in the metrics, defining if one report is better\n            than another. If given a `#!python Callable`, it should\n            return a `bool`, indicating if the first argument report\n            is better than the second argument report.\n        sortby: The key to sort by. If given a str, it will sort by\n            the value of that key in the `.metrics` and also filter\n            out anything that does not contain this key.\n            By default, it will sort by the end time of the report.\n        reverse: Whether to sort in some given order. By\n            default (`None`), if given a metric key, the reports with\n            the best metric values will be sorted first. If\n            given a `#!python Callable`, the reports with the\n            smallest values will be sorted first. Using\n            `reverse=True` will always reverse this order, while\n            `reverse=False` will always preserve it.\n        ffill: Whether to forward fill the incumbents. This means that\n            if a report is not an incumbent, it will be replaced with\n            the current best. This is useful if you want to\n            visualize the incumbents over some x axis, where the\n            you have a point at every place along the axis.\n\n    Returns:\n        The history of incumbents.\n    \"\"\"  # noqa: E501\n    match key:\n        case str():\n            metric = self.metrics[key]\n            __op = operator.lt if metric.minimize else operator.gt  # type: ignore\n            op = lambda r1, r2: __op(r1.values[key], r2.values[key])\n        case _:\n            op = key\n\n    sorted_reports = self.sortby(sortby, reverse=reverse)\n    return History.from_reports(\n        compare_accumulate(sorted_reports, op=op, ffill=ffill),\n    )\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.sortby","title":"sortby","text":"<pre><code>sortby(\n    key: Callable[[Report], Comparable] | str,\n    *,\n    reverse: bool | None = None\n) -&gt; History\n</code></pre> <p>Sorts the history by a key and returns a sorted History.</p> sortby<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    report = trial.success(cost=x**2 - x*2 + 4)\n    history.add(report)\n\ntrace = (\n    history\n    .filter(lambda report: report.status == \"success\")\n    .sortby(\"cost\")\n)\n\nfor report in trace:\n    print(f\"{report.values}, {report}\")\n</code></pre> <pre><code>{'cost': 3}, Trial.Report(trial=Trial(name='trial_1', config={'x': 1}, bucket=PathBucket(PosixPath('trial-trial_1-2024-08-13T07:34:20.515869')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 515868), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516481), exception=None, values={'cost': 3})\n{'cost': 4}, Trial.Report(trial=Trial(name='trial_0', config={'x': 0}, bucket=PathBucket(PosixPath('trial-trial_0-2024-08-13T07:34:20.515754')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 515749), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516375), exception=None, values={'cost': 4})\n{'cost': 4}, Trial.Report(trial=Trial(name='trial_2', config={'x': 2}, bucket=PathBucket(PosixPath('trial-trial_2-2024-08-13T07:34:20.515930')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 515928), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516557), exception=None, values={'cost': 4})\n{'cost': 7}, Trial.Report(trial=Trial(name='trial_3', config={'x': 3}, bucket=PathBucket(PosixPath('trial-trial_3-2024-08-13T07:34:20.515990')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 515989), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516623), exception=None, values={'cost': 7})\n{'cost': 12}, Trial.Report(trial=Trial(name='trial_4', config={'x': 4}, bucket=PathBucket(PosixPath('trial-trial_4-2024-08-13T07:34:20.516044')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516042), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516743), exception=None, values={'cost': 12})\n{'cost': 19}, Trial.Report(trial=Trial(name='trial_5', config={'x': 5}, bucket=PathBucket(PosixPath('trial-trial_5-2024-08-13T07:34:20.516095')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516094), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516840), exception=None, values={'cost': 19})\n{'cost': 28}, Trial.Report(trial=Trial(name='trial_6', config={'x': 6}, bucket=PathBucket(PosixPath('trial-trial_6-2024-08-13T07:34:20.516143')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516142), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516919), exception=None, values={'cost': 28})\n{'cost': 39}, Trial.Report(trial=Trial(name='trial_7', config={'x': 7}, bucket=PathBucket(PosixPath('trial-trial_7-2024-08-13T07:34:20.516197')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516195), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516984), exception=None, values={'cost': 39})\n{'cost': 52}, Trial.Report(trial=Trial(name='trial_8', config={'x': 8}, bucket=PathBucket(PosixPath('trial-trial_8-2024-08-13T07:34:20.516257')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516256), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 517045), exception=None, values={'cost': 52})\n{'cost': 67}, Trial.Report(trial=Trial(name='trial_9', config={'x': 9}, bucket=PathBucket(PosixPath('trial-trial_9-2024-08-13T07:34:20.516306')), metrics=MetricCollection(metrics={'cost': Metric(name='cost', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 516305), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 20, 517108), exception=None, values={'cost': 67})\n</code></pre> PARAMETER DESCRIPTION <code>key</code> <p>The key to sort by. If given a str, it will sort by the value of that key in the <code>.values</code> and also filter out anything that does not contain this key.</p> <p> TYPE: <code>Callable[[Report], Comparable] | str</code> </p> <code>reverse</code> <p>Whether to sort in some given order. By default (<code>None</code>), if given a metric key, the reports with the best metric values will be sorted first. If given a <code>Callable</code>, the reports with the smallest values will be sorted first. Using <code>reverse=True/False</code> will apply to python's <code>sorted()</code>.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A sorted list of reports</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def sortby(\n    self,\n    key: Callable[[Trial.Report], Comparable] | str,\n    *,\n    reverse: bool | None = None,\n) -&gt; History:\n    \"\"\"Sorts the history by a key and returns a sorted History.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"sortby\" hl_lines=\"15\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial.create(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n        trial.bucket.rmdir() # markdown-exec: hide\n\n    trace = (\n        history\n        .filter(lambda report: report.status == \"success\")\n        .sortby(\"cost\")\n    )\n\n    for report in trace:\n        print(f\"{report.values}, {report}\")\n    ```\n\n    Args:\n        key: The key to sort by. If given a str, it will sort by\n            the value of that key in the `.values` and also filter\n            out anything that does not contain this key.\n        reverse: Whether to sort in some given order. By\n            default (`None`), if given a metric key, the reports with\n            the best metric values will be sorted first. If\n            given a `#!python Callable`, the reports with the\n            smallest values will be sorted first. Using\n            `reverse=True/False` will apply to python's\n            [`sorted()`][sorted].\n\n    Returns:\n        A sorted list of reports\n    \"\"\"  # noqa: E501\n    # If given a str, filter out anything that doesn't have that key\n    if isinstance(key, str):\n        history = self.filter(lambda report: key in report.values)\n        sort_key = lambda r: r.values[key]\n        reverse = reverse if reverse is not None else not self.metrics[key].minimize\n    else:\n        history = self\n        sort_key = key\n        # Default is False\n        reverse = False if reverse is None else reverse\n\n    return History.from_reports(\n        sorted(history.reports, key=sort_key, reverse=reverse),\n    )\n</code></pre>"},{"location":"api/amltk/optimization/metric/","title":"Metric","text":""},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric","title":"amltk.optimization.metric","text":"<p>The metric definition.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric","title":"Metric  <code>dataclass</code>","text":"<pre><code>Metric(\n    name: str,\n    *,\n    minimize: bool = True,\n    bounds: tuple[float, float] | None = None,\n    fn: Callable[P, float] | None = None\n)\n</code></pre> <p>               Bases: <code>Generic[P]</code></p> <p>A metric with a given name, optimal direction, and possible bounds.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.bounds","title":"bounds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bounds: tuple[float, float] | None = field(\n    kw_only=True, default=None\n)\n</code></pre> <p>The bounds of the metric, if any.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.fn","title":"fn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fn: Callable[P, float] | None = field(\n    kw_only=True, default=None, compare=False\n)\n</code></pre> <p>A function to attach to this metric to be used within a trial.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.minimize","title":"minimize  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>minimize: bool = field(kw_only=True, default=True)\n</code></pre> <p>Whether to minimize or maximize the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.optimal","title":"optimal  <code>property</code>","text":"<pre><code>optimal: float\n</code></pre> <p>The optimal value of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.worst","title":"worst  <code>property</code>","text":"<pre><code>worst: float\n</code></pre> <p>The worst possible value of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Comparison","title":"Comparison","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The comparison between two values.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.__call__","title":"__call__","text":"<pre><code>__call__(*args: args, **kwargs: kwargs) -&gt; float\n</code></pre> <p>Call the associated function with this metric.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def __call__(self, *args: P.args, **kwargs: P.kwargs) -&gt; float:\n    \"\"\"Call the associated function with this metric.\"\"\"\n    if self.fn is None:\n        raise ValueError(\n            f\"Metric {self.name} does not have a function to call.\"\n            \" Please provide a function to `Metric(fn=...)` if you\"\n            \" want to call this metric like this.\",\n        )\n    return self.fn(*args, **kwargs)\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.as_scorer","title":"as_scorer","text":"<pre><code>as_scorer(\n    *,\n    response_method: (\n        SklearnResponseMethods\n        | Sequence[SklearnResponseMethods]\n        | None\n    ) = None,\n    **scorer_kwargs: Any\n) -&gt; _Scorer\n</code></pre> <p>Convert a metric to a sklearn scorer.</p> PARAMETER DESCRIPTION <code>response_method</code> <p>The response method to use for the scorer. This can be a single method or an iterable of methods.</p> <p> TYPE: <code>SklearnResponseMethods | Sequence[SklearnResponseMethods] | None</code> DEFAULT: <code>None</code> </p> <code>scorer_kwargs</code> <p>Additional keyword arguments to pass to the scorer during the call. Forwards to <code>sklearn.metrics.make_scorer</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>_Scorer</code> <p>The sklearn scorer.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def as_scorer(\n    self,\n    *,\n    response_method: (\n        SklearnResponseMethods | Sequence[SklearnResponseMethods] | None\n    ) = None,\n    **scorer_kwargs: Any,\n) -&gt; _Scorer:\n    \"\"\"Convert a metric to a sklearn scorer.\n\n    Args:\n        response_method: The response method to use for the scorer.\n            This can be a single method or an iterable of methods.\n        scorer_kwargs: Additional keyword arguments to pass to the\n            scorer during the call. Forwards to [`sklearn.metrics.make_scorer`][].\n\n    Returns:\n        The sklearn scorer.\n    \"\"\"\n    from sklearn.metrics import get_scorer, make_scorer\n\n    match self.fn:\n        case None:\n            try:\n                return get_scorer(self.name)\n            except ValueError as e:\n                raise ValueError(\n                    f\"Could not find scorer for {self.name}.\"\n                    \" Please provide a function to `Metric(fn=...)`.\",\n                ) from e\n        case fn:\n            return make_scorer(\n                fn,\n                greater_is_better=not self.minimize,\n                response_method=response_method,\n                **scorer_kwargs,\n            )\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.compare","title":"compare","text":"<pre><code>compare(v1: float, v2: float) -&gt; Comparison\n</code></pre> <p>Check if <code>v1</code> is better than <code>v2</code>.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def compare(self, v1: float, v2: float) -&gt; Metric.Comparison:\n    \"\"\"Check if `v1` is better than `v2`.\"\"\"\n    minimize = self.minimize\n    if v1 == v2:\n        return Metric.Comparison.EQUAL\n    if v1 &gt; v2:\n        return Metric.Comparison.WORSE if minimize else Metric.Comparison.BETTER\n\n    # v1 &lt; v2\n    return Metric.Comparison.BETTER if minimize else Metric.Comparison.WORSE\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.distance_to_optimal","title":"distance_to_optimal","text":"<pre><code>distance_to_optimal(v: float) -&gt; float\n</code></pre> <p>The distance to the optimal value, using the bounds if possible.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def distance_to_optimal(self, v: float) -&gt; float:\n    \"\"\"The distance to the optimal value, using the bounds if possible.\"\"\"\n    match self.bounds:\n        case None:\n            raise ValueError(\n                f\"Metric {self.name} is unbounded, can not compute distance\"\n                \" to optimal.\",\n            )\n        case (lower, upper) if lower &lt;= v &lt;= upper:\n            if self.minimize:\n                return abs(v - lower)\n            return abs(v - upper)\n        case (lower, upper):\n            raise ValueError(f\"Value {v} is not within {self.bounds=}\")\n        case _:\n            raise ValueError(f\"Invalid {self.bounds=}\")\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(s: str) -&gt; Self\n</code></pre> <p>Create an metric from a str.</p> <pre><code>from amltk.optimization import Metric\n\ns = \"loss (minimize)\"\nmetric = Metric.from_str(s)\nprint(metric)\n\ns = \"accuracy [0.0, 1.0] (maximize)\"\nmetric = Metric.from_str(s)\nprint(metric)\n</code></pre> <pre><code>loss (minimize)\naccuracy [0.0, 1.0] (maximize)\n</code></pre> PARAMETER DESCRIPTION <code>s</code> <p>The string to parse.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The parsed metric.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>@classmethod\ndef from_str(cls, s: str) -&gt; Self:\n    \"\"\"Create an metric from a str.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\"\n    from amltk.optimization import Metric\n\n    s = \"loss (minimize)\"\n    metric = Metric.from_str(s)\n    print(metric)\n\n    s = \"accuracy [0.0, 1.0] (maximize)\"\n    metric = Metric.from_str(s)\n    print(metric)\n    ```\n\n    Args:\n        s: The string to parse.\n\n    Returns:\n        The parsed metric.\n    \"\"\"\n    splits = s.split(\" \")\n    # No bounds\n    if len(splits) == 2:  # noqa: PLR2004\n        name, minimize_str = splits\n        bounds = None\n    else:\n        name, lower_str, upper_str, minimize_str = splits\n        bounds = (float(lower_str[1:-1]), float(upper_str[:-1]))\n\n    minimize = minimize_str == \"(minimize)\"\n    return cls(name=name, minimize=minimize, bounds=bounds)\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.loss","title":"loss","text":"<pre><code>loss(v: float) -&gt; float\n</code></pre> <p>Convert a value to a loss.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def loss(self, v: float, /) -&gt; float:\n    \"\"\"Convert a value to a loss.\"\"\"\n    return float(v) if self.minimize else -float(v)\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.normalized_loss","title":"normalized_loss","text":"<pre><code>normalized_loss(v: float) -&gt; float\n</code></pre> <p>The normalized loss of a value if possible.</p> <p>If both sides of the bounds are finite, we can normalize the value to be between 0 and 1.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def normalized_loss(self, v: float) -&gt; float:\n    \"\"\"The normalized loss of a value if possible.\n\n    If both sides of the bounds are finite, we can normalize the value\n    to be between 0 and 1.\n    \"\"\"\n    match self.bounds:\n        # If both sides are finite, we can 0-1 normalize\n        case (lower, upper) if not np.isinf(lower) and not np.isinf(upper):\n            cost = (v - lower) / (upper - lower)\n            cost = 1 - cost if self.minimize is False else cost\n        # No bounds or one unbounded bound, we can't normalize\n        case _:\n            cost = v if self.minimize else -v\n\n    return cost\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.score","title":"score","text":"<pre><code>score(v: float) -&gt; float\n</code></pre> <p>Convert a value to a score.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def score(self, v: float, /) -&gt; float:\n    \"\"\"Convert a value to a score.\"\"\"\n    return -float(v) if self.minimize else float(v)\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.MetricCollection","title":"MetricCollection  <code>dataclass</code>","text":"<pre><code>MetricCollection(*, metrics: Mapping[str, Metric] = dict())\n</code></pre> <p>               Bases: <code>Mapping[str, Metric]</code></p> <p>A collection of metrics.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.MetricCollection.metrics","title":"metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metrics: Mapping[str, Metric] = field(default_factory=dict)\n</code></pre> <p>The metrics in this collection.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.MetricCollection.as_sklearn_scorer","title":"as_sklearn_scorer","text":"<pre><code>as_sklearn_scorer(\n    *,\n    response_methods: (\n        Mapping[\n            str,\n            SklearnResponseMethods\n            | Sequence[SklearnResponseMethods],\n        ]\n        | None\n    ) = None,\n    scorer_kwargs: (\n        Mapping[str, Mapping[str, Any]] | None\n    ) = None,\n    raise_exc: bool = True\n) -&gt; _MultimetricScorer\n</code></pre> <p>Convert this collection to a sklearn scorer.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def as_sklearn_scorer(\n    self,\n    *,\n    response_methods: (\n        Mapping[str, SklearnResponseMethods | Sequence[SklearnResponseMethods]]\n        | None\n    ) = None,\n    scorer_kwargs: Mapping[str, Mapping[str, Any]] | None = None,\n    raise_exc: bool = True,\n) -&gt; _MultimetricScorer:\n    \"\"\"Convert this collection to a sklearn scorer.\"\"\"\n    from sklearn.metrics._scorer import _MultimetricScorer\n\n    rms = response_methods or {}\n    skwargs = scorer_kwargs or {}\n\n    scorers = {\n        k: v.as_scorer(response_method=rms.get(k), **skwargs.get(k, {}))\n        for k, v in self.items()\n    }\n    return _MultimetricScorer(scorers=scorers, raise_exc=raise_exc)\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.MetricCollection.from_collection","title":"from_collection  <code>classmethod</code>","text":"<pre><code>from_collection(\n    metrics: (\n        Metric | Iterable[Metric] | Mapping[str, Metric]\n    )\n) -&gt; MetricCollection\n</code></pre> <p>Create a metric collection from an iterable of metrics.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>@classmethod\ndef from_collection(\n    cls,\n    metrics: Metric | Iterable[Metric] | Mapping[str, Metric],\n) -&gt; MetricCollection:\n    \"\"\"Create a metric collection from an iterable of metrics.\"\"\"\n    match metrics:\n        case Metric():\n            return cls(metrics={metrics.name: metrics})\n        case Mapping():\n            return MetricCollection(metrics={m.name: m for m in metrics.values()})\n        case Iterable():\n            return cls(metrics={m.name: m for m in metrics})  # type: ignore\n        case _:\n            raise TypeError(\n                f\"Expected a Metric, Iterable[Metric], or Mapping[str, Metric].\"\n                f\" Got {type(metrics)} instead.\",\n            )\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.MetricCollection.from_empty","title":"from_empty  <code>classmethod</code>","text":"<pre><code>from_empty() -&gt; MetricCollection\n</code></pre> <p>Create an empty metric collection.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>@classmethod\ndef from_empty(cls) -&gt; MetricCollection:\n    \"\"\"Create an empty metric collection.\"\"\"\n    return cls(metrics={})\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.MetricCollection.optimums","title":"optimums","text":"<pre><code>optimums() -&gt; Mapping[str, float]\n</code></pre> <p>The optimums of the metrics.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def optimums(self) -&gt; Mapping[str, float]:\n    \"\"\"The optimums of the metrics.\"\"\"\n    return {k: v.optimal for k, v in self.items()}\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.MetricCollection.worsts","title":"worsts","text":"<pre><code>worsts() -&gt; Mapping[str, float]\n</code></pre> <p>The worsts of the metrics.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def worsts(self) -&gt; Mapping[str, float]:\n    \"\"\"The worsts of the metrics.\"\"\"\n    return {k: v.worst for k, v in self.items()}\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/","title":"Optimizer","text":""},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer","title":"amltk.optimization.optimizer","text":"<p>The base <code>Optimizer</code> class, defines the API we require optimizers to implement.</p> <ul> <li><code>ask()</code> - Ask the optimizer for a     new <code>Trial</code> to evaluate.</li> <li><code>tell()</code> - Tell the optimizer     the result of the sampled config. This comes in the form of a     <code>Trial.Report</code>.</li> </ul> <p>Additionally, to aid users from switching between optimizers, the <code>preferred_parser()</code> method should return either a <code>parser</code> function or a string that can be used with <code>node.search_space(parser=..._)</code> to extract the search space for the optimizer.</p>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer","title":"Optimizer","text":"<pre><code>Optimizer(\n    metrics: Sequence[Metric],\n    bucket: PathBucket | None = None,\n)\n</code></pre> <p>               Bases: <code>Generic[I]</code></p> <p>An optimizer protocol.</p> <p>An optimizer is an object that can be asked for a trail using <code>ask</code> and a <code>tell</code> to inform the optimizer of the report from that trial.</p> PARAMETER DESCRIPTION <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store results of individual trials from this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __init__(\n    self,\n    metrics: Sequence[Metric],\n    bucket: PathBucket | None = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        metrics: The metrics to optimize.\n        bucket: The bucket to store results of individual trials from this\n            optimizer.\n    \"\"\"\n    super().__init__()\n    if not all_unique(metric.name for metric in metrics):\n        raise ValueError(\n            \"All metrics must have unique names.\"\n            f\"Got {metrics} with names {[metric.name for metric in metrics]}\",\n        )\n\n    self.metrics = MetricCollection.from_collection(metrics)\n    self.bucket = (\n        bucket\n        if bucket is not None\n        else PathBucket(f\"{self.__class__.__name__}-{datetime.now().isoformat()}\")\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.bucket","title":"bucket  <code>instance-attribute</code>","text":"<pre><code>bucket: PathBucket = (\n    bucket\n    if bucket is not None\n    else PathBucket(f\"{__name__}-{isoformat()}\")\n)\n</code></pre> <p>The bucket to give to trials generated by this optimizer.</p>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics: MetricCollection = from_collection(metrics)\n</code></pre> <p>The metrics to optimize.</p>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.CreateSignature","title":"CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.ask","title":"ask  <code>abstractmethod</code>","text":"<pre><code>ask(\n    n: int | None = None,\n) -&gt; Trial[I] | Iterable[Trial[I]]\n</code></pre> <p>Ask the optimizer for a trial to evaluate.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of trials to ask for. If <code>None</code>, ask for a single trial.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial[I] | Iterable[Trial[I]]</code> <p>A config to sample.</p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>@abstractmethod\ndef ask(self, n: int | None = None) -&gt; Trial[I] | Iterable[Trial[I]]:\n    \"\"\"Ask the optimizer for a trial to evaluate.\n\n    Args:\n        n: The number of trials to ask for. If `None`, ask for a single trial.\n\n    Returns:\n        A config to sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.create","title":"create  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>create(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: str | Path | PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Self\n</code></pre> <p>Create this optimizer.</p> <p>Note</p> <p>Subclasses should override this with more specific configuration but these arguments should be all that's necessary to create the optimizer.</p> PARAMETER DESCRIPTION <code>space</code> <p>The space to optimize over.</p> <p> TYPE: <code>Node</code> </p> <code>bucket</code> <p>The bucket for where to store things related to the trial.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>seed</code> <p>The seed to use for the optimizer.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The optimizer.</p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>@classmethod\n@abstractmethod\ndef create(\n    cls,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: str | Path | PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Self:\n    \"\"\"Create this optimizer.\n\n    !!! note\n\n        Subclasses should override this with more specific configuration\n        but these arguments should be all that's necessary to create the optimizer.\n\n    Args:\n        space: The space to optimize over.\n        bucket: The bucket for where to store things related to the trial.\n        metrics: The metrics to optimize.\n        seed: The seed to use for the optimizer.\n\n    Returns:\n        The optimizer.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.preferred_parser","title":"preferred_parser  <code>classmethod</code>","text":"<pre><code>preferred_parser() -&gt; (\n    str\n    | Callable[Concatenate[Node, ...], Any]\n    | Callable[[Node], Any]\n    | None\n)\n</code></pre> <p>The preferred parser for this optimizer.</p> <p>Note</p> <p>Subclasses should override this as required.</p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>@classmethod\ndef preferred_parser(\n    cls,\n) -&gt; str | Callable[Concatenate[Node, ...], Any] | Callable[[Node], Any] | None:\n    \"\"\"The preferred parser for this optimizer.\n\n    !!! note\n\n        Subclasses should override this as required.\n\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.tell","title":"tell  <code>abstractmethod</code>","text":"<pre><code>tell(report: Report[I]) -&gt; None\n</code></pre> <p>Tell the optimizer the report for an asked trial.</p> PARAMETER DESCRIPTION <code>report</code> <p>The report for a trial</p> <p> TYPE: <code>Report[I]</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>@abstractmethod\ndef tell(self, report: Trial.Report[I]) -&gt; None:\n    \"\"\"Tell the optimizer the report for an asked trial.\n\n    Args:\n        report: The report for a trial\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/optimization/trial/","title":"Trial","text":""},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial","title":"amltk.optimization.trial","text":"<p>The Trial and Report class.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial","title":"Trial  <code>dataclass</code>","text":"<pre><code>Trial(\n    *,\n    name: str,\n    config: Mapping[str, Any],\n    bucket: PathBucket,\n    info: I | None,\n    metrics: MetricCollection,\n    created_at: datetime,\n    seed: int | None = None,\n    fidelities: Mapping[str, Any],\n    profiler: Profiler,\n    summary: MutableMapping[str, Any],\n    storage: set[Any],\n    extras: MutableMapping[str, Any]\n)\n</code></pre> <p>               Bases: <code>RichRenderable</code>, <code>Generic[I]</code></p> <p>The trial class.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.bucket","title":"bucket  <code>instance-attribute</code>","text":"<pre><code>bucket: PathBucket\n</code></pre> <p>The bucket to store trial related output to.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: Mapping[str, Any]\n</code></pre> <p>The config of the trial provided by the optimizer.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.created_at","title":"created_at  <code>instance-attribute</code>","text":"<pre><code>created_at: datetime\n</code></pre> <p>When the trial was created.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.extras","title":"extras  <code>instance-attribute</code>","text":"<pre><code>extras: MutableMapping[str, Any]\n</code></pre> <p>Any extras attached to the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.fidelities","title":"fidelities  <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any]\n</code></pre> <p>The fidelities at which to evaluate the trial, if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.info","title":"info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>info: I | None = field(repr=False)\n</code></pre> <p>The info of the trial provided by the optimizer.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics: MetricCollection\n</code></pre> <p>The metrics associated with the trial.</p> <p>You can access the metrics by name, e.g. <code>trial.metrics[\"loss\"]</code>.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The unique name of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.profiler","title":"profiler  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>profiler: Profiler = field(repr=False)\n</code></pre> <p>A profiler for this trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.profiles","title":"profiles  <code>property</code>","text":"<pre><code>profiles: Mapping[str, Interval]\n</code></pre> <p>The profiles of the trial.</p> <p>These are indexed by the name of the profile indicated by:</p> <pre><code>with trial.profile(\"key_to_index\"):\n    # ...\n\nprofile = trial.profiles[\"key_to_index\"]\n</code></pre> <p>The values are a <code>Profile.Interval</code>, which contain a <code>Memory.Interval</code> and a <code>Timer.Interval</code>. Please see the respective documentation for more.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: int | None = None\n</code></pre> <p>The seed to use if suggested by the optimizer.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.storage","title":"storage  <code>instance-attribute</code>","text":"<pre><code>storage: set[Any]\n</code></pre> <p>Anything stored in the trial, the elements of the list are keys that can be used to retrieve them later, such as a Path.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.summary","title":"summary  <code>instance-attribute</code>","text":"<pre><code>summary: MutableMapping[str, Any]\n</code></pre> <p>The summary of the trial. These are for summary statistics of a trial and are single values.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report","title":"Report  <code>dataclass</code>","text":"<pre><code>Report(\n    trial: Trial[I2],\n    status: Status,\n    reported_at: datetime = datetime.now(),\n    exception: BaseException | None = None,\n    traceback: str | None = None,\n    values: Mapping[str, float] = dict(),\n)\n</code></pre> <p>               Bases: <code>RichRenderable</code>, <code>Generic[I2]</code></p> <p>The report generated from a <code>Trial</code>.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.bucket","title":"bucket  <code>property</code>","text":"<pre><code>bucket: PathBucket\n</code></pre> <p>The bucket attached to the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.config","title":"config  <code>property</code>","text":"<pre><code>config: Mapping[str, Any]\n</code></pre> <p>The config of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.exception","title":"exception  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exception: BaseException | None = None\n</code></pre> <p>The exception reported if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.info","title":"info  <code>property</code>","text":"<pre><code>info: I2 | None\n</code></pre> <p>The info of the trial, specific to the optimizer that issued it.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.metrics","title":"metrics  <code>property</code>","text":"<pre><code>metrics: MetricCollection\n</code></pre> <p>The metrics of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.profiles","title":"profiles  <code>property</code>","text":"<pre><code>profiles: Mapping[str, Interval]\n</code></pre> <p>The profiles of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.reported_at","title":"reported_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reported_at: datetime = field(default_factory=now)\n</code></pre> <p>When this Report was generated.</p> <p>This will primarily be <code>None</code> if there was no corresponding key when loading this report from a serialized form, such as with <code>from_df()</code> or <code>from_dict()</code>.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status: Status\n</code></pre> <p>The status of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.storage","title":"storage  <code>property</code>","text":"<pre><code>storage: set[str]\n</code></pre> <p>The storage of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.summary","title":"summary  <code>property</code>","text":"<pre><code>summary: MutableMapping[str, Any]\n</code></pre> <p>The summary of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.traceback","title":"traceback  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>traceback: str | None = field(repr=False, default=None)\n</code></pre> <p>The traceback reported if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.trial","title":"trial  <code>instance-attribute</code>","text":"<pre><code>trial: Trial[I2]\n</code></pre> <p>The trial that was run.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.values","title":"values  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>values: Mapping[str, float] = field(default_factory=dict)\n</code></pre> <p>The reported metric values of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.df","title":"df","text":"<pre><code>df(\n    *,\n    profiles: bool = True,\n    configs: bool = True,\n    summary: bool = True,\n    metrics: bool = True\n) -&gt; DataFrame\n</code></pre> <p>Get a dataframe of the trial.</p> <p>Prefixes</p> <ul> <li><code>summary</code>: Entries will be prefixed with <code>\"summary:\"</code></li> <li><code>config</code>: Entries will be prefixed with <code>\"config:\"</code></li> <li><code>storage</code>: Entries will be prefixed with <code>\"storage:\"</code></li> <li><code>metrics</code>: Entries will be prefixed with <code>\"metrics:\"</code></li> <li><code>profile:&lt;name&gt;</code>: Entries will be prefixed with     <code>\"profile:&lt;name&gt;:\"</code></li> </ul> PARAMETER DESCRIPTION <code>profiles</code> <p>Whether to include the profiles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>configs</code> <p>Whether to include the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>summary</code> <p>Whether to include the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>metrics</code> <p>Whether to include the metrics.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def df(\n    self,\n    *,\n    profiles: bool = True,\n    configs: bool = True,\n    summary: bool = True,\n    metrics: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Get a dataframe of the trial.\n\n    !!! note \"Prefixes\"\n\n        * `summary`: Entries will be prefixed with `#!python \"summary:\"`\n        * `config`: Entries will be prefixed with `#!python \"config:\"`\n        * `storage`: Entries will be prefixed with `#!python \"storage:\"`\n        * `metrics`: Entries will be prefixed with `#!python \"metrics:\"`\n        * `profile:&lt;name&gt;`: Entries will be prefixed with\n            `#!python \"profile:&lt;name&gt;:\"`\n\n    Args:\n        profiles: Whether to include the profiles.\n        configs: Whether to include the configs.\n        summary: Whether to include the summary.\n        metrics: Whether to include the metrics.\n    \"\"\"\n    items = {\n        \"name\": self.name,\n        \"status\": str(self.status),\n        \"trial_seed\": self.trial.seed if self.trial.seed else np.nan,\n        \"exception\": str(self.exception) if self.exception else \"NA\",\n        \"traceback\": str(self.traceback) if self.traceback else \"NA\",\n        \"bucket\": str(self.bucket.path),\n        \"created_at\": self.trial.created_at,\n        \"reported_at\": self.reported_at,\n    }\n    if metrics:\n        for metric_name, value in self.values.items():\n            metric_def = self.metrics[metric_name]\n            items[f\"metric:{metric_def}\"] = value\n    if summary:\n        items.update(**prefix_keys(self.trial.summary, \"summary:\"))\n    if configs:\n        items.update(**prefix_keys(self.trial.config, \"config:\"))\n    if profiles:\n        for name, profile in sorted(self.profiles.items(), key=lambda x: x[0]):\n            items.update(profile.to_dict(prefix=f\"profile:{name}\"))\n\n    return pd.DataFrame(items, index=[0]).convert_dtypes().set_index(\"name\")\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.from_df","title":"from_df  <code>classmethod</code>","text":"<pre><code>from_df(df: DataFrame | Series) -&gt; Report\n</code></pre> <p>Create a report from a dataframe.</p> See Also <ul> <li><code>.from_dict()</code></li> </ul> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@classmethod\ndef from_df(cls, df: pd.DataFrame | pd.Series) -&gt; Trial.Report:\n    \"\"\"Create a report from a dataframe.\n\n    See Also:\n        * [`.from_dict()`][amltk.optimization.Trial.Report.from_dict]\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        if len(df) != 1:\n            raise ValueError(\n                f\"Expected a dataframe with one row, got {len(df)} rows.\",\n            )\n        series = df.iloc[0]\n    else:\n        series = df\n\n    data_dict = {\"name\": series.name, **series.to_dict()}\n    return cls.from_dict(data_dict)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(d: Mapping[str, Any]) -&gt; Report\n</code></pre> <p>Create a report from a dictionary.</p> <p>Prefixes</p> <p>Please see <code>.df()</code> for information on what the prefixes should be for certain fields.</p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary to create the report from.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Report</code> <p>The created report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Trial.Report:\n    \"\"\"Create a report from a dictionary.\n\n    !!! note \"Prefixes\"\n\n        Please see [`.df()`][amltk.optimization.Trial.Report.df]\n        for information on what the prefixes should be for certain fields.\n\n    Args:\n        d: The dictionary to create the report from.\n\n    Returns:\n        The created report.\n    \"\"\"\n    prof_dict = mapping_select(d, \"profile:\")\n    if any(prof_dict):\n        profile_names = sorted(\n            {name.rsplit(\":\", maxsplit=2)[0] for name in prof_dict},\n        )\n        profiles = {\n            name: Profile.from_dict(mapping_select(prof_dict, f\"{name}:\"))\n            for name in profile_names\n        }\n    else:\n        profiles = {}\n\n    # NOTE: We assume the order of the objectives are in the right\n    # order in the dict. If we attempt to force a sort-order, we may\n    # deserialize incorrectly. By not having a sort order, we rely\n    # on serialization to keep the order, which is not ideal either.\n    # May revisit this if we need to\n    raw_metrics: dict[str, float] = mapping_select(d, \"metric:\")\n    metrics: dict[Metric, float | None] = {\n        Metric.from_str(name): value for name, value in raw_metrics.items()\n    }\n\n    exception = d.get(\"exception\")\n    traceback = d.get(\"traceback\")\n    trial_seed = d.get(\"trial_seed\")\n    if pd.isna(exception) or exception == \"NA\":  # type: ignore\n        exception = None\n    if pd.isna(traceback) or traceback == \"NA\":  # type: ignore\n        traceback = None\n    if pd.isna(trial_seed):  # type: ignore\n        trial_seed = None\n\n    if (_bucket := d.get(\"bucket\")) is not None:\n        bucket = PathBucket(_bucket)\n    else:\n        bucket = PathBucket(f\"uknown_trial_bucket-{datetime.now().isoformat()}\")\n\n    created_at_timestamp = d.get(\"created_at\")\n    if created_at_timestamp is None:\n        raise ValueError(\n            \"Cannot load report from dict without a 'created_at' field.\",\n        )\n    created_at = parse_timestamp_object(created_at_timestamp)\n\n    trial: Trial = Trial.create(\n        name=d[\"name\"],\n        config=mapping_select(d, \"config:\"),\n        info=None,  # We don't save this to disk so we load it back as None\n        bucket=bucket,\n        seed=trial_seed,\n        fidelities=mapping_select(d, \"fidelities:\"),\n        profiler=Profiler(profiles=profiles),\n        metrics=metrics.keys(),\n        created_at=created_at,\n        summary=mapping_select(d, \"summary:\"),\n        storage=set(mapping_select(d, \"storage:\").values()),\n        extras=mapping_select(d, \"extras:\"),\n    )\n    _values: dict[str, float] = {\n        m.name: v\n        for m, v in metrics.items()\n        if (v is not None and not pd.isna(v))\n    }\n\n    status = Trial.Status(dict_get_not_none(d, \"status\", \"unknown\"))\n    match status:\n        case Trial.Status.SUCCESS:\n            report = trial.success(**_values)\n        case Trial.Status.FAIL:\n            exc = Exception(exception) if exception else None\n            tb = str(traceback) if traceback else None\n            report = trial.fail(exc, tb, **_values)\n        case Trial.Status.CRASHED:\n            exc = Exception(exception) if exception else Exception(\"Unknown\")\n            tb = str(traceback) if traceback else None\n            report = trial.crashed(exc, tb)\n        case Trial.Status.UNKNOWN | _:\n            report = trial.crashed(exception=Exception(\"Unknown status.\"))\n\n    timestamp = d.get(\"reported_at\")\n    if timestamp is None:\n        raise ValueError(\n            \"Cannot load report from dict without a 'reported_at' field.\",\n        )\n    report.reported_at = parse_timestamp_object(timestamp)\n\n    return report\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    key: str, *, check: type[R] | None = None\n) -&gt; R | Any\n</code></pre> <p>Retrieve items related to the trial.</p> retrieve-bucket<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\n\ntrial = Trial.create(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\nreport = trial.success()\n\nconfig = report.retrieve(\"config.json\")\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> PARAMETER DESCRIPTION <code>key</code> <p>The key of the item to retrieve as said in <code>.storage</code>.</p> <p> TYPE: <code>str</code> </p> <code>check</code> <p>If provided, will check that the retrieved item is of the provided type. If not, will raise a <code>TypeError</code>.</p> <p> TYPE: <code>type[R] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R | Any</code> <p>The retrieved item.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>check=</code> is provided and  the retrieved item is not of the provided type.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def retrieve(self, key: str, *, check: type[R] | None = None) -&gt; R | Any:\n    \"\"\"Retrieve items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve-bucket\" hl_lines=\"11\"\n\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n\n    trial = Trial.create(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    report = trial.success()\n\n    config = report.retrieve(\"config.json\")\n    print(config)\n    trial.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    Args:\n        key: The key of the item to retrieve as said in `.storage`.\n        check: If provided, will check that the retrieved item is of the\n            provided type. If not, will raise a `TypeError`.\n\n    Returns:\n        The retrieved item.\n\n    Raises:\n        TypeError: If `check=` is provided and  the retrieved item is not of the provided\n            type.\n    \"\"\"  # noqa: E501\n    return self.trial.retrieve(key, check=check)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.rich_renderables","title":"rich_renderables","text":"<pre><code>rich_renderables() -&gt; Iterable[RenderableType]\n</code></pre> <p>The renderables for rich for this report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def rich_renderables(self) -&gt; Iterable[RenderableType]:\n    \"\"\"The renderables for rich for this report.\"\"\"\n    from rich.pretty import Pretty\n    from rich.text import Text\n\n    yield Text.assemble(\n        (\"Status\", \"bold\"),\n        (\"(\", \"default\"),\n        self.status.__rich__(),\n        (\")\", \"default\"),\n    )\n    yield Pretty(self.metrics)\n    yield from self.trial.rich_renderables()\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.store","title":"store","text":"<pre><code>store(items: Mapping[str, T]) -&gt; None\n</code></pre> <p>Store items related to the trial.</p> See Also <ul> <li><code>Trial.store()</code></li> </ul> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def store(self, items: Mapping[str, T]) -&gt; None:\n    \"\"\"Store items related to the trial.\n\n    See Also:\n        * [`Trial.store()`][amltk.optimization.trial.Trial.store]\n    \"\"\"\n    self.trial.store(items)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status","title":"Status","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The status of a trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.CRASHED","title":"CRASHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CRASHED = 'crashed'\n</code></pre> <p>The trial crashed.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.FAIL","title":"FAIL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAIL = 'fail'\n</code></pre> <p>The trial failed.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.SUCCESS","title":"SUCCESS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SUCCESS = 'success'\n</code></pre> <p>The trial was successful.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.UNKNOWN","title":"UNKNOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNKNOWN = 'unknown'\n</code></pre> <p>The status of the trial is unknown.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.attach_extra","title":"attach_extra","text":"<pre><code>attach_extra(name: str, plugin_item: Any) -&gt; None\n</code></pre> <p>Attach a plugin item to the trial.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the plugin item.</p> <p> TYPE: <code>str</code> </p> <code>plugin_item</code> <p>The plugin item.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def attach_extra(self, name: str, plugin_item: Any) -&gt; None:\n    \"\"\"Attach a plugin item to the trial.\n\n    Args:\n        name: The name of the plugin item.\n        plugin_item: The plugin item.\n    \"\"\"\n    self.extras[name] = plugin_item\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Create a copy of the trial.</p> RETURNS DESCRIPTION <code>Self</code> <p>The copy of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Create a copy of the trial.\n\n    Returns:\n        The copy of the trial.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.crashed","title":"crashed","text":"<pre><code>crashed(\n    exception: Exception, traceback: str | None = None\n) -&gt; Report[I]\n</code></pre> <p>Generate a crash report.</p> <p>Note</p> <p>You will typically not create these manually, but instead if we don't recieve a report from a target function evaluation, but only an error, we assume something crashed and generate a crash report for you.</p> <p>Non specifed metrics</p> <p>We will use the <code>.metrics</code> to determine the <code>.worst</code> value of the metric, using that as the reported metrics</p> PARAMETER DESCRIPTION <code>exception</code> <p>The exception that caused the crash. If not provided, the exception will be taken from the trial. If this is still <code>None</code>, a <code>RuntimeError</code> will be raised.</p> <p> TYPE: <code>Exception</code> </p> <code>traceback</code> <p>The traceback of the exception. If not provided, the traceback will be taken from the trial if there is one there.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Report[I]</code> <p>The report of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def crashed(\n    self,\n    exception: Exception,\n    traceback: str | None = None,\n) -&gt; Trial.Report[I]:\n    \"\"\"Generate a crash report.\n\n    !!! note\n\n        You will typically not create these manually, but instead if we don't\n        recieve a report from a target function evaluation, but only an error,\n        we assume something crashed and generate a crash report for you.\n\n    !!! note \"Non specifed metrics\"\n\n        We will use the [`.metrics`][amltk.optimization.Trial.metrics] to determine\n        the [`.worst`][amltk.optimization.Metric.worst] value of the metric,\n        using that as the reported metrics\n\n    Args:\n        exception: The exception that caused the crash. If not provided, the\n            exception will be taken from the trial. If this is still `None`,\n            a `RuntimeError` will be raised.\n        traceback: The traceback of the exception. If not provided, the\n            traceback will be taken from the trial if there is one there.\n\n    Returns:\n        The report of the trial.\n    \"\"\"\n    if traceback is None:\n        traceback = \"\".join(traceback_module.format_tb(exception.__traceback__))\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.CRASHED,\n        exception=exception,\n        traceback=traceback,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    name: str,\n    config: Mapping[str, Any] | None = None,\n    *,\n    metrics: (\n        Metric\n        | Iterable[Metric]\n        | Mapping[str, Metric]\n        | None\n    ) = None,\n    info: I | None = None,\n    seed: int | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    created_at: datetime | None = None,\n    profiler: Profiler | None = None,\n    bucket: str | Path | PathBucket | None = None,\n    summary: MutableMapping[str, Any] | None = None,\n    storage: set[Hashable] | None = None,\n    extras: MutableMapping[str, Any] | None = None\n) -&gt; Trial[I]\n</code></pre> <p>Create a trial.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the trial.</p> <p> TYPE: <code>str</code> </p> <code>metrics</code> <p>The metrics of the trial.</p> <p> TYPE: <code>Metric | Iterable[Metric] | Mapping[str, Metric] | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The config of the trial.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>info</code> <p>The info of the trial.</p> <p> TYPE: <code>I | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed of the trial.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities of the trial.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>bucket</code> <p>The bucket of the trial.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>created_at</code> <p>When the trial was created.</p> <p> TYPE: <code>datetime | None</code> DEFAULT: <code>None</code> </p> <code>profiler</code> <p>The profiler of the trial.</p> <p> TYPE: <code>Profiler | None</code> DEFAULT: <code>None</code> </p> <code>summary</code> <p>The summary of the trial.</p> <p> TYPE: <code>MutableMapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>storage</code> <p>The storage of the trial.</p> <p> TYPE: <code>set[Hashable] | None</code> DEFAULT: <code>None</code> </p> <code>extras</code> <p>The extras of the trial.</p> <p> TYPE: <code>MutableMapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial[I]</code> <p>The trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@classmethod\ndef create(  # noqa: PLR0913\n    cls,\n    name: str,\n    config: Mapping[str, Any] | None = None,\n    *,\n    metrics: Metric | Iterable[Metric] | Mapping[str, Metric] | None = None,\n    info: I | None = None,\n    seed: int | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    created_at: datetime | None = None,\n    profiler: Profiler | None = None,\n    bucket: str | Path | PathBucket | None = None,\n    summary: MutableMapping[str, Any] | None = None,\n    storage: set[Hashable] | None = None,\n    extras: MutableMapping[str, Any] | None = None,\n) -&gt; Trial[I]:\n    \"\"\"Create a trial.\n\n    Args:\n        name: The name of the trial.\n        metrics: The metrics of the trial.\n        config: The config of the trial.\n        info: The info of the trial.\n        seed: The seed of the trial.\n        fidelities: The fidelities of the trial.\n        bucket: The bucket of the trial.\n        created_at: When the trial was created.\n        profiler: The profiler of the trial.\n        summary: The summary of the trial.\n        storage: The storage of the trial.\n        extras: The extras of the trial.\n\n    Returns:\n        The trial.\n    \"\"\"\n    return Trial(\n        name=name,\n        metrics=(\n            MetricCollection.from_collection(metrics)\n            if metrics is not None\n            else MetricCollection()\n        ),\n        profiler=(\n            profiler\n            if profiler is not None\n            else Profiler(memory_unit=\"B\", time_kind=\"wall\")\n        ),\n        config=config if config is not None else {},\n        info=info,\n        seed=seed,\n        created_at=created_at if created_at is not None else datetime.now(),\n        fidelities=fidelities if fidelities is not None else {},\n        bucket=(\n            bucket\n            if isinstance(bucket, PathBucket)\n            else (\n                PathBucket(bucket)\n                if bucket is not None\n                else PathBucket(f\"trial-{name}-{datetime.now().isoformat()}\")\n            )\n        ),\n        summary=summary if summary is not None else {},\n        storage=storage if storage is not None else set(),\n        extras=extras if extras is not None else {},\n    )\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.delete_from_storage","title":"delete_from_storage","text":"<pre><code>delete_from_storage(\n    items: Iterable[str],\n) -&gt; dict[str, bool]\n</code></pre> <p>Delete items related to the trial.</p> delete-storage<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\ntrial = Trial.create(name=\"trial\", config={\"x\": 1}, info={}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\ntrial.delete_from_storage(items=[\"config.json\"])\n\nprint(trial.storage)\n</code></pre> <pre><code>set()\n</code></pre> PARAMETER DESCRIPTION <code>items</code> <p>The items to delete, an iterable of keys</p> <p> TYPE: <code>Iterable[str]</code> </p> RETURNS DESCRIPTION <code>dict[str, bool]</code> <p>A dict from the key to whether it was deleted or not.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def delete_from_storage(self, items: Iterable[str]) -&gt; dict[str, bool]:\n    \"\"\"Delete items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"delete-storage\" hl_lines=\"6\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n    trial = Trial.create(name=\"trial\", config={\"x\": 1}, info={}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    trial.delete_from_storage(items=[\"config.json\"])\n\n    print(trial.storage)\n    trial.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    Args:\n        items: The items to delete, an iterable of keys\n\n    Returns:\n        A dict from the key to whether it was deleted or not.\n    \"\"\"  # noqa: E501\n    # If not a Callable, we convert to a path bucket\n    removed = self.bucket.remove(items)\n    self.storage.difference_update(items)\n    return removed\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.dump_exception","title":"dump_exception","text":"<pre><code>dump_exception(\n    exception: BaseException, *, name: str | None = None\n) -&gt; None\n</code></pre> <p>Dump an exception to the trial.</p> PARAMETER DESCRIPTION <code>exception</code> <p>The exception to dump.</p> <p> TYPE: <code>BaseException</code> </p> <code>name</code> <p>The name of the file to dump to. If <code>None</code>, will be <code>\"exception\"</code>.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def dump_exception(\n    self,\n    exception: BaseException,\n    *,\n    name: str | None = None,\n) -&gt; None:\n    \"\"\"Dump an exception to the trial.\n\n    Args:\n        exception: The exception to dump.\n        name: The name of the file to dump to. If `None`, will be `\"exception\"`.\n    \"\"\"\n    fname = name if name is not None else \"exception\"\n    traceback = \"\".join(traceback_module.format_tb(exception.__traceback__))\n    msg = f\"{traceback}\\n{exception.__class__.__name__}: {exception}\"\n    self.store({f\"{fname}.txt\": msg})\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.fail","title":"fail","text":"<pre><code>fail(\n    exception: Exception | None = None,\n    traceback: str | None = None,\n    /,\n    **metrics: float | int,\n) -&gt; Report[I]\n</code></pre> <p>Generate a failure report.</p> <p>Non specifed metrics</p> <p>If you do not specify metrics, this will use the <code>.metrics</code> to determine the <code>.worst</code> value of the metric, using that as the reported result</p> fail<pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True, bounds=(0, 1_000))\ntrial = Trial.create(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\ntry:\n    raise ValueError(\"This is an error\")  # Something went wrong\nexcept Exception as error:\n    report = trial.fail(error)\n\nprint(report.values)\nprint(report)\n</code></pre> <pre><code>{}\nTrial.Report(trial=Trial(name='trial', config={'x': 1}, bucket=PathBucket(PosixPath('trial-trial-2024-08-13T07:34:21.050712')), metrics=MetricCollection(metrics={'loss': Metric(name='loss', minimize=True, bounds=(0.0, 1000.0), fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 21, 50706), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.FAIL: 'fail'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 21, 50895), exception=ValueError('This is an error'), values={})\n</code></pre> RETURNS DESCRIPTION <code>Report[I]</code> <p>The result of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def fail(\n    self,\n    exception: Exception | None = None,\n    traceback: str | None = None,\n    /,\n    **metrics: float | int,\n) -&gt; Trial.Report[I]:\n    \"\"\"Generate a failure report.\n\n    !!! note \"Non specifed metrics\"\n\n        If you do not specify metrics, this will use\n        the [`.metrics`][amltk.optimization.Trial.metrics] to determine\n        the [`.worst`][amltk.optimization.Metric.worst] value of the metric,\n        using that as the reported result\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"fail\"\n    from amltk.optimization import Trial, Metric\n\n    loss = Metric(\"loss\", minimize=True, bounds=(0, 1_000))\n    trial = Trial.create(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\n    try:\n        raise ValueError(\"This is an error\")  # Something went wrong\n    except Exception as error:\n        report = trial.fail(error)\n\n    print(report.values)\n    print(report)\n    trial.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    Returns:\n        The result of the trial.\n    \"\"\"\n    if exception is not None and traceback is None:\n        traceback = traceback_module.format_exc()\n\n    # Need to check if anything extra was reported!\n    extra = set(metrics.keys()) - self.metrics.keys()\n    if extra:\n        raise ValueError(\n            f\"Cannot report `fail()` with extra metrics: {extra=}.\"\n            f\"\\nOnly metrics {list(self.metrics)} as these are the metrics\"\n            \" provided for this trial.\"\n            \"\\nTo record other numerics, use `trial.summary` instead.\",\n        )\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.FAIL,\n        exception=exception,\n        traceback=traceback,\n        values=metrics,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.profile","title":"profile","text":"<pre><code>profile(\n    name: str,\n    *,\n    time: (\n        Kind | Literal[\"wall\", \"cpu\", \"process\"] | None\n    ) = None,\n    memory_unit: (\n        Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None\n    ) = None,\n    summary: bool = False\n) -&gt; Iterator[None]\n</code></pre> <p>Measure some interval in the trial.</p> <p>The results of the profiling will be available in the <code>.summary</code> attribute with the name of the interval as the key.</p> profile<pre><code>from amltk.optimization import Trial\nimport time\n\ntrial = Trial.create(name=\"trial\", config={\"x\": 1})\n\nwith trial.profile(\"some_interval\"):\n    # Do some work\n    time.sleep(1)\n\nprint(trial.profiler[\"some_interval\"].time)\n</code></pre> <pre><code>Timer.Interval(start=1723534461.0715575, end=1723534462.072753, kind=wall, unit=seconds)\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>The name of the interval.</p> <p> TYPE: <code>str</code> </p> <code>time</code> <p>The timer kind to use for the trial. Defaults to the default timer kind of the profiler.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> <code>memory_unit</code> <p>The memory unit to use for the trial. Defaults to the default memory unit of the profiler.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> <code>summary</code> <p>Whether to add the interval to the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Iterator[None]</code> <p>The interval measured. Values will be nan until the with block is finished.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@contextmanager\ndef profile(\n    self,\n    name: str,\n    *,\n    time: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    summary: bool = False,\n) -&gt; Iterator[None]:\n    \"\"\"Measure some interval in the trial.\n\n    The results of the profiling will be available in the `.summary` attribute\n    with the name of the interval as the key.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"profile\"\n    from amltk.optimization import Trial\n    import time\n\n    trial = Trial.create(name=\"trial\", config={\"x\": 1})\n\n    with trial.profile(\"some_interval\"):\n        # Do some work\n        time.sleep(1)\n\n    print(trial.profiler[\"some_interval\"].time)\n    trial.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    Args:\n        name: The name of the interval.\n        time: The timer kind to use for the trial. Defaults to the default\n            timer kind of the profiler.\n        memory_unit: The memory unit to use for the trial. Defaults to the\n            default memory unit of the profiler.\n        summary: Whether to add the interval to the summary.\n\n    Yields:\n        The interval measured. Values will be nan until the with block is finished.\n    \"\"\"\n    with self.profiler(name=name, memory_unit=memory_unit, time_kind=time):\n        yield\n\n    if summary:\n        profile = self.profiler[name]\n        self.summary.update(profile.to_dict(prefix=name))\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.retrieve","title":"retrieve","text":"<pre><code>retrieve(\n    key: str, *, check: type[R] | None = None\n) -&gt; R | Any\n</code></pre> <p>Retrieve items related to the trial.</p> retrieve<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\n\n# Create a trial, normally done by an optimizer\ntrial = Trial.create(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\nconfig = trial.retrieve(\"config.json\")\n\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> PARAMETER DESCRIPTION <code>key</code> <p>The key of the item to retrieve as said in <code>.storage</code>.</p> <p> TYPE: <code>str</code> </p> <code>check</code> <p>If provided, will check that the retrieved item is of the provided type. If not, will raise a <code>TypeError</code>.</p> <p> TYPE: <code>type[R] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R | Any</code> <p>The retrieved item.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>check=</code> is provided and  the retrieved item is not of the provided type.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def retrieve(self, key: str, *, check: type[R] | None = None) -&gt; R | Any:\n    \"\"\"Retrieve items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve\" hl_lines=\"7\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n\n    # Create a trial, normally done by an optimizer\n    trial = Trial.create(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    config = trial.retrieve(\"config.json\")\n\n    print(config)\n    trial.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    Args:\n        key: The key of the item to retrieve as said in `.storage`.\n        check: If provided, will check that the retrieved item is of the\n            provided type. If not, will raise a `TypeError`.\n\n    Returns:\n        The retrieved item.\n\n    Raises:\n        TypeError: If `check=` is provided and  the retrieved item is not of the provided\n            type.\n    \"\"\"  # noqa: E501\n    return self.bucket[key].load(check=check)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.rich_renderables","title":"rich_renderables","text":"<pre><code>rich_renderables() -&gt; Iterable[RenderableType]\n</code></pre> <p>The renderables for rich for this report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def rich_renderables(self) -&gt; Iterable[RenderableType]:\n    \"\"\"The renderables for rich for this report.\"\"\"\n    from rich.panel import Panel\n    from rich.pretty import Pretty\n    from rich.table import Table\n\n    items: list[RenderableType] = []\n    table = Table.grid(padding=(0, 1), expand=False)\n\n    # Predfined things\n    table.add_row(\"config\", Pretty(self.config))\n\n    if self.fidelities:\n        table.add_row(\"fidelities\", Pretty(self.fidelities))\n\n    if any(self.extras):\n        table.add_row(\"extras\", Pretty(self.extras))\n\n    if self.seed:\n        table.add_row(\"seed\", Pretty(self.seed))\n\n    if self.bucket:\n        table.add_row(\"bucket\", Pretty(self.bucket))\n\n    if self.metrics:\n        items.append(\n            Panel(Pretty(self.metrics), title=\"Metrics\", title_align=\"left\"),\n        )\n\n    # Dynamic things\n    if self.summary:\n        table.add_row(\"summary\", Pretty(self.summary))\n\n    if any(self.storage):\n        table.add_row(\"storage\", Pretty(self.storage))\n\n    for name, profile in self.profiles.items():\n        table.add_row(\"profile:\" + name, Pretty(profile))\n\n    items.append(table)\n\n    yield from items\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.store","title":"store","text":"<pre><code>store(items: Mapping[str, T]) -&gt; None\n</code></pre> <p>Store items related to the trial.</p> store<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\ntrial = Trial.create(name=\"trial\", config={\"x\": 1}, bucket=PathBucket(\"my-trial\"))\ntrial.store({\"config.json\": trial.config})\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> PARAMETER DESCRIPTION <code>items</code> <p>The items to store, a dict from the key to store it under to the item itself.If using a <code>str</code>, <code>Path</code> or <code>PathBucket</code>, the keys of the items should be a valid filename, including the correct extension. e.g. <code>{\"config.json\": trial.config}</code></p> <p> TYPE: <code>Mapping[str, T]</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def store(self, items: Mapping[str, T]) -&gt; None:\n    \"\"\"Store items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"store\" hl_lines=\"5\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    trial = Trial.create(name=\"trial\", config={\"x\": 1}, bucket=PathBucket(\"my-trial\"))\n    trial.store({\"config.json\": trial.config})\n    print(trial.storage)\n    trial.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    Args:\n        items: The items to store, a dict from the key to store it under\n            to the item itself.If using a `str`, `Path` or `PathBucket`,\n            the keys of the items should be a valid filename, including\n            the correct extension. e.g. `#!python {\"config.json\": trial.config}`\n    \"\"\"  # noqa: E501\n    self.bucket.store(items)\n    # Add the keys to storage\n    self.storage.update(items)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.success","title":"success","text":"<pre><code>success(**metrics: float | int) -&gt; Report[I]\n</code></pre> <p>Generate a success report.</p> success<pre><code>from amltk.optimization import Trial, Metric\n\nloss_metric = Metric(\"loss\", minimize=True)\n\ntrial = Trial.create(name=\"trial\", config={\"x\": 1}, metrics=[loss_metric])\nreport = trial.success(loss=1)\n\nprint(report)\n</code></pre> <pre><code>Trial.Report(trial=Trial(name='trial', config={'x': 1}, bucket=PathBucket(PosixPath('trial-trial-2024-08-13T07:34:22.118692')), metrics=MetricCollection(metrics={'loss': Metric(name='loss', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 22, 118688), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 22, 118790), exception=None, values={'loss': 1})\n</code></pre> PARAMETER DESCRIPTION <code>**metrics</code> <p>The metrics of the trial, where the key is the name of the metrics and the value is the metric.</p> <p> TYPE: <code>float | int</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Report[I]</code> <p>The report of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def success(self, **metrics: float | int) -&gt; Trial.Report[I]:\n    \"\"\"Generate a success report.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"success\" hl_lines=\"7\"\n    from amltk.optimization import Trial, Metric\n\n    loss_metric = Metric(\"loss\", minimize=True)\n\n    trial = Trial.create(name=\"trial\", config={\"x\": 1}, metrics=[loss_metric])\n    report = trial.success(loss=1)\n\n    print(report)\n    trial.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    Args:\n        **metrics: The metrics of the trial, where the key is the name of the\n            metrics and the value is the metric.\n\n    Returns:\n        The report of the trial.\n    \"\"\"  # noqa: E501\n    values: dict[str, float] = {}\n\n    for metric_def in self.metrics.values():\n        if (reported_value := metrics.get(metric_def.name)) is not None:\n            values[metric_def.name] = reported_value\n        else:\n            raise ValueError(\n                f\" Please provide a value for the metric '{metric_def.name}' as \"\n                \" this is one of the metrics of the trial. \"\n                f\"\\n Try `trial.success({metric_def.name}=value, ...)`.\",\n            )\n\n    # Need to check if anything extra was reported!\n    extra = set(metrics.keys()) - self.metrics.keys()\n    if extra:\n        raise ValueError(\n            f\"Cannot report `success()` with extra metrics: {extra=}.\"\n            f\"\\nOnly metrics {list(self.metrics)} as these are the metrics\"\n            \" provided for this trial.\"\n            \"\\nTo record other numerics, use `trial.summary` instead.\",\n        )\n\n    return Trial.Report(trial=self, status=Trial.Status.SUCCESS, values=values)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/","title":"Neps","text":""},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps","title":"amltk.optimization.optimizers.neps","text":"<p>The <code>NEPSOptimizer</code>, is a wrapper around the <code>NePs</code> optimizer.</p> <p>Requirements</p> <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[neps]\n\n# Or directly\npip install neural-pipeline-search\n</code></pre> <p>NePs is still in development</p> <p>NePs is still in development and is not yet stable. There are likely going to be issues. Please report any issues to NePs or in AMLTK.</p> <p>This uses <code>ConfigSpace</code> as its <code>search_space()</code> to optimize.</p> <p>Users should report results using <code>trial.success(loss=...)</code> where <code>loss=</code> is a scaler value to minimize. Optionally, you can also return a <code>cost=</code> which is used for more budget aware algorithms. Again, please see NeP's documentation for more.</p> <p>Conditionals in ConfigSpace</p> <p>NePs does not support conditionals in its search space. This is account for when using the <code>preferred_parser()</code>. during search space creation. In this case, it will simply remove all conditionals from the search space, which may not be ideal for the given problem at hand.</p> <p>Visit their documentation for what you can pass to <code>NEPSOptimizer.create()</code>.</p> <p>The below example shows how you can use neps to optimize an sklearn pipeline.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.neps import NEPSOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Pipeline) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.profile(\"trial\"):\n        try:\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            accuracy = accuracy_score(y_test, y_pred)\n            loss = 1 - accuracy\n            return trial.success(loss=loss, accuracy=accuracy)\n        except Exception as e:\n            return trial.fail(e)\n\nfrom amltk._doc import make_picklable; make_picklable(target_function)  # markdown-exec: hide\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\nmetric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = NEPSOptimizer.create(space=pipeline, metrics=metric, bucket=\"neps-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\noptimizer.bucket.rmdir()  # markdown-exec: hide\n</code></pre> <p>Deep Learning</p> <p>Write an example demonstrating NEPS with continuations</p> <p>Graph Search Spaces</p> <p>Write an example demonstrating NEPS with its graph search spaces</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer","title":"NEPSOptimizer","text":"<pre><code>NEPSOptimizer(\n    *,\n    space: SearchSpace,\n    loss_metric: Metric | Sequence[Metric],\n    cost_metric: Metric | None = None,\n    time_profile: str | None = None,\n    optimizer: BaseOptimizer,\n    working_dir: Path,\n    seed: Seed | None = None,\n    bucket: PathBucket | None = None\n)\n</code></pre> <p>               Bases: <code>Optimizer[NEPSTrialInfo]</code></p> <p>An optimizer that uses SMAC to optimize a config space.</p> PARAMETER DESCRIPTION <code>space</code> <p>The space to use.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>loss_metric</code> <p>The metric to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>cost_metric</code> <p>The cost metric to use. Only certain NePs optimizers support</p> <p> TYPE: <code>Metric | None</code> DEFAULT: <code>None</code> </p> <code>time_profile</code> <p>The profile from which to get the timing of the trial from.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to use.</p> <p> TYPE: <code>BaseOptimizer</code> </p> <code>seed</code> <p>The seed to use for the trials (and not optimizers).</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>working_dir</code> <p>The directory to use for the trials.</p> <p> TYPE: <code>Path</code> </p> <code>bucket</code> <p>The bucket to give to trials generated from this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>def __init__(\n    self,\n    *,\n    space: SearchSpace,\n    loss_metric: Metric | Sequence[Metric],\n    cost_metric: Metric | None = None,\n    time_profile: str | None = None,\n    optimizer: BaseOptimizer,\n    working_dir: Path,\n    seed: Seed | None = None,\n    bucket: PathBucket | None = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        space: The space to use.\n        loss_metric: The metric to optimize.\n        cost_metric: The cost metric to use. Only certain NePs optimizers support\n        time_profile: The profile from which to get the timing of\n            the trial from.\n        optimizer: The optimizer to use.\n        seed: The seed to use for the trials (and not optimizers).\n        working_dir: The directory to use for the trials.\n        bucket: The bucket to give to trials generated from this optimizer.\n    \"\"\"\n    if isinstance(loss_metric, Sequence):\n        raise ValueError(\"NePs does not support multiple metrics\")\n\n    if cost_metric is not None and cost_metric.minimize is False:\n        raise ValueError(\"NePs only supports minimizing cost metrics\")\n\n    if cost_metric is None and optimizer.budget is not None:\n        raise ValueError(\n            \"NePs optimizers with a budget require a cost metric to be provided\",\n        )\n\n    metrics = [loss_metric]\n    if cost_metric is not None:\n        metrics.append(cost_metric)\n\n    super().__init__(bucket=bucket, metrics=metrics)\n    self.space = space\n    self.seed = amltk.randomness.as_int(seed)\n    self.optimizer = optimizer\n    self.working_dir = working_dir\n    self.loss_metric = loss_metric\n    self.cost_metric = cost_metric\n    self.time_profile = time_profile\n\n    self.optimizer_state_file = self.working_dir / \"optimizer_state.yaml\"\n    self.base_result_directory = self.working_dir / \"results\"\n    self.serializer = metahyper.utils.YamlSerializer(self.optimizer.load_config)\n\n    self.working_dir.mkdir(parents=True, exist_ok=True)\n    self.base_result_directory.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.bucket","title":"bucket  <code>instance-attribute</code>","text":"<pre><code>bucket: PathBucket = (\n    bucket\n    if bucket is not None\n    else PathBucket(f\"{__name__}-{isoformat()}\")\n)\n</code></pre> <p>The bucket to give to trials generated by this optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics: MetricCollection = from_collection(metrics)\n</code></pre> <p>The metrics to optimize.</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.CreateSignature","title":"CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.ask","title":"ask","text":"<pre><code>ask(\n    n: int | None = None,\n) -&gt; Trial[NEPSTrialInfo] | Iterable[Trial[NEPSTrialInfo]]\n</code></pre> <p>Ask the optimizer for a new config.</p> RETURNS DESCRIPTION <code>Trial[NEPSTrialInfo] | Iterable[Trial[NEPSTrialInfo]]</code> <p>The trial info for the new config.</p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@override\ndef ask(\n    self,\n    n: int | None = None,\n) -&gt; Trial[NEPSTrialInfo] | Iterable[Trial[NEPSTrialInfo]]:\n    \"\"\"Ask the optimizer for a new config.\n\n    Returns:\n        The trial info for the new config.\n    \"\"\"\n    # TODO: Ask neps people if there's a good way to batch sample rather than 1 by 1\n    if n is not None:\n        return (self.ask(n=None) for _ in range(n))\n\n    with self.optimizer.using_state(self.optimizer_state_file, self.serializer):\n        (\n            config_id,\n            config,\n            pipeline_directory,\n            previous_pipeline_directory,\n        ) = metahyper.api._sample_config(  # type: ignore\n            optimization_dir=self.working_dir,\n            sampler=self.optimizer,\n            serializer=self.serializer,\n            logger=logger,\n        )\n\n    match config:\n        case SearchSpace():\n            _config = config.hp_values()\n        case _:  # type: ignore\n            _config = {\n                k: v.value if isinstance(v, Parameter) else v\n                for k, v in config.items()  # type: ignore\n            }\n\n    info = NEPSTrialInfo(\n        name=str(config_id),\n        config=deepcopy(_config),\n        pipeline_directory=pipeline_directory,\n        previous_pipeline_directory=previous_pipeline_directory,\n    )\n\n    match self.cost_metric:\n        case None:\n            metrics = [self.loss_metric]\n        case cost_metric:\n            metrics = [self.loss_metric, cost_metric]\n\n    trial = Trial.create(\n        name=info.name,\n        config=info.config,\n        info=info,\n        seed=self.seed,\n        bucket=self.bucket / info.name,\n        metrics={m.name: m for m in metrics},\n    )\n    logger.debug(f\"Asked for trial {trial.name}\")\n    return trial\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    *,\n    space: (\n        SearchSpace\n        | ConfigurationSpace\n        | Mapping[str, ConfigurationSpace | Parameter]\n        | Node\n    ),\n    metrics: Metric | Sequence[Metric],\n    cost_metric: Metric | None = None,\n    time_profile: str | None = None,\n    bucket: PathBucket | str | Path | None = None,\n    searcher: str | BaseOptimizer = \"default\",\n    working_dir: str | Path = \"neps\",\n    overwrite: bool = True,\n    seed: Seed | None = None,\n    max_cost_total: float | None = None,\n    searcher_kwargs: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Create a new NEPS optimizer.</p> PARAMETER DESCRIPTION <code>space</code> <p>The space to use.</p> <p> TYPE: <code>SearchSpace | ConfigurationSpace | Mapping[str, ConfigurationSpace | Parameter] | Node</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p>Warning</p> <p>NePs does not support multiple metrics. Please only pass a single metric.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>cost_metric</code> <p>The cost metric to use. Only certain NePs optimizers support this.</p> <p> TYPE: <code>Metric | None</code> DEFAULT: <code>None</code> </p> <code>time_profile</code> <p>What profiler to take the time end from.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the trials.</p> <p>Warning</p> <p>NePS optimizers do not support an explicit seeding. If you'd like to seed their optimizers, they use the global <code>torch.manual_seed</code>, <code>np.random.seed</code>, and <code>random.seed</code>. This is not considered a good practice and there is not much we can do from AMLTK to help with this.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>bucket</code> <p>The bucket to give to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | str | Path | None</code> DEFAULT: <code>None</code> </p> <code>searcher</code> <p>The searcher to use.</p> <p> TYPE: <code>str | BaseOptimizer</code> DEFAULT: <code>'default'</code> </p> <code>working_dir</code> <p>The directory to use for the optimization.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>'neps'</code> </p> <code>overwrite</code> <p>Whether to overwrite the working directory if it exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_cost_total</code> <p>The maximum cost to use for the optimization.</p> <p>Warning</p> <p>This only effects the optimization if the searcher utilizes the budget for it's actual suggestion of the next config. If the searcher does not use the budget. This parameter has no effect.</p> <p>The user is still expected to stop <code>ask()</code>'ing for configs when they have reached some budget.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>searcher_kwargs</code> <p>Additional kwargs to pass to the searcher.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@override\n@classmethod\ndef create(\n    cls,\n    *,\n    space: (\n        SearchSpace\n        | ConfigurationSpace\n        | Mapping[str, ConfigurationSpace | Parameter]\n        | Node\n    ),\n    metrics: Metric | Sequence[Metric],\n    cost_metric: Metric | None = None,\n    time_profile: str | None = None,\n    bucket: PathBucket | str | Path | None = None,\n    searcher: str | BaseOptimizer = \"default\",\n    working_dir: str | Path = \"neps\",\n    overwrite: bool = True,\n    seed: Seed | None = None,\n    max_cost_total: float | None = None,\n    searcher_kwargs: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Create a new NEPS optimizer.\n\n    Args:\n        space: The space to use.\n        metrics: The metrics to optimize.\n\n            !!! warning\n\n                NePs does not support multiple metrics. Please only pass a single\n                metric.\n\n        cost_metric: The cost metric to use. Only certain NePs optimizers support\n            this.\n        time_profile: What profiler to take the time end from.\n        seed: The seed to use for the trials.\n\n            !!! warning\n\n                NePS optimizers do not support an explicit seeding. If you'd\n                like to seed their optimizers, they use the global\n                `torch.manual_seed`, `np.random.seed`, and `random.seed`.\n                This is not considered a good practice and there is not\n                much we can do from AMLTK to help with this.\n\n        bucket: The bucket to give to trials generated by this optimizer.\n        searcher: The searcher to use.\n        working_dir: The directory to use for the optimization.\n        overwrite: Whether to overwrite the working directory if it exists.\n        max_cost_total: The maximum cost to use for the optimization.\n\n            !!! warning\n\n                This only effects the optimization if the searcher utilizes the\n                budget for it's actual suggestion of the next config. If the\n                searcher does not use the budget. This parameter has no effect.\n\n                The user is still expected to stop `ask()`'ing for configs when\n                they have reached some budget.\n        searcher_kwargs: Additional kwargs to pass to the searcher.\n    \"\"\"\n    if isinstance(space, Node):\n        space = space.search_space(parser=NEPSOptimizer.preferred_parser())\n\n    match bucket:\n        case None:\n            bucket = PathBucket(\n                f\"{cls.__name__}-{datetime.now().isoformat()}\",\n            )\n        case str() | Path():\n            bucket = PathBucket(bucket)\n        case bucket:\n            bucket = bucket  # noqa: PLW0127\n\n    space = _to_neps_space(space)\n    searcher = _to_neps_searcher(\n        space=space,\n        searcher=searcher,\n        max_cost_total=max_cost_total,\n        searcher_kwargs=searcher_kwargs,\n    )\n    working_dir = Path(working_dir)\n    if working_dir.exists() and overwrite:\n        logger.info(f\"Removing existing working directory {working_dir}\")\n        shutil.rmtree(working_dir)\n\n    return cls(\n        space=space,\n        bucket=bucket,\n        seed=seed,\n        loss_metric=metrics,\n        cost_metric=cost_metric,\n        time_profile=time_profile,\n        optimizer=searcher,\n        working_dir=working_dir,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.preferred_parser","title":"preferred_parser  <code>classmethod</code>","text":"<pre><code>preferred_parser() -&gt; NEPSPreferredParser\n</code></pre> <p>The preferred parser for this optimizer.</p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@override\n@classmethod\ndef preferred_parser(cls) -&gt; NEPSPreferredParser:\n    \"\"\"The preferred parser for this optimizer.\"\"\"\n    # TODO: We might want a custom one for neps.SearchSpace, for now we will\n    # use config space but without conditions as NePs doesn't support conditionals\n    return partial(configspace_parser, conditionals=False)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.tell","title":"tell","text":"<pre><code>tell(report: Report[NEPSTrialInfo]) -&gt; None\n</code></pre> <p>Tell the optimizer the result of the sampled config.</p> PARAMETER DESCRIPTION <code>report</code> <p>The report of the trial.</p> <p> TYPE: <code>Report[NEPSTrialInfo]</code> </p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@override\ndef tell(self, report: Trial.Report[NEPSTrialInfo]) -&gt; None:\n    \"\"\"Tell the optimizer the result of the sampled config.\n\n    Args:\n        report: The report of the trial.\n    \"\"\"\n    logger.debug(f\"Telling report for trial {report.trial.name}\")\n    info = report.info\n    assert info is not None\n\n    # Get a metric result\n    loss = report.values.get(self.loss_metric.name, self.loss_metric.worst)\n    normalized_loss = self.loss_metric.normalized_loss(loss)\n\n    result: dict[str, Any] = {\"loss\": normalized_loss}\n\n    metadata: dict[str, Any]\n    if (\n        self.time_profile is not None\n        and (profile := report.profiles.get(self.time_profile)) is not None\n    ):\n        if profile.time.unit is not Timer.Unit.SECONDS:\n            raise NotImplementedError(\n                \"NePs only supports seconds as the time unit\",\n            )\n        metadata = {\"time_end\": profile.time.end}\n    else:\n        # TODO: I'm not sure \"time_end\" is requried but probably for some optimizers\n        metadata = {}\n\n    if self.cost_metric is not None:\n        cost = report.values.get(self.cost_metric.name, self.cost_metric.worst)\n\n        # We don't normalize here\n        result[\"cost\"] = cost\n\n        # If it's a budget aware optimizer\n        if self.optimizer.budget is not None:\n            with self.optimizer.using_state(\n                self.optimizer_state_file,\n                self.serializer,\n            ):\n                self.optimizer.used_budget += cost\n\n            metadata[\"budget\"] = {\n                \"max\": self.optimizer.budget,\n                \"used\": self.optimizer.used_budget,\n                \"eval_cost\": cost,\n                \"account_for_cost\": True,\n            }\n\n    # Dump results\n    self.serializer.dump(result, info.pipeline_directory / \"result\")\n\n    # Load and dump metadata\n    config_metadata = self.serializer.load(info.pipeline_directory / \"metadata\")\n    config_metadata.update(metadata)\n    self.serializer.dump(config_metadata, info.pipeline_directory / \"metadata\")\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSPreferredParser","title":"NEPSPreferredParser","text":"<p>               Bases: <code>Protocol</code></p> <p>The preferred parser call signature for NEPSOptimizer.</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSPreferredParser.__call__","title":"__call__","text":"<pre><code>__call__(\n    node: Node,\n    *,\n    seed: int | None = None,\n    flat: bool = False,\n    delim: str = \":\"\n) -&gt; ConfigurationSpace\n</code></pre> <p>See <code>configspace_parser</code>.</p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>def __call__(\n    self,\n    node: Node,\n    *,\n    seed: int | None = None,\n    flat: bool = False,\n    delim: str = \":\",\n) -&gt; ConfigurationSpace:\n    \"\"\"See [`configspace_parser`][amltk.pipeline.parsers.configspace.parser].\"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSTrialInfo","title":"NEPSTrialInfo  <code>dataclass</code>","text":"<pre><code>NEPSTrialInfo(\n    name: str,\n    config: dict[str, Any],\n    pipeline_directory: Path,\n    previous_pipeline_directory: Path | None,\n)\n</code></pre> <p>The info for a trial.</p>"},{"location":"api/amltk/optimization/optimizers/optuna/","title":"Optuna","text":""},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna","title":"amltk.optimization.optimizers.optuna","text":"<p>Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.</p> <p>Requirements</p> <p>This requires <code>Optuna</code> which can be installed with:</p> <pre><code>pip install amltk[optuna]\n\n# Or directly\npip install optuna\n</code></pre> <p>We provide a thin wrapper called <code>OptunaOptimizer</code> from which you can integrate <code>Optuna</code> into your workflow.</p> <p>This uses an Optuna-like <code>search_space()</code> for its optimization.</p> <p>Users should report results using <code>trial.success()</code> with either <code>cost=</code> or <code>values=</code> depending on any optimization directions given to the underyling optimizer created. Please see their documentation for more.</p> <p>Visit their documentation for what you can pass to <code>OptunaOptimizer.create()</code>, which is forward to <code>optun.create_study()</code>.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.optuna import OptunaOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Pipeline) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.profile(\"trial\"):\n        try:\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            accuracy = accuracy_score(y_test, y_pred)\n            return trial.success(accuracy=accuracy)\n        except Exception as e:\n            return trial.fail(e)\n\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\naccuracy_metric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = OptunaOptimizer.create(space=pipeline, metrics=accuracy_metric, bucket=\"optuna-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\n</code></pre> <pre><code>                  status  ...  profile:trial:time:unit\nname                      ...                         \ntrial_number=1   success  ...                  seconds\ntrial_number=0   success  ...                  seconds\ntrial_number=2   success  ...                  seconds\ntrial_number=4   success  ...                  seconds\ntrial_number=3   success  ...                  seconds\n...                  ...  ...                      ...\ntrial_number=64  success  ...                  seconds\ntrial_number=65  success  ...                  seconds\ntrial_number=66  success  ...                  seconds\ntrial_number=67  success  ...                  seconds\ntrial_number=68  success  ...                  seconds\n\n[69 rows x 21 columns]\n</code></pre> <p>Some more documentation</p> <p>Sorry!</p>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer","title":"OptunaOptimizer","text":"<pre><code>OptunaOptimizer(\n    *,\n    study: Study,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n    space: OptunaSearchSpace\n)\n</code></pre> <p>               Bases: <code>Optimizer[Trial]</code></p> <p>An optimizer that uses Optuna to optimize a search space.</p> PARAMETER DESCRIPTION <code>study</code> <p>The Optuna Study to use.</p> <p> TYPE: <code>Study</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>Defines the current search space.</p> <p> TYPE: <code>OptunaSearchSpace</code> </p> <code>seed</code> <p>The seed to use for the sampler and trials.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@override\ndef __init__(\n    self,\n    *,\n    study: Study,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n    space: OptunaSearchSpace,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        study: The Optuna Study to use.\n        metrics: The metrics to optimize.\n        bucket: The bucket given to trials generated by this optimizer.\n        space: Defines the current search space.\n        seed: The seed to use for the sampler and trials.\n    \"\"\"\n    # Verify the study has the same directions as the metrics\n    match metrics:\n        case Metric(minimize=minimize):\n            _dir = StudyDirection.MINIMIZE if minimize else StudyDirection.MAXIMIZE\n            if study.direction != _dir:\n                raise ValueError(\n                    f\"The study direction is {_dir}, but the metric minimize is \"\n                    f\"{minimize}.\",\n                )\n        case metrics:\n            _dirs = [\n                StudyDirection.MINIMIZE if m.minimize else StudyDirection.MAXIMIZE\n                for m in metrics\n            ]\n            if study.directions != _dirs:\n                raise ValueError(\n                    f\"The study directions are {_dirs}, but the metrics minimize \"\n                    f\"are {[m.minimize for m in metrics]}.\",\n                )\n\n    metrics = [metrics] if isinstance(metrics, Metric) else metrics\n    super().__init__(bucket=bucket, metrics=metrics)\n    self.seed = amltk.randomness.as_int(seed)\n    self.study = study\n    self.space = space\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.bucket","title":"bucket  <code>instance-attribute</code>","text":"<pre><code>bucket: PathBucket = (\n    bucket\n    if bucket is not None\n    else PathBucket(f\"{__name__}-{isoformat()}\")\n)\n</code></pre> <p>The bucket to give to trials generated by this optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics: MetricCollection = from_collection(metrics)\n</code></pre> <p>The metrics to optimize.</p>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.CreateSignature","title":"CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.ask","title":"ask","text":"<pre><code>ask(\n    n: int | None = None,\n) -&gt; Trial[Trial] | Iterable[Trial[Trial]]\n</code></pre> <p>Ask the optimizer for a new config.</p> RETURNS DESCRIPTION <code>Trial[Trial] | Iterable[Trial[Trial]]</code> <p>The trial info for the new config.</p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@override\ndef ask(\n    self,\n    n: int | None = None,\n) -&gt; Trial[OptunaTrial] | Iterable[Trial[OptunaTrial]]:\n    \"\"\"Ask the optimizer for a new config.\n\n    Returns:\n        The trial info for the new config.\n    \"\"\"\n    if n is not None:\n        return (self.ask(n=None) for _ in range(n))\n\n    optuna_trial: optuna.Trial = self.study.ask(self.space)\n    config = optuna_trial.params\n    trial_number = optuna_trial.number\n    unique_name = f\"{trial_number=}\"\n    return Trial.create(\n        name=unique_name,\n        seed=self.seed,\n        config=config,\n        info=optuna_trial,\n        bucket=self.bucket / unique_name,\n        metrics=self.metrics,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    *,\n    space: OptunaSearchSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    sampler: BaseSampler | None = None,\n    seed: Seed | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a new Optuna optimizer. For more information, check Optuna     documentation     here.</p> PARAMETER DESCRIPTION <code>space</code> <p>Defines the current search space.</p> <p> TYPE: <code>OptunaSearchSpace | Node</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | str | Path | None</code> DEFAULT: <code>None</code> </p> <code>sampler</code> <p>The sampler to use. Default is to use:</p> <ul> <li>Single metric: TPESampler</li> <li>Multiple metrics: NSGAIISampler</li> </ul> <p> TYPE: <code>BaseSampler | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the sampler and trials.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments to pass to <code>optuna.create_study</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The newly created optimizer.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@override\n@classmethod\ndef create(\n    cls,\n    *,\n    space: OptunaSearchSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    sampler: BaseSampler | None = None,\n    seed: Seed | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a new Optuna optimizer. For more information, check Optuna\n        documentation\n        [here](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html#).\n\n    Args:\n        space: Defines the current search space.\n        metrics: The metrics to optimize.\n        bucket: The bucket given to trials generated by this optimizer.\n        sampler: The sampler to use. Default is to use:\n\n            * Single metric: [TPESampler][optuna.samplers.TPESampler]\n            * Multiple metrics: [NSGAIISampler][optuna.samplers.NSGAIISampler]\n\n        seed: The seed to use for the sampler and trials.\n\n        **kwargs: Additional arguments to pass to\n            [`optuna.create_study`][optuna.create_study].\n\n    Returns:\n        Self: The newly created optimizer.\n    \"\"\"\n    if \"direction\" in kwargs:\n        raise ValueError(\n            \"The direction should be provided through the 'metrics' argument.\",\n        )\n\n    if isinstance(space, Node):\n        space = space.search_space(parser=cls.preferred_parser())\n\n    match bucket:\n        case None:\n            bucket = PathBucket(\n                f\"{cls.__name__}-{datetime.now().isoformat()}\",\n            )\n        case str() | Path():\n            bucket = PathBucket(bucket)\n        case bucket:\n            bucket = bucket  # noqa: PLW0127\n\n    if sampler is None:\n        sampler_seed = amltk.randomness.as_int(seed)\n        match metrics:\n            case Metric():\n                sampler = TPESampler(seed=sampler_seed)  # from `create_study()`\n            case metrics:\n                sampler = NSGAIISampler(seed=sampler_seed)  # from `create_study()`\n\n    match metrics:\n        case Metric(minimize=minimize):\n            direction = [\n                StudyDirection.MINIMIZE if minimize else StudyDirection.MAXIMIZE,\n            ]\n        case metrics:\n            direction = [\n                StudyDirection.MINIMIZE if m.minimize else StudyDirection.MAXIMIZE\n                for m in metrics\n            ]\n\n    return cls(\n        study=optuna.create_study(directions=direction, sampler=sampler, **kwargs),\n        metrics=metrics,\n        space=space,\n        bucket=bucket,\n        seed=seed,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.tell","title":"tell","text":"<pre><code>tell(report: Report[Trial]) -&gt; None\n</code></pre> <p>Tell the optimizer the result of the sampled config.</p> PARAMETER DESCRIPTION <code>report</code> <p>The report of the trial.</p> <p> TYPE: <code>Report[Trial]</code> </p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@override\ndef tell(self, report: Trial.Report[OptunaTrial]) -&gt; None:\n    \"\"\"Tell the optimizer the result of the sampled config.\n\n    Args:\n        report: The report of the trial.\n    \"\"\"\n    trial = report.trial.info\n    assert trial is not None\n\n    match report.status:\n        case Trial.Status.CRASHED | Trial.Status.UNKNOWN | Trial.Status.FAIL:\n            # NOTE: Can't tell any values if the trial crashed or failed\n            self.study.tell(trial=trial, state=TrialState.FAIL)\n        case Trial.Status.SUCCESS:\n            values = {\n                name: report.values.get(name, metric.worst)\n                for name, metric in self.metrics.items()\n            }\n            v: list[float] = list(values.values())\n            if len(v) == 1:\n                self.study.tell(trial=trial, state=TrialState.COMPLETE, values=v[0])\n            else:\n                self.study.tell(trial=trial, state=TrialState.COMPLETE, values=v)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaParser","title":"OptunaParser","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for Optuna search space parser.</p>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaParser.__call__","title":"__call__","text":"<pre><code>__call__(\n    node: Node, *, flat: bool = False, delim: str = \":\"\n) -&gt; OptunaSearchSpace\n</code></pre> <p>See <code>optuna_parser</code>.</p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>def __call__(\n    self,\n    node: Node,\n    *,\n    flat: bool = False,\n    delim: str = \":\",\n) -&gt; OptunaSearchSpace:\n    \"\"\"See [`optuna_parser`][amltk.pipeline.parsers.optuna.parser].\"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/random_search/","title":"Random search","text":""},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search","title":"amltk.optimization.optimizers.random_search","text":"<p>An optimizer that uses ConfigSpace for random search.</p>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch","title":"RandomSearch","text":"<pre><code>RandomSearch(\n    *,\n    space: ConfigurationSpace,\n    bucket: PathBucket | None = None,\n    metrics: Metric | Sequence[Metric],\n    seed: Seed | None = None\n)\n</code></pre> <p>               Bases: <code>Optimizer[None]</code></p> <p>An optimizer that uses ConfigSpace for random search.</p> PARAMETER DESCRIPTION <code>space</code> <p>The search space to search over.</p> <p> TYPE: <code>ConfigurationSpace</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>The metrics to optimize. Unused for RandomSearch.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>seed</code> <p>The seed to use for the optimization.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/random_search.py</code> <pre><code>def __init__(\n    self,\n    *,\n    space: ConfigurationSpace,\n    bucket: PathBucket | None = None,\n    metrics: Metric | Sequence[Metric],\n    seed: Seed | None = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        space: The search space to search over.\n        bucket: The bucket given to trials generated by this optimizer.\n        metrics: The metrics to optimize. Unused for RandomSearch.\n        seed: The seed to use for the optimization.\n    \"\"\"\n    metrics = metrics if isinstance(metrics, Sequence) else [metrics]\n    super().__init__(metrics=metrics, bucket=bucket)\n    seed = as_int(seed)\n    space.seed(seed)\n    self._counter = 0\n    self.seed = seed\n    self.space = space\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.bucket","title":"bucket  <code>instance-attribute</code>","text":"<pre><code>bucket: PathBucket = (\n    bucket\n    if bucket is not None\n    else PathBucket(f\"{__name__}-{isoformat()}\")\n)\n</code></pre> <p>The bucket to give to trials generated by this optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics: MetricCollection = from_collection(metrics)\n</code></pre> <p>The metrics to optimize.</p>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.CreateSignature","title":"CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.ask","title":"ask","text":"<pre><code>ask(\n    n: int | None = None,\n) -&gt; Trial[None] | Iterable[Trial[None]]\n</code></pre> <p>Ask the optimizer for a new config.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configs to ask for. If <code>None</code>, ask for a single config.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial[None] | Iterable[Trial[None]]</code> <p>The trial info for the new config.</p> Source code in <code>src/amltk/optimization/optimizers/random_search.py</code> <pre><code>@override\ndef ask(\n    self,\n    n: int | None = None,\n) -&gt; Trial[None] | Iterable[Trial[None]]:\n    \"\"\"Ask the optimizer for a new config.\n\n    Args:\n        n: The number of configs to ask for. If `None`, ask for a single config.\n\n\n    Returns:\n        The trial info for the new config.\n    \"\"\"\n    if n is None:\n        configs = [self.space.sample_configuration()]\n    else:\n        configs = self.space.sample_configuration(n)\n\n    trials: list[Trial[None]] = []\n    for config in configs:\n        self._counter += 1\n        randuid_seed = self.seed + self._counter\n        unique_name = f\"trial-{randuid(4, seed=randuid_seed)}-{self._counter}\"\n        trial: Trial[None] = Trial.create(\n            name=unique_name,\n            config=dict(config),\n            info=None,\n            seed=self.seed,\n            bucket=self.bucket / unique_name,\n            metrics=self.metrics,\n        )\n        trials.append(trial)\n\n    if n is None:\n        return trials[0]\n\n    return trials\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    *,\n    space: ConfigurationSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    seed: Seed | None = None\n) -&gt; Self\n</code></pre> <p>Create a random search optimizer.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>ConfigurationSpace | Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | str | Path | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/random_search.py</code> <pre><code>@override\n@classmethod\ndef create(\n    cls,\n    *,\n    space: ConfigurationSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    seed: Seed | None = None,\n) -&gt; Self:\n    \"\"\"Create a random search optimizer.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    seed = as_int(seed)\n    match bucket:\n        case None:\n            bucket = PathBucket(\n                f\"{cls.__name__}-{datetime.now().isoformat()}\",\n            )\n        case str() | Path():\n            bucket = PathBucket(bucket)\n        case bucket:\n            bucket = bucket  # noqa: PLW0127\n\n    if isinstance(space, Node):\n        space = space.search_space(parser=cls.preferred_parser())\n\n    return cls(\n        space=space,\n        seed=seed,\n        bucket=bucket,\n        metrics=metrics,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.preferred_parser","title":"preferred_parser  <code>classmethod</code>","text":"<pre><code>preferred_parser() -&gt; Literal['configspace']\n</code></pre> <p>The preferred parser for this optimizer.</p> Source code in <code>src/amltk/optimization/optimizers/random_search.py</code> <pre><code>@override\n@classmethod\ndef preferred_parser(cls) -&gt; Literal[\"configspace\"]:\n    \"\"\"The preferred parser for this optimizer.\"\"\"\n    return \"configspace\"\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/random_search/#amltk.optimization.optimizers.random_search.RandomSearch.tell","title":"tell","text":"<pre><code>tell(report: Report[None]) -&gt; None\n</code></pre> <p>Tell the optimizer about the result of a trial.</p> <p>Does nothing for random search.</p> PARAMETER DESCRIPTION <code>report</code> <p>The report of the trial.</p> <p> TYPE: <code>Report[None]</code> </p> Source code in <code>src/amltk/optimization/optimizers/random_search.py</code> <pre><code>@override\ndef tell(self, report: Trial.Report[None]) -&gt; None:\n    \"\"\"Tell the optimizer about the result of a trial.\n\n    Does nothing for random search.\n\n    Args:\n        report: The report of the trial.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/","title":"Smac","text":""},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac","title":"amltk.optimization.optimizers.smac","text":"<p>The <code>SMACOptimizer</code>, is a wrapper around the <code>smac</code> optimizer.</p> <p>Requirements</p> <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[smac]\n\n# Or directly\npip install smac\n</code></pre> <p>This uses <code>ConfigSpace</code> as its <code>search_space()</code> to optimize.</p> <p>Users should report results using <code>trial.success()</code>.</p> <p>Visit their documentation for what you can pass to <code>SMACOptimizer.create()</code>.</p> <p>The below example shows how you can use SMAC to optimize an sklearn pipeline.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component, Node\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Node) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.profile(\"trial\"):\n        try:\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            accuracy = accuracy_score(y_test, y_pred)\n            return trial.success(accuracy=accuracy)\n        except Exception as e:\n            return trial.fail(e)\n\n    return trial.fail()\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100), \"max_samples\": (0.1, 0.9)})\n\nmetric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = SMACOptimizer.create(space=pipeline, metrics=metric, bucket=\"smac-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\n</code></pre> <pre><code>                                                     status  ...  profile:trial:time:unit\nname                                                         ...                         \nconfig_id=1_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=3_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=2_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=4_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=5_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=7_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=6_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=9_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=8_seed=1947329455_budget=None_instanc...  success  ...                  seconds\nconfig_id=11_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=10_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=13_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=12_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=15_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=14_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=17_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=16_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=19_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=18_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=20_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=21_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=22_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=23_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=24_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=26_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=25_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=27_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=28_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=29_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=30_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=31_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=32_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=33_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=34_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=35_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=36_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=37_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=38_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=40_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=39_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=41_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=42_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=43_seed=1947329455_budget=None_instan...  success  ...                  seconds\nconfig_id=44_seed=1947329455_budget=None_instan...  success  ...                  seconds\n\n[44 rows x 22 columns]\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer","title":"SMACOptimizer","text":"<pre><code>SMACOptimizer(\n    *,\n    facade: AbstractFacade,\n    bucket: PathBucket | None = None,\n    metrics: Metric | Sequence[Metric],\n    fidelities: Mapping[str, FidT] | None = None,\n    time_profile: str | None = None\n)\n</code></pre> <p>               Bases: <code>Optimizer[TrialInfo]</code></p> <p>An optimizer that uses SMAC to optimize a config space.</p> PARAMETER DESCRIPTION <code>facade</code> <p>The SMAC facade to use.</p> <p> TYPE: <code>AbstractFacade</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>fidelities</code> <p>The fidelities to use, if any.</p> <p> TYPE: <code>Mapping[str, FidT] | None</code> DEFAULT: <code>None</code> </p> <code>time_profile</code> <p>The profile to use to get time information to the optimizer. Must use <code>trial.profile(time_profile)</code> in your target function then.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>def __init__(\n    self,\n    *,\n    facade: AbstractFacade,\n    bucket: PathBucket | None = None,\n    metrics: Metric | Sequence[Metric],\n    fidelities: Mapping[str, FidT] | None = None,\n    time_profile: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        facade: The SMAC facade to use.\n        bucket: The bucket given to trials generated by this optimizer.\n        metrics: The metrics to optimize.\n        fidelities: The fidelities to use, if any.\n        time_profile: The profile to use to get time information to the\n            optimizer. Must use `trial.profile(time_profile)` in your\n            target function then.\n    \"\"\"\n    # We need to very that the scenario is correct incase user pass in\n    # their own facade construction\n    assert list(self.crash_costs(metrics).values()) == facade.scenario.crash_cost\n\n    metrics = metrics if isinstance(metrics, Sequence) else [metrics]\n    super().__init__(metrics=metrics, bucket=bucket)\n    self.facade = facade\n    self.fidelities = fidelities\n    self.time_profile = time_profile\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.bucket","title":"bucket  <code>instance-attribute</code>","text":"<pre><code>bucket: PathBucket = (\n    bucket\n    if bucket is not None\n    else PathBucket(f\"{__name__}-{isoformat()}\")\n)\n</code></pre> <p>The bucket to give to trials generated by this optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.metrics","title":"metrics  <code>instance-attribute</code>","text":"<pre><code>metrics: MetricCollection = from_collection(metrics)\n</code></pre> <p>The metrics to optimize.</p>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.CreateSignature","title":"CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.ask","title":"ask","text":"<pre><code>ask(\n    n: int | None = None,\n) -&gt; Trial[TrialInfo] | Iterable[Trial[TrialInfo]]\n</code></pre> <p>Ask the optimizer for a new config.</p> PARAMETER DESCRIPTION <code>n</code> <p>The number of configs to ask for. If <code>None</code>, ask for a single config.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Trial[TrialInfo] | Iterable[Trial[TrialInfo]]</code> <p>The trial info for the new config.</p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@override\ndef ask(\n    self,\n    n: int | None = None,\n) -&gt; Trial[SMACTrialInfo] | Iterable[Trial[SMACTrialInfo]]:\n    \"\"\"Ask the optimizer for a new config.\n\n    Args:\n        n: The number of configs to ask for. If `None`, ask for a single config.\n\n\n    Returns:\n        The trial info for the new config.\n    \"\"\"\n    if n is not None:\n        return (self.ask(n=None) for _ in range(n))\n\n    smac_trial_info = self.facade.ask()\n    config = smac_trial_info.config\n    budget = smac_trial_info.budget\n    instance = smac_trial_info.instance\n    seed = smac_trial_info.seed\n\n    if self.fidelities and budget:\n        if len(self.fidelities) == 1:\n            k, _ = next(iter(self.fidelities.items()))\n            trial_fids = {k: budget}\n        else:\n            trial_fids = {\"budget\": budget}\n    else:\n        trial_fids = None\n\n    config_id = self.facade.runhistory.config_ids[config]\n    unique_name = f\"{config_id=}_{seed=}_{budget=}_{instance=}\"\n    trial: Trial[SMACTrialInfo] = Trial.create(\n        name=unique_name,\n        config=dict(config),\n        info=smac_trial_info,\n        seed=seed,\n        fidelities=trial_fids,\n        bucket=self.bucket / unique_name,\n        metrics=self.metrics,\n    )\n    logger.debug(f\"Asked for trial {trial.name}\")\n    return trial\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.crash_costs","title":"crash_costs  <code>classmethod</code>","text":"<pre><code>crash_costs(\n    metric: Metric | Iterable[Metric],\n) -&gt; dict[str, float]\n</code></pre> <p>Get the crash cost for a metric for SMAC.</p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@classmethod\ndef crash_costs(cls, metric: Metric | Iterable[Metric]) -&gt; dict[str, float]:\n    \"\"\"Get the crash cost for a metric for SMAC.\"\"\"\n    match metric:\n        case Metric():\n            return {metric.name: metric.normalized_loss(metric.worst)}\n        case Iterable():\n            return {\n                metric.name: metric.normalized_loss(metric.worst)\n                for metric in metric\n            }\n        case _:\n            raise TypeError(\n                f\"Expected a Metric, Mapping, or Iterable of Metrics. Got {metric}\",\n            )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    *,\n    space: ConfigurationSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    time_profile: str | None = None,\n    deterministic: bool = True,\n    seed: Seed | None = None,\n    fidelities: Mapping[str, FidT] | None = None,\n    continue_from_last_run: bool = False,\n    logging_level: (\n        int | Path | Literal[False] | None\n    ) = False\n) -&gt; Self\n</code></pre> <p>Create a new SMAC optimizer using either the HPO facade or a mutli-fidelity facade.</p> PARAMETER DESCRIPTION <code>space</code> <p>The config space to optimize.</p> <p> TYPE: <code>ConfigurationSpace | Node</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | str | Path | None</code> DEFAULT: <code>None</code> </p> <code>time_profile</code> <p>The profile to use to get time information to the optimizer. Must use <code>trial.profile(time_profile)</code> in your target function then.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>deterministic</code> <p>Whether the function your optimizing is deterministic, given a seed and config.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>seed</code> <p>The seed to use for the optimizer.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities to use, if any.</p> <p> TYPE: <code>Mapping[str, FidT] | None</code> DEFAULT: <code>None</code> </p> <code>continue_from_last_run</code> <p>Whether to continue from a previous run.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>logging_level</code> <p>The logging level to use. This argument is passed forward to SMAC, use False to disable SMAC's handling of logging.</p> <p> TYPE: <code>int | Path | Literal[False] | None</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@override\n@classmethod\ndef create(\n    cls,\n    *,\n    space: ConfigurationSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    time_profile: str | None = None,\n    deterministic: bool = True,\n    seed: Seed | None = None,\n    fidelities: Mapping[str, FidT] | None = None,\n    continue_from_last_run: bool = False,\n    logging_level: int | Path | Literal[False] | None = False,\n) -&gt; Self:\n    \"\"\"Create a new SMAC optimizer using either the HPO facade or\n    a mutli-fidelity facade.\n\n    Args:\n        space: The config space to optimize.\n        metrics: The metrics to optimize.\n        bucket: The bucket given to trials generated by this optimizer.\n        time_profile: The profile to use to get time information to the\n            optimizer. Must use `trial.profile(time_profile)` in your\n            target function then.\n        deterministic: Whether the function your optimizing is deterministic, given\n            a seed and config.\n        seed: The seed to use for the optimizer.\n        fidelities: The fidelities to use, if any.\n        continue_from_last_run: Whether to continue from a previous run.\n        logging_level: The logging level to use.\n            This argument is passed forward to SMAC, use False to disable\n            SMAC's handling of logging.\n    \"\"\"\n    seed = as_int(seed)\n    match bucket:\n        case None:\n            bucket = PathBucket(\n                f\"{cls.__name__}-{datetime.now().isoformat()}\",\n            )\n        case str() | Path():\n            bucket = PathBucket(bucket)\n        case bucket:\n            bucket = bucket  # noqa: PLW0127\n\n    # NOTE SMAC always minimizes! Hence we make it a minimization problem\n    metric_names: str | list[str]\n    if isinstance(metrics, Sequence):\n        metric_names = [metric.name for metric in metrics]\n    else:\n        metric_names = metrics.name\n\n    if isinstance(space, Node):\n        space = space.search_space(parser=cls.preferred_parser())\n\n    facade_cls: type[AbstractFacade]\n    if fidelities:\n        if len(fidelities) == 1:\n            v = next(iter(fidelities.values()))\n            min_budget, max_budget = v\n        else:\n            min_budget, max_budget = 1.0, 100.0\n\n        scenario = Scenario(\n            objectives=metric_names,\n            configspace=space,\n            output_directory=bucket.path / \"smac3_output\",\n            seed=seed,\n            min_budget=min_budget,\n            max_budget=max_budget,\n            crash_cost=list(cls.crash_costs(metrics).values()),\n        )\n        facade_cls = MultiFidelityFacade\n    else:\n        scenario = Scenario(\n            configspace=space,\n            seed=seed,\n            output_directory=bucket.path / \"smac3_output\",\n            deterministic=deterministic,\n            objectives=metric_names,\n            crash_cost=list(cls.crash_costs(metrics).values()),\n        )\n        facade_cls = HyperparameterOptimizationFacade\n\n    facade = facade_cls(\n        scenario=scenario,\n        target_function=\"dummy\",  # NOTE: https://github.com/automl/SMAC3/issues/946\n        overwrite=not continue_from_last_run,\n        logging_level=logging_level,\n        multi_objective_algorithm=facade_cls.get_multi_objective_algorithm(\n            scenario=scenario,\n        ),\n    )\n    return cls(\n        facade=facade,\n        fidelities=fidelities,\n        bucket=bucket,\n        metrics=metrics,\n        time_profile=time_profile,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.preferred_parser","title":"preferred_parser  <code>classmethod</code>","text":"<pre><code>preferred_parser() -&gt; Literal['configspace']\n</code></pre> <p>The preferred parser for this optimizer.</p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@override\n@classmethod\ndef preferred_parser(cls) -&gt; Literal[\"configspace\"]:\n    \"\"\"The preferred parser for this optimizer.\"\"\"\n    return \"configspace\"\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.tell","title":"tell","text":"<pre><code>tell(report: Report[TrialInfo]) -&gt; None\n</code></pre> <p>Tell the optimizer the result of the sampled config.</p> PARAMETER DESCRIPTION <code>report</code> <p>The report of the trial.</p> <p> TYPE: <code>Report[TrialInfo]</code> </p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@override\ndef tell(self, report: Trial.Report[SMACTrialInfo]) -&gt; None:\n    \"\"\"Tell the optimizer the result of the sampled config.\n\n    Args:\n        report: The report of the trial.\n    \"\"\"\n    assert report.trial.info is not None\n\n    costs: dict[str, float] = {}\n    for name, metric in self.metrics.items():\n        value = report.values.get(metric.name)\n        if value is None:\n            if report.status == Trial.Status.SUCCESS:\n                raise ValueError(\n                    f\"Could not find metric '{metric.name}' in report values.\"\n                    \" Make sure you use `trial.success()` in your target function.\"\n                    \" So that we can report the metric value to SMAC.\",\n                )\n            value = metric.worst\n\n        costs[name] = metric.normalized_loss(value)\n\n    logger.debug(f\"Reporting for trial {report.trial.name} with costs: {costs}\")\n\n    cost = next(iter(costs.values())) if len(costs) == 1 else list(costs.values())\n\n    # If we're successful, get the cost and times and report them\n    params: dict[str, Any]\n    match report.status:\n        case Trial.Status.SUCCESS | Trial.Status.FAIL:\n            smac_status = (\n                StatusType.SUCCESS\n                if report.status == Trial.Status.SUCCESS\n                else StatusType.CRASHED\n            )\n            params = {\"cost\": cost, \"status\": smac_status}\n        case Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n            params = {\"cost\": cost, \"status\": StatusType.CRASHED}\n\n    if self.time_profile:\n        profile = report.trial.profiles.get(self.time_profile)\n        match profile:\n            # If it was a success, we kind of expect there to have been this\n            # timing. Otherwise, for failure we don't necessarily expect it.\n            case None if report.status in Trial.Status.SUCCESS:\n                raise ValueError(\n                    f\"Could not find profile '{self.time_profile}' in trial\"\n                    \" as specified by `time_profile` during construction.\"\n                    \" Make sure you use `with trial.profile(time_profile):`\"\n                    \" in your target function. So that we can report the\"\n                    \" timing information to SMAC.\",\n                )\n            case Profile.Interval(time=timer):\n                params.update(\n                    {\n                        \"time\": timer.duration,\n                        \"starttime\": timer.start,\n                        \"endtime\": timer.end,\n                    },\n                )\n            case None:\n                pass\n\n    match report.exception:\n        case None:\n            pass\n        case MemoryLimitException():\n            params[\"status\"] = StatusType.MEMORYOUT\n            params[\"additional_info\"] = {\n                \"exception\": str(report.exception),\n                \"traceback\": report.traceback,\n            }\n        case TimeoutException():\n            params[\"status\"] = StatusType.TIMEOUT\n            params[\"additional_info\"] = {\n                \"exception\": str(report.exception),\n                \"traceback\": report.traceback,\n            }\n        case _:\n            params[\"additional_info\"] = {\n                \"exception\": str(report.exception),\n                \"traceback\": report.traceback,\n            }\n\n    self.facade.tell(report.trial.info, value=SMACTrialValue(**params), save=True)\n</code></pre>"},{"location":"api/amltk/pipeline/components/","title":"Components","text":""},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components","title":"amltk.pipeline.components","text":"<p>The provided subclasses of a <code>Node</code> that can be used can be assembled into a pipeline.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice","title":"Choice  <code>dataclass</code>","text":"<pre><code>Choice(\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: (\n        Callable[[Config, Any], Config] | None\n    ) = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>Node[Item, Space]</code></p> <p>A <code>Choice</code> between different subcomponents.</p> <p>This indicates that a choice should be made between the different children in <code>.nodes</code>, usually done when you <code>configure()</code> with some <code>config</code> from a <code>search_space()</code>.</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = Choice(rf, mlp, name=\"estimator\")\n</code></pre> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>Order of nodes</p> <p>The given nodes of a choice are always ordered according to their name, so indexing <code>choice.nodes</code> may not be reliable if modifying the choice dynamically.</p> <p>Please use <code>choice[\"name\"]</code> to access the nodes instead.</p> See Also <ul> <li><code>Node</code></li> </ul> PARAMETER DESCRIPTION <code>nodes</code> <p>The nodes that should be chosen between for this node.</p> <p> TYPE: <code>Node | NodeLike</code> DEFAULT: <code>()</code> </p> <code>item</code> <p>The item attached to this node (if any).</p> <p> TYPE: <code>Item | Callable[[Item], Item] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the node. If not specified, the name will be randomly generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The configuration for this node.</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The search space for this node. This will be used when <code>search_space()</code> is called.</p> <p> TYPE: <code>Space | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities for this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>config_transform</code> <p>A function that transforms the <code>config=</code> parameter during <code>configure(config)</code> before return the new configured node. Useful for times where you need to combine multiple parameters into one.</p> <p> TYPE: <code>Callable[[Config, Any], Config] | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a choice node.\n\n    Args:\n        nodes: The nodes that should be chosen between for this node.\n        item: The item attached to this node (if any).\n        name: The name of the node. If not specified, the name will be\n            randomly generated.\n        config: The configuration for this node.\n        space: The search space for this node. This will be used when\n            [`search_space()`][amltk.pipeline.node.Node.search_space] is called.\n        fidelities: The fidelities for this node.\n        config_transform: A function that transforms the `config=` parameter\n            during [`configure(config)`][amltk.pipeline.node.Node.configure]\n            before return the new configured node. Useful for times where\n            you need to combine multiple parameters into one.\n        meta: Any meta information about this node.\n    \"\"\"\n    _nodes: tuple[Node, ...] = tuple(\n        sorted((as_node(n) for n in nodes), key=lambda n: n.name),\n    )\n\n    if name is None:\n        name = f\"Choice-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Config | None = field(hash=False)\n</code></pre> <p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: Callable[[Config, Any], Config] | None = (\n    field(hash=False)\n)\n</code></pre> <p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.item","title":"item  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>item: Callable[..., Item] | Item | None = field(hash=False)\n</code></pre> <p>The item attached to this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.nodes","title":"nodes  <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[Node, ...]\n</code></pre> <p>The choice of possible nodes that this choice could take.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: Space | None = field(hash=False)\n</code></pre> <p>The search space for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.chosen","title":"chosen","text":"<pre><code>chosen() -&gt; Node\n</code></pre> <p>The chosen branch.</p> RETURNS DESCRIPTION <code>Node</code> <p>The chosen branch</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def chosen(self) -&gt; Node:\n    \"\"\"The chosen branch.\n\n    Returns:\n        The chosen branch\n    \"\"\"\n    match self.config:\n        case {\"__choice__\": choice}:\n            chosen = first_true(\n                self.nodes,\n                pred=lambda node: node.name == choice,\n                default=None,\n            )\n            if chosen is None:\n                raise NodeNotFoundError(choice, self.name)\n\n            return chosen\n        case _:\n            raise NoChoiceMadeError(self.name)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> <p>Configuring a choice</p> <p>For a Choice, if the config has a <code>__choice__</code> key, then only the node chosen will be configured. The others will not be configured at all and their config will be discarded.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>@override\ndef configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    !!! note \"Configuring a choice\"\n\n        For a Choice, if the config has a `__choice__` key, then only the node\n        chosen will be configured. The others will not be configured at all and\n        their config will be discarded.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    # This part is what differs for a Choice\n    if len(self.nodes) &gt; 0:\n        choice_made = config.get(\"__choice__\", None)\n        if choice_made is not None:\n            matching_child = first_true(\n                self.nodes,\n                pred=lambda node: node.name == choice_made,\n                default=None,\n            )\n            if matching_child is None:\n                raise ValueError(\n                    f\"Can not find matching child for choice {self.name} with child\"\n                    f\" {choice_made}.\"\n                    \"\\nPlease check the config and ensure that the choice is one of\"\n                    f\" {[n.name for n in self.nodes]}.\"\n                    f\"\\nThe config recieved at this choice node was {config=}.\",\n                )\n\n            # We still iterate over all of them just to ensure correct ordering\n            nodes = tuple(\n                node.copy()\n                if node.name != choice_made\n                else matching_child.configure(\n                    config,\n                    prefixed_name=True,\n                    transform_context=transform_context,\n                    params=params,\n                )\n                for node in self.nodes\n            )\n            _kwargs[\"nodes\"] = nodes\n        else:\n            nodes = tuple(\n                node.configure(\n                    config,\n                    prefixed_name=True,\n                    transform_context=transform_context,\n                    params=params,\n                )\n                for node in self.nodes\n            )\n            _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component","title":"Component  <code>dataclass</code>","text":"<pre><code>Component(\n    item: Callable[..., Item],\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: (\n        Callable[[Config, Any], Config] | None\n    ) = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>Node[Item, Space]</code></p> <p>A <code>Component</code> of the pipeline with a possible item and no children.</p> <p>This is the basic building block of most pipelines, it accepts as it's <code>item=</code> some function that will be called with <code>build_item()</code> to build that one part of the pipeline.</p> <p>When <code>build_item()</code> is called, whatever the config of the component is at that time, will be used to construct the item.</p> <p>A common pattern is to use a <code>Component</code> to wrap a constructor, specifying the <code>space=</code> and <code>config=</code> to be used when building the item.</p> <pre><code>from amltk.pipeline import Component\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = Component(\n    RandomForestClassifier,\n    config={\"max_depth\": 3},\n    space={\"n_estimators\": (10, 100)}\n)\n\nconfig = {\"n_estimators\": 50}  # Sample from some space or something\nconfigured_rf = rf.configure(config)\n\nestimator = configured_rf.build_item()\n</code></pre> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class RandomForestClassifier(...) \u2502\n\u2502 config {'max_depth': 3}                  \u2502\n\u2502 space  {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriNot fitted<pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre> See Also <ul> <li><code>Node</code></li> </ul> PARAMETER DESCRIPTION <code>item</code> <p>The item attached to this node.</p> <p> TYPE: <code>Callable[..., Item]</code> </p> <code>name</code> <p>The name of the node. If not specified, the name will be generated from the item.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The configuration for this node.</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The search space for this node. This will be used when <code>search_space()</code> is called.</p> <p> TYPE: <code>Space | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities for this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>config_transform</code> <p>A function that transforms the <code>config=</code> parameter during <code>configure(config)</code> before return the new configured node. Useful for times where you need to combine multiple parameters into one.</p> <p> TYPE: <code>Callable[[Config, Any], Config] | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    item: Callable[..., Item],\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a component.\n\n    Args:\n        item: The item attached to this node.\n        name: The name of the node. If not specified, the name will be\n            generated from the item.\n        config: The configuration for this node.\n        space: The search space for this node. This will be used when\n            [`search_space()`][amltk.pipeline.node.Node.search_space] is called.\n        fidelities: The fidelities for this node.\n        config_transform: A function that transforms the `config=` parameter\n            during [`configure(config)`][amltk.pipeline.node.Node.configure]\n            before return the new configured node. Useful for times where\n            you need to combine multiple parameters into one.\n        meta: Any meta information about this node.\n    \"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Config | None = field(hash=False)\n</code></pre> <p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: Callable[[Config, Any], Config] | None = (\n    field(hash=False)\n)\n</code></pre> <p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.item","title":"item  <code>instance-attribute</code>","text":"<pre><code>item: Callable[..., Item]\n</code></pre> <p>A node which constructs an item in the pipeline.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.nodes","title":"nodes  <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[]\n</code></pre> <p>A component has no children.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: Space | None = field(hash=False)\n</code></pre> <p>The search space for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.build_item","title":"build_item","text":"<pre><code>build_item(**kwargs: Any) -&gt; Item\n</code></pre> <p>Build the item attached to this component.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>Any additional arguments to pass to the item</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Item</code> <p>Item     The built item</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def build_item(self, **kwargs: Any) -&gt; Item:\n    \"\"\"Build the item attached to this component.\n\n    Args:\n        **kwargs: Any additional arguments to pass to the item\n\n    Returns:\n        Item\n            The built item\n    \"\"\"\n    config = self.config or {}\n    try:\n        return self.item(**{**config, **kwargs})\n    except TypeError as e:\n        new_msg = f\"Failed to build `{self.item=}` with `{self.config=}`.\\n\"\n        if any(kwargs):\n            new_msg += f\"Extra {kwargs=} were also provided.\\n\"\n        new_msg += (\n            \"If the item failed to initialize, a common reason can be forgetting\"\n            \" to call `configure()` on the `Component` or the pipeline it is in or\"\n            \" not calling `build()`/`build_item()` on the **returned** value of\"\n            \" `configure()`.\\n\"\n            \"Reasons may also include not having fully specified the `config`\"\n            \" initially, it having not being configured fully from `configure()`\"\n            \" or from misspecfying parameters in the `space`.\"\n        )\n        raise ComponentBuildError(new_msg) from e\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed","title":"Fixed  <code>dataclass</code>","text":"<pre><code>Fixed(\n    item: Item,\n    *,\n    name: str | None = None,\n    config: None = None,\n    space: None = None,\n    fidelities: None = None,\n    config_transform: None = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>Node[Item, None]</code></p> <p>A <code>Fixed</code> part of the pipeline that represents something that can not be configured and used directly as is.</p> <p>It consists of an <code>.item</code> that is fixed, non-configurable and non-searchable. It also has no children.</p> <p>This is useful for representing parts of the pipeline that are fixed, for example if you have a pipeline that is a <code>Sequential</code> of nodes, but you want to fix the first component to be a <code>PCA</code> with <code>n_components=3</code>, you can use a <code>Fixed</code> to represent that.</p> <pre><code>from amltk.pipeline import Component, Fixed, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\npca = Fixed(PCA(n_components=3))\n\npipeline = Sequential(pca, rf, name=\"my_pipeline\")\n</code></pre> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> See Also <ul> <li><code>Node</code></li> </ul> PARAMETER DESCRIPTION <code>item</code> <p>The item attached to this node. Will be fixed and can not be configured.</p> <p> TYPE: <code>Item</code> </p> <code>name</code> <p>The name of the node. If not specified, the name will be generated from the item.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(  # noqa: D417\n    self,\n    item: Item,\n    *,\n    name: str | None = None,\n    config: None = None,\n    space: None = None,\n    fidelities: None = None,\n    config_transform: None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a fixed node.\n\n    Args:\n        item: The item attached to this node. Will be fixed and can not\n            be configured.\n        name: The name of the node. If not specified, the name will be\n            generated from the item.\n        meta: Any meta information about this node.\n    \"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: None = None\n</code></pre> <p>A fixed node has no config.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: None = None\n</code></pre> <p>A fixed node has no config so no transform.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: None = None\n</code></pre> <p>A fixed node has no search space.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.item","title":"item  <code>instance-attribute</code>","text":"<pre><code>item: Item\n</code></pre> <p>The fixed item that this node represents.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.nodes","title":"nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[] = ()\n</code></pre> <p>A fixed node has no children.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: None = None\n</code></pre> <p>A fixed node has no search space.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join","title":"Join  <code>dataclass</code>","text":"<pre><code>Join(\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: (\n        Callable[[Config, Any], Config] | None\n    ) = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>Node[Item, Space]</code></p> <p><code>Join</code> together different parts of the pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in tandem with one another, for example, concatenating the outputs of the various members of the <code>Join</code>.</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\njoin = Join(pca, kbest, name=\"my_feature_union\")\n</code></pre> <pre>\n<code>\u256d\u2500 Join(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> See Also <ul> <li><code>Node</code></li> </ul> PARAMETER DESCRIPTION <code>nodes</code> <p>The nodes that should be joined together in parallel.</p> <p> TYPE: <code>Node | NodeLike</code> DEFAULT: <code>()</code> </p> <code>item</code> <p>The item attached to this node (if any).</p> <p> TYPE: <code>Item | Callable[[Item], Item] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the node. If not specified, the name will be randomly generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The configuration for this node.</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The search space for this node. This will be used when <code>search_space()</code> is called.</p> <p> TYPE: <code>Space | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities for this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>config_transform</code> <p>A function that transforms the <code>config=</code> parameter during <code>configure(config)</code> before return the new configured node. Useful for times where you need to combine multiple parameters into one.</p> <p> TYPE: <code>Callable[[Config, Any], Config] | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a join node.\n\n    Args:\n        nodes: The nodes that should be joined together in parallel.\n        item: The item attached to this node (if any).\n        name: The name of the node. If not specified, the name will be\n            randomly generated.\n        config: The configuration for this node.\n        space: The search space for this node. This will be used when\n            [`search_space()`][amltk.pipeline.node.Node.search_space] is called.\n        fidelities: The fidelities for this node.\n        config_transform: A function that transforms the `config=` parameter\n            during [`configure(config)`][amltk.pipeline.node.Node.configure]\n            before return the new configured node. Useful for times where\n            you need to combine multiple parameters into one.\n        meta: Any meta information about this node.\n    \"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n\n    if name is None:\n        name = f\"Join-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Config | None = field(hash=False)\n</code></pre> <p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: Callable[[Config, Any], Config] | None = (\n    field(hash=False)\n)\n</code></pre> <p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.item","title":"item  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>item: Callable[..., Item] | Item | None = field(hash=False)\n</code></pre> <p>The item attached to this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.nodes","title":"nodes  <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[Node, ...]\n</code></pre> <p>The nodes that should be joined together in parallel.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: Space | None = field(hash=False)\n</code></pre> <p>The search space for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable","title":"Searchable  <code>dataclass</code>","text":"<pre><code>Searchable(\n    space: Space | None = None,\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: (\n        Callable[[Config, Any], Config] | None\n    ) = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>Node[None, Space]</code></p> <p>A <code>Searchable</code> node of the pipeline which just represents a search space, no item attached.</p> <p>While not usually applicable to pipelines you want to build, this node is useful for creating a search space, especially if the real pipeline you want to optimize can not be built directly. For example, if you are optimize a script, you may wish to use a <code>Searchable</code> to represent the search space of that script.</p> <pre><code>from amltk.pipeline import Searchable\n\nscript_space = Searchable({\"mode\": [\"orange\", \"blue\", \"red\"], \"n\": (10, 100)})\n</code></pre> <pre>\n<code>\u256d\u2500 Searchable(Searchable-rA20SWeU) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 space {'mode': ['orange', 'blue', 'red'], 'n': (10, 100)} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> See Also <ul> <li><code>Node</code></li> </ul> PARAMETER DESCRIPTION <code>space</code> <p>The search space for this node. This will be used when <code>search_space()</code> is called.</p> <p> TYPE: <code>Space | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the node. If not specified, a random one will be generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The configuration for this node. Useful for setting some default values.</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities for this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>config_transform</code> <p>A function that transforms the <code>config=</code> parameter during <code>configure(config)</code> before return the new configured node. Useful for times where you need to combine multiple parameters into one.</p> <p> TYPE: <code>Callable[[Config, Any], Config] | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    space: Space | None = None,\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a choice.\n\n    Args:\n        space: The search space for this node. This will be used when\n            [`search_space()`][amltk.pipeline.node.Node.search_space] is called.\n        name: The name of the node. If not specified, a random one will\n            be generated.\n        config: The configuration for this node. Useful for setting some\n            default values.\n        fidelities: The fidelities for this node.\n        config_transform: A function that transforms the `config=` parameter\n            during [`configure(config)`][amltk.pipeline.node.Node.configure]\n            before return the new configured node. Useful for times where\n            you need to combine multiple parameters into one.\n        meta: Any meta information about this node.\n    \"\"\"\n    if name is None:\n        name = f\"Searchable-{randuid(8)}\"\n\n    super().__init__(\n        name=name,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Config | None = field(hash=False)\n</code></pre> <p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: Callable[[Config, Any], Config] | None = (\n    field(hash=False)\n)\n</code></pre> <p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.item","title":"item  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>item: None = None\n</code></pre> <p>A searchable has no item.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.nodes","title":"nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[] = ()\n</code></pre> <p>A searchable has no children.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: Space | None = field(hash=False)\n</code></pre> <p>The search space for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential","title":"Sequential  <code>dataclass</code>","text":"<pre><code>Sequential(\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: (\n        Callable[[Config, Any], Config] | None\n    ) = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>Node[Item, Space]</code></p> <p>A <code>Sequential</code> set of operations in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act one after another, feeding the output of one into the next.</p> <pre><code>from amltk.pipeline import Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)}),\n    name=\"my_pipeline\"\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> See Also <ul> <li><code>Node</code></li> </ul> PARAMETER DESCRIPTION <code>nodes</code> <p>The nodes that this node leads to. In the case of a <code>Sequential</code>, the order here matters and it signifies that data should first be passed through the first node, then the second, etc.</p> <p> TYPE: <code>Node | NodeLike</code> DEFAULT: <code>()</code> </p> <code>item</code> <p>The item attached to this node (if any).</p> <p> TYPE: <code>Item | Callable[[Item], Item] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the node. If not specified, the name will be randomly generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The configuration for this node.</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The search space for this node. This will be used when <code>search_space()</code> is called.</p> <p> TYPE: <code>Space | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities for this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>config_transform</code> <p>A function that transforms the <code>config=</code> parameter during <code>configure(config)</code> before return the new configured node. Useful for times where you need to combine multiple parameters into one.</p> <p> TYPE: <code>Callable[[Config, Any], Config] | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a sequential node.\n\n    Args:\n        nodes: The nodes that this node leads to. In the case of a `Sequential`,\n            the order here matters and it signifies that data should first\n            be passed through the first node, then the second, etc.\n        item: The item attached to this node (if any).\n        name: The name of the node. If not specified, the name will be\n            randomly generated.\n        config: The configuration for this node.\n        space: The search space for this node. This will be used when\n            [`search_space()`][amltk.pipeline.node.Node.search_space] is called.\n        fidelities: The fidelities for this node.\n        config_transform: A function that transforms the `config=` parameter\n            during [`configure(config)`][amltk.pipeline.node.Node.configure]\n            before return the new configured node. Useful for times where\n            you need to combine multiple parameters into one.\n        meta: Any meta information about this node.\n    \"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n\n    if name is None:\n        name = f\"Seq-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Config | None = field(hash=False)\n</code></pre> <p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: Callable[[Config, Any], Config] | None = (\n    field(hash=False)\n)\n</code></pre> <p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.item","title":"item  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>item: Callable[..., Item] | Item | None = field(hash=False)\n</code></pre> <p>The item attached to this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.nodes","title":"nodes  <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[Node, ...]\n</code></pre> <p>The nodes ordered in series.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: Space | None = field(hash=False)\n</code></pre> <p>The search space for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.tail","title":"tail  <code>property</code>","text":"<pre><code>tail: Node\n</code></pre> <p>The last step in the pipeline.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get the number of nodes in the pipeline.</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of nodes in the pipeline.\"\"\"\n    return len(self.nodes)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>@override\ndef walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    path = [*path, self]\n    for node in self.nodes:\n        yield from node.walk(path=path)\n\n        # Append the previous node so that the next node in the sequence is\n        # lead to from the previous node\n        path = [*path, node]\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split","title":"Split  <code>dataclass</code>","text":"<pre><code>Split(\n    *nodes: Node | NodeLike | dict[str, Node | NodeLike],\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: (\n        Callable[[Config, Any], Config] | None\n    ) = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>Node[Item, Space]</code></p> <p>A <code>Split</code> of data in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in parallel but on different subsets of data.</p> <pre><code>from amltk.pipeline import Component, Split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(drop=\"first\"),\n]\nnumerical_pipeline = Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})\n\npreprocessor = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numerical\": numerical_pipeline,\n    },\n    config={\n        \"categories\": make_column_selector(dtype_include=\"category\"),\n        \"numerical\": make_column_selector(dtype_exclude=\"category\"),\n    },\n    name=\"my_split\"\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7efd77e95840&gt;,                                                      \u2502\n\u2502            'numerical':                                                      \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7efd77e94670&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502 space {                        \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           'strategy': [        \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502               'mean',          \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'median'         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502           ]                    \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502       }                        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> See Also <ul> <li><code>Node</code></li> </ul> PARAMETER DESCRIPTION <code>nodes</code> <p>The nodes that this node leads to. You may also provide a dictionary where the keys are the names of the nodes and the values are the nodes or list of nodes themselves.</p> <p> TYPE: <code>Node | NodeLike | dict[str, Node | NodeLike]</code> DEFAULT: <code>()</code> </p> <code>item</code> <p>The item attached to this node. The object created by <code>item</code> should be capable of figuring out how to deal with its child nodes.</p> <p> TYPE: <code>Item | Callable[[Item], Item] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the node. If not specified, the name will be generated from the item.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The configuration for this split.</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The search space for this node. This will be used when <code>search_space()</code> is called.</p> <p> TYPE: <code>Space | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities for this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>config_transform</code> <p>A function that transforms the <code>config=</code> parameter during <code>configure(config)</code> before return the new configured node. Useful for times where you need to combine multiple parameters into one.</p> <p> TYPE: <code>Callable[[Config, Any], Config] | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike | dict[str, Node | NodeLike],\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a split node.\n\n    Args:\n        nodes: The nodes that this node leads to. You may also provide\n            a dictionary where the keys are the names of the nodes and\n            the values are the nodes or list of nodes themselves.\n        item: The item attached to this node. The object created by `item`\n            should be capable of figuring out how to deal with its child nodes.\n        name: The name of the node. If not specified, the name will be\n            generated from the item.\n        config: The configuration for this split.\n        space: The search space for this node. This will be used when\n            [`search_space()`][amltk.pipeline.node.Node.search_space] is called.\n        fidelities: The fidelities for this node.\n        config_transform: A function that transforms the `config=` parameter\n            during [`configure(config)`][amltk.pipeline.node.Node.configure]\n            before return the new configured node. Useful for times where\n            you need to combine multiple parameters into one.\n        meta: Any meta information about this node.\n    \"\"\"\n    if any(isinstance(n, dict) for n in nodes):\n        if len(nodes) &gt; 1:\n            raise ValueError(\n                \"Can't handle multiple nodes with a dictionary as a node.\\n\"\n                f\"{nodes=}\",\n            )\n        _node = nodes[0]\n        assert isinstance(_node, dict)\n\n        def _construct(key: str, value: Node | NodeLike) -&gt; Node:\n            match value:\n                case list():\n                    return Sequential(*value, name=key)\n                case set() | tuple():\n                    return as_node(value, name=key)\n                case _:\n                    return Sequential(value, name=key)\n\n        _nodes = tuple(_construct(key, value) for key, value in _node.items())\n    else:\n        _nodes = tuple(as_node(n) for n in nodes)\n\n    if name is None:\n        name = f\"Split-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Config | None = field(hash=False)\n</code></pre> <p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: Callable[[Config, Any], Config] | None = (\n    field(hash=False)\n)\n</code></pre> <p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.item","title":"item  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>item: Callable[..., Item] | Item | None = field(hash=False)\n</code></pre> <p>The item attached to this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.nodes","title":"nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[Node, ...] = field(hash=True)\n</code></pre> <p>The nodes that this node leads to.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: Space | None = field(hash=False)\n</code></pre> <p>The search space for this node</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/components/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.as_node","title":"as_node","text":"<pre><code>as_node(\n    thing: Node | NodeLike[Item], name: str | None = None\n) -&gt; Node | Choice | Join | Sequential | Fixed[Item]\n</code></pre> <p>Convert a node, pipeline, set or tuple into a component, copying anything in the process and removing all linking to other nodes.</p> PARAMETER DESCRIPTION <code>thing</code> <p>The thing to convert</p> <p> TYPE: <code>Node | NodeLike[Item]</code> </p> <code>name</code> <p>The name of the node. If it already a node, it will be renamed to that one.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | Choice | Join | Sequential | Fixed[Item]</code> <p>The component</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def as_node(  # noqa: PLR0911\n    thing: Node | NodeLike[Item],\n    name: str | None = None,\n) -&gt; Node | Choice | Join | Sequential | Fixed[Item]:\n    \"\"\"Convert a node, pipeline, set or tuple into a component, copying anything\n    in the process and removing all linking to other nodes.\n\n    Args:\n        thing: The thing to convert\n        name: The name of the node. If it already a node, it will be renamed to that\n            one.\n\n    Returns:\n        The component\n    \"\"\"\n    match thing:\n        case set():\n            return Choice(*thing, name=name)\n        case tuple():\n            return Join(*thing, name=name)\n        case list():\n            return Sequential(*thing, name=name)\n        case Node():\n            name = thing.name if name is None else name\n            return thing.mutate(name=name)\n        case type():\n            return Component(thing, name=name)\n        case thing if (inspect.isfunction(thing) or inspect.ismethod(thing)):\n            return Component(thing, name=name)\n        case _:\n            return Fixed(thing, name=name)\n</code></pre>"},{"location":"api/amltk/pipeline/node/","title":"Node","text":""},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node","title":"amltk.pipeline.node","text":"<p>A pipeline consists of <code>Node</code>s, which hold the various attributes required to build a pipeline, such as the <code>.item</code>, its <code>.space</code>, its <code>.config</code> and so on.</p> <p>The <code>Node</code>s are connected to each in a parent-child relation ship where the children are simply the <code>.nodes</code> that the parent leads to.</p> <p>To give these attributes and relations meaning, there are various subclasses of <code>Node</code> which give different syntactic meanings when you want to construct something like a <code>search_space()</code> or <code>build()</code> some concrete object out of the pipeline.</p> <p>For example, a <code>Sequential</code> node gives the meaning that each of its children in <code>.nodes</code> should follow one another while something like a <code>Choice</code> gives the meaning that only one of its children should be chosen.</p> <p>You will likely never have to create a <code>Node</code> directly, but instead use the various components to create the pipeline.</p> Hashing <p>When hashing a node, i.e. to put it in a <code>set</code> or as a key in a <code>dict</code>, only the name of the node and the hash of its children is used. This means that two nodes with the same connectivity will be equalling hashed,</p> Equality <p>When considering equality, this will be done by comparing all the fields of the node. This include even the <code>parent</code> and <code>branches</code> fields. This means two nodes are considered equal if they look the same and they are connected in to nodes that also look the same.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node","title":"Node  <code>dataclass</code>","text":"<pre><code>Node(\n    *nodes: Node,\n    name: str,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: (\n        Callable[[Config, Any], Config] | None\n    ) = None,\n    meta: Mapping[str, Any] | None = None\n)\n</code></pre> <p>               Bases: <code>RichRenderable</code>, <code>Generic[Item, Space]</code></p> <p>The core node class for the pipeline.</p> <p>These are simple objects that are named and linked together to form a chain. They are then wrapped in a <code>Pipeline</code> object to provide a convenient interface for interacting with the chain.</p> PARAMETER DESCRIPTION <code>nodes</code> <p>The nodes that this node leads to</p> <p> TYPE: <code>Node</code> DEFAULT: <code>()</code> </p> <code>name</code> <p>The name of the node</p> <p> TYPE: <code>str</code> </p> <code>item</code> <p>The item attached to this node</p> <p> TYPE: <code>Item | Callable[[Item], Item] | None</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The configuration for this node</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The search space for this node</p> <p> TYPE: <code>Space | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities for this node</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>config_transform</code> <p>A function that transforms the configuration of this node</p> <p> TYPE: <code>Callable[[Config, Any], Config] | None</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Any meta information about this node</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node,\n    name: str,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a choice.\n\n    Args:\n        nodes: The nodes that this node leads to\n        name: The name of the node\n        item: The item attached to this node\n        config: The configuration for this node\n        space: The search space for this node\n        fidelities: The fidelities for this node\n        config_transform: A function that transforms the configuration of this node\n        meta: Any meta information about this node\n    \"\"\"\n    super().__init__()\n    object.__setattr__(self, \"name\", name)\n    object.__setattr__(self, \"item\", item)\n    object.__setattr__(self, \"config\", config)\n    object.__setattr__(self, \"space\", space)\n    object.__setattr__(self, \"fidelities\", fidelities)\n    object.__setattr__(self, \"config_transform\", config_transform)\n    object.__setattr__(self, \"meta\", meta)\n    object.__setattr__(self, \"nodes\", nodes)\n\n    if not all_unique(node.name for node in self.nodes):\n        raise DuplicateNamesError(\n            f\"Duplicate node names in {self}. \" \"All nodes must have unique names.\",\n        )\n\n    for child in self.nodes:\n        if child.name == self.name:\n            raise DuplicateNamesError(\n                f\"Cannot have a child node with the same name as its parent. \"\n                f\"{self.name=} {child.name=}\",\n            )\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.RICH_OPTIONS","title":"RICH_OPTIONS  <code>class-attribute</code>","text":"<pre><code>RICH_OPTIONS: RichOptions = RichOptions(\n    panel_color=\"default\", node_orientation=\"horizontal\"\n)\n</code></pre> <p>How to display this node in rich.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.config","title":"config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config: Config | None = field(hash=False)\n</code></pre> <p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.config_transform","title":"config_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_transform: Callable[[Config, Any], Config] | None = (\n    field(hash=False)\n)\n</code></pre> <p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.fidelities","title":"fidelities  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fidelities: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.item","title":"item  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>item: Callable[..., Item] | Item | None = field(hash=False)\n</code></pre> <p>The item attached to this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.meta","title":"meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta: Mapping[str, Any] | None = field(hash=False)\n</code></pre> <p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = field(hash=True)\n</code></pre> <p>Name of the node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.nodes","title":"nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>nodes: tuple[Node, ...] = field(hash=True)\n</code></pre> <p>The nodes that this node leads to.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.space","title":"space  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>space: Space | None = field(hash=False)\n</code></pre> <p>The search space for this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Node\n</code></pre> <p>Get the first from <code>.nodes</code> with <code>key</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the first from [`.nodes`][amltk.pipeline.node.Node.nodes] with `key`.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name `{key}` in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.build","title":"build","text":"<pre><code>build(\n    builder: (\n        Callable[Concatenate[Node, P], BuilderOutput]\n        | Literal[\"sklearn\"]\n    ),\n    *builder_args: args,\n    **builder_kwargs: kwargs\n) -&gt; BuilderOutput | Pipeline\n</code></pre> <p>Build a concrete object out of this node.</p> PARAMETER DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.configure","title":"configure","text":"<pre><code>configure(\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None\n) -&gt; Self\n</code></pre> <p>Configure this node and anything following it with the given config.</p> PARAMETER DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.copy","title":"copy","text":"<pre><code>copy() -&gt; Self\n</code></pre> <p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.display","title":"display","text":"<pre><code>display(*, full: bool = False) -&gt; RenderableType\n</code></pre> <p>Display this node.</p> PARAMETER DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.factorize","title":"factorize","text":"<pre><code>factorize(\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None\n) -&gt; Iterator[Self]\n</code></pre> <p>Please see <code>factorize()</code>.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def factorize(\n    self,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[Node, Node], Node] | None = None,\n) -&gt; Iterator[Self]:\n    \"\"\"Please see [`factorize()`][amltk.pipeline.ops.factorize].\"\"\"  # noqa: D402\n    from amltk.pipeline.ops import factorize\n\n    yield from factorize(\n        self,\n        min_depth=min_depth,\n        max_depth=max_depth,\n        current_depth=current_depth,\n        factor_by=factor_by,\n        assign_child=assign_child,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.fidelity_space","title":"fidelity_space","text":"<pre><code>fidelity_space() -&gt; dict[str, Any]\n</code></pre> <p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.find","title":"find","text":"<pre><code>find(\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None\n</code></pre> <p>Find a node in that's nested deeper from this node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.iter","title":"iter","text":"<pre><code>iter(*, skip_unchosen: bool = False) -&gt; Iterator[Node]\n</code></pre> <p>Recursively iterate through the nodes starting from this node.</p> <p>This method traverses the nodes in a depth-first manner, including the current node and its children nodes.</p> PARAMETER DESCRIPTION <code>skip_unchosen</code> <p>Flag to skip unchosen nodes in Choice nodes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Node</code> <p>Iterator[Node]: Nodes connected to this node.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self, *, skip_unchosen: bool = False) -&gt; Iterator[Node]:\n    \"\"\"Recursively iterate through the nodes starting from this node.\n\n    This method traverses the nodes in a depth-first manner, including\n    the current node and its children nodes.\n\n    Args:\n        skip_unchosen (bool): Flag to skip unchosen nodes in Choice nodes.\n\n    Yields:\n        Iterator[Node]: Nodes connected to this node.\n    \"\"\"\n    # Import Choice node to avoid circular imports\n    from amltk.pipeline.components import Choice\n\n    # Yield the current node\n    yield self\n\n    # Iterate through the child nodes\n    for node in self.nodes:\n        if skip_unchosen and isinstance(node, Choice):\n            # If the node is a Choice and skipping unchosen nodes is enabled\n            chosen_node = node.chosen()\n            if chosen_node is None:\n                raise RuntimeError(\n                    f\"No Node chosen in Choice node {node.name}. \"\n                    f\"Did you call configure?\",\n                )\n            yield from chosen_node.iter(skip_unchosen=skip_unchosen)\n        else:\n            # Recursively iterate through the child nodes\n            yield from node.iter(skip_unchosen=skip_unchosen)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.linearized_fidelity","title":"linearized_fidelity","text":"<pre><code>linearized_fidelity(\n    value: float,\n) -&gt; dict[str, int | float | Any]\n</code></pre> <p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.mutate","title":"mutate","text":"<pre><code>mutate(**kwargs: Any) -&gt; Self\n</code></pre> <p>Mutate the node with the given keyword arguments.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self     The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.optimize","title":"optimize","text":"<pre><code>optimize(\n    target: (\n        Callable[[Trial, Node], Report]\n        | Task[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\"\n) -&gt; History\n</code></pre> <p>Optimize a pipeline on a given target function or evaluation protocol.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Callable[[Trial, Node], Report] | Task[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code></p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>timeout</code> <p>How long to run the scheduler for. This parameter only takes effect if <code>setup_only=False</code> which is the default. Otherwise, it will be ignored.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>display</code> <p>Whether to display the scheduler during running. By default it is <code>\"auto\"</code> which means to enable the display if running in a juptyer notebook or colab. Otherwise, it will be <code>False</code>.</p> <p>This may work poorly if including print statements or logging.</p> <p> TYPE: <code>bool | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> <code>wait</code> <p>Whether to wait for the scheduler to finish all pending jobs if was stopped for any reason, e.g. a <code>timeout=</code> or <code>scheduler.stop()</code> was called.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_scheduler_exception</code> <p>What to do if an exception occured, either in the submitted task, the callback, or any other unknown source during the loop.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default behavior and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def optimize(\n    self,\n    target: (\n        Callable[[Trial, Node], Trial.Report] | Task[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    timeout: float | None = None,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n    # `scheduler.run()` arguments\n    display: bool | Literal[\"auto\"] = \"auto\",\n    wait: bool = True,\n    on_scheduler_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n) -&gt; History:\n    \"\"\"Optimize a pipeline on a given target function or evaluation protocol.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature]\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        timeout:\n            How long to run the scheduler for. This parameter only takes\n            effect if `setup_only=False` which is the default. Otherwise,\n            it will be ignored.\n        display:\n            Whether to display the scheduler during running. By default\n            it is `\"auto\"` which means to enable the display if running\n            in a juptyer notebook or colab. Otherwise, it will be\n            `False`.\n\n            This may work poorly if including print statements or logging.\n        wait:\n            Whether to wait for the scheduler to finish all pending jobs\n            if was stopped for any reason, e.g. a `timeout=` or\n            [`scheduler.stop()`][amltk.scheduling.Scheduler.stop] was called.\n        on_scheduler_exception:\n            What to do if an exception occured, either in the submitted task,\n            the callback, or any other unknown source during the loop.\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            behavior and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n    \"\"\"  # noqa: E501\n    if timeout is None and max_trials is None:\n        raise ValueError(\n            \"You must one or both of `timeout` or `max_trials` to\"\n            \" limit the optimization process.\",\n        )\n\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    scheduler, _, _ = self.register_optimization_loop(\n        target=target,\n        metric=metric,\n        optimizer=optimizer,\n        seed=seed,\n        max_trials=max_trials,\n        n_workers=n_workers,\n        working_dir=working_dir,\n        scheduler=scheduler,\n        history=history,\n        on_begin=on_begin,\n        on_trial_exception=on_trial_exception,\n        plugins=plugins,\n        process_memory_limit=process_memory_limit,\n        process_walltime_limit=process_walltime_limit,\n        process_cputime_limit=process_cputime_limit,\n        threadpool_limit_ctl=threadpool_limit_ctl,\n    )\n    scheduler.run(\n        wait=wait,\n        timeout=timeout,\n        on_exception=on_scheduler_exception,\n        display=display,\n    )\n    return history\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/node/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.path_to","title":"path_to","text":"<pre><code>path_to(\n    key: str | Node | Callable[[Node], bool]\n) -&gt; list[Node] | None\n</code></pre> <p>Find a path to the given node.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.register_optimization_loop","title":"register_optimization_loop","text":"<pre><code>register_optimization_loop(\n    target: (\n        Task[[Trial, Node], Report]\n        | Callable[[Trial, Node], Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: (\n        int | tuple[int, str] | None\n    ) = None,\n    process_walltime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    process_cputime_limit: (\n        int | tuple[float, str] | None\n    ) = None,\n    threadpool_limit_ctl: bool | int | None = None\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Report], History]\n</code></pre> <p>Setup a pipeline to be optimized in a loop.</p> PARAMETER DESCRIPTION <code>target</code> <p>The function against which to optimize.</p> <ul> <li> <p>If <code>target</code> is a function, then it must take in a <code>Trial</code> as the first argument and a <code>Node</code> second argument, returning a <code>Trial.Report</code>. Please refer to the optimization guide for more.</p> </li> <li> <p>If <code>target</code> is a <code>Task</code>, then this will be used instead, updating the plugins with any additional plugins specified.</p> </li> </ul> <p> TYPE: <code>Task[[Trial, Node], Report] | Callable[[Trial, Node], Report]</code> </p> <code>metric</code> <p>The metric(s) that will be passed to <code>optimizer=</code>. These metrics should align with what is being computed in <code>target=</code>.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>optimizer</code> <p>The optimizer to use. If <code>None</code>, then AMLTK will go through a list of known optimizers and use the first one it can find which was installed.</p> <p>Alternatively, this can be a class inheriting from <code>Optimizer</code> or else a signature match <code>Optimizer.CreateSignature</code>.</p> <code>Optimizer.CreateSignature</code> <p>Lastly, you can also pass in your own already instantiated optimizer if you prefer, however you should make sure to set it up correctly with the given metrics and search space. It is recommened to just pass in the class if you are unsure how to do this properly.</p> <p> TYPE: <code>type[Optimizer] | CreateSignature | Optimizer | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>A <code>seed</code> for the optimizer to use.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>n_workers</code> <p>The numer of workers to use to evaluate this pipeline. If no <code>scheduler=</code> is provided, then one will be created for you as <code>Scheduler.with_processes(n_workers)</code>. If you provide your own <code>scheduler=</code> then this will limit the maximum amount of concurrent trials for this pipeline that will be evaluating at once.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>working_dir</code> <p>A working directory to use for the optimizer and the trials. Any items you store in trials will be located in this directory, where the <code>trial.name</code> will be used as a subfolder where any contents stored with <code>trial.store()</code> will be put there. Please see the optimization guide for more on trial storage.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>scheduler</code> <p>The specific <code>Scheduler</code> to use. If <code>None</code>, then one will be created for you with <code>Scheduler.with_processes(n_workers)</code></p> <p> TYPE: <code>Scheduler | None</code> DEFAULT: <code>None</code> </p> <code>history</code> <p>A <code>History</code> to store the <code>Trial.Report</code>s in. You may pass in your own if you wish for this method to store it there instead of creating its own.</p> <p> TYPE: <code>History | None</code> DEFAULT: <code>None</code> </p> <code>on_begin</code> <p>A callback that will be called before the scheduler is run. This can be used to hook into the life-cycle of the optimization and perform custom routines. Please see the scheduling guide for more.</p> on_begin signature <p> TYPE: <code>OnBeginCallbackSignature | None</code> DEFAULT: <code>None</code> </p> <code>on_trial_exception</code> <p>What to do when a trial returns a fail report from <code>trial.fail()</code> or <code>trial.crashed()</code> that contains an exception.</p> <p>Please see the optimization guide for more. In all cases, the exception will be attached to the <code>Trial.Report</code> object under <code>report.exception</code>.</p> <ul> <li>If <code>\"raise\"</code>, then the exception will be raised immediatly and the optimization process will halt. The default and good for initial development.</li> <li>If <code>\"end\"</code>, then the exception will be caught and the optimization process will end gracefully.</li> <li>If <code>\"continue\"</code>, the exception will be ignored and the optimization procedure will continue.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>max_trials</code> <p>The maximum number of trials to run. If <code>None</code>, then the optimization will continue for as long as the scheduler is running. You'll likely want to configure this.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>process_memory_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the memory the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_walltime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the wall time the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>process_cputime_limit</code> <p>If specified, the <code>Task</code> will use the <code>PynisherPlugin</code> to limit the cputime the process can use. Please refer to the plugins <code>pynisher</code> reference for more as there are platform limitations and additional dependancies required.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>threadpool_limit_ctl</code> <p>If specified, the <code>Task</code> will use the <code>ThreadPoolCTLPlugin</code> to limit the number of threads used by compliant libraries. Notably, this includes scikit-learn, for which running multiple in parallel can be problematic if not adjusted accordingly.</p> <p>The default behavior (when <code>None</code>) is to auto-detect whether this is applicable. This is done by checking if <code>sklearn</code> is installed and if the first node in the pipeline has a <code>BaseEstimator</code> item. Please set this to <code>True</code>/<code>False</code> depending on your preference.</p> <p> TYPE: <code>bool | int | None</code> DEFAULT: <code>None</code> </p> <code>plugins</code> <p>Additional plugins to attach to the eventual <code>Task</code> that will be executed by the <code>Scheduler</code>. Please refer to the plugins reference for more.</p> <p> TYPE: <code>Plugin | Iterable[Plugin] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Scheduler, Task[[Trial, Node], Report], History]</code> <p>A tuple of the <code>Scheduler</code>, the <code>Task</code> and the <code>History</code> that reports will be put into.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def register_optimization_loop(  # noqa: C901, PLR0915, PLR0912\n    self,\n    target: (\n        Task[[Trial, Node], Trial.Report] | Callable[[Trial, Node], Trial.Report]\n    ),\n    metric: Metric | Sequence[Metric],\n    *,\n    optimizer: (\n        type[Optimizer] | Optimizer.CreateSignature | Optimizer | None\n    ) = None,\n    seed: Seed | None = None,\n    max_trials: int | None = None,\n    n_workers: int = 1,\n    working_dir: str | Path | PathBucket | None = None,\n    scheduler: Scheduler | None = None,\n    history: History | None = None,\n    on_begin: OnBeginCallbackSignature | None = None,\n    on_trial_exception: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    # Plugin creating arguments\n    plugins: Plugin | Iterable[Plugin] | None = None,\n    process_memory_limit: int | tuple[int, str] | None = None,\n    process_walltime_limit: int | tuple[float, str] | None = None,\n    process_cputime_limit: int | tuple[float, str] | None = None,\n    threadpool_limit_ctl: bool | int | None = None,\n) -&gt; tuple[Scheduler, Task[[Trial, Node], Trial.Report], History]:\n    \"\"\"Setup a pipeline to be optimized in a loop.\n\n    Args:\n        target:\n            The function against which to optimize.\n\n            * If `target` is a function, then it must take in a\n            [`Trial`][amltk.optimization.trial.Trial] as the first argument\n            and a [`Node`][amltk.pipeline.node.Node] second argument, returning a\n            [`Trial.Report`][amltk.optimization.trial.Trial.Report]. Please refer to\n            the [optimization guide](../../../guides/optimization.md) for more.\n\n            * If `target` is a [`Task`][amltk.scheduling.task.Task], then\n            this will be used instead, updating the plugins with any additional\n            plugins specified.\n        metric:\n            The metric(s) that will be passed to `optimizer=`. These metrics\n            should align with what is being computed in `target=`.\n        optimizer:\n            The optimizer to use. If `None`, then AMLTK will go through a list\n            of known optimizers and use the first one it can find which was installed.\n\n            Alternatively, this can be a class inheriting from\n            [`Optimizer`][amltk.optimization.optimizer.Optimizer] or else\n            a signature match [`Optimizer.CreateSignature`][amltk.optimization.Optimizer.CreateSignature].\n\n            ??? tip \"`Optimizer.CreateSignature`\"\n\n                ::: amltk.optimization.Optimizer.CreateSignature\n\n            Lastly, you can also pass in your own already instantiated optimizer if you prefer, however\n            you should make sure to set it up correctly with the given metrics and search space.\n            It is recommened to just pass in the class if you are unsure how to do this properly.\n        seed:\n            A [`seed`][amltk.types.Seed] for the optimizer to use.\n        n_workers:\n            The numer of workers to use to evaluate this pipeline.\n            If no `scheduler=` is provided, then one will be created for\n            you as [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes].\n            If you provide your own `scheduler=` then this will limit the maximum\n            amount of concurrent trials for this pipeline that will be evaluating\n            at once.\n        working_dir:\n            A working directory to use for the optimizer and the trials.\n            Any items you store in trials will be located in this directory,\n            where the [`trial.name`][amltk.optimization.Trial.name] will be\n            used as a subfolder where any contents stored with\n            [`trial.store()`][amltk.optimization.trial.Trial.store] will be put there.\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more on trial storage.\n        scheduler:\n            The specific [`Scheduler`][amltk.scheduling.Scheduler] to use.\n            If `None`, then one will be created for you with\n            [`Scheduler.with_processes(n_workers)`][amltk.scheduling.Scheduler.with_processes]\n        history:\n            A [`History`][amltk.optimization.history.History] to store the\n            [`Trial.Report`][amltk.optimization.Trial.Report]s in. You\n            may pass in your own if you wish for this method to store\n            it there instead of creating its own.\n        on_begin:\n            A callback that will be called before the scheduler is run. This\n            can be used to hook into the life-cycle of the optimization and\n            perform custom routines. Please see the\n            [scheduling guide](../../../guides/scheduling.md) for more.\n\n            ??? tip \"on_begin signature\"\n\n                ::: amltk.pipeline.node.OnBeginCallbackSignature\n\n        on_trial_exception:\n            What to do when a trial returns a fail report from\n            [`trial.fail()`][amltk.optimization.trial.Trial.fail] or\n            [`trial.crashed()`][amltk.optimization.trial.Trial.crashed]\n            that contains an exception.\n\n            Please see the [optimization guide](../../../guides/optimization.md)\n            for more. In all cases, the exception will be attached to the\n            [`Trial.Report`][amltk.optimization.Trial.Report] object under\n            [`report.exception`][amltk.optimization.Trial.Report.exception].\n\n            * If `#!python \"raise\"`, then the exception will be raised\n            immediatly and the optimization process will halt. The default\n            and good for initial development.\n            * If `#!python \"end\"`, then the exception will be caught and\n            the optimization process will end gracefully.\n            * If `#!python \"continue\"`, the exception will be ignored and\n            the optimization procedure will continue.\n\n        max_trials:\n            The maximum number of trials to run. If `None`, then the\n            optimization will continue for as long as the scheduler is\n            running. You'll likely want to configure this.\n        process_memory_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the memory the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_walltime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the wall time the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        process_cputime_limit:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`PynisherPlugin`][amltk.scheduling.plugins.pynisher.PynisherPlugin]\n            to limit the cputime the process can use. Please\n            refer to the\n            [plugins `pynisher` reference](../../../reference/scheduling/plugins.md#pynisher)\n            for more as there are platform limitations and additional\n            dependancies required.\n        threadpool_limit_ctl:\n            If specified, the [`Task`][amltk.scheduling.task.Task] will\n            use the\n            [`ThreadPoolCTLPlugin`][amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin]\n            to limit the number of threads used by compliant libraries.\n            **Notably**, this includes scikit-learn, for which running multiple\n            in parallel can be problematic if not adjusted accordingly.\n\n            The default behavior (when `None`) is to auto-detect whether this\n            is applicable. This is done by checking if `sklearn` is installed\n            and if the first node in the pipeline has a `BaseEstimator` item.\n            Please set this to `True`/`False` depending on your preference.\n        plugins:\n            Additional plugins to attach to the eventual\n            [`Task`][amltk.scheduling.task.Task] that will be executed by\n            the [`Scheduler`][amltk.scheduling.Scheduler]. Please\n            refer to the\n            [plugins reference](../../../reference/scheduling/plugins.md) for more.\n\n    Returns:\n        A tuple of the [`Scheduler`][amltk.scheduling.Scheduler], the\n        [`Task`][amltk.scheduling.task.Task] and the\n        [`History`][amltk.optimization.history.History] that reports will be put into.\n    \"\"\"  # noqa: E501\n    match history:\n        case None:\n            history = History()\n        case History():\n            pass\n        case _:\n            raise ValueError(f\"Invalid history {history}. Must be a History\")\n\n    _plugins: tuple[Plugin, ...]\n    match plugins:\n        case None:\n            _plugins = ()\n        case Plugin():\n            _plugins = (plugins,)\n        case Iterable():\n            _plugins = tuple(plugins)\n        case _:\n            raise ValueError(\n                f\"Invalid plugins {plugins}. Must be a Plugin or an Iterable of\"\n                \" Plugins\",\n            )\n\n    if any(\n        limit is not None\n        for limit in (\n            process_memory_limit,\n            process_walltime_limit,\n            process_cputime_limit,\n        )\n    ):\n        try:\n            from amltk.scheduling.plugins.pynisher import PynisherPlugin\n        except ImportError as e:\n            raise ImportError(\n                \"You must install `pynisher` to use `trial_*_limit`\"\n                \" You can do so with `pip install amltk[pynisher]`\"\n                \" or `pip install pynisher` directly\",\n            ) from e\n        # TODO: I'm hesitant to add even more arguments to the `optimize`\n        # signature, specifically for `mp_context`.\n        plugin = PynisherPlugin(\n            memory_limit=process_memory_limit,\n            walltime_limit=process_walltime_limit,\n            cputime_limit=process_cputime_limit,\n        )\n        _plugins = (*_plugins, plugin)\n\n    # If threadpool_limit_ctl None, we should default to inspecting if it's\n    # an sklearn pipeline. This is because sklearn pipelines\n    # run in parallel will over-subscribe the CPU and cause\n    # the system to slow down.\n    # We use a heuristic to check this by checking if the item at the head\n    # of this node is a subclass of sklearn.base.BaseEstimator\n    match threadpool_limit_ctl:\n        case None:\n            from amltk._util import threadpoolctl_heuristic\n\n            threadpool_limit_ctl = False\n            if threadpoolctl_heuristic(self.item):\n                threadpool_limit_ctl = 1\n                warnings.warn(\n                    \"Detected an sklearn pipeline. Setting `threadpool_limit_ctl`\"\n                    \" to True. This will limit the number of threads spawned by\"\n                    \" sklearn to the number of cores on the machine. This is\"\n                    \" because sklearn pipelines run in parallel will over-subscribe\"\n                    \" the CPU and cause the system to slow down.\"\n                    \"\\nPlease set `threadpool_limit_ctl=False` if you do not want\"\n                    \" this behaviour and set it to `True` to silence this warning.\",\n                    AutomaticThreadPoolCTLWarning,\n                    stacklevel=2,\n                )\n        case True:\n            threadpool_limit_ctl = 1\n        case False:\n            pass\n        case int():\n            pass\n        case _:\n            raise ValueError(\n                f\"Invalid threadpool_limit_ctl {threadpool_limit_ctl}.\"\n                \" Must be a bool or an int\",\n            )\n\n    if threadpool_limit_ctl is not False:\n        from amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\n        _plugins = (*_plugins, ThreadPoolCTLPlugin(threadpool_limit_ctl))\n\n    match max_trials:\n        case None:\n            pass\n        case int() if max_trials &gt; 0:\n            from amltk.scheduling.plugins import Limiter\n\n            _plugins = (*_plugins, Limiter(max_calls=max_trials))\n        case _:\n            raise ValueError(f\"{max_trials=} must be a positive int\")\n\n    from amltk.scheduling.scheduler import Scheduler\n\n    match scheduler:\n        case None:\n            scheduler = Scheduler.with_processes(n_workers)\n        case Scheduler():\n            pass\n        case _:\n            raise ValueError(f\"Invalid scheduler {scheduler}. Must be a Scheduler\")\n\n    match target:\n        case Task():  # type: ignore # NOTE not sure why pyright complains here\n            for _p in _plugins:\n                target.attach_plugin(_p)\n            task = target\n        case _ if callable(target):\n            task = scheduler.task(target, plugins=_plugins)\n        case _:\n            raise ValueError(f\"Invalid {target=}. Must be a function or Task.\")\n\n    if isinstance(optimizer, Optimizer):\n        _optimizer = optimizer\n    else:\n        # NOTE: I'm not particularly fond of this hack but I assume most people\n        # when prototyping don't care for the actual underlying optimizer and\n        # so we should just *pick one*.\n        create_optimizer: Optimizer.CreateSignature\n        match optimizer:\n            case None:\n                first_opt_class = next(\n                    Optimizer._get_known_importable_optimizer_classes(),\n                    None,\n                )\n                if first_opt_class is None:\n                    raise ValueError(\n                        \"No optimizer was given and no known importable optimizers \"\n                        \" were found. Please consider giving one explicitly or\"\n                        \" installing one of the following packages:\\n\"\n                        \"\\n - optuna\"\n                        \"\\n - smac\"\n                        \"\\n - neural-pipeline-search\",\n                    )\n\n                create_optimizer = first_opt_class.create\n                opt_name = classname(first_opt_class)\n            case type():\n                if not issubclass(optimizer, Optimizer):\n                    raise ValueError(\n                        f\"Invalid optimizer {optimizer}. Must be a subclass of\"\n                        \" Optimizer or a function that returns an Optimizer\",\n                    )\n                create_optimizer = optimizer.create\n                opt_name = classname(optimizer)\n            case _:\n                assert not isinstance(optimizer, type)\n                create_optimizer = optimizer\n                opt_name = funcname(optimizer)\n\n        match working_dir:\n            case None:\n                now = datetime.utcnow().isoformat()\n\n                working_dir = PathBucket(f\"{opt_name}-{self.name}-{now}\")\n            case str() | Path():\n                working_dir = PathBucket(working_dir)\n            case PathBucket():\n                pass\n            case _:\n                raise ValueError(\n                    f\"Invalid working_dir {working_dir}.\"\n                    \" Must be a str, Path or PathBucket\",\n                )\n\n        _optimizer = create_optimizer(\n            space=self,\n            metrics=metric,\n            bucket=working_dir,\n            seed=seed,\n        )\n        assert _optimizer is not None\n\n    if on_begin is not None:\n        hook = partial(on_begin, task, scheduler, history)\n        scheduler.on_start(hook)\n\n    @scheduler.on_start\n    def launch_initial_trials() -&gt; None:\n        trials = _optimizer.ask(n=n_workers)\n        for trial in trials:\n            task.submit(trial, self)\n\n    from amltk.optimization.trial import Trial\n\n    @task.on_result\n    def tell_optimizer(_: Any, report: Trial.Report) -&gt; None:\n        _optimizer.tell(report)\n\n    @task.on_result\n    def add_report_to_history(_: Any, report: Trial.Report) -&gt; None:\n        history.add(report)\n        match report.status:\n            case Trial.Status.SUCCESS:\n                return\n            case Trial.Status.FAIL | Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n                match on_trial_exception:\n                    case \"raise\":\n                        if report.exception is None:\n                            raise RuntimeError(\n                                f\"Trial finished with status {report.status} but\"\n                                \" no exception was attached!\",\n                            )\n                        raise report.exception\n                    case \"end\":\n                        scheduler.stop(\n                            stop_msg=f\"Trial finished with status {report.status}\",\n                            exception=report.exception,\n                        )\n                    case \"continue\":\n                        pass\n            case _:\n                raise ValueError(f\"Invalid status {report.status}\")\n\n    @task.on_result\n    def run_next_trial(*_: Any) -&gt; None:\n        if scheduler.running():\n            trial = _optimizer.ask()\n            task.submit(trial, self)\n\n    return scheduler, task, history\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.optimization.Optimizer.CreateSignature","title":"amltk.optimization.Optimizer.CreateSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A Protocol which defines the keywords required to create an optimizer with deterministic behavior at a desired location.</p> <p>This protocol matches the <code>Optimizer.create</code> classmethod, however we also allow any function which accepts the keyword arguments to create an Optimizer.</p>"},{"location":"api/amltk/pipeline/node/#amltk.optimization.Optimizer.CreateSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None\n) -&gt; Optimizer\n</code></pre> <p>A function which creates an optimizer for node.optimize should accept the following keyword arguments.</p> PARAMETER DESCRIPTION <code>space</code> <p>The node to optimize</p> <p> TYPE: <code>Node</code> </p> <code>metrics</code> <p>The metrics to optimize</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store the results in</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the optimization</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __call__(\n    self,\n    *,\n    space: Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n) -&gt; Optimizer:\n    \"\"\"A function which creates an optimizer for node.optimize should\n    accept the following keyword arguments.\n\n    Args:\n        space: The node to optimize\n        metrics: The metrics to optimize\n        bucket: The bucket to store the results in\n        seed: The seed to use for the optimization\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.OnBeginCallbackSignature","title":"amltk.pipeline.node.OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.search_space","title":"search_space","text":"<pre><code>search_space(\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: args,\n    **parser_kwargs: kwargs\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace\n</code></pre> <p>Get the search space for this node.</p> PARAMETER DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.walk","title":"walk","text":"<pre><code>walk(\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]\n</code></pre> <p>Walk the nodes in this chain.</p> PARAMETER DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.OnBeginCallbackSignature","title":"OnBeginCallbackSignature","text":"<p>               Bases: <code>Protocol</code></p> <p>A  calllback to further define control flow from <code>pipeline.optimize()</code>.</p> <p>In one of these callbacks, you can register to specific <code>@events</code> of the <code>Scheduler</code> or <code>Task</code>.</p> <pre><code>pipeline = ...\n\n# The callback will get the task, scheduler and the history in which results\n# will be stored\ndef my_callback(task: Task[..., Trial.Report], scheduler: Scheduler, history: History) -&gt; None:\n\n    # You can do early stopping based on a target metric\n    @task.on_result\n    def stop_if_target_reached(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt;= 0.95:\n            scheduler.stop(stop_msg=\"Target reached!\"))\n\n    # You could also perform early stopping based on iterations\n    n = 0\n    last_score = 0.0\n\n    @task.on_result\n    def stop_if_no_improvement_for_n_runs(_: Future, report: Trial.Report) -&gt; None:\n        score = report.values[\"accuracy\"]\n        if score &gt; last_score:\n            n = 0\n            last_score = score\n        elif n &gt;= 5:\n            scheduler.stop()\n        else:\n            n += 1\n\n    # Really whatever you'd like\n    @task.on_result\n    def print_if_choice_made(_: Future, report: Trial.Report) -&gt; None:\n        if report.config[\"estimator:__choice__\"] == \"random_forest\":\n            print(\"yay\")\n\n    # Every callback will be called here in the main process so it's\n    # best not to do anything too heavy here.\n    # However you can also submit new tasks or jobs to the scheduler too\n    @task.on_result(every=30)  # Do a cleanup sweep every 30 trials\n    def keep_on_ten_best_models_on_disk(_: Future, report: Trial.Report) -&gt; None:\n        sorted_reports = history.sortby(\"accuracy\")\n        reports_to_cleanup = sorted_reports[10:]\n        scheduler.submit(some_cleanup_function, reporteds_to_cleanup)\n\nhistory = pipeline.optimize(\n    ...,\n    on_begin=my_callback,\n)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.OnBeginCallbackSignature.__call__","title":"__call__","text":"<pre><code>__call__(\n    task: Task[[Trial, Node], Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None\n</code></pre> <p>Signature for the callback.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task that will be run</p> <p> TYPE: <code>Task[[Trial, Node], Report]</code> </p> <code>scheduler</code> <p>The scheduler that will be running the optimization</p> <p> TYPE: <code>Scheduler</code> </p> <code>history</code> <p>The history that will be used to collect the results</p> <p> TYPE: <code>History</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __call__(\n    self,\n    task: Task[[Trial, Node], Trial.Report],\n    scheduler: Scheduler,\n    history: History,\n) -&gt; None:\n    \"\"\"Signature for the callback.\n\n    Args:\n        task: The task that will be run\n        scheduler: The scheduler that will be running the optimization\n        history: The history that will be used to collect the results\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest","title":"ParamRequest  <code>dataclass</code>","text":"<pre><code>ParamRequest(\n    _has_default: bool,\n    key: str,\n    default: T | object = _NotSet,\n)\n</code></pre> <p>               Bases: <code>Generic[T]</code></p> <p>A parameter request for a node. This is most useful for things like seeds.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest.default","title":"default  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default: T | object = _NotSet\n</code></pre> <p>The default value to use if the key is not found.</p> <p>If left as <code>_NotSet</code> (default) then an error will be raised if the parameter is not found during configuration with <code>configure()</code>.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest.has_default","title":"has_default  <code>property</code>","text":"<pre><code>has_default: bool\n</code></pre> <p>Whether this request has a default value.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest.key","title":"key  <code>instance-attribute</code>","text":"<pre><code>key: str\n</code></pre> <p>The key to request under.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.RichOptions","title":"RichOptions","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Options for rich printing.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.request","title":"request","text":"<pre><code>request(\n    key: str, default: T | object = _NotSet\n) -&gt; ParamRequest[T]\n</code></pre> <p>Create a new parameter request.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to request under.</p> <p> TYPE: <code>str</code> </p> <code>default</code> <p>The default value to use if the key is not found. If left as <code>_NotSet</code> (default) then the key will be removed from the config once <code>configure</code> is called and nothing has been provided.</p> <p> TYPE: <code>T | object</code> DEFAULT: <code>_NotSet</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def request(key: str, default: T | object = _NotSet) -&gt; ParamRequest[T]:\n    \"\"\"Create a new parameter request.\n\n    Args:\n        key: The key to request under.\n        default: The default value to use if the key is not found.\n            If left as `_NotSet` (default) then the key will be removed from the\n            config once [`configure`][amltk.pipeline.Node.configure] is called and\n            nothing has been provided.\n    \"\"\"\n    return ParamRequest(key=key, default=default, _has_default=default is not _NotSet)\n</code></pre>"},{"location":"api/amltk/pipeline/ops/","title":"Ops","text":""},{"location":"api/amltk/pipeline/ops/#amltk.pipeline.ops","title":"amltk.pipeline.ops","text":"<p>Operations on pipelines.</p>"},{"location":"api/amltk/pipeline/ops/#amltk.pipeline.ops.factorize","title":"factorize","text":"<pre><code>factorize(\n    node: NodeT1,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: (\n        Callable[[NodeT2, Node], NodeT2] | None\n    ) = None\n) -&gt; Iterator[NodeT1]\n</code></pre> <p>Factorize a pipeline into all possibilities of its children.</p> <p>When dealing with a large pipeline with many choices at various levels, it can be useful to factorize the pipeline into all possible pipelines. This effectively returns a new pipeline for every possible choice in the pipeline.</p> <pre><code>from amltk.pipeline import Sequential, Choice, Node, factorize\n\npipeline = Sequential(\n    Choice(Node(name=\"hi\"), Node(name=\"hello\"), name=\"choice\"),\n    Node(name=\"banana\"),\n    name=\"pipeline\",\n)\n\nprint(pipeline)\nfor i, possibility in enumerate(factorize(pipeline)):\n    print(f\"Pipeline {i}:\")\n    print(possibility)\n</code></pre> <pre>\n<code>\u256d\u2500 Sequential(pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Choice(choice) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Node(hello) \u2500\u256e \u256d\u2500 Node(hi) \u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                  \u2193                   \u2502\n\u2502 \u256d\u2500 Node(banana) \u2500\u256e                   \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Pipeline 0:\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Sequential(pipeline) \u2500\u256e\n\u2502 \u256d\u2500 Choice(choice) \u2500\u2500\u256e  \u2502\n\u2502 \u2502 \u256d\u2500 Node(hello) \u2500\u256e \u2502  \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502  \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502\n\u2502           \u2193            \u2502\n\u2502 \u256d\u2500 Node(banana) \u2500\u256e     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Pipeline 1:\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Sequential(pipeline) \u2500\u256e\n\u2502 \u256d\u2500 Choice(choice) \u2500\u256e   \u2502\n\u2502 \u2502 \u256d\u2500 Node(hi) \u2500\u256e   \u2502   \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f   \u2502   \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f   \u2502\n\u2502           \u2193            \u2502\n\u2502 \u256d\u2500 Node(banana) \u2500\u256e     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> PARAMETER DESCRIPTION <code>node</code> <p>The node to factorize.</p> <p> TYPE: <code>NodeT1</code> </p> <code>min_depth</code> <p>The minimum depth at which to factorize. If the node is at a depth less than this, it will not be factorized. Depth is calculated as the node distance from node which is passed in plus the <code>current_depth</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>max_depth</code> <p>The maximum depth at which to factorize. If the node is at a depth greater than this, it will not be factorized. Depth is calculated as the node distance from node which is passed in plus the <code>current_depth</code>. If <code>None</code>, there is no maximum depth.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>current_depth</code> <p>The current depth of the node. This is used internally but can also be used externally to factorize a sub-pipeline.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>factor_by</code> <p>A function that takes a node and returns True if it should be factorized into its children, False otherwise. By default, it will split only Choice nodes. One useful example is to only factor on a particular name of a node.</p> <pre><code>pipeline_per_estimator = list(factorize(\n    pipeline,\n    factor_by=lambda _node: _node.name == \"estimator\"\n))\n</code></pre> <p> TYPE: <code>Callable[[Node], bool] | None</code> DEFAULT: <code>None</code> </p> <code>assign_child</code> <p>A function that takes a node and a child and returns a new node with that child assigned to it. By default, it will mutate the node so that it has that child as its only child. You may wish to pass in custom functionality if there is more than one way to assign a child to a node or extra logic must be done to the nodes properties.</p> <p>It should return the same type of node as the one passed in.</p> <p> TYPE: <code>Callable[[NodeT2, Node], NodeT2] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Iterator[NodeT1]</code> <p>An iterator over all possible pipelines.</p> Source code in <code>src/amltk/pipeline/ops.py</code> <pre><code>def factorize(\n    node: NodeT1,\n    *,\n    min_depth: int = 0,\n    max_depth: int | None = None,\n    current_depth: int = 0,\n    factor_by: Callable[[Node], bool] | None = None,\n    assign_child: Callable[[NodeT2, Node], NodeT2] | None = None,\n) -&gt; Iterator[NodeT1]:\n    \"\"\"Factorize a pipeline into all possibilities of its children.\n\n    When dealing with a large pipeline with many choices at various levels,\n    it can be useful to factorize the pipeline into all possible pipelines.\n    This effectively returns a new pipeline for every possible choice in the\n    pipeline.\n\n    ```python exec=\"true\" source=\"material-block\" html=\"true\"\n    from amltk.pipeline import Sequential, Choice, Node, factorize\n\n    pipeline = Sequential(\n        Choice(Node(name=\"hi\"), Node(name=\"hello\"), name=\"choice\"),\n        Node(name=\"banana\"),\n        name=\"pipeline\",\n    )\n\n    from amltk._doc import doc_print; _print = print; print = lambda thing: doc_print(_print, thing)  # markdown-exec: hide\n    print(pipeline)\n    for i, possibility in enumerate(factorize(pipeline)):\n        print(f\"Pipeline {i}:\")\n        print(possibility)\n    ```\n\n    Args:\n        node: The node to factorize.\n        min_depth: The minimum depth at which to factorize. If the node is at a\n            depth less than this, it will not be factorized.\n            Depth is calculated as the node distance from node which is passed in\n            plus the `current_depth`.\n        max_depth: The maximum depth at which to factorize. If the node is at a\n            depth greater than this, it will not be factorized.\n            Depth is calculated as the node distance from node which is passed in\n            plus the `current_depth`. If `None`, there is no maximum depth.\n        current_depth: The current depth of the node. This is used internally but\n            can also be used externally to factorize a sub-pipeline.\n        factor_by: A function that takes a node and returns True if it\n            should be factorized into its children, False otherwise. By\n            default, it will split only Choice nodes. One useful example\n            is to only factor on a particular name of a node.\n\n            ```python\n            pipeline_per_estimator = list(factorize(\n                pipeline,\n                factor_by=lambda _node: _node.name == \"estimator\"\n            ))\n            ```\n\n        assign_child: A function that takes a node and a child and\n            returns a new node with that child assigned to it. By default,\n            it will mutate the node so that it has that child as its\n            only child. You may wish to pass in custom functionality if there\n            is more than one way to assign a child to a node or extra logic must\n            be done to the nodes properties.\n\n            It should return the same type of node as the one passed in.\n\n    Returns:\n        An iterator over all possible pipelines.\n    \"\"\"  # noqa: E501\n    if max_depth is None:\n        max_depth = MAX_INT\n\n    if current_depth &lt; 0:\n        raise ValueError(\"current_depth cannot be less than 0\")\n\n    # We can exit early as there's no chance we factorize past this point\n    if current_depth &gt; max_depth:\n        yield node.copy()\n        return\n\n    # NOTE: These two functions below are defined here instead to allow custom\n    # Node types in the future. The default behaviour is defined to just split\n    # Choice nodes and assign a child to one is to just mutate the node so\n    # that it has that child as its only child.\n    if factor_by is None:\n        factor_by = lambda _node: isinstance(_node, Choice)\n\n    if assign_child is None:\n        assign_child = lambda _node, _child: _node.mutate(nodes=(_child,))\n\n    _factorize = partial(\n        factorize,\n        factor_by=factor_by,\n        assign_child=assign_child,\n        current_depth=current_depth + 1,\n        min_depth=min_depth,\n        max_depth=max_depth,\n    )\n\n    match node:\n        case Node(nodes=()):\n            # Base case, there's no further possibility to factorize\n            yield node.copy()\n        case Node(nodes=children) if factor_by(node) and min_depth &lt;= current_depth:\n            for child in children:\n                for possible_child in _factorize(child):\n                    split_node_with_child_assigned = assign_child(\n                        node.copy(),  # type: ignore\n                        possible_child,\n                    )\n                    yield split_node_with_child_assigned  # type: ignore\n\n        case Node(nodes=children):\n            # We need to return N copies of this node, with each\n            # enumerating over all the posibilities of its children\n            # e.g.\n            # | children_sets = ((1, 2), (3, 4), (5,))\n            # | for child_set in [(1, 3, 5,), (1, 4, 5,), (2, 3, 5,), (2, 4, 5,)]:\n            # |    yield node.mutate(nodes=child_set)\n            children_sets = (_factorize(c) for c in children)\n            for child_set in product(*children_sets):\n                yield node.mutate(nodes=child_set)\n</code></pre>"},{"location":"api/amltk/pipeline/xgboost/","title":"Xgboost","text":""},{"location":"api/amltk/pipeline/xgboost/#amltk.pipeline.xgboost","title":"amltk.pipeline.xgboost","text":"<p>Get an XGBoost component for your pipeline.</p> <p>A <code>Component</code> wrapped around XGBoost with two possible default spaces <code>\"large\"</code> and <code>\"performant\"</code> or you own custom <code>space=</code>.</p> <p>See <code>amltk.pipeline.xgboost.xgboost_component</code></p>"},{"location":"api/amltk/pipeline/xgboost/#amltk.pipeline.xgboost.xgboost_component","title":"xgboost_component","text":"<pre><code>xgboost_component(\n    kind: Literal[\"classifier\", \"regressor\"],\n    name: str | None = None,\n    space: Any | Literal[\"large\", \"performant\"] = \"large\",\n    config: Mapping[str, Any] | None = None,\n) -&gt; Component\n</code></pre> <p>Create a component with an XGBoost estimator.</p> PARAMETER DESCRIPTION <code>kind</code> <p>The kind of estimator to create, either \"classifier\" or \"regressor\".</p> <p> TYPE: <code>Literal['classifier', 'regressor']</code> </p> <code>name</code> <p>The name of the step in the pipeline.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The space to use for hyperparameter optimization. Choose from \"large\", \"performant\" or provide a custom space.</p> <p>Todo</p> <p>Currently <code>\"performant\"</code> links to <code>\"large\"</code> and are the same.</p> <p>Warning</p> <p>The default space is by no means optimal, please adjust it to your needs. You can find the link to the XGBoost parameters here:</p> <p>xgboost.readthedocs.io/en/stable/parameter.html</p> <p> TYPE: <code>Any | Literal['large', 'performant']</code> DEFAULT: <code>'large'</code> </p> <code>config</code> <p>The keyword arguments to pass to the XGBoost estimator when it will be created. These will be hard set on the estimator and removed from the default space if no space is provided.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Component</code> <p>A pipeline with an XGBoost estimator.</p> Source code in <code>src/amltk/pipeline/xgboost.py</code> <pre><code>def xgboost_component(\n    kind: Literal[\"classifier\", \"regressor\"],\n    name: str | None = None,\n    space: Any | Literal[\"large\", \"performant\"] = \"large\",\n    config: Mapping[str, Any] | None = None,\n) -&gt; Component:\n    \"\"\"Create a component with an XGBoost estimator.\n\n    Args:\n        kind: The kind of estimator to create, either \"classifier\" or \"regressor\".\n        name: The name of the step in the pipeline.\n        space: The space to use for hyperparameter optimization. Choose from\n            \"large\", \"performant\" or provide a custom space.\n\n            !!! todo\n\n                Currently `#!python \"performant\"` links to `#!python \"large\"` and are\n                the same.\n\n            !!! warning \"Warning\"\n\n                The default space is by no means optimal, please adjust it to your\n                needs. You can find the link to the XGBoost parameters here:\n\n                https://xgboost.readthedocs.io/en/stable/parameter.html\n\n\n        config: The keyword arguments to pass to the XGBoost estimator when it will be\n            created. These will be hard set on the estimator and removed from the\n            default space if no space is provided.\n\n    Returns:\n        A pipeline with an XGBoost estimator.\n    \"\"\"\n    estimator_types = {\n        \"classifier\": XGBClassifier,\n        \"regressor\": XGBRegressor,\n    }\n    config = config or {}\n    estimator_type = estimator_types.get(kind)\n    if estimator_type is None:\n        raise ValueError(\n            f\"Unknown kind: {kind}, please choose from {list(estimator_types.keys())}\",\n        )\n\n    if name is None:\n        name = str(estimator_type.__name__)\n\n    if isinstance(space, str):\n        device = config.get(\"device\", \"cpu\")\n        tree_method = config.get(\"tree_method\", \"auto\")\n        is_classifier = estimator_type is XGBClassifier\n        _spaces = {\n            \"large\": xgboost_large_space,\n            \"performant\": xgboost_performant_space,\n        }\n        space_f = _spaces[space]\n        space = space_f(\n            is_classifier=is_classifier,\n            tree_method=tree_method,\n            device=device,\n        )\n        for key in config:\n            space.pop(key, None)\n    elif isinstance(space, Mapping):\n        overlap = set(space.keys()).intersection(config)\n        if any(overlap):\n            raise ValueError(\n                f\"Space and kwargs overlap: {overlap}, please remove one of them\",\n            )\n\n    return Component(name=name, item=estimator_type, config=config, space=space)\n</code></pre>"},{"location":"api/amltk/pipeline/xgboost/#amltk.pipeline.xgboost.xgboost_large_space","title":"xgboost_large_space","text":"<pre><code>xgboost_large_space(\n    *, is_classifier: bool, tree_method: str, device: str\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a large space for XGBoost hyperparameter optimization.</p> Source code in <code>src/amltk/pipeline/xgboost.py</code> <pre><code>def xgboost_large_space(\n    *,\n    is_classifier: bool,\n    tree_method: str,\n    device: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Create a large space for XGBoost hyperparameter optimization.\"\"\"\n    # TODO: Do we want a general space kind\n    # For now we use ConfigSpace where needed\n    from ConfigSpace import Float, Integer\n\n    space = {\n        \"eta\": Float(\"eta\", (1e-3, 1), default=0.3, log=True),\n        \"min_split_loss\": Float(\"min_split_loss\", (0, 20), default=0),\n        \"max_depth\": Integer(\"max_depth\", (1, 20), default=6),\n        \"min_child_weight\": Float(\"min_child_weight\", (0, 20), default=1),\n        \"max_delta_step\": Float(\"max_delta_step\", (0, 10), default=0),\n        \"subsample\": Float(\"subsample\", (1e-5, 1), default=1),\n        \"colsample_bytree\": Float(\"colsample_bytree\", (1e-5, 1), default=1),\n        \"colsample_bylevel\": Float(\"colsample_bylevel\", (1e-5, 1), default=1),\n        \"colsample_bynode\": Float(\"colsample_bynode\", (1e-5, 1), default=1),\n        \"reg_lambda\": Float(\"reg_lambda\", (1e-5, 1e3), default=1),\n        \"reg_alpha\": Float(\"reg_alpha\", (0, 1e3), default=0),\n    }\n\n    if tree_method == \"hist\" and (\"cuda\" in device or \"gpu\" in device):\n        space[\"sampling_method\"] = [\"uniform\", \"gradient_based\"]\n\n    if tree_method in (\"hist\", \"approx\"):\n        space[\"max_bin\"] = Integer(\"max_bin\", (2, 512), default=256)\n\n    if is_classifier:\n        space[\"scale_pos_weight\"] = Float(\"scale_pos_weight\", (1e-5, 1e5), default=1)\n\n    return space\n</code></pre>"},{"location":"api/amltk/pipeline/xgboost/#amltk.pipeline.xgboost.xgboost_performant_space","title":"xgboost_performant_space","text":"<pre><code>xgboost_performant_space(\n    *, is_classifier: bool, tree_method: str, device: str\n) -&gt; dict[str, Any]\n</code></pre> <p>Create a performant space for XGBoost hyperparameter optimization.</p> Source code in <code>src/amltk/pipeline/xgboost.py</code> <pre><code>def xgboost_performant_space(\n    *,\n    is_classifier: bool,\n    tree_method: str,\n    device: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Create a performant space for XGBoost hyperparameter optimization.\"\"\"\n    warnings.warn(\n        \"This space is not yet optimized for performance\"\n        \" and is subject to change in the future.\",\n        FutureWarning,\n        stacklevel=2,\n    )\n    return xgboost_large_space(\n        is_classifier=is_classifier,\n        tree_method=tree_method,\n        device=device,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/builders/sklearn/","title":"Sklearn","text":""},{"location":"api/amltk/pipeline/builders/sklearn/#amltk.pipeline.builders.sklearn","title":"amltk.pipeline.builders.sklearn","text":"<p>The sklearn <code>builder()</code>, converts a pipeline made of <code>Node</code>s into a sklearn <code>Pipeline</code>.</p> <p>Requirements</p> <p>This requires <code>sklearn</code> which can be installed with:</p> <pre><code>pip install \"amltk[scikit-learn]\"\n\n# Or directly\npip install scikit-learn\n</code></pre> <p>Each kind of node corresponds to a different part of the end pipeline:</p> <code>Fixed</code><code>Component</code><code>Sequential</code><code>Split</code><code>Join</code><code>Choice</code> <p><code>Fixed</code> - The estimator will simply be cloned, allowing you to directly configure some object in a pipeline.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Fixed\n\nest = Fixed(RandomForestClassifier(n_estimators=25))\nbuilt_pipeline = est.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> <p><code>Component</code> - The estimator will be built from the component's config. This is mostly useful to allow a space to be defined for the component.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Component\n\nest = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\n# ... Likely get the configuration through an optimizer or sampling\nconfigured_est = est.configure({\"n_estimators\": 25})\n\nbuilt_pipeline = configured_est.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> <p><code>Sequential</code> - The sequential will be converted into a <code>Pipeline</code>, building whatever nodes are contained within in.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom amltk.pipeline import Component, Sequential\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, config={\"n_estimators\": 25})\n)\nbuilt_pipeline = pipeline.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre> \u00a0PCA?Documentation for PCA<pre>PCA(n_components=3)</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> <p><code>Split</code> - The split will be converted into a <code>ColumnTransformer</code>, where each path and the data that should go through it is specified by the split's config. You can provide a <code>ColumnTransformer</code> directly as the item to the <code>Split</code>, or otherwise if left blank, it will default to the standard sklearn one.</p> <p>You can use a <code>Fixed</code> with the special keyword <code>\"passthrough\"</code> as you might normally do with a <code>ColumnTransformer</code>.</p> <p>By default, we provide two special keywords you can provide to a <code>Split</code>, namely <code>\"categorical\"</code> and <code>\"numerical\"</code>, which will automatically configure a <code>ColumnTransorfmer</code> to pass the appropraite columns of a data-frame to the given paths.</p> <pre><code>from amltk.pipeline import Split, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    Component(\n        OneHotEncoder,\n        space={\n            \"min_frequency\": (0.01, 0.1),\n            \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n        },\n        config={\"drop\": \"first\"},\n    ),\n]\nnumerical_pipeline = [SimpleImputer(strategy=\"median\"), StandardScaler()]\n\nsplit = Split(\n    {\n        \"categorical\": categorical_pipeline,\n        \"numerical\": numerical_pipeline\n    }\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Split(Split-GnZiBbJk) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(categorical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item SimpleImputer(strategy='\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502                 \u2193                  \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u256d\u2500 Fixed(StandardScaler) \u2500\u256e        \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Component(OneHotEncoder) \u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 item StandardScaler()   \u2502        \u2502 \u2502\n\u2502 \u2502 \u2502 item   class                  \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f        \u2502 \u2502\n\u2502 \u2502 \u2502        OneHotEncoder(...)     \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u2502 \u2502 config {'drop': 'first'}      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502 space  {                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'min_frequency': ( \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.01,          \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.1            \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ),                 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'handle_unknown':  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        [                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'ignore',      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'infrequent_i\u2026 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ]                  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        }                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502                                        \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>You can manually specify the column selectors if you prefer.</p> <pre><code>split = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numbers\": numerical_pipeline,\n    },\n    config={\n        \"categories\": make_column_selector(dtype_include=object),\n        \"numbers\": make_column_selector(dtype_include=np.number),\n    },\n)\n</code></pre> <p><code>Join</code> - The join will be converted into a <code>FeatureUnion</code>.</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\njoin = Join(PCA(n_components=2), SelectKBest(k=3), name=\"my_feature_union\")\n\npipeline = join.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre> \u00a0my_feature_union: FeatureUnion?Documentation for my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                               ('SelectKBest', SelectKBest(k=3))])</pre> PCA\u00a0PCA?Documentation for PCA<pre>PCA(n_components=2)</pre> SelectKBest\u00a0SelectKBest?Documentation for SelectKBest<pre>SelectKBest(k=3)</pre> <p><code>Choice</code> - The estimator will be built from the chosen component's config. This is very similar to <code>Component</code>.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom amltk.pipeline import Choice\n\n# The choice here is usually provided during the `.configure()` step.\nestimator_choice = Choice(\n    RandomForestClassifier(),\n    MLPClassifier(),\n    config={\"__choice__\": \"RandomForestClassifier\"}\n)\n\nbuilt_pipeline = estimator_choice.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier()</pre>"},{"location":"api/amltk/pipeline/builders/sklearn/#amltk.pipeline.builders.sklearn.build","title":"build","text":"<pre><code>build(\n    node: Node[Any, Any],\n    *,\n    pipeline_type: type[SklearnPipelineT] = SklearnPipeline,\n    **pipeline_kwargs: Any\n) -&gt; SklearnPipelineT\n</code></pre> <p>Build a pipeline into a usable object.</p> PARAMETER DESCRIPTION <code>node</code> <p>The node from which to build a pipeline.</p> <p> TYPE: <code>Node[Any, Any]</code> </p> <code>pipeline_type</code> <p>The type of pipeline to build. Defaults to the standard sklearn pipeline but can be any derivative of that, i.e. ImbLearn's pipeline.</p> <p> TYPE: <code>type[SklearnPipelineT]</code> DEFAULT: <code>Pipeline</code> </p> <code>**pipeline_kwargs</code> <p>The kwargs to pass to the pipeline_type.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>SklearnPipelineT</code> <p>The built pipeline</p> Source code in <code>src/amltk/pipeline/builders/sklearn.py</code> <pre><code>def build(\n    node: Node[Any, Any],\n    *,\n    pipeline_type: type[SklearnPipelineT] = SklearnPipeline,\n    **pipeline_kwargs: Any,\n) -&gt; SklearnPipelineT:\n    \"\"\"Build a pipeline into a usable object.\n\n    Args:\n        node: The node from which to build a pipeline.\n        pipeline_type: The type of pipeline to build. Defaults to the standard\n            sklearn pipeline but can be any derivative of that, i.e. ImbLearn's\n            pipeline.\n        **pipeline_kwargs: The kwargs to pass to the pipeline_type.\n\n    Returns:\n        The built pipeline\n    \"\"\"\n    return pipeline_type(list(_iter_steps(node)), **pipeline_kwargs)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/","title":"Configspace","text":""},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace","title":"amltk.pipeline.parsers.configspace","text":"<p>ConfigSpace is a library for representing and sampling configurations for hyperparameter optimization. It features a straightforward API for defining hyperparameters, their ranges and even conditional dependencies.</p> <p>It is generally flexible enough for more complex use cases, even handling the complex pipelines of AutoSklearn and AutoPyTorch, large scale hyperparameter spaces over which to optimize entire pipelines at a time.</p> <p>Requirements</p> <p>This requires <code>ConfigSpace</code> which can be installed with:</p> <pre><code>pip install \"amltk[configspace]\"\n\n# Or directly\npip install ConfigSpace\n</code></pre> <p>In general, you should have the ConfigSpace documentation ready to consult for a full understanding of how to construct hyperparameter spaces with AMLTK.</p>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace--basic-usage","title":"Basic Usage","text":"<p>You can directly us the <code>parser()</code> function and pass that into the <code>search_space()</code> method of a <code>Node</code>, however you can also simply provide <code>search_space(parser=\"configspace\", ...)</code> for simplicity.</p> <pre><code>from amltk.pipeline import Component, Choice, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\nmy_pipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Component(PCA, space={\"n_components\": (1, 3)})\n    &gt;&gt; Choice(\n        Component(\n            SVC,\n            space={\"C\": (0.1, 10.0)}\n        ),\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n        ),\n        Component(\n            MLPClassifier,\n            space={\n                \"activation\": [\"identity\", \"logistic\", \"relu\"],\n                \"alpha\": (0.0001, 0.1),\n                \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n            },\n        ),\n        name=\"estimator\"\n    )\n)\n\nspace = my_pipeline.search_space(\"configspace\")\nprint(space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Pipeline:PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    Pipeline:estimator:MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    Pipeline:estimator:MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    Pipeline:estimator:MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    Pipeline:estimator:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    Pipeline:estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:estimator:SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    Pipeline:estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n  Conditions:\n    Pipeline:estimator:MLPClassifier:activation | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:MLPClassifier:alpha | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:MLPClassifier:learning_rate | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:RandomForestClassifier:criterion | Pipeline:estimator:__choice__ == 'RandomForestClassifier'\n    Pipeline:estimator:RandomForestClassifier:n_estimators | Pipeline:estimator:__choice__ == 'RandomForestClassifier'\n    Pipeline:estimator:SVC:C | Pipeline:estimator:__choice__ == 'SVC'\n</code></pre> <p>Here we have an example of a few different kinds of hyperparmeters,</p> <ul> <li><code>PCA:n_components</code> is a integer with a range of 1 to 3, uniform distribution, as specified     by it's integer bounds in a tuple.</li> <li><code>SVC:C</code> is a float with a range of 0.1 to 10.0, uniform distribution, as specified     by it's float bounds in a tuple.</li> <li><code>RandomForestClassifier:criterion</code> is a categorical hyperparameter, with two choices,     <code>\"gini\"</code> and <code>\"log_loss\"</code>.</li> </ul> <p>There is also a <code>Choice</code> node, which is a special node that indicates that we could choose from one of these estimators. This leads to the conditionals that you can see in the printed out space.</p> <p>You may wish to remove all conditionals if an <code>Optimizer</code> does not support them, or you may wish to remove them for other reasons. You can do this by passing <code>conditionals=False</code> to the <code>parser()</code> function.</p> <pre><code>print(my_pipeline.search_space(\"configspace\", conditionals=False))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Pipeline:PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    Pipeline:estimator:MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    Pipeline:estimator:MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    Pipeline:estimator:MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    Pipeline:estimator:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    Pipeline:estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:estimator:SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    Pipeline:estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n</code></pre> <p>Likewise, you can also remove all heirarchy from the space which may make downstream tasks easier, by passing <code>flat=True</code> to the <code>parser()</code> function.</p> <pre><code>print(my_pipeline.search_space(\"configspace\", flat=True))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n  Conditions:\n    MLPClassifier:activation | estimator:__choice__ == 'MLPClassifier'\n    MLPClassifier:alpha | estimator:__choice__ == 'MLPClassifier'\n    MLPClassifier:learning_rate | estimator:__choice__ == 'MLPClassifier'\n    RandomForestClassifier:criterion | estimator:__choice__ == 'RandomForestClassifier'\n    RandomForestClassifier:n_estimators | estimator:__choice__ == 'RandomForestClassifier'\n    SVC:C | estimator:__choice__ == 'SVC'\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace--more-specific-hyperparameters","title":"More Specific Hyperparameters","text":"<p>You'll often want to be a bit more specific with your hyperparameters, here we just show a few examples of how you'd couple your pipelines a bit more towards <code>ConfigSpace</code>.</p> <pre><code>from ConfigSpace import Float, Categorical, Normal\nfrom amltk.pipeline import Searchable\n\ns = Searchable(\n    space={\n        \"lr\": Float(\"lr\", bounds=(1e-5, 1.), log=True, default=0.3),\n        \"balance\": Float(\"balance\", bounds=(-1.0, 1.0), distribution=Normal(0.0, 0.5)),\n        \"color\": Categorical(\"color\", [\"red\", \"green\", \"blue\"], weights=[2, 1, 1], default=\"blue\"),\n    },\n    name=\"Something-To-Search\",\n)\nprint(s.search_space(\"configspace\"))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Something-To-Search:balance, Type: NormalFloat, Mu: 0.0, Sigma: 0.5, Range: [-1.0, 1.0], Default: 0.0\n    Something-To-Search:color, Type: Categorical, Choices: {red, green, blue}, Default: blue, Probabilities: [0.5  0.25 0.25]\n    Something-To-Search:lr, Type: UniformFloat, Range: [1e-05, 1.0], Default: 0.3, on log-scale\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace--conditional-ands-advanced-usage","title":"Conditional ands Advanced Usage","text":"<p>We will refer you to the ConfigSpace documentation for the construction of these. However once you've constructed a <code>ConfigurationSpace</code> and added any forbiddens and conditionals, you may simply set that as the <code>.space</code> attribute.</p> <pre><code>from amltk.pipeline import Component, Choice, Sequential\nfrom ConfigSpace import ConfigurationSpace, EqualsCondition, InCondition\n\nmyspace = ConfigurationSpace({\"A\": [\"red\", \"green\", \"blue\"], \"B\": (1, 10), \"C\": (-100.0, 0.0)})\nmyspace.add_conditions([\n    EqualsCondition(myspace[\"B\"], myspace[\"A\"], \"red\"),  # B is active when A is red\n    InCondition(myspace[\"C\"], myspace[\"A\"], [\"green\", \"blue\"]), # C is active when A is green or blue\n])\n\ncomponent = Component(object, space=myspace, name=\"MyThing\")\n\nparsed_space = component.search_space(\"configspace\")\nprint(parsed_space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MyThing:A, Type: Categorical, Choices: {red, green, blue}, Default: red\n    MyThing:B, Type: UniformInteger, Range: [1, 10], Default: 6\n    MyThing:C, Type: UniformFloat, Range: [-100.0, 0.0], Default: -50.0\n  Conditions:\n    MyThing:B | MyThing:A == 'red'\n    MyThing:C | MyThing:A in {'green', 'blue'}\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace.parser","title":"parser","text":"<pre><code>parser(\n    node: Node,\n    *,\n    seed: int | None = None,\n    flat: bool = False,\n    conditionals: bool = True,\n    delim: str = \":\"\n) -&gt; ConfigurationSpace\n</code></pre> <p>Parse a Node and its children into a ConfigurationSpace.</p> PARAMETER DESCRIPTION <code>node</code> <p>The Node to parse</p> <p> TYPE: <code>Node</code> </p> <code>seed</code> <p>The seed to use for the ConfigurationSpace</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>flat</code> <p>Whether to have a heirarchical naming scheme for nodes and their children.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conditionals</code> <p>Whether to include conditionals in the space from a <code>Choice</code>. If this is <code>False</code>, this will also remove all forbidden clauses and other conditional clauses. The primary use of this functionality is that some optimizers do not support these features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>delim</code> <p>The delimiter to use for the names of the hyperparameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>':'</code> </p> Source code in <code>src/amltk/pipeline/parsers/configspace.py</code> <pre><code>def parser(\n    node: Node,\n    *,\n    seed: int | None = None,\n    flat: bool = False,\n    conditionals: bool = True,\n    delim: str = \":\",\n) -&gt; ConfigurationSpace:\n    \"\"\"Parse a Node and its children into a ConfigurationSpace.\n\n    Args:\n        node: The Node to parse\n        seed: The seed to use for the ConfigurationSpace\n        flat: Whether to have a heirarchical naming scheme for nodes and their children.\n        conditionals: Whether to include conditionals in the space from a\n            [`Choice`][amltk.pipeline.Choice]. If this is `False`, this will\n            also remove all forbidden clauses and other conditional clauses.\n            The primary use of this functionality is that some optimizers do not\n            support these features.\n        delim: The delimiter to use for the names of the hyperparameters\n    \"\"\"\n    space = ConfigurationSpace(seed=seed)\n    space.add_configuration_space(\n        prefix=node.name,\n        delimiter=delim,\n        configuration_space=_parse_space(node, seed=seed, conditionals=conditionals),\n    )\n\n    children = node.nodes\n\n    choice = None\n    if isinstance(node, Choice) and any(children):\n        choice = Categorical(\n            name=f\"{node.name}{delim}__choice__\",\n            items=[child.name for child in children],\n        )\n        space.add_hyperparameter(choice)\n\n    for child in children:\n        space.add_configuration_space(\n            prefix=node.name if not flat else \"\",\n            delimiter=delim if not flat else \"\",\n            configuration_space=parser(\n                child,\n                seed=seed,\n                flat=flat,\n                conditionals=conditionals,\n                delim=delim,\n            ),\n            parent_hyperparameter=(\n                {\"parent\": choice, \"value\": child.name}\n                if choice and conditionals\n                else None\n            ),\n        )\n\n    return space\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/optuna/","title":"Optuna","text":""},{"location":"api/amltk/pipeline/parsers/optuna/#amltk.pipeline.parsers.optuna","title":"amltk.pipeline.parsers.optuna","text":"<p>Optuna parser for parsing out a <code>search_space()</code>. from a pipeline.</p> <p>Requirements</p> <p>This requires <code>Optuna</code> which can be installed with:</p> <pre><code>pip install amltk[optuna]\n\n# Or directly\npip install optuna\n</code></pre> Limitations <p>Optuna feature a very dynamic search space (define-by-run), where people typically sample from some trial object and use traditional python control flow to define conditionality.</p> <p>This means we can not trivially represent this conditionality in a static search space. While band-aids are possible, it naturally does not sit well with the static output of a parser.</p> <p>As such, our parser does not support conditionals or choices!. Users may still use the define-by-run within their optimization function itself.</p> <p>If you have experience with Optuna and have any suggestions, please feel free to open an issue or PR on GitHub!</p>"},{"location":"api/amltk/pipeline/parsers/optuna/#amltk.pipeline.parsers.optuna--usage","title":"Usage","text":"<p>The typical way to represent a search space for Optuna is just to use a dictionary, where the keys are the names of the hyperparameters and the values are either integer/float tuples indicating boundaries or some discrete set of values. It is possible to have the value directly be a <code>BaseDistribution</code>, an optuna type, when you need to customize the distribution more.</p> <pre><code>from amltk.pipeline import Component\nfrom optuna.distributions import FloatDistribution\n\nc = Component(\n    object,\n    space={\n        \"myint\": (1, 10),\n        \"myfloat\": (1.0, 10.0),\n        \"mycategorical\": [\"a\", \"b\", \"c\"],\n        \"log-scale-custom\": FloatDistribution(1e-10, 1e-2, log=True),\n    },\n    name=\"name\",\n)\n\nspace = c.search_space(parser=\"optuna\")\n</code></pre> <pre>\n<code>{\n    'name:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'name:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, step=None),\n    'name:mycategorical': CategoricalDistribution(choices=('a', 'b', 'c')),\n    'name:log-scale-custom': FloatDistribution(high=0.01, log=True, low=1e-10, \nstep=None)\n}\n</code>\n</pre> <p>You may also just pass the <code>parser=</code> function directly if preferred</p> <pre><code>from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\nspace = c.search_space(parser=optuna_parser)\n</code></pre> <pre>\n<code>{\n    'name:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'name:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, step=None),\n    'name:mycategorical': CategoricalDistribution(choices=('a', 'b', 'c')),\n    'name:log-scale-custom': FloatDistribution(high=0.01, log=True, low=1e-10, \nstep=None)\n}\n</code>\n</pre> <p>When using <code>search_space()</code> on a some nested structures, you may want to flatten the names of the hyperparameters. For this you can use <code>flat=</code></p> <pre><code>from amltk.pipeline import Searchable, Sequential\n\nseq = Sequential(\n    Searchable({\"myint\": (1, 10)}, name=\"nested_1\"),\n    Searchable({\"myfloat\": (1.0, 10.0)}, name=\"nested_2\"),\n    name=\"seq\"\n)\n\nhierarchical_space = seq.search_space(parser=\"optuna\", flat=False)  # Default\n\nflat_space = seq.search_space(parser=\"optuna\", flat=False)  # Default\n</code></pre> <pre>\n<code>{\n    'seq:nested_1:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'seq:nested_2:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, \nstep=None)\n}\n</code>\n</pre> <pre>\n<code>{\n    'seq:nested_1:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'seq:nested_2:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, \nstep=None)\n}\n</code>\n</pre>"},{"location":"api/amltk/pipeline/parsers/optuna/#amltk.pipeline.parsers.optuna.parser","title":"parser","text":"<pre><code>parser(\n    node: Node,\n    *,\n    flat: bool = False,\n    conditionals: bool = False,\n    delim: str = \":\"\n) -&gt; OptunaSearchSpace\n</code></pre> <p>Parse a Node and its children into a ConfigurationSpace.</p> PARAMETER DESCRIPTION <code>node</code> <p>The Node to parse</p> <p> TYPE: <code>Node</code> </p> <code>flat</code> <p>Whether to have a hierarchical naming scheme for nodes and their children.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conditionals</code> <p>Whether to include conditionals in the space from a <code>Choice</code>. If this is <code>False</code>, this will also remove all forbidden clauses and other conditional clauses. The primary use of this functionality is that some optimizers do not support these features.</p> <p>Not yet supported</p> <p>This functionality is not yet supported as we can't encode this into a static Optuna search space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>delim</code> <p>The delimiter to use for the names of the hyperparameters.</p> <p> TYPE: <code>str</code> DEFAULT: <code>':'</code> </p> Source code in <code>src/amltk/pipeline/parsers/optuna.py</code> <pre><code>def parser(\n    node: Node,\n    *,\n    flat: bool = False,\n    conditionals: bool = False,\n    delim: str = \":\",\n) -&gt; OptunaSearchSpace:\n    \"\"\"Parse a Node and its children into a ConfigurationSpace.\n\n    Args:\n        node: The Node to parse\n        flat: Whether to have a hierarchical naming scheme for nodes and their children.\n        conditionals: Whether to include conditionals in the space from a\n            [`Choice`][amltk.pipeline.Choice]. If this is `False`, this will\n            also remove all forbidden clauses and other conditional clauses.\n            The primary use of this functionality is that some optimizers do not\n            support these features.\n\n            !!! TODO \"Not yet supported\"\n\n                This functionality is not yet supported as we can't encode this into\n                a static Optuna search space.\n\n        delim: The delimiter to use for the names of the hyperparameters.\n    \"\"\"\n    if conditionals:\n        raise NotImplementedError(\"Conditionals are not yet supported with Optuna.\")\n\n    space = prefix_keys(_parse_space(node), prefix=f\"{node.name}{delim}\")\n\n    for child in node.nodes:\n        subspace = parser(child, flat=flat, conditionals=conditionals, delim=delim)\n        if not flat:\n            subspace = prefix_keys(subspace, prefix=f\"{node.name}{delim}\")\n\n        for name, hp in subspace.items():\n            if name in space:\n                raise ValueError(\n                    f\"Duplicate name {name} already in space from space of {node.name}\",\n                    f\"\\nCurrently parsed space: {space}\",\n                )\n            space[name] = hp\n\n    return space\n</code></pre>"},{"location":"api/amltk/profiling/memory/","title":"Memory","text":""},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory","title":"amltk.profiling.memory","text":"<p>Module to measure memory.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory","title":"Memory  <code>dataclass</code>","text":"<pre><code>Memory(\n    start_vms: float, start_rss: float, unit: Unit | NAType\n)\n</code></pre> <p>A timer for measuring the time between two events.</p> ATTRIBUTE DESCRIPTION <code>start_vms</code> <p>The virtual memory size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>start_rss</code> <p>The resident set size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>unit</code> <p>The unit of the memory.</p> <p> TYPE: <code>Unit | NAType</code> </p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval","title":"Interval  <code>dataclass</code>","text":"<pre><code>Interval(\n    start_vms: float,\n    start_rss: float,\n    end_vms: float,\n    end_rss: float,\n    unit: Unit | NAType,\n)\n</code></pre> <p>A class for representing a time interval.</p> ATTRIBUTE DESCRIPTION <code>start_vms</code> <p>The virtual memory size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>start_rss</code> <p>The resident set size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>end_vms</code> <p>The virtual memory size at the end of the interval.</p> <p> TYPE: <code>float</code> </p> <code>end_rss</code> <p>The resident set size at the end of the interval.</p> <p> TYPE: <code>float</code> </p> <code>unit</code> <p>The unit of memory.</p> <p> TYPE: <code>Unit | NAType</code> </p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.rss_used","title":"rss_used  <code>property</code>","text":"<pre><code>rss_used: float\n</code></pre> <p>The amount of rss memory used in the interval.</p> <p>Warning</p> <p>This does not track peak memory usage. This will only give the difference between the start and end of the interval.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.vms_used","title":"vms_used  <code>property</code>","text":"<pre><code>vms_used: float\n</code></pre> <p>The amount of vms memory used in the interval.</p> <p>Warning</p> <p>This does not track peak memory usage. This will only give the difference between the start and end of the interval.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.to_dict","title":"to_dict","text":"<pre><code>to_dict(*, prefix: str = '') -&gt; dict[str, Any]\n</code></pre> <p>Convert the time interval to a dictionary.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>def to_dict(self, *, prefix: str = \"\") -&gt; dict[str, Any]:\n    \"\"\"Convert the time interval to a dictionary.\"\"\"\n    return {\n        f\"{prefix}start_vms\": self.start_vms,\n        f\"{prefix}end_vms\": self.end_vms,\n        f\"{prefix}diff_vms\": self.vms_used,\n        f\"{prefix}start_rss\": self.start_rss,\n        f\"{prefix}end_rss\": self.end_rss,\n        f\"{prefix}diff_rss\": self.rss_used,\n        f\"{prefix}unit\": self.unit,\n    }\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.to_unit","title":"to_unit","text":"<pre><code>to_unit(unit: Unit) -&gt; Interval\n</code></pre> <p>Return the memory used in a different unit.</p> PARAMETER DESCRIPTION <code>unit</code> <p>The unit to convert to.</p> <p> TYPE: <code>Unit</code> </p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory used in the new unit.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>def to_unit(self, unit: Memory.Unit) -&gt; Memory.Interval:\n    \"\"\"Return the memory used in a different unit.\n\n    Args:\n        unit: The unit to convert to.\n\n    Returns:\n        The memory used in the new unit.\n    \"\"\"\n    if self.unit == unit:\n        return self\n\n    return Memory.Interval(\n        start_vms=Memory.convert(self.start_vms, self.unit, unit),\n        end_vms=Memory.convert(self.end_vms, self.unit, unit),\n        start_rss=Memory.convert(self.start_rss, self.unit, unit),\n        end_rss=Memory.convert(self.end_rss, self.unit, unit),\n        unit=unit,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Unit","title":"Unit","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>An enum for the units of time.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Unit.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(s: Any) -&gt; Unit | NAType\n</code></pre> <p>Convert a string to a unit.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef from_str(cls, s: Any) -&gt; Memory.Unit | NAType:\n    \"\"\"Convert a string to a unit.\"\"\"\n    if isinstance(s, Memory.Unit):\n        return s\n\n    if isinstance(s, str):\n        _mapping = {\n            \"bytes\": Memory.Unit.BYTES,\n            \"b\": Memory.Unit.BYTES,\n            \"kilobytes\": Memory.Unit.KILOBYTES,\n            \"kb\": Memory.Unit.KILOBYTES,\n            \"megabytes\": Memory.Unit.MEGABYTES,\n            \"mb\": Memory.Unit.MEGABYTES,\n            \"gigabytes\": Memory.Unit.GIGABYTES,\n            \"gb\": Memory.Unit.GIGABYTES,\n        }\n        return _mapping.get(s.lower(), pd.NA)\n\n    return pd.NA\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.convert","title":"convert  <code>classmethod</code>","text":"<pre><code>convert(\n    x: float,\n    frm: (\n        Unit | NAType | Literal[\"B\", \"KB\", \"MB\", \"GB\"]\n    ) = \"B\",\n    to: (\n        Unit | NAType | Literal[\"B\", \"KB\", \"MB\", \"GB\"]\n    ) = \"B\",\n) -&gt; float\n</code></pre> <p>Convert a value from one unit to another.</p> PARAMETER DESCRIPTION <code>x</code> <p>The value to convert.</p> <p> TYPE: <code>float</code> </p> <code>frm</code> <p>The unit of the value.</p> <p> TYPE: <code>Unit | NAType | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>to</code> <p>The unit to convert to.</p> <p> TYPE: <code>Unit | NAType | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The converted value.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef convert(\n    cls,\n    x: float,\n    frm: Memory.Unit | NAType | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n    to: Memory.Unit | NAType | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; float:\n    \"\"\"Convert a value from one unit to another.\n\n    Args:\n        x: The value to convert.\n        frm: The unit of the value.\n        to: The unit to convert to.\n\n    Returns:\n        The converted value.\n    \"\"\"\n    if frm == to:\n        return x\n\n    _frm = cls.Unit.from_str(frm) if isinstance(frm, str) else frm\n    _to = cls.Unit.from_str(to) if isinstance(to, str) else to\n    return x * _CONVERSION[_frm] / _CONVERSION[_to]\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(d: Mapping[str, Any]) -&gt; Interval\n</code></pre> <p>Create a memory interval from a dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary to create from.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory interval.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Memory.Interval:\n    \"\"\"Create a memory interval from a dictionary.\n\n    Args:\n        d: The dictionary to create from.\n\n    Returns:\n        The memory interval.\n    \"\"\"\n    return Memory.Interval(\n        start_vms=dict_get_not_none(d, \"start_vms\", np.nan),\n        start_rss=dict_get_not_none(d, \"start_rss\", np.nan),\n        end_vms=dict_get_not_none(d, \"end_vms\", np.nan),\n        end_rss=dict_get_not_none(d, \"end_rss\", np.nan),\n        unit=Memory.Unit.from_str(dict_get_not_none(d, \"unit\", pd.NA)),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.measure","title":"measure  <code>classmethod</code>","text":"<pre><code>measure(\n    unit: Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\"\n) -&gt; Iterator[Interval]\n</code></pre> <p>Measure the memory used by a block of code.</p> PARAMETER DESCRIPTION <code>unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> YIELDS DESCRIPTION <code>Interval</code> <p>The Memory Interval. The start and end memory will not be</p> <code>Interval</code> <p>valid until the context manager is exited.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\n@contextmanager\ndef measure(\n    cls,\n    unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; Iterator[Memory.Interval]:\n    \"\"\"Measure the memory used by a block of code.\n\n    Args:\n        unit: The unit of memory to use.\n\n    Yields:\n        The Memory Interval. The start and end memory will not be\n        valid until the context manager is exited.\n\n    \"\"\"\n    mem = cls.start(unit=unit)\n\n    interval = Memory.na()\n    interval.unit = mem.unit\n\n    try:\n        yield interval\n    finally:\n        _interval = mem.stop()\n        interval.start_vms = _interval.start_vms\n        interval.end_vms = _interval.end_vms\n        interval.start_rss = _interval.start_rss\n        interval.end_rss = _interval.end_rss\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.na","title":"na  <code>classmethod</code>","text":"<pre><code>na() -&gt; Interval\n</code></pre> <p>Create a memory interval that represents NA.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef na(cls) -&gt; Memory.Interval:\n    \"\"\"Create a memory interval that represents NA.\"\"\"\n    return Memory.Interval(\n        start_vms=np.nan,\n        end_vms=np.nan,\n        start_rss=np.nan,\n        end_rss=np.nan,\n        unit=pd.NA,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.start","title":"start  <code>classmethod</code>","text":"<pre><code>start(\n    unit: Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\"\n) -&gt; Memory\n</code></pre> <p>Start a memory tracker.</p> PARAMETER DESCRIPTION <code>unit</code> <p>The unit of memory to use (bytes).</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> RETURNS DESCRIPTION <code>Memory</code> <p>The Memory tracker.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef start(\n    cls,\n    unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; Memory:\n    \"\"\"Start a memory tracker.\n\n    Args:\n        unit: The unit of memory to use (bytes).\n\n    Returns:\n        The Memory tracker.\n    \"\"\"\n    proc = psutil.Process()\n    info = proc.memory_info()\n\n    return Memory(\n        start_vms=Memory.convert(info.vms, frm=Memory.Unit.BYTES, to=unit),\n        start_rss=Memory.convert(info.rss, frm=Memory.Unit.BYTES, to=unit),\n        unit=unit if isinstance(unit, Memory.Unit) else Memory.Unit.from_str(unit),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.stop","title":"stop","text":"<pre><code>stop() -&gt; Interval\n</code></pre> <p>Stop the memory tracker.</p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory interval.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>def stop(self) -&gt; Memory.Interval:\n    \"\"\"Stop the memory tracker.\n\n    Returns:\n        The memory interval.\n    \"\"\"\n    proc = psutil.Process()\n    info = proc.memory_info()\n\n    return Memory.Interval(\n        start_vms=self.start_vms,\n        start_rss=self.start_rss,\n        end_vms=Memory.convert(info.vms, frm=Memory.Unit.BYTES, to=self.unit),\n        end_rss=Memory.convert(info.rss, frm=Memory.Unit.BYTES, to=self.unit),\n        unit=self.unit,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.usage","title":"usage  <code>classmethod</code>","text":"<pre><code>usage(\n    kind: Literal[\"rss\", \"vms\"] = \"vms\",\n    unit: Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; float\n</code></pre> <p>Get the memory used.</p> PARAMETER DESCRIPTION <code>kind</code> <p>The type of memory to measure.</p> <p> TYPE: <code>Literal['rss', 'vms']</code> DEFAULT: <code>'vms'</code> </p> <code>unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The memory used.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef usage(\n    cls,\n    kind: Literal[\"rss\", \"vms\"] = \"vms\",\n    unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; float:\n    \"\"\"Get the memory used.\n\n    Args:\n        kind: The type of memory to measure.\n        unit: The unit of memory to use.\n\n    Returns:\n        The memory used.\n    \"\"\"\n    proc = psutil.Process()\n    _kind = kind.lower()\n    if _kind == \"rss\":\n        return Memory.convert(\n            proc.memory_info().rss,\n            frm=Memory.Unit.BYTES,\n            to=unit,\n        )\n\n    if _kind == \"vms\":\n        return Memory.convert(\n            proc.memory_info().vms,\n            frm=Memory.Unit.BYTES,\n            to=unit,\n        )\n\n    raise ValueError(f\"Unknown memory type: {kind}\")\n</code></pre>"},{"location":"api/amltk/profiling/profiler/","title":"Profiler","text":""},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler","title":"amltk.profiling.profiler","text":"<p>The profiler module provides classes for profiling code.</p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile","title":"Profile  <code>dataclass</code>","text":"<pre><code>Profile(timer: Timer, memory: Memory)\n</code></pre> <p>A profiler for measuring statistics between two events.</p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.Interval","title":"Interval  <code>dataclass</code>","text":"<pre><code>Interval(memory: Interval, time: Interval)\n</code></pre> <p>A class for representing a profiled interval.</p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.Interval.to_dict","title":"to_dict","text":"<pre><code>to_dict(*, prefix: str = '') -&gt; dict[str, Any]\n</code></pre> <p>Convert the profile interval to a dictionary.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def to_dict(self, *, prefix: str = \"\") -&gt; dict[str, Any]:\n    \"\"\"Convert the profile interval to a dictionary.\"\"\"\n    _prefix = \"\" if prefix == \"\" else f\"{prefix}:\"\n    return {\n        **self.memory.to_dict(prefix=f\"{_prefix}memory:\"),\n        **self.time.to_dict(prefix=f\"{_prefix}time:\"),\n    }\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(d: Mapping[str, Any]) -&gt; Interval\n</code></pre> <p>Create a profile interval from a dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary to create from.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Interval</code> <p>The profile interval.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Profile.Interval:\n    \"\"\"Create a profile interval from a dictionary.\n\n    Args:\n        d: The dictionary to create from.\n\n    Returns:\n        The profile interval.\n    \"\"\"\n    return Profile.Interval(\n        memory=Memory.from_dict(mapping_select(d, \"memory:\")),\n        time=Timer.from_dict(mapping_select(d, \"time:\")),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.measure","title":"measure  <code>classmethod</code>","text":"<pre><code>measure(\n    *,\n    memory_unit: (\n        Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"]\n    ) = \"B\",\n    time_kind: (\n        Kind | Literal[\"wall\", \"cpu\", \"process\"]\n    ) = \"wall\"\n) -&gt; Iterator[Interval]\n</code></pre> <p>Profile a block of code.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER DESCRIPTION <code>memory_unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>time_kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process']</code> DEFAULT: <code>'wall'</code> </p> YIELDS DESCRIPTION <code>Interval</code> <p>The Profiler Interval. Memory and Timings will not be valid until</p> <code>Interval</code> <p>the context manager is exited.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\n@contextmanager\ndef measure(\n    cls,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] = \"wall\",\n) -&gt; Iterator[Profile.Interval]:\n    \"\"\"Profile a block of code.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        memory_unit: The unit of memory to use.\n        time_kind: The type of timer to use.\n\n    Yields:\n        The Profiler Interval. Memory and Timings will not be valid until\n        the context manager is exited.\n    \"\"\"\n    with Memory.measure(unit=memory_unit) as memory, Timer.time(\n        kind=time_kind,\n    ) as timer:\n        yield Profile.Interval(memory=memory, time=timer)\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.na","title":"na  <code>classmethod</code>","text":"<pre><code>na() -&gt; Interval\n</code></pre> <p>Create a profile interval that represents NA.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\ndef na(cls) -&gt; Profile.Interval:\n    \"\"\"Create a profile interval that represents NA.\"\"\"\n    return Profile.Interval(memory=Memory.na(), time=Timer.na())\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.start","title":"start  <code>classmethod</code>","text":"<pre><code>start(\n    memory_unit: (\n        Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"]\n    ) = \"B\",\n    time_kind: (\n        Kind | Literal[\"wall\", \"cpu\", \"process\"]\n    ) = \"wall\",\n) -&gt; Profile\n</code></pre> <p>Start a memory tracker.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER DESCRIPTION <code>memory_unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>time_kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process']</code> DEFAULT: <code>'wall'</code> </p> RETURNS DESCRIPTION <code>Profile</code> <p>The Memory tracker.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\ndef start(\n    cls,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] = \"wall\",\n) -&gt; Profile:\n    \"\"\"Start a memory tracker.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        memory_unit: The unit of memory to use.\n        time_kind: The type of timer to use.\n\n    Returns:\n        The Memory tracker.\n    \"\"\"\n    return Profile(\n        timer=Timer.start(kind=time_kind),\n        memory=Memory.start(unit=memory_unit),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.stop","title":"stop","text":"<pre><code>stop() -&gt; Interval\n</code></pre> <p>Stop the memory tracker.</p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory interval.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def stop(self) -&gt; Profile.Interval:\n    \"\"\"Stop the memory tracker.\n\n    Returns:\n        The memory interval.\n    \"\"\"\n    return Profile.Interval(\n        memory=self.memory.stop(),\n        time=self.timer.stop(),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler","title":"Profiler  <code>dataclass</code>","text":"<pre><code>Profiler(\n    profiles: dict[str, Interval] = dict(),\n    memory_unit: (\n        Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"]\n    ) = \"B\",\n    time_kind: (\n        Kind | Literal[\"wall\", \"cpu\", \"process\"]\n    ) = \"wall\",\n    disabled: bool = False,\n    _running: deque[str] = deque(),\n)\n</code></pre> <p>               Bases: <code>Mapping[str, Interval]</code></p> <p>Profile and record various events.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER DESCRIPTION <code>memory_unit</code> <p>The default unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>time_kind</code> <p>The default type of timer to use.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process']</code> DEFAULT: <code>'wall'</code> </p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__call__","title":"__call__","text":"<pre><code>__call__(\n    name: str,\n    *,\n    memory_unit: (\n        Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None\n    ) = None,\n    time_kind: (\n        Kind | Literal[\"wall\", \"cpu\", \"process\"] | None\n    ) = None\n) -&gt; Iterator[None]\n</code></pre> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@contextmanager\ndef __call__(\n    self,\n    name: str,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"::: amltk.profiling.Profiler.measure\"\"\"  # noqa: D415\n    with self.measure(name, memory_unit=memory_unit, time_kind=time_kind):\n        yield\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.Profiler.measure","title":"amltk.profiling.Profiler.measure","text":"<pre><code>measure(\n    name: str,\n    *,\n    memory_unit: (\n        Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None\n    ) = None,\n    time_kind: (\n        Kind | Literal[\"wall\", \"cpu\", \"process\"] | None\n    ) = None\n) -&gt; Iterator[None]\n</code></pre> <p>Profile a block of code. Store the result on this object.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER DESCRIPTION <code>name</code> <p>The name of the profile.</p> <p> TYPE: <code>str</code> </p> <code>memory_unit</code> <p>The unit of memory to use. Overwrites the default.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> <code>time_kind</code> <p>The type of timer to use. Overwrites the default.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@contextmanager\ndef measure(\n    self,\n    name: str,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"Profile a block of code. Store the result on this object.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        name: The name of the profile.\n        memory_unit: The unit of memory to use. Overwrites the default.\n        time_kind: The type of timer to use. Overwrites the default.\n    \"\"\"\n    if self.disabled:\n        yield\n        return\n\n    memory_unit = memory_unit or self.memory_unit\n    time_kind = time_kind or self.time_kind\n\n    self._running.append(name)\n    entry_name = \":\".join(self._running)\n\n    with Profile.measure(memory_unit=memory_unit, time_kind=time_kind) as profile:\n        self.profiles[entry_name] = profile\n        yield\n\n    self._running.pop()\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; Interval\n</code></pre> <p>Get a profile interval.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@override\ndef __getitem__(self, key: str) -&gt; Profile.Interval:\n    \"\"\"Get a profile interval.\"\"\"\n    return self.profiles[key]\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Iterate over the profile names.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@override\ndef __iter__(self) -&gt; Iterator[str]:\n    \"\"\"Iterate over the profile names.\"\"\"\n    return iter(self.profiles)\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get the number of profiles.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@override\ndef __len__(self) -&gt; int:\n    \"\"\"Get the number of profiles.\"\"\"\n    return len(self.profiles)\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__rich__","title":"__rich__","text":"<pre><code>__rich__() -&gt; RenderableType\n</code></pre> <p>Render the profiler.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def __rich__(self) -&gt; RenderableType:\n    \"\"\"Render the profiler.\"\"\"\n    from amltk._richutil import df_to_table\n\n    _df = self.df()\n    return df_to_table(_df, title=\"Profiler\", index_style=\"bold\")\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.df","title":"df","text":"<pre><code>df() -&gt; DataFrame\n</code></pre> <p>Convert the profiler to a dataframe.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the profiler to a dataframe.\"\"\"\n    return pd.DataFrame.from_dict(\n        {k: v.to_dict() for k, v in self.profiles.items()},\n        orient=\"index\",\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.disable","title":"disable","text":"<pre><code>disable() -&gt; None\n</code></pre> <p>Disable the profiler.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def disable(self) -&gt; None:\n    \"\"\"Disable the profiler.\"\"\"\n    self.disabled = True\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.each","title":"each","text":"<pre><code>each(\n    itr: Iterable[T],\n    *,\n    name: str,\n    itr_name: str | Callable[[int, T], str] | None = None\n) -&gt; Iterator[T]\n</code></pre> <p>Profile each item in an iterable.</p> PARAMETER DESCRIPTION <code>itr</code> <p>The iterable to profile.</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>name</code> <p>The name of the profile that lasts until iteration is complete</p> <p> TYPE: <code>str</code> </p> <code>itr_name</code> <p>The name of the profile for each iteration. If a function is provided, it will be called with each item's index and the item. It should return a string. If <code>None</code> is provided, just the index will be used.</p> <p> TYPE: <code>str | Callable[[int, T], str] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>T</code> <p>The the items</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def each(\n    self,\n    itr: Iterable[T],\n    *,\n    name: str,\n    itr_name: str | Callable[[int, T], str] | None = None,\n) -&gt; Iterator[T]:\n    \"\"\"Profile each item in an iterable.\n\n    Args:\n        itr: The iterable to profile.\n        name: The name of the profile that lasts until iteration is complete\n        itr_name: The name of the profile for each iteration.\n            If a function is provided, it will be called with each item's index\n            and the item. It should return a string. If `None` is provided,\n            just the index will be used.\n\n    Yields:\n        The the items\n    \"\"\"\n    match itr_name:\n        case None:\n            _itr_name = lambda i, _: str(i)\n        case str():\n            _itr_name = lambda i, _: f\"{itr_name}_{i}\"\n        case _:\n            _itr_name = itr_name\n\n    with self.measure(name=name):\n        for i, item in enumerate(itr):\n            with self.measure(name=_itr_name(i, item)):\n                yield item\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.enable","title":"enable","text":"<pre><code>enable() -&gt; None\n</code></pre> <p>Enable the profiler.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def enable(self) -&gt; None:\n    \"\"\"Enable the profiler.\"\"\"\n    self.disabled = False\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.measure","title":"measure","text":"<pre><code>measure(\n    name: str,\n    *,\n    memory_unit: (\n        Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None\n    ) = None,\n    time_kind: (\n        Kind | Literal[\"wall\", \"cpu\", \"process\"] | None\n    ) = None\n) -&gt; Iterator[None]\n</code></pre> <p>Profile a block of code. Store the result on this object.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER DESCRIPTION <code>name</code> <p>The name of the profile.</p> <p> TYPE: <code>str</code> </p> <code>memory_unit</code> <p>The unit of memory to use. Overwrites the default.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> <code>time_kind</code> <p>The type of timer to use. Overwrites the default.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@contextmanager\ndef measure(\n    self,\n    name: str,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"Profile a block of code. Store the result on this object.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        name: The name of the profile.\n        memory_unit: The unit of memory to use. Overwrites the default.\n        time_kind: The type of timer to use. Overwrites the default.\n    \"\"\"\n    if self.disabled:\n        yield\n        return\n\n    memory_unit = memory_unit or self.memory_unit\n    time_kind = time_kind or self.time_kind\n\n    self._running.append(name)\n    entry_name = \":\".join(self._running)\n\n    with Profile.measure(memory_unit=memory_unit, time_kind=time_kind) as profile:\n        self.profiles[entry_name] = profile\n        yield\n\n    self._running.pop()\n</code></pre>"},{"location":"api/amltk/profiling/timing/","title":"Timing","text":""},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing","title":"amltk.profiling.timing","text":"<p>Module for timing things.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer","title":"Timer  <code>dataclass</code>","text":"<pre><code>Timer(start_time: float, kind: Kind | NAType)\n</code></pre> <p>A timer for measuring the time between two events.</p> ATTRIBUTE DESCRIPTION <code>start_time</code> <p>The time at which the timer was started.</p> <p> TYPE: <code>float</code> </p> <code>kind</code> <p>The method of timing.</p> <p> TYPE: <code>Kind | NAType</code> </p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Interval","title":"Interval  <code>dataclass</code>","text":"<pre><code>Interval(\n    start: float,\n    end: float,\n    kind: Kind | NAType,\n    unit: Unit | NAType,\n)\n</code></pre> <p>A time interval.</p> ATTRIBUTE DESCRIPTION <code>start</code> <p>The start time.</p> <p> TYPE: <code>float</code> </p> <code>end</code> <p>The end time.</p> <p> TYPE: <code>float</code> </p> <code>kind</code> <p>The type of timer used.</p> <p> TYPE: <code>Kind | NAType</code> </p> <code>unit</code> <p>The unit of time.</p> <p> TYPE: <code>Unit | NAType</code> </p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Interval.duration","title":"duration  <code>property</code>","text":"<pre><code>duration: float\n</code></pre> <p>The duration of the time interval.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Interval.to_dict","title":"to_dict","text":"<pre><code>to_dict(*, prefix: str = '') -&gt; dict[str, Any]\n</code></pre> <p>Convert the time interval to a dictionary.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>def to_dict(self, *, prefix: str = \"\") -&gt; dict[str, Any]:\n    \"\"\"Convert the time interval to a dictionary.\"\"\"\n    return {\n        f\"{prefix}start\": self.start,\n        f\"{prefix}end\": self.end,\n        f\"{prefix}duration\": self.duration,\n        f\"{prefix}kind\": self.kind,\n        f\"{prefix}unit\": self.unit,\n    }\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Kind","title":"Kind","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>An enum for the type of timer.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Kind.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(key: Any) -&gt; Kind | NAType\n</code></pre> <p>Get the enum value from a string.</p> PARAMETER DESCRIPTION <code>key</code> <p>The string to convert.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Kind | NAType</code> <p>The enum value.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef from_str(cls, key: Any) -&gt; Timer.Kind | NAType:\n    \"\"\"Get the enum value from a string.\n\n    Args:\n        key: The string to convert.\n\n    Returns:\n        The enum value.\n    \"\"\"\n    if isinstance(key, Timer.Kind):\n        return key\n\n    if isinstance(key, str):\n        try:\n            return Timer.Kind[key.upper()]\n        except KeyError:\n            return pd.NA\n\n    return pd.NA\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Unit","title":"Unit","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>An enum for the units of time.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Unit.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(key: Any) -&gt; Unit | NAType\n</code></pre> <p>Get the enum value from a string.</p> PARAMETER DESCRIPTION <code>key</code> <p>The string to convert.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Unit | NAType</code> <p>The enum value.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef from_str(cls, key: Any) -&gt; Timer.Unit | NAType:\n    \"\"\"Get the enum value from a string.\n\n    Args:\n        key: The string to convert.\n\n    Returns:\n        The enum value.\n    \"\"\"\n    if isinstance(key, Timer.Unit):\n        return key\n\n    if isinstance(key, str):\n        try:\n            return Timer.Unit[key.upper()]\n        except KeyError:\n            return pd.NA\n\n    return pd.NA\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(d: Mapping[str, Any]) -&gt; Interval\n</code></pre> <p>Create a time interval from a dictionary.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Timer.Interval:\n    \"\"\"Create a time interval from a dictionary.\"\"\"\n    return Timer.Interval(\n        start=dict_get_not_none(d, \"start\", np.nan),\n        end=dict_get_not_none(d, \"end\", np.nan),\n        kind=Timer.Kind.from_str(dict_get_not_none(d, \"kind\", pd.NA)),\n        unit=Timer.Unit.from_str(dict_get_not_none(d, \"unit\", pd.NA)),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.na","title":"na  <code>classmethod</code>","text":"<pre><code>na() -&gt; Interval\n</code></pre> <p>Create a time interval with all values set to <code>None</code>.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef na(cls) -&gt; Timer.Interval:\n    \"\"\"Create a time interval with all values set to `None`.\"\"\"\n    return Timer.Interval(\n        start=np.nan,\n        end=np.nan,\n        kind=pd.NA,\n        unit=pd.NA,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.start","title":"start  <code>classmethod</code>","text":"<pre><code>start(\n    kind: Kind | Literal[\"cpu\", \"wall\", \"process\"] = \"wall\"\n) -&gt; Timer\n</code></pre> <p>Start a timer.</p> PARAMETER DESCRIPTION <code>kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['cpu', 'wall', 'process']</code> DEFAULT: <code>'wall'</code> </p> RETURNS DESCRIPTION <code>Timer</code> <p>The timer.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef start(\n    cls,\n    kind: Timer.Kind | Literal[\"cpu\", \"wall\", \"process\"] = \"wall\",\n) -&gt; Timer:\n    \"\"\"Start a timer.\n\n    Args:\n        kind: The type of timer to use.\n\n    Returns:\n        The timer.\n    \"\"\"\n    if kind in (Timer.Kind.WALL, \"wall\"):\n        return Timer(time.time(), Timer.Kind.WALL)\n\n    if kind in (Timer.Kind.CPU, \"cpu\"):\n        return Timer(time.perf_counter(), Timer.Kind.CPU)\n\n    if kind in (Timer.Kind.PROCESS, \"process\"):\n        return Timer(time.process_time(), Timer.Kind.PROCESS)\n\n    raise ValueError(f\"Unknown timer type: {kind}\")\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.stop","title":"stop","text":"<pre><code>stop() -&gt; Interval\n</code></pre> <p>Stop the timer.</p> RETURNS DESCRIPTION <code>Interval</code> <p>A tuple of the start time, end time, and duration.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>def stop(self) -&gt; Interval:\n    \"\"\"Stop the timer.\n\n    Returns:\n        A tuple of the start time, end time, and duration.\n    \"\"\"\n    if self.kind == Timer.Kind.WALL:\n        end = time.time()\n        return Timer.Interval(\n            self.start_time,\n            end,\n            Timer.Kind.WALL,\n            Timer.Unit.SECONDS,\n        )\n\n    if self.kind == Timer.Kind.CPU:\n        end = time.perf_counter()\n        return Timer.Interval(\n            self.start_time,\n            end,\n            Timer.Kind.CPU,\n            Timer.Unit.SECONDS,\n        )\n\n    if self.kind == Timer.Kind.PROCESS:\n        end = time.process_time()\n        return Timer.Interval(\n            self.start_time,\n            end,\n            Timer.Kind.PROCESS,\n            Timer.Unit.SECONDS,\n        )\n\n    raise ValueError(f\"Unknown timer type: {self.kind}\")\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.time","title":"time  <code>classmethod</code>","text":"<pre><code>time(\n    kind: Kind | Literal[\"cpu\", \"wall\", \"process\"] = \"wall\"\n) -&gt; Iterator[Interval]\n</code></pre> <p>Time a block of code.</p> PARAMETER DESCRIPTION <code>kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['cpu', 'wall', 'process']</code> DEFAULT: <code>'wall'</code> </p> YIELDS DESCRIPTION <code>Interval</code> <p>The Time Interval. The start and end times will not be</p> <code>Interval</code> <p>valid until the context manager is exited.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\n@contextmanager\ndef time(\n    cls,\n    kind: Timer.Kind | Literal[\"cpu\", \"wall\", \"process\"] = \"wall\",\n) -&gt; Iterator[Interval]:\n    \"\"\"Time a block of code.\n\n    Args:\n        kind: The type of timer to use.\n\n    Yields:\n        The Time Interval. The start and end times will not be\n        valid until the context manager is exited.\n    \"\"\"\n    timer = cls.start(kind=kind)\n\n    interval = Timer.na()\n    interval.kind = timer.kind\n    interval.unit = Timer.Unit.SECONDS\n\n    try:\n        yield interval\n    finally:\n        _interval = timer.stop()\n        interval.start = _interval.start\n        interval.end = _interval.end\n</code></pre>"},{"location":"api/amltk/pytorch/builders/","title":"Builders","text":""},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders","title":"amltk.pytorch.builders","text":"<p>This module contains functionality to construct a pytorch model from a pipeline.</p> <p>It also includes classes for handling dimension matching between layers.</p>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchChosenDimensions","title":"MatchChosenDimensions  <code>dataclass</code>","text":"<pre><code>MatchChosenDimensions(\n    choice_name: str, choices: Mapping[str, Any]\n)\n</code></pre> <p>Handles matching dimensions for chosen nodes in a pipeline.</p> <p>This class helps ensure compatibility between layers with search spaces during HPO optimization. It takes the choice name and the corresponding dimensions for that choice and stores them for later reference.</p>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchChosenDimensions.choice_name","title":"choice_name  <code>instance-attribute</code>","text":"<pre><code>choice_name: str\n</code></pre> <p>The name of the choice node.</p>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchChosenDimensions.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: Mapping[str, Any]\n</code></pre> <p>The mapping of choice taken to the dimension to use.</p>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchChosenDimensions.collect_chosen_nodes_names","title":"collect_chosen_nodes_names  <code>staticmethod</code>","text":"<pre><code>collect_chosen_nodes_names(\n    pipeline: Node,\n) -&gt; dict[str, str]\n</code></pre> <p>Collects the names of chosen nodes in the pipeline.</p> <p>Each pipeline has a unique set of chosen nodes, which we collect separately to handle dimension matching between layers with search spaces.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline containing the model architecture.</p> <p> TYPE: <code>Node</code> </p> RETURNS DESCRIPTION <code>dict[str, str]</code> <p>The names of the chosen nodes in the pipeline.</p> Source code in <code>src/amltk/pytorch/builders.py</code> <pre><code>@staticmethod\ndef collect_chosen_nodes_names(pipeline: Node) -&gt; dict[str, str]:\n    \"\"\"Collects the names of chosen nodes in the pipeline.\n\n    Each pipeline has a unique set of chosen nodes, which we collect separately\n    to handle dimension matching between layers with search spaces.\n\n    Args:\n        pipeline: The pipeline containing the model architecture.\n\n    Returns:\n        The names of the chosen nodes in the pipeline.\n    \"\"\"\n    chosen_nodes_names = {}  # Class variable to store chosen node names\n\n    for node in pipeline.iter():\n        if isinstance(node, Choice):\n            chosen_node = node.chosen()\n            if chosen_node:\n                chosen_nodes_names[node.name] = chosen_node.name\n\n    return chosen_nodes_names\n</code></pre>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchChosenDimensions.evaluate","title":"evaluate","text":"<pre><code>evaluate(chosen_nodes: dict[str, str]) -&gt; int\n</code></pre> <p>Retrieves the corresponding dimension for the chosen node.</p> <p>If the chosen node is not found in the choices dictionary, an error is raised. If the dimensions provided are not valid, an error is not raised. It is up to the user to ensure that the dimensions are valid.</p> PARAMETER DESCRIPTION <code>chosen_nodes</code> <p>The chosen nodes.</p> <p> TYPE: <code>dict[str, str]</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The value of the matching dimension for a chosen node.</p> Source code in <code>src/amltk/pytorch/builders.py</code> <pre><code>def evaluate(self, chosen_nodes: dict[str, str]) -&gt; int:\n    \"\"\"Retrieves the corresponding dimension for the chosen node.\n\n    If the chosen node is not found in the choices dictionary, an error is raised.\n    If the dimensions provided are not valid, an error is not raised.\n    It is up to the user to ensure that the dimensions are valid.\n\n    Args:\n        chosen_nodes: The chosen nodes.\n\n    Returns:\n        The value of the matching dimension for a chosen node.\n    \"\"\"\n    chosen_node_name = chosen_nodes.get(self.choice_name, None)\n\n    if chosen_node_name is None:\n        raise MatchChosenDimensionsError(self.choice_name, chosen_node_name=None)\n\n    try:\n        return self.choices[chosen_node_name]\n    except KeyError as e:\n        raise MatchChosenDimensionsError(self.choice_name, chosen_node_name) from e\n</code></pre>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchDimensions","title":"MatchDimensions  <code>dataclass</code>","text":"<pre><code>MatchDimensions(layer_name: str, param: str)\n</code></pre> <p>Handles matching dimensions between layers in a pipeline.</p> <p>This class helps ensure compatibility between layers with search spaces during HPO optimization. It takes the layer name and parameter name and stores them for later reference.</p> <p>Not intended to be used inside a Choice node.</p>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchDimensions.layer_name","title":"layer_name  <code>instance-attribute</code>","text":"<pre><code>layer_name: str\n</code></pre> <p>The name of the layer.</p>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchDimensions.param","title":"param  <code>instance-attribute</code>","text":"<pre><code>param: str\n</code></pre> <p>The name of the parameter to match.</p>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.MatchDimensions.evaluate","title":"evaluate","text":"<pre><code>evaluate(pipeline: Node) -&gt; int\n</code></pre> <p>Retrieves the corresponding configuration value from the pipeline.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline to search for the matching configuration.</p> <p> TYPE: <code>Node</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The value of the matching configuration parameter.</p> Source code in <code>src/amltk/pytorch/builders.py</code> <pre><code>def evaluate(self, pipeline: Node) -&gt; int:\n    \"\"\"Retrieves the corresponding configuration value from the pipeline.\n\n    Args:\n        pipeline: The pipeline to search for the matching configuration.\n\n    Returns:\n        The value of the matching configuration parameter.\n    \"\"\"\n    layer = pipeline[self.layer_name]\n    layer_config = layer.config\n    if layer_config is None:\n        raise MatchDimensionsError(self.layer_name, None)\n    value = layer_config.get(self.param)\n    if value is None:\n        raise MatchDimensionsError(self.layer_name, self.param)\n    return value\n</code></pre>"},{"location":"api/amltk/pytorch/builders/#amltk.pytorch.builders.build_model_from_pipeline","title":"build_model_from_pipeline","text":"<pre><code>build_model_from_pipeline(pipeline: Node) -&gt; Module\n</code></pre> <p>Builds a model from the provided pipeline.</p> <p>This function iterates through the pipeline nodes, constructing the model layers dynamically based on the node types and configurations. It also utilizes the <code>MatchDimensions</code> and <code>MatchChosenDimensions</code> objects to handle dimension matching between layers with search spaces.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The pipeline containing the model architecture.</p> <p> TYPE: <code>Node</code> </p> RETURNS DESCRIPTION <code>Module</code> <p>The constructed PyTorch model.</p> Source code in <code>src/amltk/pytorch/builders.py</code> <pre><code>def build_model_from_pipeline(pipeline: Node, /) -&gt; nn.Module:\n    \"\"\"Builds a model from the provided pipeline.\n\n    This function iterates through the pipeline nodes, constructing the model\n    layers dynamically based on the node types and configurations. It also\n    utilizes the `MatchDimensions` and `MatchChosenDimensions` objects to handle\n    dimension matching between layers with search spaces.\n\n    Args:\n        pipeline: The pipeline containing the model architecture.\n\n    Returns:\n        The constructed PyTorch model.\n    \"\"\"\n    model_layers = []\n\n    # Mapping of choice node names to what was chosen for that choice\n    chosen_nodes_names = MatchChosenDimensions.collect_chosen_nodes_names(pipeline)\n\n    # NOTE: pipeline.iter() may not be sufficient as we relying on some implied ordering\n    # for this to work, i.e. we might not know when we're iterating through nodes of a\n    # Join or Split\n    for node in pipeline.iter(skip_unchosen=True):\n        match node:\n            case Component(config=config):\n                layer_config = dict(config) if config else {}\n\n                for key, instance in layer_config.items():\n                    match instance:\n                        case MatchDimensions():\n                            layer_config[key] = instance.evaluate(pipeline)\n                        case MatchChosenDimensions():\n                            layer_config[key] = instance.evaluate(chosen_nodes_names)\n                        case _:\n                            # Just used the value directly\n                            pass\n\n                layer = node.build_item(**layer_config)\n                model_layers.append(layer)\n            # Check if node is a Fixed layer (e.g., Flatten, ReLU),\n            # Flatten layer or any other layer without config parameter\n            case Node(item=built_object) if built_object is not None:\n                model_layers.append(built_object)\n            case Sequential() | Choice():\n                pass  # Skip these as it will come up in iteration...\n            case _:\n                # TODO: Support other node types\n                raise NotImplementedError(f\"Node type {type(node)} not supported yet.\")\n\n    return nn.Sequential(*model_layers)\n</code></pre>"},{"location":"api/amltk/scheduling/events/","title":"Events","text":""},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events","title":"amltk.scheduling.events","text":"<p>THe event system in AMLTK.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter","title":"Emitter","text":"<pre><code>Emitter(name: str | None = None)\n</code></pre> <p>An event emitter.</p> <p>This class is used to emit events and register callbacks for those events. It also provides a convenience function <code>subscriber()</code> such that objects using an <code>Emitter</code> can easily create access points for users to directly subscribe to their <code>Events</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the emitter. If not provided, then a UUID will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def __init__(self, name: str | None = None) -&gt; None:\n    \"\"\"Initialise the emitter.\n\n    Args:\n        name: The name of the emitter. If not provided, then a UUID\n            will be used.\n    \"\"\"\n    super().__init__()\n    self.unique_ref = f\"{name}-{randuid()}\"\n    self.emitted_events: set[Event] = set()\n\n    self.name = name\n    self.handlers = defaultdict(list)\n    self.event_counts = Counter()\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.HandlerResponses","title":"HandlerResponses  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HandlerResponses: TypeAlias = Iterable[\n    tuple[Handler[P, R], R | None]\n]\n</code></pre> <p>The stream of responses from handlers when an event is triggered.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.event_counts","title":"event_counts  <code>instance-attribute</code>","text":"<pre><code>event_counts: Counter[Event] = Counter()\n</code></pre> <p>A count of all events emitted by this emitter.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.events","title":"events  <code>property</code>","text":"<pre><code>events: list[Event]\n</code></pre> <p>Return a list of the events.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.handlers","title":"handlers  <code>instance-attribute</code>","text":"<pre><code>handlers: dict[Event, list[Handler]] = defaultdict(list)\n</code></pre> <p>A mapping of events to their handlers.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str | None = name\n</code></pre> <p>The name of the emitter.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.add_event","title":"add_event","text":"<pre><code>add_event(*event: Event) -&gt; None\n</code></pre> <p>Add an event to the event manager so that it shows up in visuals.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to add.</p> <p> TYPE: <code>Event</code> DEFAULT: <code>()</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def add_event(self, *event: Event) -&gt; None:\n    \"\"\"Add an event to the event manager so that it shows up in visuals.\n\n    Args:\n        event: The event to add.\n    \"\"\"\n    for e in event:\n        if e not in self.handlers:\n            self.handlers[e] = []\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.as_event","title":"as_event","text":"<pre><code>as_event(key: str | Event) -&gt; Event\n</code></pre> <p>Return the event associated with the key.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def as_event(self, key: str | Event) -&gt; Event:\n    \"\"\"Return the event associated with the key.\"\"\"\n    match key:\n        case Event():\n            return key\n        case str():\n            match = first_true(self.events, None, lambda e: e.name == key)\n            if match is None:\n                raise EventNotKnownError(\n                    f\"{key=} is not a valid event for {self.name}.\"\n                    f\"\\nKnown events are: {[e.name for e in self.events]}\",\n                )\n            return match\n        case _:\n            raise TypeError(f\"{key=} must be a string or an Event.\")\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.emit","title":"emit","text":"<pre><code>emit(\n    event: Event[P, R], *args: args, **kwargs: kwargs\n) -&gt; list[tuple[Handler[P, R], R | None]]\n</code></pre> <p>Emit an event.</p> <p>This will call all the handlers for the event.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to emit. If passing a list, then the handlers for all events will be called, regardless of the order</p> <p> TYPE: <code>Event[P, R]</code> </p> <code>*args</code> <p>The positional arguments to pass to the handlers.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to pass to the handlers.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>list[tuple[Handler[P, R], R | None]]</code> <p>A list of the results from the handlers.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def emit(\n    self,\n    event: Event[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; list[tuple[Handler[P, R], R | None]]:\n    \"\"\"Emit an event.\n\n    This will call all the handlers for the event.\n\n    Args:\n        event: The event to emit.\n            If passing a list, then the handlers for all events will be called,\n            regardless of the order\n        *args: The positional arguments to pass to the handlers.\n        **kwargs: The keyword arguments to pass to the handlers.\n\n    Returns:\n        A list of the results from the handlers.\n    \"\"\"\n    logger.debug(f\"{self.name}: Emitting {event}\")\n\n    self.event_counts[event] += 1\n    return [(handler, handler(*args, **kwargs)) for handler in self.handlers[event]]\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.on","title":"on","text":"<pre><code>on(\n    event: Event[P, R] | str,\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False\n) -&gt; Callable[[Callable[P, R | None]], None]\n</code></pre> <p>Register a callback for an event as a decorator.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P, R] | str</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def on(\n    self,\n    event: Event[P, R] | str,\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False,\n) -&gt; Callable[[Callable[P, R | None]], None]:\n    \"\"\"Register a callback for an event as a decorator.\n\n    Args:\n        event: The event to register the callback for.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n    \"\"\"\n    return partial(\n        self.subscriber(event=self.as_event(event)),  # type: ignore\n        when=when,\n        every=every,\n        repeat=repeat,\n        max_calls=max_calls,\n        hidden=hidden,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.register","title":"register","text":"<pre><code>register(\n    event: Event[P, R] | str,\n    callback: Callable[P, R],\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False\n) -&gt; None\n</code></pre> <p>Register a callback for an event.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P, R] | str</code> </p> <code>callback</code> <p>The callback to register.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def register(\n    self,\n    event: Event[P, R] | str,\n    callback: Callable[P, R],\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False,\n) -&gt; None:\n    \"\"\"Register a callback for an event.\n\n    Args:\n        event: The event to register the callback for.\n        callback: The callback to register.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n    \"\"\"\n    event = self.as_event(event)\n\n    if repeat &lt;= 0:\n        raise ValueError(f\"{repeat=} must be a positive integer.\")\n\n    if every &lt;= 0:\n        raise ValueError(f\"{every=} must be a positive integer.\")\n\n    # Make sure it shows up in the event counts, setting it to 0 if it\n    # doesn't exist\n    self.event_counts.setdefault(event, 0)\n    self.handlers[event].append(\n        Handler(\n            callback,\n            when=when,\n            every=every,\n            repeat=repeat,\n            max_calls=max_calls,\n            hidden=hidden,\n        ),\n    )\n\n    _name = funcname(callback)\n    msg = f\"{self.name}: Registered callback '{_name}' for event {event}\"\n    if every &gt; 1:\n        msg += f\" every {every} times\"\n    if when:\n        msg += f\" with predicate ({funcname(when)})\"\n    if repeat &gt; 1:\n        msg += f\" called {repeat} times successively\"\n    if hidden:\n        msg += \" (hidden from visual output)\"\n    logger.debug(msg)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.subscriber","title":"subscriber","text":"<pre><code>subscriber(event: Event[P, R]) -&gt; Subscriber[P, R]\n</code></pre> <p>Create a subscriber for an event.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P, R]</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def subscriber(self, event: Event[P, R]) -&gt; Subscriber[P, R]:\n    \"\"\"Create a subscriber for an event.\n\n    Args:\n        event: The event to register the callback for.\n    \"\"\"\n    if event not in self.handlers:\n        self.handlers[event] = []\n\n    return Subscriber(self, event)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Event","title":"Event  <code>dataclass</code>","text":"<pre><code>Event(name: str)\n</code></pre> <p>               Bases: <code>Generic[P, R]</code></p> <p>An event that can be emitted.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name of the event.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Event.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check if two events are equal.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>@override\ndef __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check if two events are equal.\"\"\"\n    if isinstance(other, str):\n        return self.name == other\n    if isinstance(other, Event):\n        return self.name == other.name\n\n    return False\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Handler","title":"Handler  <code>dataclass</code>","text":"<pre><code>Handler(\n    callback: Callable[P, R],\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    n_calls_to_handler: int = 0,\n    n_calls_to_callback: int = 0,\n    max_calls: int | None = None,\n    repeat: int = 1,\n    registered_at: int = time.time_ns(),\n    hidden: bool = False,\n)\n</code></pre> <p>               Bases: <code>Generic[P, R]</code></p> <p>A handler for an event.</p> <p>This is a simple class that holds a callback and any predicate that must be satisfied for it to be triggered.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Handler.__call__","title":"__call__","text":"<pre><code>__call__(*args: args, **kwargs: kwargs) -&gt; R | None\n</code></pre> <p>Call the callback if the predicate is satisfied.</p> <p>If the predicate is not satisfied, then <code>None</code> is returned.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def __call__(self, *args: P.args, **kwargs: P.kwargs) -&gt; R | None:\n    \"\"\"Call the callback if the predicate is satisfied.\n\n    If the predicate is not satisfied, then `None` is returned.\n    \"\"\"\n    self.n_calls_to_handler += 1\n    if self.every &gt; 1 and self.n_calls_to_handler % self.every != 0:\n        return None\n\n    if self.when is not None and not self.when():\n        return None\n\n    max_calls = self.max_calls if self.max_calls is not None else math.inf\n\n    if self.repeat == 1:\n        if self.n_calls_to_callback &gt;= max_calls:\n            return None\n\n        self.n_calls_to_callback += 1\n        return self.callback(*args, **kwargs)\n\n    if self.n_calls_to_callback &gt;= max_calls:\n        return None\n\n    responses = iter(self.callback(*args, **kwargs) for _ in range(self.repeat))\n    self.n_calls_to_callback += 1\n    first_response = next(responses)\n    if first_response is not None:\n        raise ValueError(\"A callback with a response cannot have `repeat` &gt; 1.\")\n\n    # Otherwise just exhaust the iterator\n    list(responses)\n    return None\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.RegisteredTimeCallOrderStrategy","title":"RegisteredTimeCallOrderStrategy  <code>dataclass</code>","text":"<pre><code>RegisteredTimeCallOrderStrategy()\n</code></pre> <p>A calling strategy that calls callbacks in the order they were registered.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.RegisteredTimeCallOrderStrategy.execute","title":"execute  <code>classmethod</code>","text":"<pre><code>execute(\n    events: Iterable[\n        tuple[\n            Iterable[Handler[P, R]],\n            tuple[Any, ...] | None,\n            dict[str, Any] | None,\n        ]\n    ]\n) -&gt; list[tuple[Handler[P, R], R | None]]\n</code></pre> <p>Call all events in the scheduler.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>@classmethod\ndef execute(\n    cls,\n    events: Iterable[\n        tuple[\n            Iterable[Handler[P, R]],\n            tuple[Any, ...] | None,\n            dict[str, Any] | None,\n        ]\n    ],\n) -&gt; list[tuple[Handler[P, R], R | None]]:\n    \"\"\"Call all events in the scheduler.\"\"\"\n    all_handlers = []\n    for handlers, args, kwargs in events:\n        all_handlers += [\n            (handler, args or (), kwargs or {}) for handler in handlers\n        ]\n\n    sorted_handlers = sorted(all_handlers, key=lambda item: item[0].registered_at)\n    return [\n        (handler, handler(*args, **kwargs))\n        for handler, args, kwargs in sorted_handlers\n    ]\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber","title":"Subscriber  <code>dataclass</code>","text":"<pre><code>Subscriber(emitter: Emitter, event: Event[P, R])\n</code></pre> <p>               Bases: <code>Generic[P, R]</code></p> <p>An object that can be used to easily subscribe to a certain event.</p> <pre><code>from amltk.scheduling.events import Event, Subscriber\n\ntest_event: Event[[int, str]] = Event(\"test\")\n\nemitter = Emitter(\"hello world\")\nsubscribe = emitter.subscriber(test_event)\n\n# Use it as a decorator\n\n@subscribe\ndef callback(a: int, b: str) -&gt; None:\n    print(f\"Got {a} and {b}!\")\n\n# ... or just pass a function\n\nsubscribe(callback)\n\n# Will emit `test_event` with the arguments 1 and \"hello\"\n# and call the callback with those same arguments.\nemitter.emit(test_event, 1, \"hello\")\n</code></pre> ATTRIBUTE DESCRIPTION <code>manager</code> <p>The event manager to use.</p> <p> </p> <code>event</code> <p>The event to subscribe to.</p> <p> TYPE: <code>Event[P, R]</code> </p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber.event_counts","title":"event_counts  <code>property</code>","text":"<pre><code>event_counts: int\n</code></pre> <p>The number of times this event has been emitted.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber.__call__","title":"__call__","text":"<pre><code>__call__(\n    callback: Callable[P, R] | None = None,\n    *,\n    when: Callable[[], bool] | None = None,\n    max_calls: int | None = None,\n    repeat: int = 1,\n    every: int = 1,\n    hidden: bool = False\n) -&gt; Callable[[Callable[P, R]], None] | None\n</code></pre> <p>A decorator to register a callback for this subscriber.</p> PARAMETER DESCRIPTION <code>callback</code> <p>The callback to register. If <code>None</code>, then this acts as a decorator, as you would normally use it. Prefer to leave this as <code>None</code> and use <code>register()</code> if you have a direct reference to the function and are not decorating it.</p> <p> TYPE: <code>Callable[P, R] | None</code> DEFAULT: <code>None</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def __call__(\n    self,\n    callback: Callable[P, R] | None = None,\n    *,\n    when: Callable[[], bool] | None = None,\n    max_calls: int | None = None,\n    repeat: int = 1,\n    every: int = 1,\n    hidden: bool = False,\n) -&gt; Callable[[Callable[P, R]], None] | None:\n    \"\"\"A decorator to register a callback for this subscriber.\n\n    Args:\n        callback: The callback to register. If `None`, then this\n            acts as a decorator, as you would normally use it. Prefer\n            to leave this as `None` and use\n            [`register()`][amltk.scheduling.events.Subscriber.register] if\n            you have a direct reference to the function and are not decorating it.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n    \"\"\"\n    if callback is None:\n        return partial(\n            self.register,\n            when=when,\n            max_calls=max_calls,\n            repeat=repeat,\n            every=every,\n            hidden=hidden,\n        )\n    self.register(\n        callback,\n        when=when,\n        max_calls=max_calls,\n        repeat=repeat,\n        every=every,\n        hidden=hidden,\n    )\n    return None\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber.emit","title":"emit","text":"<pre><code>emit(\n    *args: args, **kwargs: kwargs\n) -&gt; list[tuple[Handler[P, R], R | None]]\n</code></pre> <p>Emit this subscribers event.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def emit(\n    self,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; list[tuple[Handler[P, R], R | None]]:\n    \"\"\"Emit this subscribers event.\"\"\"\n    return self.emitter.emit(self.event, *args, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber.register","title":"register","text":"<pre><code>register(\n    callback: Callable[P, R],\n    *,\n    when: Callable[[], bool] | None = None,\n    max_calls: int | None = None,\n    repeat: int = 1,\n    every: int = 1,\n    hidden: bool = False\n) -&gt; None\n</code></pre> <p>Register a callback for this subscriber.</p> PARAMETER DESCRIPTION <code>callback</code> <p>The callback to register.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def register(\n    self,\n    callback: Callable[P, R],\n    *,\n    when: Callable[[], bool] | None = None,\n    max_calls: int | None = None,\n    repeat: int = 1,\n    every: int = 1,\n    hidden: bool = False,\n) -&gt; None:\n    \"\"\"Register a callback for this subscriber.\n\n    Args:\n        callback: The callback to register.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n    \"\"\"\n    self.emitter.register(\n        event=self.event,\n        callback=callback,\n        when=when,\n        max_calls=max_calls,\n        repeat=repeat,\n        every=every,\n        hidden=hidden,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/","title":"Queue monitor","text":""},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor","title":"amltk.scheduling.queue_monitor","text":"<p>The queue monitoring.</p>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor","title":"QueueMonitor","text":"<pre><code>QueueMonitor(scheduler: Scheduler)\n</code></pre> <p>A monitor for the scheduler queue.</p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def __init__(self, scheduler: Scheduler) -&gt; None:\n    \"\"\"Initializes the monitor.\"\"\"\n    super().__init__()\n    self.scheduler = scheduler\n    self.data: list[QueueMonitorRecord] = []\n\n    scheduler.on_start(self.update)\n    scheduler.on_finishing(self.update)\n    scheduler.on_finished(self.update)\n    scheduler.on_future_submitted(self.update)\n    scheduler.on_future_cancelled(self.update)\n    scheduler.on_future_done(self.update)\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor.df","title":"df","text":"<pre><code>df(*, n_workers: int | None = None) -&gt; DataFrame\n</code></pre> <p>Converts the data to a pandas DataFrame.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers that were in use. This helps idenify how many workers were idle at a given time. If None, the maximum length of the queue at any recorded time is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def df(\n    self,\n    *,\n    n_workers: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Converts the data to a pandas DataFrame.\n\n    Args:\n        n_workers: The number of workers that were in use. This helps idenify how\n            many workers were idle at a given time. If None, the maximum length of\n            the queue at any recorded time is used.\n    \"\"\"\n    _df = pd.DataFrame(self.data, columns=list(QueueMonitorRecord._fields)).astype(\n        {\n            # Windows might have a weird default here but it should be 64 at least\n            \"time\": \"int64\",\n            \"queue_size\": int,\n            \"queued\": int,\n            \"finished\": int,\n            \"cancelled\": int,\n        },\n    )\n    if n_workers is None:\n        n_workers = int(_df[\"queue_size\"].max())\n    _df[\"idle\"] = n_workers - _df[\"queue_size\"]\n    _df[\"time\"] = pd.to_datetime(_df[\"time\"], unit=\"ns\", origin=\"unix\")\n    return _df.set_index(\"time\")\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor.plot","title":"plot","text":"<pre><code>plot(\n    *,\n    ax: Axes | None = None,\n    interval: tuple[int, UnitChoices] = (1, \"s\"),\n    n_workers: int | None = None,\n    **kwargs: Any\n) -&gt; Axes\n</code></pre> <p>Plots the data as a stacked barchart.</p> PARAMETER DESCRIPTION <code>ax</code> <p>The axes to plot on. If None, a new figure is created.</p> <p> TYPE: <code>Axes | None</code> DEFAULT: <code>None</code> </p> <code>interval</code> <p>The interval to use for the x-axis. The first value is the interval and the second value is the unit. Must be a valid pandas timedelta unit. See to_timedelta() for more information.</p> <p> TYPE: <code>tuple[int, UnitChoices]</code> DEFAULT: <code>(1, 's')</code> </p> <code>n_workers</code> <p>The number of workers that were in use. This helps idenify how many workers were idle at a given time. If None, the maximum length of the queue at any recorded time is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the pandas plot function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Axes</code> <p>The axes.</p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def plot(\n    self,\n    *,\n    ax: plt.Axes | None = None,\n    interval: tuple[int, UnitChoices] = (1, \"s\"),\n    n_workers: int | None = None,\n    **kwargs: Any,\n) -&gt; plt.Axes:\n    \"\"\"Plots the data as a stacked barchart.\n\n    Args:\n        ax: The axes to plot on. If None, a new figure is created.\n        interval: The interval to use for the x-axis. The first value is the\n            interval and the second value is the unit. Must be a valid pandas\n            timedelta unit. See [to_timedelta()][pandas.to_timedelta] for more\n            information.\n        n_workers: The number of workers that were in use. This helps idenify how\n            many workers were idle at a given time. If None, the maximum length of\n            the queue at any recorded time is used.\n        **kwargs: Additional keyword arguments to pass to the pandas plot function.\n\n    Returns:\n        The axes.\n    \"\"\"\n    if ax is None:\n        _, _ax = plt.subplots(1, 1)\n    else:\n        _ax = ax\n\n    _df = self.df(n_workers=n_workers)\n    _df = _df.resample(f\"{interval[0]}{interval[1]}\").mean()\n    _df.index = _df.index - _df.index[0]\n    _reversed_df = _df[::-1]\n\n    _reversed_df.plot.barh(\n        stacked=True,\n        y=[\"finished\", \"queued\", \"cancelled\", \"idle\"],\n        ax=_ax,\n        width=1,\n        edgecolor=\"k\",\n        **kwargs,\n    )\n\n    _ax.set_ylabel(\"Time\")\n    _ax.yaxis.set_major_locator(MaxNLocator(nbins=\"auto\"))\n\n    _ax.set_xlabel(\"Count\")\n    _ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    return _ax\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor.update","title":"update","text":"<pre><code>update(*_: Any) -&gt; None\n</code></pre> <p>Updates the data when the scheduler has an event.</p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def update(self, *_: Any) -&gt; None:\n    \"\"\"Updates the data when the scheduler has an event.\"\"\"\n    queue = self.scheduler.queue\n    # OPTIM: Not sure if this is fastenough\n    counter = Counter([f._state for f in queue])\n    self.data.append(\n        QueueMonitorRecord(\n            time.time_ns(),\n            len(queue),\n            counter[\"PENDING\"],\n            counter[\"FINISHED\"],\n            counter[\"CANCELLED\"],\n        ),\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitorRecord","title":"QueueMonitorRecord","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A record of the queue state at a given time.</p>"},{"location":"api/amltk/scheduling/scheduler/","title":"Scheduler","text":""},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler","title":"amltk.scheduling.scheduler","text":"<p>The scheduler for AMLTK.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState","title":"ExitState  <code>dataclass</code>","text":"<pre><code>ExitState(\n    code: Code, exception: BaseException | None = None\n)\n</code></pre> <p>The exit state of a scheduler.</p> ATTRIBUTE DESCRIPTION <code>reason</code> <p>The reason for the exit.</p> <p> </p> <code>exception</code> <p>The exception that caused the exit, if any.</p> <p> TYPE: <code>BaseException | None</code> </p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code","title":"Code","text":"<p>               Bases: <code>Enum</code></p> <p>The reason the scheduler ended.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.CANCELLED","title":"CANCELLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CANCELLED = auto()\n</code></pre> <p>The scheduler was cancelled.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.EXCEPTION","title":"EXCEPTION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXCEPTION = auto()\n</code></pre> <p>The scheduler finished because of an exception.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.EXHAUSTED","title":"EXHAUSTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>EXHAUSTED = auto()\n</code></pre> <p>The scheduler finished because it exhausted its queue.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.STOPPED","title":"STOPPED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>STOPPED = auto()\n</code></pre> <p>The scheduler was stopped forcefully with <code>Scheduler.stop</code>.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.TIMEOUT","title":"TIMEOUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIMEOUT = auto()\n</code></pre> <p>The scheduler finished because of a timeout.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.UNKNOWN","title":"UNKNOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNKNOWN = auto()\n</code></pre> <p>The scheduler finished for an unknown reason.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler","title":"Scheduler","text":"<pre><code>Scheduler(\n    executor: Executor,\n    *,\n    terminate: Callable[[Executor], None] | bool = True\n)\n</code></pre> <p>               Bases: <code>RichRenderable</code></p> <p>A scheduler for submitting tasks to an Executor.</p> PARAMETER DESCRIPTION <code>executor</code> <p>The dispatcher to use for submitting tasks.</p> <p> TYPE: <code>Executor</code> </p> <code>terminate</code> <p>Whether to call shutdown on the executor when <code>run(..., wait=False)</code>. If True, the executor will be <code>shutdown(wait=False)</code> and we will attempt to terminate any workers of the executor. For some <code>Executors</code> this is enough, i.e. Dask, however for something like <code>ProcessPoolExecutor</code>, we will use <code>psutil</code> to kill its worker processes. If a callable, we will use this function for custom worker termination. If False, shutdown will not be called and the executor will remain active.</p> <p> TYPE: <code>Callable[[Executor], None] | bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def __init__(\n    self,\n    executor: Executor,\n    *,\n    terminate: Callable[[Executor], None] | bool = True,\n) -&gt; None:\n    \"\"\"Initialize a scheduler.\n\n    Args:\n        executor: The dispatcher to use for submitting tasks.\n        terminate: Whether to call shutdown on the executor when\n            `run(..., wait=False)`. If True, the executor will be\n            `shutdown(wait=False)` and we will attempt to terminate\n            any workers of the executor. For some `Executors` this\n            is enough, i.e. Dask, however for something like\n            `ProcessPoolExecutor`, we will use `psutil` to kill\n            its worker processes. If a callable, we will use this\n            function for custom worker termination.\n            If False, shutdown will not be called and the executor will\n            remain active.\n    \"\"\"\n    super().__init__()\n    self.executor = executor\n    self.unique_ref = f\"Scheduler-{uuid4()}\"\n    self.emitter = Emitter()\n    self.event_counts = self.emitter.event_counts\n\n    # The current state of things and references to them\n    self.queue = {}\n\n    # Set up subscribers for events\n    self.on_start = self.emitter.subscriber(self.STARTED)\n    self.on_finishing = self.emitter.subscriber(self.FINISHING)\n    self.on_finished = self.emitter.subscriber(self.FINISHED)\n    self.on_stop = self.emitter.subscriber(self.STOP)\n    self.on_timeout = self.emitter.subscriber(self.TIMEOUT)\n    self.on_empty = self.emitter.subscriber(self.EMPTY)\n\n    self.on_future_submitted = self.emitter.subscriber(self.FUTURE_SUBMITTED)\n    self.on_future_done = self.emitter.subscriber(self.FUTURE_DONE)\n    self.on_future_cancelled = self.emitter.subscriber(self.FUTURE_CANCELLED)\n    self.on_future_exception = self.emitter.subscriber(self.FUTURE_EXCEPTION)\n    self.on_future_result = self.emitter.subscriber(self.FUTURE_RESULT)\n\n    self._terminate: Callable[[Executor], None] | None\n    if terminate is True:\n        self._terminate = termination_strategy(executor)\n    else:\n        self._terminate = terminate if callable(terminate) else None\n\n    # This can be triggered either by `scheduler.stop` in a callback.\n    # Has to be created inside the event loop so there's no issues\n    self._stop_event: ContextEvent | None = None\n\n    # This is a condition to make sure monitoring the queue will wait properly\n    self._queue_has_items_event = asyncio.Event()\n\n    # This is triggered when run is called\n    self._running_event = ContextEvent()\n\n    # This is set once `run` is called.\n    # Either contains the mapping from exception to what to do,\n    # otherwise a boolean whether to end or not.\n    self._on_exc_method_map: Flag[\n        dict[type[Exception], Literal[\"raise\", \"end\", \"continue\"]],\n    ] = Flag(initial={Exception: \"raise\"})\n\n    # This is set once `run` is called.\n    # Indicates what to do when a cancellation occurs\n    self._on_cancel_method: Flag[Literal[\"raise\", \"end\", \"continue\"]] = Flag(\n        initial=\"raise\",\n    )\n\n    # This is used to manage suequential queues, where we need a Thread\n    # timer to ensure that we don't get caught in an endless loop waiting\n    # for the `timeout` in `_run_scheduler` to trigger. This won't trigger\n    # because the sync code of submit could possibly keep calling itself\n    # endlessly, preventing any of the async code from running.\n    self._timeout_timer: Timer | None = None\n\n    # A collection of things that want to register as being part of something\n    # to render when the Scheduler is rendered.\n    self._renderables: list[RenderableType] = [self.emitter]\n\n    # These are extra user provided renderables during a call to `run()`. We\n    # seperate these out so that we can remove them when the scheduler is\n    # stopped.\n    self._extra_renderables: list[RenderableType] | None = None\n\n    # An indicator an object to render live output (if requested) with\n    # `display=` on a call to `run()`\n    self._live_output: Live | None = None\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.emitter","title":"emitter  <code>instance-attribute</code>","text":"<pre><code>emitter: Emitter = Emitter()\n</code></pre> <p>The emitter to use for events.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.executor","title":"executor  <code>instance-attribute</code>","text":"<pre><code>executor: Executor = executor\n</code></pre> <p>The executor to use to run tasks.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_empty","title":"on_empty  <code>instance-attribute</code>","text":"<pre><code>on_empty: Subscriber[[], Any] = subscriber(EMPTY)\n</code></pre> <p>A <code>Subscriber</code> which is called when the queue is empty. This can be useful to re-fill the queue and prevent the scheduler from exiting.</p> <pre><code>@scheduler.on_empty\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_finished","title":"on_finished  <code>instance-attribute</code>","text":"<pre><code>on_finished: Subscriber[[], Any] = subscriber(FINISHED)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finished, has shutdown the executor and possibly terminated any remaining compute.</p> <pre><code>@scheduler.on_finished\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_finishing","title":"on_finishing  <code>instance-attribute</code>","text":"<pre><code>on_finishing: Subscriber[[], Any] = subscriber(FINISHING)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finishing up. This occurs right before the scheduler shuts down the executor.</p> <pre><code>@scheduler.on_finishing\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_cancelled","title":"on_future_cancelled  <code>instance-attribute</code>","text":"<pre><code>on_future_cancelled: Subscriber[[Future], Any] = subscriber(\n    FUTURE_CANCELLED\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when a future is cancelled. This usually occurs due to the underlying Scheduler, and is not something we do directly, other than when shutting down the scheduler.</p> <pre><code>@scheduler.on_future_cancelled\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_done","title":"on_future_done  <code>instance-attribute</code>","text":"<pre><code>on_future_done: Subscriber[[Future], Any] = subscriber(\n    FUTURE_DONE\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is done, regardless of whether it was successful or not.</p> <pre><code>@scheduler.on_future_done\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_exception","title":"on_future_exception  <code>instance-attribute</code>","text":"<pre><code>on_future_exception: Subscriber[\n    [Future, BaseException], Any\n] = subscriber(FUTURE_EXCEPTION)\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute raised an uncaught exception.</p> <pre><code>@scheduler.on_future_exception\ndef my_callback(future: Future, exception: BaseException):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_result","title":"on_future_result  <code>instance-attribute</code>","text":"<pre><code>on_future_result: Subscriber[[Future, Any], Any] = (\n    subscriber(FUTURE_RESULT)\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when a future returned with a result, no exception raise.</p> <pre><code>@scheduler.on_future_result\ndef my_callback(future: Future, result: Any):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_submitted","title":"on_future_submitted  <code>instance-attribute</code>","text":"<pre><code>on_future_submitted: Subscriber[[Future], Any] = subscriber(\n    FUTURE_SUBMITTED\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is submitted.</p> <pre><code>@scheduler.on_future_submitted\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_start","title":"on_start  <code>instance-attribute</code>","text":"<pre><code>on_start: Subscriber[[], Any] = subscriber(STARTED)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler starts. This is the first event emitted by the scheduler and one of the only ways to submit the initial compute to the scheduler.</p> <pre><code>@scheduler.on_start\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_stop","title":"on_stop  <code>instance-attribute</code>","text":"<pre><code>on_stop: Subscriber[[str, BaseException | None], Any] = (\n    subscriber(STOP)\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is has been stopped due to the <code>stop()</code> method being called.</p> <pre><code>@scheduler.on_stop\ndef my_callback(stop_msg: str, exception: BaseException | None):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_timeout","title":"on_timeout  <code>instance-attribute</code>","text":"<pre><code>on_timeout: Subscriber[[], Any] = subscriber(TIMEOUT)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler reaches the timeout.</p> <pre><code>@scheduler.on_timeout\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.queue","title":"queue  <code>instance-attribute</code>","text":"<pre><code>queue: dict[Future, tuple[Callable, tuple, dict]] = {}\n</code></pre> <p>The queue of tasks running.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.add_renderable","title":"add_renderable","text":"<pre><code>add_renderable(renderable: RenderableType) -&gt; None\n</code></pre> <p>Add a renderable object to the scheduler.</p> <p>This will be displayed whenever the scheduler is displayed.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def add_renderable(self, renderable: RenderableType) -&gt; None:\n    \"\"\"Add a renderable object to the scheduler.\n\n    This will be displayed whenever the scheduler is displayed.\n    \"\"\"\n    self._renderables.append(renderable)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.async_run","title":"async_run  <code>async</code>","text":"<pre><code>async_run(\n    *,\n    timeout: float | None = None,\n    end_on_empty: bool = True,\n    wait: bool = True,\n    on_exception: (\n        Literal[\"raise\", \"end\", \"continue\"]\n        | Mapping[\n            type[Exception],\n            Literal[\"raise\", \"end\", \"continue\"],\n        ]\n    ) = \"raise\",\n    on_cancelled: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    display: bool | Iterable[RenderableType] = False\n) -&gt; ExitState\n</code></pre> <p>Async version of <code>run</code>.</p> <p>This can be useful if you are already running in an async context, such as in a web server or Jupyter notebook.</p> <p>Please see <code>run()</code> for more details.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>async def async_run(  # noqa: C901, PLR0915\n    self,\n    *,\n    timeout: float | None = None,\n    end_on_empty: bool = True,\n    wait: bool = True,\n    on_exception: (\n        Literal[\"raise\", \"end\", \"continue\"]\n        | Mapping[type[Exception], Literal[\"raise\", \"end\", \"continue\"]]\n    ) = \"raise\",\n    on_cancelled: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    display: bool | Iterable[RenderableType] = False,\n) -&gt; ExitState:\n    \"\"\"Async version of `run`.\n\n    This can be useful if you are already running in an async context,\n    such as in a web server or Jupyter notebook.\n\n    Please see [`run()`][amltk.Scheduler.run] for more details.\n    \"\"\"\n    if self.running():\n        raise RuntimeError(\"Scheduler already seems to be running\")\n\n    match on_cancelled:\n        case \"raise\" | \"end\" | \"continue\":\n            self._on_cancel_method.set(value=on_cancelled)\n        case _:\n            raise ValueError(\n                f\"Invalid value for `on_cancelled`: {on_cancelled}.\"\n                \" Please provide one of `raise`, `end`, or `ignore`\",\n            )\n\n    match on_exception:\n        case \"raise\" | \"end\" | \"continue\":\n            self._on_exc_method_map.set(value={Exception: on_exception})\n        case Mapping():\n            self._on_exc_method_map.set(value=dict(on_exception))\n            for k, v in on_exception.items():\n                if k in (BaseException, KeyboardInterrupt):\n                    raise ValueError(\n                        f\"Invalid key in `on_exception` mapping: {k}. \"\n                        \"Must not be `BaseException` or `KeyboardInterrupt`.\",\n                    )\n\n                if not issubclass(k, Exception):\n                    raise ValueError(\n                        f\"Invalid key in `on_exception` mapping: {k}. \"\n                        \"Must be a subclass of `Exception`.\",\n                    )\n\n                if v not in (\"raise\", \"end\", \"continue\"):\n                    raise ValueError(\n                        f\"Invalid value in `on_exception` mapping: {v}. \"\n                        \"Must be one of `raise`, `end`, or `ignore`.\",\n                    )\n        case _:\n            raise ValueError(\n                f\"Invalid value for `on_exception`: {on_exception}.\"\n                \" Please provide one of `raise`, `end`, or `ignore`\"\n                \" or a `Mapping` from exception types to one of those values.\",\n            )\n\n    # If the user has requested to have a live display,\n    # we will need to setup a `Live` instance to render to\n    match display:\n        case False:\n            self._extra_renderables = None\n            self._live_output = None\n        case True:\n            from rich.live import Live\n\n            self._extra_renderables = None\n            self._live_output = Live(\n                auto_refresh=False,\n                get_renderable=self.__rich__,\n            )\n        case Iterable():\n            from rich.live import Live\n\n            self._extra_renderables = list(display)\n            self._live_output = Live(\n                auto_refresh=False,\n                get_renderable=self.__rich__,\n            )\n        case _:\n            raise ValueError(\n                f\"Invalid value for `display`: {display}.\"\n                \" Please provide one of `True`, `False`, or an iterable of\"\n                \" `RenderableType`.\",\n            )\n\n    loop = asyncio.get_running_loop()\n\n    # If it's been specified that we might need to abruptly end\n    # the asyncio handler due to an error (as indicated by \"raise\" or \"end\")\n    # then we need to setup a custom exception handler that tells the scheduler\n    # to stop.\n    # By default, the asyncio handler will silently eat exceptions which\n    # is the desired behaviour if everything is just \"continue\"\n    previous_exception_handler = None\n    if on_exception in (\"raise\", \"end\") or (\n        isinstance(on_exception, Mapping)\n        and any(v in (\"raise\", \"end\") for v in on_exception.values())\n    ):\n        previous_exception_handler = loop.get_exception_handler()\n\n        def custom_exception_handler(\n            loop: asyncio.AbstractEventLoop,\n            context: dict[str, Any],\n        ) -&gt; None:\n            # The exception could be None as specified by:\n            # https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.call_exception_handler\n            # Not sure when exactly this happens but since there's no exception\n            # we should just ignore it.\n            exception: BaseException | None = context.get(\"exception\")\n            message: str | None = context.get(\"message\")\n\n            # handle with previous handler before possibly\n            # stopping\n            if previous_exception_handler:\n                previous_exception_handler(loop, context)\n            else:\n                loop.default_exception_handler(context)\n\n            match exception:\n                case None:\n                    return\n                # We do not allow special handling of BaseException by users\n                case BaseException() if not isinstance(exception, Exception):\n                    self.stop(stop_msg=message, exception=exception)\n                    return\n                case Exception():\n                    match subclass_map(exception, self._on_exc_method_map.value):\n                        case None | (_, \"continue\"):\n                            return\n                        case (_, \"raise\") | (_, \"end\"):\n                            self.stop(stop_msg=message, exception=exception)\n                            return\n                        case _:\n                            raise RuntimeError(\"Unexpected, please raise issue!\")\n                case _:\n                    msg = (\n                        \"Recieved something odd.\"\n                        \" Please raise an issue with this info!\"\n                        f\"{type(exception)} {exception=}\"\n                        f\"\\n{context=}\"\n                        f\"\\n{loop=}\"\n                        \"\\n===\\n\"\n                        f\"{message}\"\n                    )\n                    self.stop(stop_msg=msg, exception=exception)\n                    return\n\n        loop.set_exception_handler(custom_exception_handler)\n\n    logger.debug(\"Starting scheduler\")\n    # Run the actual scheduling loop\n    stop_reason = await self._run_scheduler(\n        timeout=timeout,\n        end_on_empty=end_on_empty,\n        wait=wait,\n    )\n    if previous_exception_handler is not None:\n        loop.set_exception_handler(previous_exception_handler)\n\n    # Need a reference to later this before resetting...\n    exc_method_map = dict(self._on_exc_method_map.value)\n\n    # Reset variables back to its default\n    self._live_output = None\n    self._extra_renderables = None\n    self._on_exc_method_map.reset()\n    self._on_cancel_method.reset()\n\n    # Decide what to do with the result from the scheduler\n    match stop_reason:\n        case ExitState.Code():\n            return ExitState(code=stop_reason)\n        # Special case for handling the cancellation error as it does\n        # not occur as an exception as far as futures are concerened\n        case CancelledError() if on_cancelled == \"raise\":\n            raise stop_reason\n        case CancelledError() if on_cancelled in (\"end\", \"continue\"):\n            return ExitState(code=ExitState.Code.CANCELLED, exception=stop_reason)\n        # General exception handling\n        case Exception():\n            match subclass_map(stop_reason, exc_method_map):\n                case None | (_, \"raise\"):\n                    raise stop_reason\n                case (_, \"end\"):\n                    return ExitState(\n                        ExitState.Code.EXCEPTION,\n                        exception=stop_reason,\n                    )\n                case (_, \"continue\"):\n                    logger.warning(\n                        f\"Was told to ignore `{type(stop_reason)}` yet it\"\n                        \" was returned by the Scheduler. Please raise an issue!\"\n                        \"\\nReturning ExitState.Code.UNKNOWN but this behaviour\",\n                        \" may crash in the future.\",\n                    )\n                    return ExitState(\n                        ExitState.Code.UNKNOWN,\n                        exception=stop_reason,\n                    )\n                case _:\n                    raise RuntimeError(\"Unexpected, please raise issue!\")\n        case BaseException():\n            raise stop_reason\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.call_later","title":"call_later","text":"<pre><code>call_later(\n    delay: float,\n    fn: Callable[P, Any],\n    *args: args,\n    **kwargs: kwargs\n) -&gt; TimerHandle\n</code></pre> <p>Schedule a function to be run after a delay.</p> PARAMETER DESCRIPTION <code>delay</code> <p>The delay in seconds.</p> <p> TYPE: <code>float</code> </p> <code>fn</code> <p>The function to run.</p> <p> TYPE: <code>Callable[P, Any]</code> </p> <code>args</code> <p>The positional arguments to pass to the function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>The keyword arguments to pass to the function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>TimerHandle</code> <p>A timer handle that can be used to cancel the function.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def call_later(\n    self,\n    delay: float,\n    fn: Callable[P, Any],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; asyncio.TimerHandle:\n    \"\"\"Schedule a function to be run after a delay.\n\n    Args:\n        delay: The delay in seconds.\n        fn: The function to run.\n        args: The positional arguments to pass to the function.\n        kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        A timer handle that can be used to cancel the function.\n    \"\"\"\n    if not self.running():\n        raise RuntimeError(\"Scheduler is not running!\")\n\n    _fn = partial(fn, *args, **kwargs)\n    loop = asyncio.get_running_loop()\n    return loop.call_later(delay, _fn)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.empty","title":"empty","text":"<pre><code>empty() -&gt; bool\n</code></pre> <p>Check if the scheduler is empty.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if there are no tasks in the queue.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def empty(self) -&gt; bool:\n    \"\"\"Check if the scheduler is empty.\n\n    Returns:\n        True if there are no tasks in the queue.\n    \"\"\"\n    return len(self.queue) == 0\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.run","title":"run","text":"<pre><code>run(\n    *,\n    timeout: float | None = None,\n    end_on_empty: bool = True,\n    wait: bool = True,\n    on_exception: (\n        Literal[\"raise\", \"end\", \"continue\"]\n        | Mapping[\n            type[Exception],\n            Literal[\"raise\", \"end\", \"continue\"],\n        ]\n    ) = \"raise\",\n    on_cancelled: Literal[\n        \"raise\", \"end\", \"continue\"\n    ] = \"raise\",\n    asyncio_debug_mode: bool = False,\n    display: (\n        bool | Iterable[RenderableType] | Literal[\"auto\"]\n    ) = \"auto\"\n) -&gt; ExitState\n</code></pre> <p>Run the scheduler.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>The maximum time to run the scheduler for in seconds. Defaults to <code>None</code> which means no timeout and it will end once the queue is empty if <code>end_on_empty=True</code>.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>end_on_empty</code> <p>Whether to end the scheduler when the queue becomes empty.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>wait</code> <p>Whether to wait for currently running compute to finish once the <code>Scheduler</code> is shutting down.</p> <ul> <li>If <code>True</code>, will wait for all currently running compute.</li> <li>If <code>False</code>, will attempt to cancel/terminate all currently     running compute and shutdown the executor. This may be useful     if you want to end the scheduler as quickly as possible or     respect the <code>timeout=</code> more precisely.</li> </ul> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_exception</code> <p>What to do when an exception occurs in the scheduler or callbacks (Does not apply to submitted compute!)</p> <ul> <li>If <code>\"raise\"</code>, the scheduler will stop and raise the     exception at the point where you called <code>run()</code>.</li> <li>If <code>\"continue\"</code>, the scheduler will continue to run     and just emit the exception as usual.     This may be useful when requiring more robust execution.</li> <li> <p>If <code>\"end\"</code>, similar to <code>\"raise\"</code>, the scheduler     will stop but no exception will occur and the control flow     will return gracefully to the point where you called <code>run()</code>.</p> </li> <li> <p>If a <code>dict</code>, then it's a mapping from error types to what should be     done about them. This can be used with something like the     [<code>amltk.scheduling.plugins.plugins.PynisherPlugin</code>] to prevent     it from killing the scheduler when an individual trial fails.</p> <pre><code>from amltk.scheduling.plugins import PynisherPlugin\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes()\n\nscheduler.run(\n    on_exception={\n        PynisherPlugin.Exception: \"continue\",\n        Exception: \"raise\"  # Default...\n    }\n)\n</code></pre> <p>Order</p> <p>The order of the mapping matters. The first matching exception with an <code>isinstance</code> check will be used. You should place more explicit exception types first.</p> <p>KeyboardInterrupt</p> <p>The <code>KeyboardInterrupt</code> or <code>BaseException</code> can not be caught and will always be raised, regardless of the <code>on_exception</code> setting.</p> </li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue'] | Mapping[type[Exception], Literal['raise', 'end', 'continue']]</code> DEFAULT: <code>'raise'</code> </p> <code>on_cancelled</code> <p>What to do when a task is cancelled.</p> <ul> <li>If <code>\"raise\"</code>, the scheduler will stop and raise the     exception at the point where you called <code>run()</code>.</li> <li>If <code>\"continue\"</code>, the scheduler will continue running,     ignoring the cancellation. This may be useful when requiring more     robust execution.</li> <li>If <code>\"end\"</code>, similar to <code>\"raise\"</code>, the scheduler     will stop but no exception will occur and the control flow     will return gracefully to the point where you called <code>run()</code>.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'continue']</code> DEFAULT: <code>'raise'</code> </p> <code>asyncio_debug_mode</code> <p>Whether to run the async loop in debug mode. Defaults to <code>False</code>. Please see asyncio.run for more.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>display</code> <p>Whether to display the scheduler live in the console.</p> <ul> <li>If <code>\"auto\"</code>, will display the scheduler if in     a notebook or colab environemnt. Otherwise, it will not display     it. If left as \"auto\" and the display occurs, a warning will     be printed alongside it.</li> <li>If <code>False</code>, will not display anything.</li> <li>If <code>True</code>, will display the scheduler and all its tasks.</li> <li>If a <code>list[RenderableType]</code> , will display the scheduler     itself plus those renderables.</li> </ul> <p> TYPE: <code>bool | Iterable[RenderableType] | Literal['auto']</code> DEFAULT: <code>'auto'</code> </p> RETURNS DESCRIPTION <code>ExitState</code> <p>The reason for the scheduler ending.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the scheduler is already running.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def run(\n    self,\n    *,\n    timeout: float | None = None,\n    end_on_empty: bool = True,\n    wait: bool = True,\n    on_exception: (\n        Literal[\"raise\", \"end\", \"continue\"]\n        | Mapping[type[Exception], Literal[\"raise\", \"end\", \"continue\"]]\n    ) = \"raise\",\n    on_cancelled: Literal[\"raise\", \"end\", \"continue\"] = \"raise\",\n    asyncio_debug_mode: bool = False,\n    display: bool | Iterable[RenderableType] | Literal[\"auto\"] = \"auto\",\n) -&gt; ExitState:\n    \"\"\"Run the scheduler.\n\n    Args:\n        timeout: The maximum time to run the scheduler for in\n            seconds. Defaults to `None` which means no timeout and it\n            will end once the queue is empty if `end_on_empty=True`.\n        end_on_empty: Whether to end the scheduler when the queue becomes empty.\n        wait: Whether to wait for currently running compute to finish once\n            the `Scheduler` is shutting down.\n\n            * If `#!python True`, will wait for all currently running compute.\n            * If `#!python False`, will attempt to cancel/terminate all currently\n                running compute and shutdown the executor. This may be useful\n                if you want to end the scheduler as quickly as possible or\n                respect the `timeout=` more precisely.\n        on_exception: What to do when an exception occurs in the scheduler\n            or callbacks (**Does not apply to submitted compute!**)\n\n            * If `#!python \"raise\"`, the scheduler will stop and raise the\n                exception at the point where you called `run()`.\n            * If `#!python \"continue\"`, the scheduler will continue to run\n                and just emit the exception as usual.\n                This may be useful when requiring more robust execution.\n            * If `#!python \"end\"`, similar to `#!python \"raise\"`, the scheduler\n                will stop but no exception will occur and the control flow\n                will return gracefully to the point where you called `run()`.\n\n            * If a `dict`, then it's a mapping from error types to what should be\n                done about them. This can be used with something like the\n                [`amltk.scheduling.plugins.plugins.PynisherPlugin`] to prevent\n                it from killing the scheduler when an individual trial fails.\n\n                ```python\n                from amltk.scheduling.plugins import PynisherPlugin\n                from amltk.scheduling import Scheduler\n\n                scheduler = Scheduler.with_processes()\n\n                scheduler.run(\n                    on_exception={\n                        PynisherPlugin.Exception: \"continue\",\n                        Exception: \"raise\"  # Default...\n                    }\n                )\n                ```\n\n                !!! tip \"Order\"\n\n                    The order of the mapping matters. The first matching exception\n                    with an `isinstance` check will be used. You should place\n                    more explicit exception types first.\n\n                !!! warning \"KeyboardInterrupt\"\n\n                    The `KeyboardInterrupt` or `BaseException` can not be\n                    caught and will always be raised, regardless of the\n                    `on_exception` setting.\n\n        on_cancelled:\n            What to do when a task is cancelled.\n\n            * If `#!python \"raise\"`, the scheduler will stop and raise the\n                exception at the point where you called `run()`.\n            * If `#!python \"continue\"`, the scheduler will continue running,\n                ignoring the cancellation. This may be useful when requiring more\n                robust execution.\n            * If `#!python \"end\"`, similar to `#!python \"raise\"`, the scheduler\n                will stop but no exception will occur and the control flow\n                will return gracefully to the point where you called `run()`.\n\n        asyncio_debug_mode: Whether to run the async loop in debug mode.\n            Defaults to `False`. Please see [asyncio.run][] for more.\n        display: Whether to display the scheduler live in the console.\n\n            * If `#!python \"auto\"`, will display the scheduler if in\n                a notebook or colab environemnt. Otherwise, it will not display\n                it. If left as \"auto\" and the display occurs, a warning will\n                be printed alongside it.\n            * If `#!python False`, will not display anything.\n            * If `#!python True`, will display the scheduler and all its tasks.\n            * If a `#!python list[RenderableType]` , will display the scheduler\n                itself plus those renderables.\n\n    Returns:\n        The reason for the scheduler ending.\n\n    Raises:\n        RuntimeError: If the scheduler is already running.\n    \"\"\"\n    if display == \"auto\":\n        from amltk._richutil import is_jupyter\n\n        display = is_jupyter()\n        if display is True:\n            warnings.warn(\n                \"Detected that current running context is in a notebook!\"\n                \" When `display='auto'`, the default, the scheduler will\"\n                \" automatically be set to display. If you do not want this or\"\n                \" wish to disable this warning, please set `display=False`.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    return asyncio.run(\n        self.async_run(\n            timeout=timeout,\n            end_on_empty=end_on_empty,\n            wait=wait,\n            on_exception=on_exception,\n            display=display,\n            on_cancelled=on_cancelled,\n        ),\n        debug=asyncio_debug_mode,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.running","title":"running","text":"<pre><code>running() -&gt; bool\n</code></pre> <p>Whether the scheduler is running and accepting tasks to dispatch.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the scheduler is running and accepting tasks.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def running(self) -&gt; bool:\n    \"\"\"Whether the scheduler is running and accepting tasks to dispatch.\n\n    Returns:\n        True if the scheduler is running and accepting tasks.\n    \"\"\"\n    return self._running_event.is_set()\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.stop","title":"stop","text":"<pre><code>stop(\n    *args: Any,\n    stop_msg: str | None = None,\n    exception: BaseException | None = None,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Stop the scheduler.</p> <p>The scheduler will stop, finishing currently running tasks depending on the <code>wait=</code> parameter to <code>Scheduler.run</code>.</p> <p>The call signature is kept open with <code>*args, **kwargs</code> to make it easier to include in any callback.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Logged in a debug message</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Logged in a debug message</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <code>stop_msg</code> <p>The message to log when stopping the scheduler.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>exception</code> <p>The exception which incited <code>stop()</code> to be called. Will be used by the <code>Scheduler</code> to possibly raise the exception to the user.</p> <p> TYPE: <code>BaseException | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def stop(\n    self,\n    *args: Any,\n    stop_msg: str | None = None,\n    exception: BaseException | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Stop the scheduler.\n\n    The scheduler will stop, finishing currently running tasks depending\n    on the `wait=` parameter to [`Scheduler.run`][amltk.Scheduler.run].\n\n    The call signature is kept open with `*args, **kwargs` to make it\n    easier to include in any callback.\n\n    Args:\n        *args: Logged in a debug message\n        **kwargs: Logged in a debug message\n        stop_msg: The message to log when stopping the scheduler.\n        exception: The exception which incited `stop()` to be called.\n            Will be used by the `Scheduler` to possibly raise the exception\n            to the user.\n    \"\"\"\n    if not self.running():\n        return\n\n    assert self._stop_event is not None\n\n    msg = stop_msg if stop_msg is not None else \"scheduler.stop() was called.\"\n    logger.debug(f\"Stopping scheduler: {msg} {args=} {kwargs=}\")\n\n    self._stop_event.set(msg=msg, exception=exception)\n    self._running_event.clear()\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.submit","title":"submit","text":"<pre><code>submit(\n    fn: Callable[P, R], /, *args: args, **kwargs: kwargs\n) -&gt; Future[R]\n</code></pre> <p>Submits a callable to be executed with the given arguments.</p> PARAMETER DESCRIPTION <code>fn</code> <p>The callable to be executed as fn(args, *kwargs) that returns a Future instance representing the execution of the callable.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>args</code> <p>positional arguments to pass to the function</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>keyword arguments to pass to the function</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>SchedulerNotRunningError</code> <p>If the scheduler is not running. You can protect against this using, <code>scheduler.running()</code>.</p> RETURNS DESCRIPTION <code>Future[R]</code> <p>A Future representing the given call.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def submit(\n    self,\n    fn: Callable[P, R],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[R]:\n    \"\"\"Submits a callable to be executed with the given arguments.\n\n    Args:\n        fn: The callable to be executed as\n            fn(*args, **kwargs) that returns a Future instance representing\n            the execution of the callable.\n        args: positional arguments to pass to the function\n        kwargs: keyword arguments to pass to the function\n\n    Raises:\n        SchedulerNotRunningError: If the scheduler is not running.\n            You can protect against this using,\n            [`scheduler.running()`][amltk.scheduling.scheduler.Scheduler.running].\n\n    Returns:\n        A Future representing the given call.\n    \"\"\"\n    if not self.running():\n        msg = (\n            f\"Scheduler is not running, cannot submit task {fn}\"\n            f\" with {args=}, {kwargs=}\"\n        )\n        raise SchedulerNotRunningError(msg)\n\n    try:\n        sync_future = self.executor.submit(fn, *args, **kwargs)\n        future = asyncio.wrap_future(sync_future)\n    except Exception as e:\n        logger.exception(f\"Could not submit task {fn}\", exc_info=e)\n        raise e\n\n    self._register_future(future, fn, *args, **kwargs)\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.task","title":"task","text":"<pre><code>task(\n    function: (\n        Callable[P, R]\n        | Callable[Concatenate[Comm, P], R]\n    ),\n    *,\n    plugins: Plugin | Iterable[Plugin] = (),\n    init_plugins: bool = True\n) -&gt; Task[P, R]\n</code></pre> <p>Create a new task.</p> PARAMETER DESCRIPTION <code>function</code> <p>The function to run using the scheduler.</p> <p> TYPE: <code>Callable[P, R] | Callable[Concatenate[Comm, P], R]</code> </p> <code>plugins</code> <p>The plugins to attach to the task.</p> <p> TYPE: <code>Plugin | Iterable[Plugin]</code> DEFAULT: <code>()</code> </p> <code>init_plugins</code> <p>Whether to initialize the plugins.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Task[P, R]</code> <p>A new task.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def task(\n    self,\n    function: Callable[P, R] | Callable[Concatenate[Comm, P], R],\n    *,\n    plugins: Plugin | Iterable[Plugin] = (),\n    init_plugins: bool = True,\n) -&gt; Task[P, R]:\n    \"\"\"Create a new task.\n\n    Args:\n        function: The function to run using the scheduler.\n        plugins: The plugins to attach to the task.\n        init_plugins: Whether to initialize the plugins.\n\n    Returns:\n        A new task.\n    \"\"\"\n    # HACK: Not that the type: ignore is due to the fact that we can't use type\n    # checking to enforce that\n    # A. `function` is a callable with the first arg being a Comm\n    # B. `plugins`\n    task = Task(function, self, plugins=plugins, init_plugins=init_plugins)  # type: ignore\n    self.add_renderable(task)\n    return task  # type: ignore\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_dask_jobqueue","title":"with_dask_jobqueue  <code>classmethod</code>","text":"<pre><code>with_dask_jobqueue(\n    name: DJQ_NAMES,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler with using <code>dask-jobqueue</code>.</p> <p>See <code>dask_jobqueue</code> for more details.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the jobqueue to use. This is the name of the class in <code>dask_jobqueue</code> to use. For example, to use <code>dask_jobqueue.SLURMCluster</code>, you would use <code>slurm</code>.</p> <p> TYPE: <code>DJQ_NAMES</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If <code>dask-jobqueue</code> is not installed.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new scheduler with a <code>dask_jobqueue</code> executor.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_dask_jobqueue(\n    cls,\n    name: DJQ_NAMES,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler with using `dask-jobqueue`.\n\n    See [`dask_jobqueue`][dask_jobqueue] for more details.\n\n    [dask_jobqueue]: https://jobqueue.dask.org/en/latest/\n\n    Args:\n        name: The name of the jobqueue to use. This is the name of the\n            class in `dask_jobqueue` to use. For example, to use\n            `dask_jobqueue.SLURMCluster`, you would use `slurm`.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        n_workers: The number of workers to start.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Raises:\n        ImportError: If `dask-jobqueue` is not installed.\n\n    Returns:\n        A new scheduler with a `dask_jobqueue` executor.\n    \"\"\"\n    try:\n        from amltk.scheduling.executors.dask_jobqueue import DaskJobqueueExecutor\n\n    except ImportError as e:\n        raise ImportError(\n            f\"To use the {name} executor, you must install the \"\n            \"`dask-jobqueue` package.\",\n        ) from e\n\n    executor = DaskJobqueueExecutor.from_str(\n        name,\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n    return cls(executor)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_htcondor","title":"with_htcondor  <code>classmethod</code>","text":"<pre><code>with_htcondor(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler that runs on a HTCondor cluster.</p> <p>This is useful for running on a HTCondor cluster. Uses dask_jobqueue.HTCondorCluster.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a HTCondor cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_htcondor(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a HTCondor cluster.\n\n    This is useful for running on a HTCondor cluster. Uses\n    [dask_jobqueue.HTCondorCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a HTCondor cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"htcondor\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_loky","title":"with_loky  <code>classmethod</code>","text":"<pre><code>with_loky(\n    max_workers: int | None = None,\n    context: (\n        BaseContext\n        | Literal[\"fork\", \"spawn\", \"forkserver\"]\n        | None\n    ) = None,\n    timeout: int = 10,\n    kill_workers: bool = False,\n    reuse: bool | Literal[\"auto\"] = \"auto\",\n    job_reducers: Any | None = None,\n    result_reducers: Any | None = None,\n    initializer: Callable[..., Any] | None = None,\n    initargs: tuple[Any, ...] = (),\n    env: dict[str, str] | None = None,\n) -&gt; Self\n</code></pre> <p>Create a scheduler with a <code>loky.get_reusable_executor</code>.</p> <p>See [loky documentation][loky.readthedocs.io/en/stable/API.html] for more details.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_loky(  # noqa: PLR0913\n    cls,\n    max_workers: int | None = None,\n    context: BaseContext | Literal[\"fork\", \"spawn\", \"forkserver\"] | None = None,\n    timeout: int = 10,\n    kill_workers: bool = False,  # noqa: FBT002, FBT001\n    reuse: bool | Literal[\"auto\"] = \"auto\",\n    job_reducers: Any | None = None,\n    result_reducers: Any | None = None,\n    initializer: Callable[..., Any] | None = None,\n    initargs: tuple[Any, ...] = (),\n    env: dict[str, str] | None = None,\n) -&gt; Self:\n    \"\"\"Create a scheduler with a `loky.get_reusable_executor`.\n\n    See [loky documentation][https://loky.readthedocs.io/en/stable/API.html]\n    for more details.\n    \"\"\"\n    from loky import get_reusable_executor\n\n    executor = get_reusable_executor(\n        max_workers=max_workers,\n        context=context,\n        timeout=timeout,\n        kill_workers=kill_workers,\n        reuse=reuse,  # type: ignore\n        job_reducers=job_reducers,\n        result_reducers=result_reducers,\n        initializer=initializer,\n        initargs=initargs,\n        env=env,\n    )\n    return cls(executor=executor)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_lsf","title":"with_lsf  <code>classmethod</code>","text":"<pre><code>with_lsf(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler that runs on a LSF cluster.</p> <p>This is useful for running on a LSF cluster. Uses dask_jobqueue.LSFCluster.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a LSF cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_lsf(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a LSF cluster.\n\n    This is useful for running on a LSF cluster. Uses\n    [dask_jobqueue.LSFCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a LSF cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"lsf\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_moab","title":"with_moab  <code>classmethod</code>","text":"<pre><code>with_moab(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler that runs on a Moab cluster.</p> <p>This is useful for running on a Moab cluster. Uses dask_jobqueue.MoabCluster.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a Moab cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_moab(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a Moab cluster.\n\n    This is useful for running on a Moab cluster. Uses\n    [dask_jobqueue.MoabCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a Moab cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"moab\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_oar","title":"with_oar  <code>classmethod</code>","text":"<pre><code>with_oar(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler that runs on a OAR cluster.</p> <p>This is useful for running on a OAR cluster. Uses dask_jobqueue.OARCluster.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a OAR cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_oar(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a OAR cluster.\n\n    This is useful for running on a OAR cluster. Uses\n    [dask_jobqueue.OARCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a OAR cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"oar\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_pbs","title":"with_pbs  <code>classmethod</code>","text":"<pre><code>with_pbs(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler that runs on a PBS cluster.</p> <p>This is useful for running on a PBS cluster. Uses dask_jobqueue.PBSCluster.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a PBS cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_pbs(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a PBS cluster.\n\n    This is useful for running on a PBS cluster. Uses\n    [dask_jobqueue.PBSCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a PBS cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"pbs\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_processes","title":"with_processes  <code>classmethod</code>","text":"<pre><code>with_processes(\n    max_workers: int | None = None,\n    mp_context: (\n        BaseContext\n        | Literal[\"fork\", \"spawn\", \"forkserver\"]\n        | None\n    ) = None,\n    initializer: Callable[..., Any] | None = None,\n    initargs: tuple[Any, ...] = (),\n) -&gt; Self\n</code></pre> <p>Create a scheduler with a <code>ProcessPoolExecutor</code>.</p> <p>See <code>ProcessPoolExecutor</code> for more details.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_processes(\n    cls,\n    max_workers: int | None = None,\n    mp_context: BaseContext | Literal[\"fork\", \"spawn\", \"forkserver\"] | None = None,\n    initializer: Callable[..., Any] | None = None,\n    initargs: tuple[Any, ...] = (),\n) -&gt; Self:\n    \"\"\"Create a scheduler with a `ProcessPoolExecutor`.\n\n    See [`ProcessPoolExecutor`][concurrent.futures.ProcessPoolExecutor]\n    for more details.\n    \"\"\"\n    if isinstance(mp_context, str):\n        from multiprocessing import get_context\n\n        mp_context = get_context(mp_context)\n\n    executor = ProcessPoolExecutor(\n        max_workers=max_workers,\n        mp_context=mp_context,\n        initializer=initializer,\n        initargs=initargs,\n    )\n    return cls(executor=executor)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_sequential","title":"with_sequential  <code>classmethod</code>","text":"<pre><code>with_sequential() -&gt; Self\n</code></pre> <p>Create a Scheduler that runs sequentially.</p> <p>This is useful for debugging and testing. Uses a <code>SequentialExecutor</code>.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_sequential(cls) -&gt; Self:\n    \"\"\"Create a Scheduler that runs sequentially.\n\n    This is useful for debugging and testing. Uses\n    a [`SequentialExecutor`][amltk.scheduling.SequentialExecutor].\n    \"\"\"\n    return cls(executor=SequentialExecutor())\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_sge","title":"with_sge  <code>classmethod</code>","text":"<pre><code>with_sge(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler that runs on a SGE cluster.</p> <p>This is useful for running on a SGE cluster. Uses dask_jobqueue.SGECluster.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a SGE cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_sge(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a SGE cluster.\n\n    This is useful for running on a SGE cluster. Uses\n    [dask_jobqueue.SGECluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a SGE cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"sge\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_slurm","title":"with_slurm  <code>classmethod</code>","text":"<pre><code>with_slurm(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a Scheduler that runs on a SLURM cluster.</p> <p>This is useful for running on a SLURM cluster. Uses dask_jobqueue.SLURMCluster.</p> PARAMETER DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a SLURM cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_slurm(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a SLURM cluster.\n\n    This is useful for running on a SLURM cluster. Uses\n    [dask_jobqueue.SLURMCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a SLURM cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"slurm\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/task/","title":"Task","text":""},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task","title":"amltk.scheduling.task","text":"<p>The task module.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task","title":"Task","text":"<pre><code>Task(\n    function: Callable[P, R],\n    scheduler: Scheduler,\n    *,\n    plugins: Plugin | Iterable[Plugin] = (),\n    init_plugins: bool = True\n)\n</code></pre> <p>               Bases: <code>Emitter</code>, <code>RichRenderable</code>, <code>Generic[P, R]</code></p> <p>The task class.</p> PARAMETER DESCRIPTION <code>function</code> <p>The function of this task</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>scheduler</code> <p>The scheduler that this task is registered with.</p> <p> TYPE: <code>Scheduler</code> </p> <code>plugins</code> <p>The plugins to use for this task.</p> <p> TYPE: <code>Plugin | Iterable[Plugin]</code> DEFAULT: <code>()</code> </p> <code>init_plugins</code> <p>Whether to initialize the plugins or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def __init__(\n    self: Self,\n    function: Callable[P, R],\n    scheduler: Scheduler,\n    *,\n    plugins: Plugin | Iterable[Plugin] = (),\n    init_plugins: bool = True,\n) -&gt; None:\n    \"\"\"Initialize a task.\n\n    Args:\n        function: The function of this task\n        scheduler: The scheduler that this task is registered with.\n        plugins: The plugins to use for this task.\n        init_plugins: Whether to initialize the plugins or not.\n    \"\"\"\n    super().__init__(name=f\"Task-{funcname(function)}\")\n    self.plugins: list[Plugin] = (\n        [plugins] if isinstance(plugins, Plugin) else list(plugins)\n    )\n    self.function: Callable[P, R] = function\n    self.scheduler: Scheduler = scheduler\n    self.init_plugins: bool = init_plugins\n    self.queue: list[Future[R]] = []\n\n    # Set up subscription methods to events\n    self.on_submitted = self.subscriber(self.SUBMITTED)\n    self.on_done = self.subscriber(self.DONE)\n    self.on_result = self.subscriber(self.RESULT)\n    self.on_exception = self.subscriber(self.EXCEPTION)\n    self.on_cancelled = self.subscriber(self.CANCELLED)\n\n    if init_plugins:\n        for plugin in self.plugins:\n            plugin.attach_task(self)\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.HandlerResponses","title":"HandlerResponses  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>HandlerResponses: TypeAlias = Iterable[\n    tuple[Handler[P, R], R | None]\n]\n</code></pre> <p>The stream of responses from handlers when an event is triggered.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.emitter","title":"emitter  <code>instance-attribute</code>","text":"<pre><code>emitter: Emitter\n</code></pre> <p>The emitter for events of this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.event_counts","title":"event_counts  <code>instance-attribute</code>","text":"<pre><code>event_counts: Counter[Event] = Counter()\n</code></pre> <p>A count of all events emitted by this emitter.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.events","title":"events  <code>property</code>","text":"<pre><code>events: list[Event]\n</code></pre> <p>Return a list of the events.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.function","title":"function  <code>instance-attribute</code>","text":"<pre><code>function: Callable[P, R] = function\n</code></pre> <p>The function of this task</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.handlers","title":"handlers  <code>instance-attribute</code>","text":"<pre><code>handlers: dict[Event, list[Handler]] = defaultdict(list)\n</code></pre> <p>A mapping of events to their handlers.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.init_plugins","title":"init_plugins  <code>instance-attribute</code>","text":"<pre><code>init_plugins: bool = init_plugins\n</code></pre> <p>Whether to initialize the plugins or not.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.n_running","title":"n_running  <code>property</code>","text":"<pre><code>n_running: int\n</code></pre> <p>Get the number of futures for this task that are currently running.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str | None = name\n</code></pre> <p>The name of the emitter.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_cancelled","title":"on_cancelled  <code>instance-attribute</code>","text":"<pre><code>on_cancelled: Subscriber[[Future[R]], Any] = subscriber(\n    CANCELLED\n)\n</code></pre> <p>Called when a task is cancelled. <pre><code>@task.on_cancelled\ndef on_cancelled(future: Future[R]):\n    print(f\"Future {future} was cancelled\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_done","title":"on_done  <code>instance-attribute</code>","text":"<pre><code>on_done: Subscriber[[Future[R]], Any] = subscriber(DONE)\n</code></pre> <p>Called when a task is done running with a result or exception. <pre><code>@task.on_done\ndef on_done(future: Future[R]):\n    print(f\"Future {future} is done\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_exception","title":"on_exception  <code>instance-attribute</code>","text":"<pre><code>on_exception: Subscriber[\n    [Future[R], BaseException], Any\n] = subscriber(EXCEPTION)\n</code></pre> <p>Called when a task failed to return anything but an exception. Comes with Future <pre><code>@task.on_exception\ndef on_exception(future: Future[R], error: BaseException):\n    print(f\"Future {future} exceptioned {error}\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_result","title":"on_result  <code>instance-attribute</code>","text":"<pre><code>on_result: Subscriber[[Future[R], R], Any] = subscriber(\n    RESULT\n)\n</code></pre> <p>Called when a task has successfully returned a value. Comes with Future <pre><code>@task.on_result\ndef on_result(future: Future[R], result: R):\n    print(f\"Future {future} returned {result}\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_submitted","title":"on_submitted  <code>instance-attribute</code>","text":"<pre><code>on_submitted: Subscriber[\n    Concatenate[Future[R], P], Any\n] = subscriber(SUBMITTED)\n</code></pre> <p>An event that is emitted when a future is submitted to the scheduler. It will pass the future as the first argument with args and kwargs following.</p> <p>This is done before any callbacks are attached to the future. <pre><code>@task.on_submitted\ndef on_submitted(future: Future[R], *args, **kwargs):\n    print(f\"Future {future} was submitted with {args=} and {kwargs=}\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.plugins","title":"plugins  <code>instance-attribute</code>","text":"<pre><code>plugins: list[Plugin] = (\n    [plugins]\n    if isinstance(plugins, Plugin)\n    else list(plugins)\n)\n</code></pre> <p>The plugins to use for this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.queue","title":"queue  <code>instance-attribute</code>","text":"<pre><code>queue: list[Future[R]] = []\n</code></pre> <p>The queue of futures for this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.scheduler","title":"scheduler  <code>instance-attribute</code>","text":"<pre><code>scheduler: Scheduler = scheduler\n</code></pre> <p>The scheduler that this task is registered with.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.unique_ref","title":"unique_ref  <code>instance-attribute</code>","text":"<pre><code>unique_ref: str\n</code></pre> <p>A unique reference to this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.add_event","title":"add_event","text":"<pre><code>add_event(*event: Event) -&gt; None\n</code></pre> <p>Add an event to the event manager so that it shows up in visuals.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to add.</p> <p> TYPE: <code>Event</code> DEFAULT: <code>()</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def add_event(self, *event: Event) -&gt; None:\n    \"\"\"Add an event to the event manager so that it shows up in visuals.\n\n    Args:\n        event: The event to add.\n    \"\"\"\n    for e in event:\n        if e not in self.handlers:\n            self.handlers[e] = []\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.as_event","title":"as_event","text":"<pre><code>as_event(key: str | Event) -&gt; Event\n</code></pre> <p>Return the event associated with the key.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def as_event(self, key: str | Event) -&gt; Event:\n    \"\"\"Return the event associated with the key.\"\"\"\n    match key:\n        case Event():\n            return key\n        case str():\n            match = first_true(self.events, None, lambda e: e.name == key)\n            if match is None:\n                raise EventNotKnownError(\n                    f\"{key=} is not a valid event for {self.name}.\"\n                    f\"\\nKnown events are: {[e.name for e in self.events]}\",\n                )\n            return match\n        case _:\n            raise TypeError(f\"{key=} must be a string or an Event.\")\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.attach_plugin","title":"attach_plugin","text":"<pre><code>attach_plugin(plugin: Plugin) -&gt; None\n</code></pre> <p>Attach a plugin to this task.</p> PARAMETER DESCRIPTION <code>plugin</code> <p>The plugin to attach.</p> <p> TYPE: <code>Plugin</code> </p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def attach_plugin(self, plugin: Plugin) -&gt; None:\n    \"\"\"Attach a plugin to this task.\n\n    Args:\n        plugin: The plugin to attach.\n    \"\"\"\n    self.plugins.append(plugin)\n    plugin.attach_task(self)\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.emit","title":"emit","text":"<pre><code>emit(\n    event: Event[P, R], *args: args, **kwargs: kwargs\n) -&gt; list[tuple[Handler[P, R], R | None]]\n</code></pre> <p>Emit an event.</p> <p>This will call all the handlers for the event.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to emit. If passing a list, then the handlers for all events will be called, regardless of the order</p> <p> TYPE: <code>Event[P, R]</code> </p> <code>*args</code> <p>The positional arguments to pass to the handlers.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to pass to the handlers.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>list[tuple[Handler[P, R], R | None]]</code> <p>A list of the results from the handlers.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def emit(\n    self,\n    event: Event[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; list[tuple[Handler[P, R], R | None]]:\n    \"\"\"Emit an event.\n\n    This will call all the handlers for the event.\n\n    Args:\n        event: The event to emit.\n            If passing a list, then the handlers for all events will be called,\n            regardless of the order\n        *args: The positional arguments to pass to the handlers.\n        **kwargs: The keyword arguments to pass to the handlers.\n\n    Returns:\n        A list of the results from the handlers.\n    \"\"\"\n    logger.debug(f\"{self.name}: Emitting {event}\")\n\n    self.event_counts[event] += 1\n    return [(handler, handler(*args, **kwargs)) for handler in self.handlers[event]]\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.futures","title":"futures","text":"<pre><code>futures() -&gt; list[Future[R]]\n</code></pre> <p>Get the futures for this task.</p> RETURNS DESCRIPTION <code>list[Future[R]]</code> <p>A list of futures for this task.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def futures(self) -&gt; list[Future[R]]:\n    \"\"\"Get the futures for this task.\n\n    Returns:\n        A list of futures for this task.\n    \"\"\"\n    return self.queue\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on","title":"on","text":"<pre><code>on(\n    event: Event[P, R] | str,\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False\n) -&gt; Callable[[Callable[P, R | None]], None]\n</code></pre> <p>Register a callback for an event as a decorator.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P, R] | str</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def on(\n    self,\n    event: Event[P, R] | str,\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False,\n) -&gt; Callable[[Callable[P, R | None]], None]:\n    \"\"\"Register a callback for an event as a decorator.\n\n    Args:\n        event: The event to register the callback for.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n    \"\"\"\n    return partial(\n        self.subscriber(event=self.as_event(event)),  # type: ignore\n        when=when,\n        every=every,\n        repeat=repeat,\n        max_calls=max_calls,\n        hidden=hidden,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.register","title":"register","text":"<pre><code>register(\n    event: Event[P, R] | str,\n    callback: Callable[P, R],\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False\n) -&gt; None\n</code></pre> <p>Register a callback for an event.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P, R] | str</code> </p> <code>callback</code> <p>The callback to register.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def register(\n    self,\n    event: Event[P, R] | str,\n    callback: Callable[P, R],\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False,\n) -&gt; None:\n    \"\"\"Register a callback for an event.\n\n    Args:\n        event: The event to register the callback for.\n        callback: The callback to register.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n    \"\"\"\n    event = self.as_event(event)\n\n    if repeat &lt;= 0:\n        raise ValueError(f\"{repeat=} must be a positive integer.\")\n\n    if every &lt;= 0:\n        raise ValueError(f\"{every=} must be a positive integer.\")\n\n    # Make sure it shows up in the event counts, setting it to 0 if it\n    # doesn't exist\n    self.event_counts.setdefault(event, 0)\n    self.handlers[event].append(\n        Handler(\n            callback,\n            when=when,\n            every=every,\n            repeat=repeat,\n            max_calls=max_calls,\n            hidden=hidden,\n        ),\n    )\n\n    _name = funcname(callback)\n    msg = f\"{self.name}: Registered callback '{_name}' for event {event}\"\n    if every &gt; 1:\n        msg += f\" every {every} times\"\n    if when:\n        msg += f\" with predicate ({funcname(when)})\"\n    if repeat &gt; 1:\n        msg += f\" called {repeat} times successively\"\n    if hidden:\n        msg += \" (hidden from visual output)\"\n    logger.debug(msg)\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.running","title":"running","text":"<pre><code>running() -&gt; bool\n</code></pre> <p>Check if this task has any futures that are currently running.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def running(self) -&gt; bool:\n    \"\"\"Check if this task has any futures that are currently running.\"\"\"\n    return self.n_running &gt; 0\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.submit","title":"submit","text":"<pre><code>submit(*args: args, **kwargs: kwargs) -&gt; Future[R] | None\n</code></pre> <p>Dispatch this task.</p> PARAMETER DESCRIPTION <code>*args</code> <p>The positional arguments to pass to the task.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to call the task with.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Future[R] | None</code> <p>The future of the task, or <code>None</code> if it was rejected for either reaching some max call limit or a plugin prevented if from being submitted.</p> RAISES DESCRIPTION <code>SchedulerNotRunningError</code> <p>If the scheduler is not running. You can protect against this using, <code>scheduler.running()</code>.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def submit(self, *args: P.args, **kwargs: P.kwargs) -&gt; Future[R] | None:\n    \"\"\"Dispatch this task.\n\n    Args:\n        *args: The positional arguments to pass to the task.\n        **kwargs: The keyword arguments to call the task with.\n\n    Returns:\n        The future of the task, or `None` if it was rejected for either reaching\n        some max call limit or a plugin prevented if from being submitted.\n\n    Raises:\n        SchedulerNotRunningError: If the scheduler is not running.\n            You can protect against this using,\n            [`scheduler.running()`][amltk.scheduling.scheduler.Scheduler.running].\n    \"\"\"\n    # Inform all plugins that the task is about to be called\n    # They have chance to cancel submission based on their return\n    # value.\n    fn = self.function\n    for plugin in self.plugins:\n        items = plugin.pre_submit(fn, *args, **kwargs)\n        if items is None:\n            logger.debug(\n                f\"Plugin '{plugin.name}' prevented {self} from being submitted\"\n                f\" with {callstring(self.function, *args, **kwargs)}\",\n            )\n            return None\n\n        fn, args, kwargs = items  # type: ignore\n\n    try:\n        future = self.scheduler.submit(fn, *args, **kwargs)\n    except SchedulerNotRunningError as e:\n        logger.exception(\"Scheduler is not running\", exc_info=e)\n        raise e\n    except Exception as e:\n        logger.exception(\"Error submitting task\", exc_info=e)\n        raise e\n\n    self.queue.append(future)\n\n    # We have the function wrapped in something will\n    # attach tracebacks to errors, so we need to get the\n    # original function name.\n    msg = f\"Submitted {callstring(self.function, *args, **kwargs)} from {self}.\"\n    logger.debug(msg)\n\n    self.on_submitted.emit(future, *args, **kwargs)\n\n    # Process the task once it's completed\n    # NOTE: If the task is done super quickly or in the sequential mode,\n    # this will immediatly call `self._process_future`.\n    future.add_done_callback(self._process_future)\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.subscriber","title":"subscriber","text":"<pre><code>subscriber(event: Event[P, R]) -&gt; Subscriber[P, R]\n</code></pre> <p>Create a subscriber for an event.</p> PARAMETER DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P, R]</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def subscriber(self, event: Event[P, R]) -&gt; Subscriber[P, R]:\n    \"\"\"Create a subscriber for an event.\n\n    Args:\n        event: The event to register the callback for.\n    \"\"\"\n    if event not in self.handlers:\n        self.handlers[event] = []\n\n    return Subscriber(self, event)\n</code></pre>"},{"location":"api/amltk/scheduling/termination_strategies/","title":"Termination strategies","text":""},{"location":"api/amltk/scheduling/termination_strategies/#amltk.scheduling.termination_strategies","title":"amltk.scheduling.termination_strategies","text":"<p>This module is concerned with the termination of the workers of a scheduler.</p> <p>Most Executors on <code>shutdown(wait=False)</code> will not terminate their workers but just cancel pending futures. This means that currently running tasks will continue to run until they finish and have their result discarded as the program will have moved on.</p> <p>We provide some custom strategies for known executors.</p> Note <p>There is no known way in basic Python to forcibully terminate a thread that does not account for early terminiation explicitly.</p>"},{"location":"api/amltk/scheduling/termination_strategies/#amltk.scheduling.termination_strategies.polite_kill","title":"polite_kill","text":"<pre><code>polite_kill(\n    process: Process, timeout: int | None = None\n) -&gt; None\n</code></pre> <p>Politely kill a process.</p> <p>This works by first sending a SIGTERM to the process, and then if it doesn't respond to that, sending a SIGKILL.</p> <p>On Windows, SIGTERM is not available, so <code>terminate()</code> will send a <code>SIGKILL</code> directly.</p> PARAMETER DESCRIPTION <code>process</code> <p>The process to kill.</p> <p> TYPE: <code>Process</code> </p> <code>timeout</code> <p>The time to wait for the process after sending SIGTERM. before resorting to SIGKILL. If None, wait indefinitely.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/termination_strategies.py</code> <pre><code>def polite_kill(process: psutil.Process, timeout: int | None = None) -&gt; None:\n    \"\"\"Politely kill a process.\n\n    This works by first sending a SIGTERM to the process, and then if it\n    doesn't respond to that, sending a SIGKILL.\n\n    On Windows, SIGTERM is not available, so `terminate()` will\n    send a `SIGKILL` directly.\n\n    Args:\n        process: The process to kill.\n        timeout: The time to wait for the process after sending SIGTERM.\n            before resorting to SIGKILL. If None, wait indefinitely.\n    \"\"\"\n    with suppress(psutil.NoSuchProcess):\n        process.terminate()\n        process.wait(timeout=timeout)\n\n        # Forcibly kill it if it's not responding to the SIGTERM\n        if process.is_running():\n            process.kill()\n</code></pre>"},{"location":"api/amltk/scheduling/termination_strategies/#amltk.scheduling.termination_strategies.termination_strategy","title":"termination_strategy","text":"<pre><code>termination_strategy(\n    executor: _Executor,\n) -&gt; Callable[[_Executor], None] | None\n</code></pre> <p>Return a termination strategy for the given executor.</p> PARAMETER DESCRIPTION <code>executor</code> <p>The executor to get a termination strategy for.</p> <p> TYPE: <code>_Executor</code> </p> RETURNS DESCRIPTION <code>Callable[[_Executor], None] | None</code> <p>A termination strategy for the given executor, or None if no termination strategy is available.</p> Source code in <code>src/amltk/scheduling/termination_strategies.py</code> <pre><code>def termination_strategy(executor: _Executor) -&gt; Callable[[_Executor], None] | None:\n    \"\"\"Return a termination strategy for the given executor.\n\n    Args:\n        executor: The executor to get a termination strategy for.\n\n    Returns:\n        A termination strategy for the given executor, or None if no\n        termination strategy is available.\n    \"\"\"\n    if isinstance(executor, ThreadPoolExecutor):\n        return None\n\n    if isinstance(executor, ProcessPoolExecutor):\n        return _terminate_with_psutil  # type: ignore\n\n    # Dask process based things seem pretty happy to close nicely and need\n    # no special treatment.\n\n    return None\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/","title":"Dask jobqueue","text":""},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue","title":"amltk.scheduling.executors.dask_jobqueue","text":"<p>Dask Jobqueue Executors.</p> <p>These are essentially wrappers around the dask_jobqueue classes. We use them to provide a consistent interface for all the different jobqueue implementations and get access to their executors.</p> <p>Documentation from <code>dask_jobqueue</code></p> <p>See the dask jobqueue documentation specifically:</p> <ul> <li>Example deployments</li> <li>Tips and Tricks</li> <li>Debugging</li> </ul>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor","title":"DaskJobqueueExecutor","text":"<pre><code>DaskJobqueueExecutor(\n    cluster: _JQC,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None\n)\n</code></pre> <p>               Bases: <code>Executor</code>, <code>Generic[_JQC]</code></p> <p>A concurrent.futures Executor that executes tasks on a dask_jobqueue cluster.</p> <p>Implementations</p> <p>Prefer to use the class methods to create an instance of this class.</p> <ul> <li><code>DaskJobqueueExecutor.SLURM()</code></li> <li><code>DaskJobqueueExecutor.HTCondor()</code></li> <li><code>DaskJobqueueExecutor.LSF()</code></li> <li><code>DaskJobqueueExecutor.OAR()</code></li> <li><code>DaskJobqueueExecutor.PBS()</code></li> <li><code>DaskJobqueueExecutor.SGE()</code></li> <li><code>DaskJobqueueExecutor.Moab()</code></li> </ul> PARAMETER DESCRIPTION <code>cluster</code> <p>The implementation of a dask_jobqueue.JobQueueCluster.</p> <p> TYPE: <code>_JQC</code> </p> <code>n_workers</code> <p>The number of workers to maximally adapt to on the cluster.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>To overwrite the submission command if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>To overwrite the cancel command if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>def __init__(\n    self,\n    cluster: _JQC,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n):\n    \"\"\"Initialize a DaskJobqueueExecutor.\n\n\n    !!! note \"Implementations\"\n\n        Prefer to use the class methods to create an instance of this class.\n\n        * [`DaskJobqueueExecutor.SLURM()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SLURM]\n        * [`DaskJobqueueExecutor.HTCondor()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.HTCondor]\n        * [`DaskJobqueueExecutor.LSF()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.LSF]\n        * [`DaskJobqueueExecutor.OAR()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.OAR]\n        * [`DaskJobqueueExecutor.PBS()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.PBS]\n        * [`DaskJobqueueExecutor.SGE()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SGE]\n        * [`DaskJobqueueExecutor.Moab()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.Moab]\n\n    Args:\n        cluster: The implementation of a\n            [dask_jobqueue.JobQueueCluster](https://jobqueue.dask.org/en/latest/api.html).\n        n_workers: The number of workers to maximally adapt to on the cluster.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed allocate all workers.\n            This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers specified.\n        submit_command: To overwrite the submission command if necessary.\n        cancel_command: To overwrite the cancel command if necessary.\n    \"\"\"\n    super().__init__()\n    self.cluster = cluster\n    self.adaptive = adaptive\n    if submit_command:\n        self.cluster.job_cls.submit_command = submit_command  # type: ignore\n\n    if cancel_command:\n        self.cluster.job_cls.cancel_command = cancel_command  # type: ignore\n\n    if adaptive:\n        self.cluster.adapt(minimum=0, maximum=n_workers)\n    else:\n        self.cluster.scale(n_workers)\n\n    self.n_workers = n_workers\n    self._client = self.cluster.get_client()\n    self.executor: ClientExecutor = self._client.get_executor()\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.HTCondor","title":"HTCondor  <code>classmethod</code>","text":"<pre><code>HTCondor(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor[HTCondorCluster]\n</code></pre> <p>Create a DaskJobqueueExecutor for a HTCondor cluster.</p> <p>See the dask_jobqueue.HTCondorCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef HTCondor(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[HTCondorCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a HTCondor cluster.\n\n    See the [dask_jobqueue.HTCondorCluster documentation][dask_jobqueue.HTCondorCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        HTCondorCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.LSF","title":"LSF  <code>classmethod</code>","text":"<pre><code>LSF(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor[LSFCluster]\n</code></pre> <p>Create a DaskJobqueueExecutor for a LSF cluster.</p> <p>See the dask_jobqueue.LSFCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef LSF(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[LSFCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a LSF cluster.\n\n    See the [dask_jobqueue.LSFCluster documentation][dask_jobqueue.LSFCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        LSFCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.Moab","title":"Moab  <code>classmethod</code>","text":"<pre><code>Moab(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor[MoabCluster]\n</code></pre> <p>Create a DaskJobqueueExecutor for a Moab cluster.</p> <p>See the dask_jobqueue.MoabCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef Moab(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[MoabCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a Moab cluster.\n\n    See the [dask_jobqueue.MoabCluster documentation][dask_jobqueue.MoabCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        MoabCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.OAR","title":"OAR  <code>classmethod</code>","text":"<pre><code>OAR(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor[OARCluster]\n</code></pre> <p>Create a DaskJobqueueExecutor for a OAR cluster.</p> <p>See the dask_jobqueue.OARCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef OAR(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[OARCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a OAR cluster.\n\n    See the [dask_jobqueue.OARCluster documentation][dask_jobqueue.OARCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        OARCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.PBS","title":"PBS  <code>classmethod</code>","text":"<pre><code>PBS(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor[PBSCluster]\n</code></pre> <p>Create a DaskJobqueueExecutor for a PBS cluster.</p> <p>See the dask_jobqueue.PBSCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef PBS(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[PBSCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a PBS cluster.\n\n    See the [dask_jobqueue.PBSCluster documentation][dask_jobqueue.PBSCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        PBSCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SGE","title":"SGE  <code>classmethod</code>","text":"<pre><code>SGE(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor[SGECluster]\n</code></pre> <p>Create a DaskJobqueueExecutor for a SGE cluster.</p> <p>See the dask_jobqueue.SGECluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef SGE(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[SGECluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a SGE cluster.\n\n    See the [dask_jobqueue.SGECluster documentation][dask_jobqueue.SGECluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        SGECluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SLURM","title":"SLURM  <code>classmethod</code>","text":"<pre><code>SLURM(\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor[SLURMCluster]\n</code></pre> <p>Create a DaskJobqueueExecutor for a SLURM cluster.</p> <p>See the dask_jobqueue.SLURMCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef SLURM(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[SLURMCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a SLURM cluster.\n\n    See the [dask_jobqueue.SLURMCluster documentation][dask_jobqueue.SLURMCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        SLURMCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        n_workers=n_workers,\n        adaptive=adaptive,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(\n    name: DJQ_NAMES,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any\n) -&gt; DaskJobqueueExecutor\n</code></pre> <p>Create a DaskJobqueueExecutor using a string lookup.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of cluster to create, must be one of [\"slurm\", \"htcondor\", \"lsf\", \"oar\", \"pbs\", \"sge\", \"moab\"].</p> <p> TYPE: <code>DJQ_NAMES</code> </p> <code>n_workers</code> <p>The number of workers to maximally adapt to on the cluster.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the submit command of workers if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the cancel command of workers if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>The keyword arguments to pass to the cluster constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>KeyError</code> <p>If <code>name</code> is not one of the supported cluster types.</p> RETURNS DESCRIPTION <code>DaskJobqueueExecutor</code> <p>A DaskJobqueueExecutor for the requested cluster type.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef from_str(\n    cls,\n    name: DJQ_NAMES,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor:\n    \"\"\"Create a DaskJobqueueExecutor using a string lookup.\n\n    Args:\n        name: The name of cluster to create, must be one of\n            [\"slurm\", \"htcondor\", \"lsf\", \"oar\", \"pbs\", \"sge\", \"moab\"].\n        n_workers: The number of workers to maximally adapt to on the cluster.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed allocate all workers.\n            This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers specified.\n        submit_command: Overwrite the submit command of workers if necessary.\n        cancel_command: Overwrite the cancel command of workers if necessary.\n        kwargs: The keyword arguments to pass to the cluster constructor.\n\n    Raises:\n        KeyError: If `name` is not one of the supported cluster types.\n\n    Returns:\n        A DaskJobqueueExecutor for the requested cluster type.\n    \"\"\"\n    methods = {\n        \"slurm\": cls.SLURM,\n        \"htcondor\": cls.HTCondor,\n        \"lsf\": cls.LSF,\n        \"oar\": cls.OAR,\n        \"pbs\": cls.PBS,\n        \"sge\": cls.SGE,\n        \"moab\": cls.Moab,\n    }\n    method = methods.get(name.lower())\n    if method is None:\n        raise KeyError(\n            f\"Unknown cluster name: {name}, must be from {list(methods)}\",\n        )\n\n    return method(\n        n_workers=n_workers,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.map","title":"map","text":"<pre><code>map(\n    fn: Callable[..., R],\n    *iterables: Iterable,\n    timeout: float | None = None,\n    chunksize: int = 1\n) -&gt; Iterator[R]\n</code></pre> <p>See concurrent.futures.Executor.map.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@override\ndef map(\n    self,\n    fn: Callable[..., R],\n    *iterables: Iterable,\n    timeout: float | None = None,\n    chunksize: int = 1,\n) -&gt; Iterator[R]:\n    \"\"\"See [concurrent.futures.Executor.map][].\"\"\"\n    return self.executor.map(  # type: ignore\n        fn,\n        *iterables,\n        timeout=timeout,\n        chunksize=chunksize,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.shutdown","title":"shutdown","text":"<pre><code>shutdown(wait: bool = True, **kwargs: Any) -&gt; None\n</code></pre> <p>See concurrent.futures.Executor.shutdown.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@override\ndef shutdown(\n    self,\n    wait: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"See [concurrent.futures.Executor.shutdown][].\"\"\"\n    self.executor.shutdown(wait=wait, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.submit","title":"submit","text":"<pre><code>submit(\n    fn: Callable[P, R], /, *args: args, **kwargs: kwargs\n) -&gt; Future[R]\n</code></pre> <p>See concurrent.futures.Executor.submit.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@override\ndef submit(\n    self,\n    fn: Callable[P, R],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[R]:\n    \"\"\"See [concurrent.futures.Executor.submit][].\"\"\"\n    future = self.executor.submit(fn, *args, **kwargs)\n    assert isinstance(future, Future)\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/executors/sequential_executor/","title":"Sequential executor","text":""},{"location":"api/amltk/scheduling/executors/sequential_executor/#amltk.scheduling.executors.sequential_executor","title":"amltk.scheduling.executors.sequential_executor","text":"<p>A concurrent.futures.Executor interface that forces sequential execution.</p>"},{"location":"api/amltk/scheduling/executors/sequential_executor/#amltk.scheduling.executors.sequential_executor.SequentialExecutor","title":"SequentialExecutor","text":"<p>               Bases: <code>Executor</code></p> <p>A Executor interface for sequential execution.</p>"},{"location":"api/amltk/scheduling/executors/sequential_executor/#amltk.scheduling.executors.sequential_executor.SequentialExecutor.submit","title":"submit","text":"<pre><code>submit(\n    fn: Callable[P, R], /, *args: args, **kwargs: kwargs\n) -&gt; Future[R]\n</code></pre> <p>Submit a function to be executed.</p> PARAMETER DESCRIPTION <code>fn</code> <p>The function to execute.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>*args</code> <p>The positional arguments to pass to the function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to pass to the function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Future[R]</code> <p>A future that is already resolved with the result/exception of the function.</p> Source code in <code>src/amltk/scheduling/executors/sequential_executor.py</code> <pre><code>@override\ndef submit(\n    self,\n    fn: Callable[P, R],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[R]:\n    \"\"\"Submit a function to be executed.\n\n    Args:\n        fn: The function to execute.\n        *args: The positional arguments to pass to the function.\n        **kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        A future that is already resolved with the result/exception of the function.\n    \"\"\"\n    # TODO: It would be good if we can somehow wrap this in some sort\n    # of async context such that it allows other callbacks to operate.\n    future: Future[R] = Future()\n    future.set_running_or_notify_cancel()\n\n    try:\n        future.set_result(fn(*args, **kwargs))\n    except BaseException as exc:  # noqa: BLE001\n        future.set_exception(exc)\n\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/","title":"Comm","text":""},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm","title":"amltk.scheduling.plugins.comm","text":"<p>The <code>Comm.Plugin</code> enables two way-communication with running <code>Task</code>.</p> <p>The <code>Comm</code> provides an easy interface to communicate while the <code>Comm.Msg</code> encapsulates messages between the main process and the <code>Task</code>.</p> Usage <p>To setup a <code>Task</code> to work with a <code>Comm</code>, the <code>Task</code> must accept a <code>comm</code> as a keyword argument. This is to prevent it conflicting with any args passed through during the call to <code>submit()</code>.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef powers_of_two(start: int, n: int, *, comm: Comm) -&gt; None:\n    with comm.open():\n        for i in range(n):\n            comm.send(start ** (i+1))\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(powers_of_two, plugins=Comm.Plugin())\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(2, 5)\n\n@task.on(\"comm-open\")\ndef on_open(msg: Comm.Msg):\n    print(f\"Task has opened | {msg}\")\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>Task has opened | Comm.Msg(kind=&lt;Kind.OPEN: 'open'&gt;, data=None)\n[2, 4, 8, 16, 32]\n</code></pre> <p>You can also block a worker, waiting for a response from the main process, allowing for the worker to <code>request()</code> data from the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef my_worker(comm: Comm, n_tasks: int) -&gt; None:\n    with comm.open():\n        for task_number in range(n_tasks):\n            task = comm.request(task_number)\n            comm.send(f\"Task recieved {task} for {task_number}\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(my_worker, plugins=Comm.Plugin())\n\nitems = [\"A\", \"B\", \"C\"]\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(n_tasks=3)\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    task_number = msg.data\n    msg.respond(items[task_number])\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>['Task recieved A for 0', 'Task recieved B for 1', 'Task recieved C for 2']\n</code></pre> <code>@events</code> <code>@comm-message</code><code>@comm-request</code><code>@comm-open</code><code>@comm-close</code>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.MESSAGE","title":"amltk.scheduling.plugins.comm.Comm.MESSAGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MESSAGE: Event[[Msg], Any] = Event('comm-message')\n</code></pre> <p>A Task has sent a message to the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(x: int, comm: Comm | None = None) -&gt; int:\n    assert comm is not None\n    with comm.open():\n        comm.send(x + 1)\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@task.on(\"comm-message\")\ndef callback(msg: Comm.Msg):\n    print(msg.data)\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int, comm: amltk.scheduling.plugins.comm.Comm | None = None) -&gt; \u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-ZVHs2JO2 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.REQUEST","title":"amltk.scheduling.plugins.comm.Comm.REQUEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REQUEST: Event[[Msg], Any] = Event('comm-request')\n</code></pre> <p>A Task has sent a request.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef greeter(greeting: str, comm: Comm | None = None) -&gt; None:\n    assert comm is not None\n    with comm.open():\n        name = comm.request()\n        comm.send(f\"{greeting} {name}!\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(greeter, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit(\"Hello\")\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    msg.respond(\"Alice\")\n\n@task.on(\"comm-message\")\ndef on_msg(msg: Comm.Msg):\n    print(msg.data)\n\nscheduler.run()\n</code></pre>  Hello Alice!  <pre>\n<code>\u256d\u2500 Task greeter(greeting: str, comm: amltk.scheduling.plugins.comm.Comm | None\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-greeter-4pr2HmvR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.OPEN","title":"amltk.scheduling.plugins.comm.Comm.OPEN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPEN: Event[[Msg], Any] = Event('comm-open')\n</code></pre> <p>The task has signalled it's open.</p> <p>```python exec=\"true\" source=\"material-block\" html=\"true\" hl_lines=\"5 15-17\"  from amltk.scheduling import Scheduler  from amltk.scheduling.plugins import Comm</p> <p>def fn(comm: Comm) -&gt; None:      with comm.open():          pass from amltk._doc import make_picklable; make_picklable(fn)  # markdown-exec: hide</p> <p>scheduler = Scheduler.with_processes(1)  task = scheduler.task(fn, plugins=Comm.Plugin())</p> <p>@scheduler.on_start  def on_start():      task.submit()</p> <p>@task.on(\"comm-open\")  def callback(msg: Comm.Msg):      print(\"Comm has just used comm.open()\")</p> <p>scheduler.run()  from amltk._doc import doc_print; doc_print(print, task)  # markdown-exec: hide  ```</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.CLOSE","title":"amltk.scheduling.plugins.comm.Comm.CLOSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLOSE: Event[[Msg], Any] = Event('comm-close')\n</code></pre> <p>The task has signalled it's close.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm) -&gt; None:\n    with comm.open():\n        pass\n        # Will send a close signal to the main process as it exists this block\n\n    print(\"Done\")\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit()\n\n@task.on(\"comm-close\")\ndef on_close(msg: Comm.Msg):\n    print(f\"Worker close with {msg}\")\n\nscheduler.run()\n</code></pre>  Worker close with Comm.Msg(kind=, data=None)  <pre>\n<code>\u256d\u2500 Task fn(comm: amltk.scheduling.plugins.comm.Comm) -&gt; None \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-WpucW405 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> Supported Backends <p>The current implementation relies on <code>Pipe</code> which only works between processes on the same system/cluster. There is also limited support with <code>dask</code> backends.</p> <p>This could be extended to allow for web sockets or other forms of connections but requires time. Please let us know in the Github issues if this is something you are interested in!</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.AsyncComm","title":"AsyncComm  <code>dataclass</code>","text":"<pre><code>AsyncComm(comm: Comm)\n</code></pre> <p>A async wrapper of a Comm.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.AsyncComm.request","title":"request  <code>async</code>","text":"<pre><code>request(*, timeout: float | None = None) -&gt; Any\n</code></pre> <p>Recieve a message.</p> PARAMETER DESCRIPTION <code>timeout</code> <p>The timeout in seconds to wait for a message, raises a <code>Comm.TimeoutError</code> if the timeout is reached. If <code>None</code>, will wait forever.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The message from the worker or the default value.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>async def request(\n    self,\n    *,\n    timeout: float | None = None,\n) -&gt; Any:\n    \"\"\"Recieve a message.\n\n    Args:\n        timeout: The timeout in seconds to wait for a message, raises\n            a [`Comm.TimeoutError`][amltk.scheduling.plugins.comm.Comm.TimeoutError]\n            if the timeout is reached.\n            If `None`, will wait forever.\n\n    Returns:\n        The message from the worker or the default value.\n    \"\"\"\n    connection = AsyncConnection(self.comm.connection)\n    try:\n        return await asyncio.wait_for(connection.recv(), timeout=timeout)\n    except asyncio.TimeoutError as e:\n        raise Comm.TimeoutError(\n            f\"Timed out waiting for response from {self.comm}\",\n        ) from e\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.AsyncComm.send","title":"send  <code>async</code>","text":"<pre><code>send(obj: Any) -&gt; None\n</code></pre> <p>Send a message.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The message to send.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>async def send(self, obj: Any) -&gt; None:\n    \"\"\"Send a message.\n\n    Args:\n        obj: The message to send.\n    \"\"\"\n    return await AsyncConnection(self.comm.connection).send(obj)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm","title":"Comm","text":"<pre><code>Comm(connection: Connection)\n</code></pre> <p>A communication channel between a worker and scheduler.</p> <p>For duplex connections, such as returned by python's builtin <code>Pipe</code>, use the <code>create(duplex=...)</code> class method.</p> <p>Adds three new events to the task:</p> <ul> <li><code>@comm-message</code></li> <li><code>@comm-request</code></li> <li><code>@comm-close</code></li> <li><code>@comm-open</code></li> </ul> ATTRIBUTE DESCRIPTION <code>connection</code> <p>The underlying Connection</p> <p> </p> <code>id</code> <p>The id of the comm.</p> <p> TYPE: <code>CommID</code> </p> PARAMETER DESCRIPTION <code>connection</code> <p>The underlying Connection</p> <p> TYPE: <code>Connection</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def __init__(self, connection: Connection) -&gt; None:\n    \"\"\"Initialize the Comm.\n\n    Args:\n        connection: The underlying Connection\n    \"\"\"\n    super().__init__()\n    self.connection = connection\n    self.id: CommID = id(self)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.CLOSE","title":"CLOSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLOSE: Event[[Msg], Any] = Event('comm-close')\n</code></pre> <p>The task has signalled it's close.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm) -&gt; None:\n    with comm.open():\n        pass\n        # Will send a close signal to the main process as it exists this block\n\n    print(\"Done\")\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit()\n\n@task.on(\"comm-close\")\ndef on_close(msg: Comm.Msg):\n    print(f\"Worker close with {msg}\")\n\nscheduler.run()\n</code></pre>  Worker close with Comm.Msg(kind=, data=None)  <pre>\n<code>\u256d\u2500 Task fn(comm: amltk.scheduling.plugins.comm.Comm) -&gt; None \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-lFoKfnT7 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.MESSAGE","title":"MESSAGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MESSAGE: Event[[Msg], Any] = Event('comm-message')\n</code></pre> <p>A Task has sent a message to the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(x: int, comm: Comm | None = None) -&gt; int:\n    assert comm is not None\n    with comm.open():\n        comm.send(x + 1)\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@task.on(\"comm-message\")\ndef callback(msg: Comm.Msg):\n    print(msg.data)\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int, comm: amltk.scheduling.plugins.comm.Comm | None = None) -&gt; \u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-ULAboBss \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.OPEN","title":"OPEN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPEN: Event[[Msg], Any] = Event('comm-open')\n</code></pre> <p>The task has signalled it's open.</p> <p>```python exec=\"true\" source=\"material-block\" html=\"true\" hl_lines=\"5 15-17\"  from amltk.scheduling import Scheduler  from amltk.scheduling.plugins import Comm</p> <p>def fn(comm: Comm) -&gt; None:      with comm.open():          pass from amltk._doc import make_picklable; make_picklable(fn)  # markdown-exec: hide</p> <p>scheduler = Scheduler.with_processes(1)  task = scheduler.task(fn, plugins=Comm.Plugin())</p> <p>@scheduler.on_start  def on_start():      task.submit()</p> <p>@task.on(\"comm-open\")  def callback(msg: Comm.Msg):      print(\"Comm has just used comm.open()\")</p> <p>scheduler.run()  from amltk._doc import doc_print; doc_print(print, task)  # markdown-exec: hide  ```</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.REQUEST","title":"REQUEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REQUEST: Event[[Msg], Any] = Event('comm-request')\n</code></pre> <p>A Task has sent a request.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef greeter(greeting: str, comm: Comm | None = None) -&gt; None:\n    assert comm is not None\n    with comm.open():\n        name = comm.request()\n        comm.send(f\"{greeting} {name}!\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(greeter, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit(\"Hello\")\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    msg.respond(\"Alice\")\n\n@task.on(\"comm-message\")\ndef on_msg(msg: Comm.Msg):\n    print(msg.data)\n\nscheduler.run()\n</code></pre>  Hello Alice!  <pre>\n<code>\u256d\u2500 Task greeter(greeting: str, comm: amltk.scheduling.plugins.comm.Comm | None\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-greeter-qMdkvLoM \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.as_async","title":"as_async  <code>property</code>","text":"<pre><code>as_async: AsyncComm\n</code></pre> <p>Return an async version of this comm.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.CloseRequestError","title":"CloseRequestError","text":"<p>               Bases: <code>RuntimeError</code></p> <p>An exception happened in the main process and it send a response to the worker to raise this exception.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Msg","title":"Msg  <code>dataclass</code>","text":"<pre><code>Msg(\n    kind: Kind,\n    data: T,\n    comm: Comm,\n    future: Future,\n    task: Task,\n)\n</code></pre> <p>               Bases: <code>Generic[T]</code></p> <p>A message sent over a communication channel.</p> ATTRIBUTE DESCRIPTION <code>task</code> <p>The task that sent the message.</p> <p> TYPE: <code>Task</code> </p> <code>comm</code> <p>The communication channel.</p> <p> TYPE: <code>Comm</code> </p> <code>future</code> <p>The future of the task.</p> <p> TYPE: <code>Future</code> </p> <code>data</code> <p>The data sent by the task.</p> <p> TYPE: <code>T</code> </p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Msg.Kind","title":"Kind","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The kind of message.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Msg.respond","title":"respond","text":"<pre><code>respond(response: Any) -&gt; None\n</code></pre> <p>Respond to the message.</p> PARAMETER DESCRIPTION <code>response</code> <p>The response to send back to the task.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def respond(self, response: Any) -&gt; None:\n    \"\"\"Respond to the message.\n\n    Args:\n        response: The response to send back to the task.\n    \"\"\"\n    self.comm._send_pipe(response)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin","title":"Plugin","text":"<pre><code>Plugin(\n    parameter_name: str = \"comm\",\n    create_comms: (\n        Callable[[], tuple[Comm, Comm]] | None\n    ) = None,\n)\n</code></pre> <p>               Bases: <code>Plugin</code></p> <p>A plugin that handles communication with a worker.</p> PARAMETER DESCRIPTION <code>parameter_name</code> <p>The name of the parameter to inject the comm into.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'comm'</code> </p> <code>create_comms</code> <p>A function that creates a pair of communication channels. Defaults to <code>Comm.create</code>.</p> <p> TYPE: <code>Callable[[], tuple[Comm, Comm]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def __init__(\n    self,\n    parameter_name: str = \"comm\",\n    create_comms: Callable[[], tuple[Comm, Comm]] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Args:\n        parameter_name: The name of the parameter to inject the comm into.\n        create_comms: A function that creates a pair of communication\n            channels. Defaults to `Comm.create`.\n    \"\"\"\n    super().__init__()\n    if create_comms is None:\n        create_comms = Comm.create\n\n    self.parameter_name = parameter_name\n    self.create_comms = create_comms\n    self.comms: dict[CommID, tuple[Comm, Comm]] = {}\n    self.communication_tasks: list[asyncio.Task] = []\n    self.task: Task\n    self.open_comms: set[CommID] = set()\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Attach the plugin to a task.</p> <p>This method is called when the plugin is attached to a task. This is the place to subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task the plugin is being attached to.</p> <p> TYPE: <code>Task</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\n\n    This method is called when the plugin is attached to a task. This\n    is the place to subscribe to events on the task, create new subscribers\n    for people to use or even store a reference to the task for later use.\n\n    Args:\n        task: The task the plugin is being attached to.\n    \"\"\"\n    self.task = task\n    task.add_event(Comm.MESSAGE, Comm.REQUEST, Comm.OPEN, Comm.CLOSE)\n    task.on_submitted(self._begin_listening, hidden=True)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: args, **kwargs: kwargs\n) -&gt; tuple[Callable[P, R], tuple, dict] | None\n</code></pre> <p>Pre-submit hook.</p> <p>This method is called before the task is submitted.</p> PARAMETER DESCRIPTION <code>fn</code> <p>The task function.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>*args</code> <p>The arguments to the task function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to the task function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>A tuple of the task function, arguments and keyword arguments if the task should be submitted, or <code>None</code> if the task should not be submitted.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    This method is called before the task is submitted.\n\n    Args:\n        fn: The task function.\n        *args: The arguments to the task function.\n        **kwargs: The keyword arguments to the task function.\n\n    Returns:\n        A tuple of the task function, arguments and keyword arguments\n        if the task should be submitted, or `None` if the task should\n        not be submitted.\n    \"\"\"\n    host_comm, worker_comm = self.create_comms()\n    if self.parameter_name in kwargs:\n        raise ValueError(\n            f\"Parameter {self.parameter_name} already exists in kwargs!\",\n        )\n\n    kwargs[self.parameter_name] = worker_comm\n\n    # We don't necessarily know if the future will be submitted. If so,\n    # we will use this index later to retrieve the host_comm\n    self.comms[worker_comm.id] = (host_comm, worker_comm)\n\n    # Make sure to include the Comm\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.TimeoutError","title":"TimeoutError","text":"<p>               Bases: <code>TimeoutError</code></p> <p>A timeout error for communications.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.close","title":"close","text":"<pre><code>close(\n    msg: Any | None = None,\n    *,\n    wait_for_ack: bool = False,\n    okay_if_broken_pipe: bool = False,\n    side: str = \"\"\n) -&gt; None\n</code></pre> <p>Close the connection.</p> PARAMETER DESCRIPTION <code>msg</code> <p>The message to send to the other end of the connection.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>wait_for_ack</code> <p>If <code>True</code>, wait for an acknowledgement from the other end before closing the connection.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>okay_if_broken_pipe</code> <p>If <code>True</code>, will not log an error if the connection is already closed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>side</code> <p>The side of the connection for naming purposes.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def close(  # noqa: PLR0912, C901\n    self,\n    msg: Any | None = None,\n    *,\n    wait_for_ack: bool = False,\n    okay_if_broken_pipe: bool = False,\n    side: str = \"\",\n) -&gt; None:\n    \"\"\"Close the connection.\n\n    Args:\n        msg: The message to send to the other end of the connection.\n        wait_for_ack: If `True`, wait for an acknowledgement from the\n            other end before closing the connection.\n        okay_if_broken_pipe: If `True`, will not log an error if the\n            connection is already closed.\n        side: The side of the connection for naming purposes.\n    \"\"\"\n    if not self.connection.closed:\n        kind = Comm.Msg.Kind.CLOSE_WITH_ACK if wait_for_ack else Comm.Msg.Kind.CLOSE\n        try:\n            self._send_pipe((kind, msg))\n        except BrokenPipeError as e:\n            if not okay_if_broken_pipe:\n                logger.error(f\"{side} - Error sending close signal: {type(e)}{e}\")\n        except Exception as e:  # noqa: BLE001\n            logger.error(f\"{side} - Error sending close signal: {type(e)}{e}\")\n        else:\n            if wait_for_ack:\n                logger.debug(f\"{side} - Waiting for ACK\")\n                try:\n                    recieved_msg = self.connection.recv()\n                except Exception as e:  # noqa: BLE001\n                    logger.error(\n                        f\"{side} - Error waiting for ACK, closing: {type(e)}{e}\",\n                    )\n                else:\n                    match recieved_msg:\n                        case Comm.Msg.Kind.WORKER_CLOSE_REQUEST:\n                            logger.error(\n                                f\"{side} - Worker recieved request to close!\",\n                            )\n                        case Comm.Msg.Kind.ACK:\n                            logger.debug(f\"{side} - Recieved ACK, closing\")\n                        case _:\n                            logger.warning(\n                                f\"{side} - Expected ACK but {recieved_msg=}\",\n                            )\n        finally:\n            try:\n                self.connection.close()\n            except OSError:\n                # It's possble that the connection was closed by the other end\n                # before we could close it.\n                pass\n            except Exception as e:  # noqa: BLE001\n                logger.error(f\"{side} - Error closing connection: {type(e)}{e}\")\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(*, duplex: bool = True) -&gt; tuple[Self, Self]\n</code></pre> <p>Create a pair of communication channels.</p> <p>Wraps the output of <code>multiprocessing.Pipe(duplex=duplex)</code>.</p> PARAMETER DESCRIPTION <code>duplex</code> <p>Whether to allow for two-way communication</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>tuple[Self, Self]</code> <p>A pair of communication channels.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@classmethod\ndef create(cls, *, duplex: bool = True) -&gt; tuple[Self, Self]:\n    \"\"\"Create a pair of communication channels.\n\n    Wraps the output of\n    [`multiprocessing.Pipe(duplex=duplex)`][multiprocessing.Pipe].\n\n    Args:\n        duplex: Whether to allow for two-way communication\n\n    Returns:\n        A pair of communication channels.\n    \"\"\"\n    reader, writer = Pipe(duplex=duplex)\n    return cls(reader), cls(writer)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.open","title":"open","text":"<pre><code>open(\n    opening_msg: Any | None = None,\n    *,\n    wait_for_ack: bool = False,\n    side: str = \"worker\"\n) -&gt; Iterator[Self]\n</code></pre> <p>Open the connection.</p> PARAMETER DESCRIPTION <code>opening_msg</code> <p>The message to send to the main process when the connection is opened.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>wait_for_ack</code> <p>If <code>True</code>, wait for an acknowledgement from the other end before closing the connection and exiting the context manager.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>side</code> <p>The side of the connection for naming purposes. Usually this is only done on the <code>\"worker\"</code> side.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'worker'</code> </p> YIELDS DESCRIPTION <code>Self</code> <p>The comm.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@contextmanager\ndef open(\n    self,\n    opening_msg: Any | None = None,\n    *,\n    wait_for_ack: bool = False,\n    side: str = \"worker\",\n) -&gt; Iterator[Self]:\n    \"\"\"Open the connection.\n\n    Args:\n        opening_msg: The message to send to the main process\n            when the connection is opened.\n        wait_for_ack: If `True`, wait for an acknowledgement from the\n            other end before closing the connection and exiting the\n            context manager.\n        side: The side of the connection for naming purposes.\n            Usually this is only done on the `\"worker\"` side.\n\n    Yields:\n        The comm.\n    \"\"\"\n    self._send_pipe((Comm.Msg.Kind.OPEN, opening_msg))\n    yield self\n    self.close(wait_for_ack=wait_for_ack, side=side)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.request","title":"request","text":"<pre><code>request(\n    msg: Any | None = None, *, timeout: None | float = None\n) -&gt; Any\n</code></pre> <p>Receive a message.</p> PARAMETER DESCRIPTION <code>msg</code> <p>The message to send to the other end of the connection. If left empty, will be <code>None</code>.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>timeout</code> <p>If float, will wait for that many seconds, raising an exception if exceeded. Otherwise, None will wait forever.</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TimeoutError</code> <p>If the timeout is reached.</p> <code>CloseRequestError</code> <p>If the other end needs to abruptly end and can not fufill the request. If thise error is thrown, the worker should finish as soon as possible.</p> RETURNS DESCRIPTION <code>Any</code> <p>The received message or the default.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def request(\n    self,\n    msg: Any | None = None,\n    *,\n    timeout: None | float = None,\n) -&gt; Any:\n    \"\"\"Receive a message.\n\n    Args:\n        msg: The message to send to the other end of the connection.\n            If left empty, will be `None`.\n        timeout: If float, will wait for that many seconds, raising an exception\n            if exceeded. Otherwise, None will wait forever.\n\n    Raises:\n        Comm.TimeoutError: If the timeout is reached.\n        Comm.CloseRequestError: If the other end needs to abruptly end and\n            can not fufill the request. If thise error is thrown, the worker\n            should finish as soon as possible.\n\n    Returns:\n        The received message or the default.\n    \"\"\"\n    self._send_pipe((Comm.Msg.Kind.REQUEST, msg))\n    if not self.connection.poll(timeout):\n        raise Comm.TimeoutError(f\"Timed out waiting for response for {msg}\")\n\n    response = self.connection.recv()\n    if response == Comm.Msg.Kind.WORKER_CLOSE_REQUEST:\n        logger.error(\"Worker recieved request to close!\")\n        raise Comm.CloseRequestError()\n\n    return response\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.send","title":"send","text":"<pre><code>send(obj: Any) -&gt; None\n</code></pre> <p>Send a message.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to send.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def send(self, obj: Any) -&gt; None:\n    \"\"\"Send a message.\n\n    Args:\n        obj: The object to send.\n    \"\"\"\n    self._send_pipe((Comm.Msg.Kind.MESSAGE, obj))\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/","title":"Emissions tracker plugin","text":""},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/#amltk.scheduling.plugins.emissions_tracker_plugin","title":"amltk.scheduling.plugins.emissions_tracker_plugin","text":"<p>Emissions Tracker Plugin Module.</p> <p>This module defines a plugin for tracking carbon emissions using the codecarbon library.</p> <p>For usage examples, refer to the docstring of the EmissionsTrackerPlugin class.</p>"},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/#amltk.scheduling.plugins.emissions_tracker_plugin.EmissionsTrackerPlugin","title":"EmissionsTrackerPlugin","text":"<pre><code>EmissionsTrackerPlugin(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Plugin</code></p> <p>A plugin that tracks carbon emissions using codecarbon library.</p> <p>Usage Example:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins.emissions_tracker_plugin import EmissionsTrackerPlugin\n\ndef some_function(x: int) -&gt; int:\n    return x * 2\n\nexecutor = ThreadPoolExecutor(max_workers=1)\n\n# Create a Scheduler instance with the executor\nscheduler = Scheduler(executor=executor)\n\n# Create a task with the emissions tracker plugin\ntask = scheduler.task(some_function, plugins=[\n    # Pass any codecarbon parameters as args here\n    EmissionsTrackerPlugin(log_level=\"info\", save_to_file=False)\n])\n\n@scheduler.on_start\ndef on_start():\n    task.submit(5)  # Submit any args here\n\n@task.on_submitted\ndef on_submitted(future, *args, **kwargs):\n    print(f\"Task was submitted\", future, args, kwargs)\n\n@task.on_done\ndef on_done(future):\n    # Result is the return value of the function\n    print(\"Task done: \", future.result())\n\nscheduler.run()\n</code></pre> PARAMETER DESCRIPTION <code>*args</code> <p>Additional arguments to pass to codecarbon library.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to codecarbon library.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>You can pass any codecarbon parameters as args to EmissionsTrackerPlugin. Please refer to the official codecarbon documentation for more details: mlco2.github.io/codecarbon/parameters.html</p> Source code in <code>src/amltk/scheduling/plugins/emissions_tracker_plugin.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    \"\"\"Initialize the EmissionsTrackerPlugin.\n\n    Args:\n        *args: Additional arguments to pass to codecarbon library.\n        **kwargs: Additional keyword arguments to pass to codecarbon library.\n\n    You can pass any codecarbon parameters as args to EmissionsTrackerPlugin.\n    Please refer to the official codecarbon documentation for more details:\n    https://mlco2.github.io/codecarbon/parameters.html\n    \"\"\"\n    super().__init__()\n    self.task: Task | None = None\n    self.codecarbon_args = args\n    self.codecarbon_kwargs = kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/#amltk.scheduling.plugins.emissions_tracker_plugin.EmissionsTrackerPlugin.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'emissions-tracker'\n</code></pre> <p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/#amltk.scheduling.plugins.emissions_tracker_plugin.EmissionsTrackerPlugin.__rich__","title":"__rich__","text":"<pre><code>__rich__() -&gt; Panel\n</code></pre> <p>Return a rich panel.</p> Source code in <code>src/amltk/scheduling/plugins/emissions_tracker_plugin.py</code> <pre><code>def __rich__(self) -&gt; Panel:\n    \"\"\"Return a rich panel.\"\"\"\n    from rich.panel import Panel\n\n    return Panel(\n        f\"codecarbon_args: {self.codecarbon_args} \"\n        f\"codecarbon_kwargs: {self.codecarbon_kwargs}\",\n        title=f\"Plugin {self.name}\",\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/#amltk.scheduling.plugins.emissions_tracker_plugin.EmissionsTrackerPlugin.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/emissions_tracker_plugin.py</code> <pre><code>def attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/#amltk.scheduling.plugins.emissions_tracker_plugin.EmissionsTrackerPlugin.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/emissions_tracker_plugin/#amltk.scheduling.plugins.emissions_tracker_plugin.EmissionsTrackerPlugin.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: args, **kwargs: kwargs\n) -&gt; tuple[Callable[P, R], tuple, dict]\n</code></pre> <p>Pre-submit hook.</p> Source code in <code>src/amltk/scheduling/plugins/emissions_tracker_plugin.py</code> <pre><code>def pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict]:\n    \"\"\"Pre-submit hook.\"\"\"\n    wrapped_f = _EmissionsTrackerWrapper(\n        fn,\n        self.task,\n        *self.codecarbon_args,\n        **self.codecarbon_kwargs,\n    )\n    return wrapped_f, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/","title":"Limiter","text":""},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter","title":"amltk.scheduling.plugins.limiter","text":"<p>The <code>Limiter</code> can limit the number of times a function is called, how many concurrent instances of it can be running, or whether it can run while another task is running.</p> <p>The functionality of the <code>Limiter</code> could also be implemented without a plugin but it gives some nice utility.</p> Usage <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-th9kx514 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <code>@events</code> <code>@call-limit-reached</code><code>@concurrent-limit-reached</code><code>@disabled-due-to-running-task</code>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.Limiter.CALL_LIMIT_REACHED","title":"amltk.scheduling.plugins.Limiter.CALL_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CALL_LIMIT_REACHED: Event[..., Any] = Event(\n    \"call-limit-reached\"\n)\n</code></pre> <p>The event emitted when the task has reached its call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-Zo7qMuE0 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.Limiter.CONCURRENT_LIMIT_REACHED","title":"amltk.scheduling.plugins.Limiter.CONCURRENT_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONCURRENT_LIMIT_REACHED: Event[..., Any] = Event(\n    \"concurrent-limit-reached\"\n)\n</code></pre> <p>The event emitted when the task has reached its concurrent call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_concurrent=2)])\n\n@task.on(\"concurrent-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Concurrent 0/2                                                           \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-D4jefqPO \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.Limiter.DISABLED_DUE_TO_RUNNING_TASK","title":"amltk.scheduling.plugins.Limiter.DISABLED_DUE_TO_RUNNING_TASK  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED_DUE_TO_RUNNING_TASK: Event[..., Any] = Event(\n    \"disabled-due-to-running-task\"\n)\n</code></pre> <p>The event emitter when the task was not submitted due to some other running task.</p> <p>Will call any subscribers with the task as first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\nother_task = scheduler.task(fn)\ntask = scheduler.task(fn, plugins=[Limiter(not_while_running=other_task)])\n\n@task.on(\"disabled-due-to-running-task\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Not While def fn(...) Ref: Task-fn-QRIkk6ee                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-y9aSlaHR \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter","title":"Limiter","text":"<pre><code>Limiter(\n    *,\n    max_calls: int | None = None,\n    max_concurrent: int | None = None,\n    not_while_running: Task | Iterable[Task] | None = None\n)\n</code></pre> <p>               Bases: <code>Plugin</code></p> <p>A plugin that limits the submission of a task.</p> <p>Adds three new events to the task:</p> <ul> <li><code>@call-limit-reached</code></li> <li><code>@concurrent-limit-reached</code></li> <li><code>@disabled-due-to-running-task</code></li> </ul> PARAMETER DESCRIPTION <code>max_calls</code> <p>The maximum number of calls to the task.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>max_concurrent</code> <p>The maximum number of calls of this task that can be in the queue.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>not_while_running</code> <p>A task or iterable of tasks that if active, will prevent this task from being submitted.</p> <p> TYPE: <code>Task | Iterable[Task] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/limiter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    max_calls: int | None = None,\n    max_concurrent: int | None = None,\n    not_while_running: Task | Iterable[Task] | None = None,\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        max_calls: The maximum number of calls to the task.\n        max_concurrent: The maximum number of calls of this task that can\n            be in the queue.\n        not_while_running: A task or iterable of tasks that if active, will prevent\n            this task from being submitted.\n    \"\"\"\n    super().__init__()\n\n    match not_while_running:\n        case None:\n            not_while_running = []\n        case Task():\n            not_while_running = [not_while_running]\n        case _:\n            not_while_running = list(not_while_running)\n\n    self.max_calls = max_calls\n    self.max_concurrent = max_concurrent\n    self.not_while_running = not_while_running\n    self.task: Task | None = None\n\n    if isinstance(max_calls, int) and not max_calls &gt; 0:\n        raise ValueError(\"max_calls must be greater than 0\")\n\n    if isinstance(max_concurrent, int) and not max_concurrent &gt; 0:\n        raise ValueError(\"max_concurrent must be greater than 0\")\n\n    self._calls = 0\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.CALL_LIMIT_REACHED","title":"CALL_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CALL_LIMIT_REACHED: Event[..., Any] = Event(\n    \"call-limit-reached\"\n)\n</code></pre> <p>The event emitted when the task has reached its call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-ILHItQwp \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.CONCURRENT_LIMIT_REACHED","title":"CONCURRENT_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONCURRENT_LIMIT_REACHED: Event[..., Any] = Event(\n    \"concurrent-limit-reached\"\n)\n</code></pre> <p>The event emitted when the task has reached its concurrent call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_concurrent=2)])\n\n@task.on(\"concurrent-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Concurrent 0/2                                                           \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-MoW997uV \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.DISABLED_DUE_TO_RUNNING_TASK","title":"DISABLED_DUE_TO_RUNNING_TASK  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED_DUE_TO_RUNNING_TASK: Event[..., Any] = Event(\n    \"disabled-due-to-running-task\"\n)\n</code></pre> <p>The event emitter when the task was not submitted due to some other running task.</p> <p>Will call any subscribers with the task as first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\nother_task = scheduler.task(fn)\ntask = scheduler.task(fn, plugins=[Limiter(not_while_running=other_task)])\n\n@task.on(\"disabled-due-to-running-task\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Not While def fn(...) Ref: Task-fn-U82CZSJS                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-ggSuhxLr \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.n_running","title":"n_running  <code>property</code>","text":"<pre><code>n_running: int\n</code></pre> <p>Return the number of running tasks.</p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'limiter'\n</code></pre> <p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/limiter.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n\n    if self.task in self.not_while_running:\n        raise ValueError(\n            f\"Task {self.task} was found in the {self.not_while_running=}\"\n            \" list. This is disabled but please raise an issue if you think this\"\n            \" has sufficient use case.\",\n        )\n\n    task.add_event(\n        self.CALL_LIMIT_REACHED,\n        self.CONCURRENT_LIMIT_REACHED,\n        self.DISABLED_DUE_TO_RUNNING_TASK,\n    )\n\n    # Make sure to increment the count when a task was submitted\n    task.on_submitted(self._increment_call_count, hidden=True)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: args, **kwargs: kwargs\n) -&gt; tuple[Callable[P, R], tuple, dict] | None\n</code></pre> <p>Pre-submit hook.</p> <p>Prevents submission of the task if it exceeds any of the set limits.</p> Source code in <code>src/amltk/scheduling/plugins/limiter.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    Prevents submission of the task if it exceeds any of the set limits.\n    \"\"\"\n    assert self.task is not None\n\n    if self.max_calls is not None and self._calls &gt;= self.max_calls:\n        self.task.emit(self.CALL_LIMIT_REACHED, self.task, *args, **kwargs)\n        return None\n\n    if self.max_concurrent is not None and self.n_running &gt;= self.max_concurrent:\n        self.task.emit(self.CONCURRENT_LIMIT_REACHED, self.task, *args, **kwargs)\n        return None\n\n    for other_task in self.not_while_running:\n        if other_task.running():\n            self.task.emit(\n                self.DISABLED_DUE_TO_RUNNING_TASK,\n                other_task,\n                self.task,\n                *args,\n                **kwargs,\n            )\n            return None\n\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/plugin/","title":"Plugin","text":""},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin","title":"amltk.scheduling.plugins.plugin","text":"<p>A plugin that can be attached to a Task.</p> <p>By inheriting from a <code>Plugin</code>, you can hook into a <code>Task</code>. A plugin can affect, modify and extend its behaviours. Please see the documentation of the methods for more information. Creating a plugin is only necesary if you need to modify actual behaviour of the task. For siply hooking into the lifecycle of a task, you can use the <code>@events</code> that a <code>Task</code> emits.</p> Creating a Plugin <p>For a full example of a simple plugin, see the <code>Limiter</code> plugin which prevents the task being submitted if for example, it has already been submitted too many times.</p> <p>The below example shows how to create a plugin that prints the task name before submitting it. It also emits an event when the task is submitted.</p> <pre><code>from __future__ import annotations\nfrom typing import Callable\n\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Plugin\nfrom amltk.scheduling.events import Event\n\n# A simple plugin that prints the task name before submitting\nclass Printer(Plugin):\n    name = \"my-plugin\"\n\n    # Define an event the plugin will emit\n    # Event[Task] indicates the callback for the event will be called with the task\n    PRINTED: Event[str] = Event(\"printer-msg\")\n\n    def __init__(self, greeting: str):\n        self.greeting = greeting\n        self.n_greetings = 0\n\n    def attach_task(self, task) -&gt; None:\n        self.task = task\n        # Register an event with the task, this lets the task know valid events\n        # people can subscribe to and helps it show up in visuals\n        task.add_event(self.PRINTED)\n        task.on_submitted(self._print_submitted, hidden=True)  # You can hide this callback from visuals\n\n    def pre_submit(self, fn, *args, **kwargs) -&gt; tuple[Callable, tuple, dict]:\n        print(f\"{self.greeting} for {self.task} {args} {kwargs}\")\n        self.n_greetings += 1\n        return fn, args, kwargs\n\n    def _print_submitted(self, future, *args, **kwargs) -&gt; None:\n        msg = f\"Task was submitted {self.task} {args} {kwargs}\"\n        self.task.emit(self.PRINTED, msg)  # Emit the event with a msg\n\n    def __rich__(self):\n        # Custome how the plugin is displayed in rich (Optional)\n        # rich is an optional dependancy of amltk so we move the imports into here\n        from rich.panel import Panel\n\n        return Panel(\n            f\"Greeting: {self.greeting} ({self.n_greetings})\",\n            title=f\"Plugin {self.name}\"\n        )\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=[Printer(\"Hello\")])\n\n@scheduler.on_start\ndef on_start():\n    task.submit(15)\n\n@task.on(\"printer-msg\")\ndef callback(msg: str):\n    print(\"\\nmsg\")\n\nscheduler.run()\n</code></pre>  Hello for Task(unique_ref=Task-fn-E0b1ecUe, plugins=[&lt;_code_block_n54_.Printer object at 0x7efd77cc6c80&gt;]) (15,) {}  msg  <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin my-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Greeting: Hello (1)                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-E0b1ecUe \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>All methods are optional, and you can choose to implement only the ones you need. Most plugins will likely need to implement the <code>attach_task()</code> method, which is called when the plugin is attached to a task. In this method, you can for example subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> <p>Plugins are also encouraged to utilize the events of a <code>Task</code> to further hook into the lifecycle of the task. For exampe, by saving a reference to the task in the <code>attach_task()</code> method, you can use the <code>emit()</code> method of the task to emit your own specialized events.</p>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin","title":"Plugin","text":"<p>               Bases: <code>RichRenderable</code>, <code>ABC</code></p> <p>A plugin that can be attached to a Task.</p>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.name","title":"name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the plugin.</p> <p>This is used to identify the plugin during logging.</p>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Attach the plugin to a task.</p> <p>This method is called when the plugin is attached to a task. This is the place to subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> PARAMETER DESCRIPTION <code>task</code> <p>The task the plugin is being attached to.</p> <p> TYPE: <code>Task</code> </p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\n\n    This method is called when the plugin is attached to a task. This\n    is the place to subscribe to events on the task, create new subscribers\n    for people to use or even store a reference to the task for later use.\n\n    Args:\n        task: The task the plugin is being attached to.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: args, **kwargs: kwargs\n) -&gt; tuple[Callable[P, R], tuple, dict] | None\n</code></pre> <p>Pre-submit hook.</p> <p>This method is called before the task is submitted.</p> PARAMETER DESCRIPTION <code>fn</code> <p>The task function.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>*args</code> <p>The arguments to the task function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to the task function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>A tuple of the task function, arguments and keyword arguments if the task should be submitted, or <code>None</code> if the task should not be submitted.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    This method is called before the task is submitted.\n\n    Args:\n        fn: The task function.\n        *args: The arguments to the task function.\n        **kwargs: The keyword arguments to the task function.\n\n    Returns:\n        A tuple of the task function, arguments and keyword arguments\n        if the task should be submitted, or `None` if the task should\n        not be submitted.\n    \"\"\"\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/","title":"Pynisher","text":""},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher","title":"amltk.scheduling.plugins.pynisher","text":"<p>The <code>PynisherPlugin</code> uses pynisher to place memory, walltime and cputime constraints on processes, crashing them if these limits are reached. These default units are <code>bytes (\"B\")</code> and <code>seconds (\"s\")</code> but you can also use other units, please see the relevant API doc.</p> <p>It's best use is when used with <code>Scheduler.with_processes()</code> to have work performed in processes.</p> <p>Requirements</p> <p>This required <code>pynisher</code> which can be installed with:</p> <pre><code>pip install amltk[pynisher]\n\n# Or directly\npip install pynisher\n</code></pre> Usage <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-16TI8nBm \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <code>@events</code> <code>@pynisher-timeout</code><code>@pynisher-memory-limit</code><code>@pynisher-cputime-limit</code><code>@pynisher-walltime-limit</code> Scheduler Executor <p>This will place process limits on the task as soon as it starts running, whever it may be running. If you are using <code>Scheduler.with_sequential()</code> then this will place limits on the main process, likely not what you want. This also does not work with a <code>ThreadPoolExecutor</code>.</p> <p>If using this with something like [<code>dask-jobqueue</code>], then this will place limits on the workers it spawns. It would be better to place limits directly through dask job-queue then.</p> Platform Limitations (Mac, Windows) <p>Pynisher has some limitations with memory on Mac and Windows: automl/pynisher#features</p> <p>You can check this with <code>PynisherPlugin.supports(\"memory\")</code>, <code>PynisherPlugin.supports(\"cpu_time\")</code> and <code>PynisherPlugin.supports(\"wall_time\")</code>. See <code>PynisherPlugin.supports()</code></p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.TIMEOUT","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.TIMEOUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIMEOUT: Event[[TimeoutException], Any] = Event(\n    \"pynisher-timeout\"\n)\n</code></pre> <p>A Task timed out, either due to the wall time or cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-YcoKVrWm \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.MEMORY_LIMIT_REACHED","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.MEMORY_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEMORY_LIMIT_REACHED: Event[[MemoryLimitException], Any] = (\n    Event(\"pynisher-memory-limit\")\n)\n</code></pre> <p>A Task was submitted but reached it's memory limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport numpy as np\n\ndef f(x: int) -&gt; int:\n    x = np.arange(100000000)\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(memory_limit=(1, \"KB\")))\n\n@task.on(\"pynisher-memory-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory     Wall Time  CPU Time                                          \u2502 \u2502\n\u2502 \u2502  (1, 'KB')  None       None                                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-8VKlkB4F \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.CPU_TIME_LIMIT_REACHED","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.CPU_TIME_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CPU_TIME_LIMIT_REACHED: Event[\n    [CpuTimeoutException], Any\n] = Event(\"pynisher-cputime-limit\")\n</code></pre> <p>A Task was submitted but reached it's cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    i = 0\n    while True:\n        # Keep busying computing the answer to everything\n        i += 1\n\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(cputime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-cputime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    None       (1, 's')                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-nYZuClzB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.WALL_TIME_LIMIT_REACHED","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.WALL_TIME_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WALL_TIME_LIMIT_REACHED: Event[\n    [WallTimeoutException], Any\n] = Event(\"pynisher-walltime-limit\")\n</code></pre> <p>A Task was submitted but reached it's wall time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-walltime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-dny85XyH \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin","title":"PynisherPlugin","text":"<pre><code>PynisherPlugin(\n    *,\n    memory_limit: int | tuple[int, str] | None = None,\n    cputime_limit: int | tuple[float, str] | None = None,\n    walltime_limit: int | tuple[float, str] | None = None,\n    context: BaseContext | None = None,\n    disable_trial_handling: bool = False\n)\n</code></pre> <p>               Bases: <code>Plugin</code></p> <p>A plugin that wraps a task in a pynisher to enforce limits on it.</p> <p>This plugin wraps a task function in a <code>Pynisher</code> instance to enforce limits on the task. The limits are set by any of <code>memory_limit=</code>, <code>cpu_time_limit=</code> and <code>wall_time_limit=</code>.</p> <p>Adds four new events to the task</p> <ul> <li><code>@pynisher-timeout</code></li> <li><code>@pynisher-memory-limit</code></li> <li><code>@pynisher-cputime-limit</code></li> <li><code>@pynisher-walltime-limit</code></li> </ul> ATTRIBUTE DESCRIPTION <code>memory_limit</code> <p>The memory limit of the task.</p> <p> </p> <code>cpu_time_limit</code> <p>The cpu time limit of the task.</p> <p> </p> <code>wall_time_limit</code> <p>The wall time limit of the task.</p> <p> </p> PARAMETER DESCRIPTION <code>memory_limit</code> <p>The memory limit to wrap the task in. Base unit is in bytes but you can specify <code>(value, unit)</code> where <code>unit</code> is one of <code>(\"B\", \"KB\", \"MB\", \"GB\")</code>. Defaults to <code>None</code></p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>cputime_limit</code> <p>The cpu time limit to wrap the task in. Base unit is in seconds but you can specify <code>(value, unit)</code> where <code>unit</code> is one of <code>(\"s\", \"m\", \"h\")</code>. Defaults to <code>None</code></p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>walltime_limit</code> <p>The wall time limit for the task. Base unit is in seconds but you can specify <code>(value, unit)</code> where <code>unit</code> is one of <code>(\"s\", \"m\", \"h\")</code>. Defaults to <code>None</code>.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>The context to use for multiprocessing. Defaults to <code>None</code>. See <code>multiprocessing.get_context()</code></p> <p> TYPE: <code>BaseContext | None</code> DEFAULT: <code>None</code> </p> <code>disable_trial_handling</code> <p>By default, the <code>PynisherPlugin</code> will auto-detect if the task is one for a <code>Trial</code>. If so, it will catch any pynisher specific exceptions and return a <code>Trial.Report</code> with <code>trial.fail()</code>, instead of raising the expcetion. This has the effect that the report can be caught with <code>task.on_result</code> where it can be handled. This will also prevent the specific events <code>@pynisher-timeout</code>, <code>@pynisher-memory-limit</code>, <code>@pynisher-cputime-limit</code> and <code>@pynisher-walltime-limit</code> from being emitted.</p> <p>If this is <code>True</code>, then the pynisher exceptions will be raised as normal and should be handled with <code>task.on_exception</code> where there is no direct access to the <code>Trial</code> submitted.</p> Auto-Detection <p>This will be triggered if the first positional argument is a <code>Trial</code> or if any of the keyword arguments are <code>\"trial\"</code>.</p> <pre><code>from amltk.optimization import Trial\nfrom amltk.scheduling import Scheduler\n\ndef trial_evaluator_one(trial: Trial, ...) -&gt; int:\n    ...\n\ndef trial_evaluator_two(..., trial: Trial) -&gt; int:\n    ...\n\nscheduler = Scheduler.with_processes(1)\n\ntask_one = scheduler.task(\n    trial_evaluator_one,\n    plugins=PynisherPlugin(memory_limit=(1, \"gb\")\n)\ntask_two = scheduler.task(\n    trial_evaluator_two,\n    plugins=PynisherPlugin(memory_limit=(1, \"gb\")\n)\n\n# Will auto-detect\ntrial = Trial.create(...)\ntask_one.submit(trial, ...)\ntask_two.submit(..., trial=trial)\n\n# Will not auto-detect\ntask_one.submit(42, trial)\n</code></pre> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>def __init__(\n    self,\n    *,\n    memory_limit: int | tuple[int, str] | None = None,\n    cputime_limit: int | tuple[float, str] | None = None,\n    walltime_limit: int | tuple[float, str] | None = None,\n    context: BaseContext | None = None,\n    disable_trial_handling: bool = False,\n):\n    \"\"\"Initialize a `PynisherPlugin` instance.\n\n    Args:\n        memory_limit: The memory limit to wrap the task in. Base unit is in bytes\n            but you can specify `(value, unit)` where `unit` is one of\n            `(\"B\", \"KB\", \"MB\", \"GB\")`. Defaults to `None`\n        cputime_limit: The cpu time limit to wrap the task in. Base unit is in\n            seconds but you can specify `(value, unit)` where `unit` is one of\n            `(\"s\", \"m\", \"h\")`. Defaults to `None`\n        walltime_limit: The wall time limit for the task. Base unit is in seconds\n            but you can specify `(value, unit)` where `unit` is one of\n            `(\"s\", \"m\", \"h\")`. Defaults to `None`.\n        context: The context to use for multiprocessing. Defaults to `None`.\n            See [`multiprocessing.get_context()`][multiprocessing.get_context]\n        disable_trial_handling: By default, the `PynisherPlugin` will auto-detect\n            if the task is one for a `Trial`. If so, it will catch any pynisher\n            specific exceptions and return a `Trial.Report` with `trial.fail()`,\n            instead of raising the expcetion. This has the effect that the report\n            can be caught with `task.on_result` where it can be handled. This will\n            also prevent the specific events `@pynisher-timeout`,\n            `@pynisher-memory-limit`, `@pynisher-cputime-limit`\n            and `@pynisher-walltime-limit` from being emitted.\n\n            If this\n            is `True`, then the pynisher exceptions will be raised as normal and\n            should be handled with `task.on_exception` where there is no direct\n            access to the `Trial` submitted.\n\n            ??? note \"Auto-Detection\"\n\n                This will be triggered if the first positional argument is a\n                `Trial` or if any of the keyword arguments are `\"trial\"`.\n\n                ```python\n                from amltk.optimization import Trial\n                from amltk.scheduling import Scheduler\n\n                def trial_evaluator_one(trial: Trial, ...) -&gt; int:\n                    ...\n\n                def trial_evaluator_two(..., trial: Trial) -&gt; int:\n                    ...\n\n                scheduler = Scheduler.with_processes(1)\n\n                task_one = scheduler.task(\n                    trial_evaluator_one,\n                    plugins=PynisherPlugin(memory_limit=(1, \"gb\")\n                )\n                task_two = scheduler.task(\n                    trial_evaluator_two,\n                    plugins=PynisherPlugin(memory_limit=(1, \"gb\")\n                )\n\n                # Will auto-detect\n                trial = Trial.create(...)\n                task_one.submit(trial, ...)\n                task_two.submit(..., trial=trial)\n\n                # Will not auto-detect\n                task_one.submit(42, trial)\n                ```\n    \"\"\"\n    super().__init__()\n\n    for limit, name in [\n        (memory_limit, \"memory\"),\n        (cputime_limit, \"cpu_time\"),\n        (walltime_limit, \"wall_time\"),\n    ]:\n        if limit is not None and not self.supports(name):  # type: ignore\n            raise RuntimeError(\n                f\"Your platform does not support {name} limits.\"\n                \" Please see pynisher documentation for more:\"\n                \"\\nhttps://github.com/automl/pynisher#features\",\n            )\n\n    self.memory_limit = memory_limit\n    self.cputime_limit = cputime_limit\n    self.walltime_limit = walltime_limit\n    self.context = context\n    self.disable_trial_handling = disable_trial_handling\n\n    self.task: Task\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.CPU_TIME_LIMIT_REACHED","title":"CPU_TIME_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CPU_TIME_LIMIT_REACHED: Event[\n    [CpuTimeoutException], Any\n] = Event(\"pynisher-cputime-limit\")\n</code></pre> <p>A Task was submitted but reached it's cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    i = 0\n    while True:\n        # Keep busying computing the answer to everything\n        i += 1\n\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(cputime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-cputime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    None       (1, 's')                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-aNriQRwk \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.CpuTimeoutException","title":"CpuTimeoutException  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CpuTimeoutException: TypeAlias = CpuTimeoutException\n</code></pre> <p>The exception that is raised when a task reaches it's cpu time limit.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.MEMORY_LIMIT_REACHED","title":"MEMORY_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEMORY_LIMIT_REACHED: Event[[MemoryLimitException], Any] = (\n    Event(\"pynisher-memory-limit\")\n)\n</code></pre> <p>A Task was submitted but reached it's memory limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport numpy as np\n\ndef f(x: int) -&gt; int:\n    x = np.arange(100000000)\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(memory_limit=(1, \"KB\")))\n\n@task.on(\"pynisher-memory-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory     Wall Time  CPU Time                                          \u2502 \u2502\n\u2502 \u2502  (1, 'KB')  None       None                                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-jxUYRtdA \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.MemoryLimitException","title":"MemoryLimitException  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MemoryLimitException: TypeAlias = MemoryLimitException\n</code></pre> <p>The exception that is raised when a task reaches it's memory limit.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.PynisherException","title":"PynisherException  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PynisherException: TypeAlias = PynisherException\n</code></pre> <p>The base exception for all pynisher exceptions.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.TIMEOUT","title":"TIMEOUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIMEOUT: Event[[TimeoutException], Any] = Event(\n    \"pynisher-timeout\"\n)\n</code></pre> <p>A Task timed out, either due to the wall time or cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-zMEtuX5S \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.TimeoutException","title":"TimeoutException  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TimeoutException: TypeAlias = TimeoutException\n</code></pre> <p>The exception that is raised when a task times out.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.WALL_TIME_LIMIT_REACHED","title":"WALL_TIME_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WALL_TIME_LIMIT_REACHED: Event[\n    [WallTimeoutException], Any\n] = Event(\"pynisher-walltime-limit\")\n</code></pre> <p>A Task was submitted but reached it's wall time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-walltime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-QLqjyJsb \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.WallTimeoutException","title":"WallTimeoutException  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WallTimeoutException: TypeAlias = WallTimeoutException\n</code></pre> <p>The exception that is raised when a task reaches it's wall time limit.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'pynisher-plugin'\n</code></pre> <p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n    task.add_event(\n        self.TIMEOUT,\n        self.MEMORY_LIMIT_REACHED,\n        self.CPU_TIME_LIMIT_REACHED,\n        self.WALL_TIME_LIMIT_REACHED,\n    )\n\n    # Check the exception and emit pynisher specific ones too\n    task.on_exception(self._check_to_emit_pynisher_exception, hidden=True)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: args, **kwargs: kwargs\n) -&gt; tuple[Callable[P, R], tuple, dict]\n</code></pre> <p>Wrap a task function in a <code>Pynisher</code> instance.</p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict]:\n    \"\"\"Wrap a task function in a `Pynisher` instance.\"\"\"\n    _fn = _PynisherWrap(\n        fn,\n        disable_trial_handling=self.disable_trial_handling,\n        memory_limit=self.memory_limit,\n        cputime_limit=self.cputime_limit,\n        walltime_limit=self.walltime_limit,\n        context=self.context,\n        terminate_child_processes=True,\n    )\n    return _fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.supports","title":"supports  <code>classmethod</code>","text":"<pre><code>supports(\n    kind: Literal[\"wall_time\", \"cpu_time\", \"memory\"]\n) -&gt; bool\n</code></pre> <p>Check if the task is supported by the plugin.</p> PARAMETER DESCRIPTION <code>kind</code> <p>The kind of limit to check.</p> <p> TYPE: <code>Literal['wall_time', 'cpu_time', 'memory']</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the limit is supported by the plugin for your os, else <code>False</code>.</p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>@classmethod\ndef supports(cls, kind: Literal[\"wall_time\", \"cpu_time\", \"memory\"]) -&gt; bool:\n    \"\"\"Check if the task is supported by the plugin.\n\n    Args:\n        kind: The kind of limit to check.\n\n    Returns:\n        `True` if the limit is supported by the plugin for your os, else `False`.\n    \"\"\"\n    return pynisher.supports(kind)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/","title":"Threadpoolctl","text":""},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl","title":"amltk.scheduling.plugins.threadpoolctl","text":"<p>The <code>ThreadPoolCTLPlugin</code> if useful for parallel training of models. Without limiting with threadpoolctl, the number of threads used by a given model may oversubscribe to resources and cause significant slowdowns.</p> <p>This is the mechanism employed by scikit-learn to limit the number of threads used by a given model.</p> <p>See threadpoolctl documentation.</p> <p>Requirements</p> <p>This requires <code>threadpoolctl</code> which can be installed with:</p> <pre><code>pip install amltk[threadpoolctl]\n\n# Or directly\npip install threadpoolctl\n</code></pre> Usage <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\nscheduler = Scheduler.with_processes(1)\n\ndef f() -&gt; None:\n    # ... some task that respects the limits set by threadpoolctl\n    pass\n\ntask = scheduler.task(f, plugins=ThreadPoolCTLPlugin(max_threads=1))\n</code></pre> <pre>\n<code>\u256d\u2500 Task f() -&gt; None \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin threadpoolctl-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Max Threads  User-API                                                   \u2502 \u2502\n\u2502 \u2502  1            None                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-mgm7wb3O \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin","title":"ThreadPoolCTLPlugin","text":"<pre><code>ThreadPoolCTLPlugin(\n    max_threads: int | dict[str, int] | None = None,\n    user_api: str | None = None,\n)\n</code></pre> <p>               Bases: <code>Plugin</code></p> <p>A plugin that limits the usage of threads in a task.</p> <p>This plugin is used to make utilize threadpoolctl with tasks, useful for parallel training of models. Without limiting with threadpoolctl, the number of threads used by a given model may oversubscribe to resources and cause significant slowdowns.</p> ATTRIBUTE DESCRIPTION <code>max_calls</code> <p>The maximum number of calls to the task.</p> <p> </p> <code>max_concurrent</code> <p>The maximum number of calls of this task that can be in the queue.</p> <p> </p> PARAMETER DESCRIPTION <code>max_threads</code> <p>The maximum number of threads to use.</p> <p> TYPE: <code>int | dict[str, int] | None</code> DEFAULT: <code>None</code> </p> <code>user_api</code> <p>The user API to limit.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/threadpoolctl.py</code> <pre><code>@override\ndef __init__(\n    self,\n    max_threads: int | dict[str, int] | None = None,\n    user_api: str | None = None,\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        max_threads: The maximum number of threads to use.\n        user_api: The user API to limit.\n    \"\"\"\n    super().__init__()\n    self.max_threads = max_threads\n    self.user_api = user_api\n    self.task: Task | None = None\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'threadpoolctl-plugin'\n</code></pre> <p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/threadpoolctl.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: args, **kwargs: kwargs\n) -&gt; tuple[Callable[P, R], tuple, dict] | None\n</code></pre> <p>Pre-submit hook.</p> <p>Wrap the function in something that will activate threadpoolctl when the function is called.</p> Source code in <code>src/amltk/scheduling/plugins/threadpoolctl.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    Wrap the function in something that will activate threadpoolctl\n    when the function is called.\n    \"\"\"\n    assert self.task is not None\n\n    fn = _ThreadPoolLimiter(\n        fn=fn,\n        max_threads=self.max_threads,\n        user_api=self.user_api,\n    )\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/","title":"Wandb","text":""},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb","title":"amltk.scheduling.plugins.wandb","text":"<p>Wandb plugin.</p> <p>Todo</p> <p>This plugin is experimental and out of date.</p>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbLiveRunWrap","title":"WandbLiveRunWrap","text":"<pre><code>WandbLiveRunWrap(\n    params: WandbParams,\n    fn: Callable[Concatenate[Trial, P], Report],\n    *,\n    modify: (\n        Callable[[Trial, WandbParams], WandbParams] | None\n    ) = None\n)\n</code></pre> <p>               Bases: <code>Generic[P]</code></p> <p>Wrap a function to log the results to a wandb run.</p> <p>This class is used to wrap a function that returns a report to log the results to a wandb run. It is used by the <code>WandbTrialTracker</code> to wrap the target function.</p> PARAMETER DESCRIPTION <code>params</code> <p>The parameters to initialize the wandb run.</p> <p> TYPE: <code>WandbParams</code> </p> <code>fn</code> <p>The function to wrap.</p> <p> TYPE: <code>Callable[Concatenate[Trial, P], Report]</code> </p> <code>modify</code> <p>A function that modifies the parameters of the wandb run before each trial.</p> <p> TYPE: <code>Callable[[Trial, WandbParams], WandbParams] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __init__(\n    self,\n    params: WandbParams,\n    fn: Callable[Concatenate[Trial, P], Trial.Report],\n    *,\n    modify: Callable[[Trial, WandbParams], WandbParams] | None = None,\n):\n    \"\"\"Initialize the wrapper.\n\n    Args:\n        params: The parameters to initialize the wandb run.\n        fn: The function to wrap.\n        modify: A function that modifies the parameters of the wandb run\n            before each trial.\n    \"\"\"\n    super().__init__()\n    self.params = params\n    self.fn = fn\n    self.modify = modify\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbLiveRunWrap.__call__","title":"__call__","text":"<pre><code>__call__(\n    trial: Trial, *args: args, **kwargs: kwargs\n) -&gt; Report\n</code></pre> <p>Call the wrapped function and log the results to a wandb run.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __call__(self, trial: Trial, *args: P.args, **kwargs: P.kwargs) -&gt; Trial.Report:\n    \"\"\"Call the wrapped function and log the results to a wandb run.\"\"\"\n    params = self.params if self.modify is None else self.modify(trial, self.params)\n    with params.run(name=trial.name, config=trial.config) as run:\n        # Make sure the run is available from the trial\n        trial.extras[\"wandb\"] = run\n\n        report = self.fn(trial, *args, **kwargs)\n\n        report_df = report.df()\n        run.log({\"table\": wandb.Table(dataframe=report_df)})\n        wandb_summary = {\n            k: v\n            for k, v in report.summary.items()\n            if isinstance(v, int | float | np.number)\n        }\n        run.summary.update(wandb_summary)\n\n    wandb.finish()\n    return report\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbParams","title":"WandbParams  <code>dataclass</code>","text":"<pre><code>WandbParams(\n    project: str | None = None,\n    group: str | None = None,\n    job_type: str | None = None,\n    entity: str | None = None,\n    tags: list[str] | None = None,\n    notes: str | None = None,\n    reinit: bool | None = None,\n    config_exclude_keys: list[str] | None = None,\n    config_include_keys: list[str] | None = None,\n    resume: bool | str | None = None,\n    mode: Literal[\n        \"online\", \"offline\", \"disabled\"\n    ] = \"online\",\n    allow_val_change: bool = False,\n    force: bool = False,\n    dir: str | Path | None = None,\n)\n</code></pre> <p>Parameters for initializing a wandb run.</p> <p>This class is a dataclass that contains all the parameters that are used to initialize a wandb run. It is used by the <code>WandbPlugin</code> to initialize a run. It can be modified using the <code>modify()</code> method.</p> <p>Please refer to the documentation of the <code>wandb.init()</code> method for more information on the parameters.</p>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbParams.modify","title":"modify","text":"<pre><code>modify(**kwargs: Any) -&gt; WandbParams\n</code></pre> <p>Modify the parameters of this instance.</p> <p>This method returns a new instance of this class with the parameters modified. This is useful for example when you want to modify the parameters of a run to add tags or notes.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def modify(self, **kwargs: Any) -&gt; WandbParams:\n    \"\"\"Modify the parameters of this instance.\n\n    This method returns a new instance of this class with the parameters\n    modified. This is useful for example when you want to modify the\n    parameters of a run to add tags or notes.\n    \"\"\"\n    return replace(self, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbParams.run","title":"run","text":"<pre><code>run(\n    name: str, config: Mapping[str, Any] | None = None\n) -&gt; WRun\n</code></pre> <p>Initialize a wandb run.</p> <p>This method initializes a wandb run using the parameters of this instance. It returns the wandb run object.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the run.</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The configuration of the run.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>WRun</code> <p>The wandb run object.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def run(\n    self,\n    name: str,\n    config: Mapping[str, Any] | None = None,\n) -&gt; WRun:\n    \"\"\"Initialize a wandb run.\n\n    This method initializes a wandb run using the parameters of this\n    instance. It returns the wandb run object.\n\n    Args:\n        name: The name of the run.\n        config: The configuration of the run.\n\n    Returns:\n        The wandb run object.\n    \"\"\"\n    run = wandb.init(\n        config=dict(config) if config else None,\n        name=name,\n        project=self.project,\n        group=self.group,\n        tags=self.tags,\n        entity=self.entity,\n        notes=self.notes,\n        reinit=self.reinit,\n        dir=self.dir,\n        config_exclude_keys=self.config_exclude_keys,\n        config_include_keys=self.config_include_keys,\n        mode=self.mode,\n        allow_val_change=self.allow_val_change,\n        force=self.force,\n    )\n    if run is None:\n        raise RuntimeError(\"Wandb run was not initialized\")\n\n    return run\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbPlugin","title":"WandbPlugin","text":"<pre><code>WandbPlugin(\n    *,\n    project: str,\n    group: str | None = None,\n    entity: str | None = None,\n    dir: str | Path | None = None,\n    mode: Literal[\n        \"online\", \"offline\", \"disabled\"\n    ] = \"online\"\n)\n</code></pre> <p>Log trials using wandb.</p> <p>This class is the entry point to log trials using wandb. It can be used to create a <code>trial_tracker()</code> to pass into a <code>Task(plugins=...)</code> or to create <code>wandb.Run</code>'s for custom purposes with <code>run()</code>.</p> PARAMETER DESCRIPTION <code>project</code> <p>The name of the project.</p> <p> TYPE: <code>str</code> </p> <code>group</code> <p>The name of the group.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>entity</code> <p>The name of the entity.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>dir</code> <p>The directory to store the runs in.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The mode to use for the runs.</p> <p> TYPE: <code>Literal['online', 'offline', 'disabled']</code> DEFAULT: <code>'online'</code> </p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __init__(\n    self,\n    *,\n    project: str,\n    group: str | None = None,\n    entity: str | None = None,\n    dir: str | Path | None = None,  # noqa: A002\n    mode: Literal[\"online\", \"offline\", \"disabled\"] = \"online\",\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        project: The name of the project.\n        group: The name of the group.\n        entity: The name of the entity.\n        dir: The directory to store the runs in.\n        mode: The mode to use for the runs.\n    \"\"\"\n    super().__init__()\n    _dir = Path(project) if dir is None else Path(dir)\n    _dir.mkdir(parents=True, exist_ok=True)\n\n    self.dir = _dir.resolve().absolute()\n    self.project = project\n    self.group = group\n    self.entity = entity\n    self.mode = mode\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbPlugin.run","title":"run","text":"<pre><code>run(\n    *,\n    name: str,\n    job_type: str | None = None,\n    group: str | None = None,\n    config: Mapping[str, Any] | None = None,\n    tags: list[str] | None = None,\n    resume: bool | str | None = None,\n    notes: str | None = None\n) -&gt; WRun\n</code></pre> <p>Create a wandb run.</p> <p>See <code>wandb.init()</code> for more.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def run(\n    self,\n    *,\n    name: str,\n    job_type: str | None = None,\n    group: str | None = None,\n    config: Mapping[str, Any] | None = None,\n    tags: list[str] | None = None,\n    resume: bool | str | None = None,\n    notes: str | None = None,\n) -&gt; WRun:\n    \"\"\"Create a wandb run.\n\n    See [`wandb.init()`](https://docs.wandb.ai/ref/python/init) for more.\n    \"\"\"\n    return WandbParams(\n        project=self.project,\n        entity=self.entity,\n        group=group,\n        dir=self.dir,\n        mode=self.mode,  # type: ignore\n        job_type=job_type,\n        tags=tags,\n        resume=resume,\n        notes=notes,\n    ).run(\n        name=name,\n        config=config,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbPlugin.trial_tracker","title":"trial_tracker","text":"<pre><code>trial_tracker(\n    job_type: str = \"trial\",\n    *,\n    modify: (\n        Callable[[Trial, WandbParams], WandbParams] | None\n    ) = None\n) -&gt; WandbTrialTracker\n</code></pre> <p>Create a live tracker.</p> PARAMETER DESCRIPTION <code>job_type</code> <p>The job type to use for the runs.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'trial'</code> </p> <code>modify</code> <p>A function that modifies the parameters of the wandb run before each trial.</p> <p> TYPE: <code>Callable[[Trial, WandbParams], WandbParams] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>WandbTrialTracker</code> <p>A live tracker.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def trial_tracker(\n    self,\n    job_type: str = \"trial\",\n    *,\n    modify: Callable[[Trial, WandbParams], WandbParams] | None = None,\n) -&gt; WandbTrialTracker:\n    \"\"\"Create a live tracker.\n\n    Args:\n        job_type: The job type to use for the runs.\n        modify: A function that modifies the parameters of the wandb run\n            before each trial.\n\n    Returns:\n        A live tracker.\n    \"\"\"\n    params = WandbParams(\n        project=self.project,\n        entity=self.entity,\n        group=self.group,\n        dir=self.dir,\n        mode=self.mode,  # type: ignore\n        job_type=job_type,\n    )\n    return WandbTrialTracker(params, modify=modify)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker","title":"WandbTrialTracker","text":"<pre><code>WandbTrialTracker(\n    params: WandbParams,\n    *,\n    modify: (\n        Callable[[Trial, WandbParams], WandbParams] | None\n    ) = None\n)\n</code></pre> <p>               Bases: <code>Plugin</code></p> <p>Track trials using wandb.</p> <p>This class is a task plugin that tracks trials using wandb.</p> PARAMETER DESCRIPTION <code>params</code> <p>The parameters to initialize the wandb run.</p> <p> TYPE: <code>WandbParams</code> </p> <code>modify</code> <p>A function that modifies the parameters of the wandb run before each trial.</p> <p> TYPE: <code>Callable[[Trial, WandbParams], WandbParams] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __init__(\n    self,\n    params: WandbParams,\n    *,\n    modify: Callable[[Trial, WandbParams], WandbParams] | None = None,\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        params: The parameters to initialize the wandb run.\n        modify: A function that modifies the parameters of the wandb run\n            before each trial.\n    \"\"\"\n    super().__init__()\n    self.params = params\n    self.modify = modify\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.name","title":"name  <code>class-attribute</code>","text":"<pre><code>name: str = 'wandb-trial-tracker'\n</code></pre> <p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Use the task to register several callbacks.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Use the task to register several callbacks.\"\"\"\n    self._check_explicit_reinit_arg_with_executor(task.scheduler)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: Any, **kwargs: Any\n) -&gt; tuple[Callable[P, R], tuple, dict] | None\n</code></pre> <p>Wrap the target function to log the results to a wandb run.</p> <p>This method wraps the target function to log the results to a wandb run and returns the wrapped function.</p> PARAMETER DESCRIPTION <code>fn</code> <p>The target function.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>args</code> <p>The positional arguments of the target function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>The keyword arguments of the target function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>The wrapped function, the positional arguments and the keyword arguments.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: Any,\n    **kwargs: Any,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Wrap the target function to log the results to a wandb run.\n\n    This method wraps the target function to log the results to a wandb run\n    and returns the wrapped function.\n\n    Args:\n        fn: The target function.\n        args: The positional arguments of the target function.\n        kwargs: The keyword arguments of the target function.\n\n    Returns:\n        The wrapped function, the positional arguments and the keyword\n        arguments.\n    \"\"\"\n    fn = WandbLiveRunWrap(self.params, fn, modify=self.modify)  # type: ignore\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/","title":"Warning filter","text":""},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter","title":"amltk.scheduling.plugins.warning_filter","text":"<p>The <code>WarningFilter</code> if used to automatically filter out warnings from a <code>Task</code> as it runs.</p> <p>This wraps your function in context manager <code>warnings.catch_warnings()</code> and applies your arguments to <code>warnings.filterwarnings()</code>, as you would normally filter warnings in Python.</p> Usage <pre><code>import warnings\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import WarningFilter\n\ndef f() -&gt; None:\n    warnings.warn(\"This is a warning\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(f, plugins=WarningFilter(\"ignore\"))\n</code></pre> <pre>\n<code>\u256d\u2500 Task f() -&gt; None \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin warning-filter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Args         Kwargs                                                     \u2502 \u2502\n\u2502 \u2502  ('ignore',)  {}                                                         \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-ycUB67MH \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter","title":"WarningFilter","text":"<pre><code>WarningFilter(*args: Any, **kwargs: Any)\n</code></pre> <p>               Bases: <code>Plugin</code></p> <p>A plugin that disables warnings emitted from tasks.</p> PARAMETER DESCRIPTION <code>*args</code> <p>arguments to pass to <code>warnings.filterwarnings</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>keyword arguments to pass to <code>warnings.filterwarnings</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/amltk/scheduling/plugins/warning_filter.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        *args: arguments to pass to\n            [`warnings.filterwarnings`][warnings.filterwarnings].\n        **kwargs: keyword arguments to pass to\n            [`warnings.filterwarnings`][warnings.filterwarnings].\n    \"\"\"\n    super().__init__()\n    self.task: Task | None = None\n    self.warning_args = args\n    self.warning_kwargs = kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'warning-filter'\n</code></pre> <p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.attach_task","title":"attach_task","text":"<pre><code>attach_task(task: Task) -&gt; None\n</code></pre> <p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/warning_filter.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.events","title":"events","text":"<pre><code>events() -&gt; list[Event]\n</code></pre> <p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.pre_submit","title":"pre_submit","text":"<pre><code>pre_submit(\n    fn: Callable[P, R], *args: args, **kwargs: kwargs\n) -&gt; tuple[Callable[P, R], tuple, dict]\n</code></pre> <p>Pre-submit hook.</p> <p>Wraps the function to ignore warnings.</p> Source code in <code>src/amltk/scheduling/plugins/warning_filter.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict]:\n    \"\"\"Pre-submit hook.\n\n    Wraps the function to ignore warnings.\n    \"\"\"\n    wrapped_f = _IgnoreWarningWrapper(fn, *self.warning_args, **self.warning_kwargs)\n    return wrapped_f, args, kwargs\n</code></pre>"},{"location":"api/amltk/sklearn/data/","title":"Data","text":""},{"location":"api/amltk/sklearn/data/#amltk.sklearn.data","title":"amltk.sklearn.data","text":"<p>Data utilities for scikit-learn.</p>"},{"location":"api/amltk/sklearn/data/#amltk.sklearn.data.split_data","title":"split_data","text":"<pre><code>split_data(\n    *items: Sequence,\n    splits: dict[str, float],\n    seed: Seed | None = None,\n    shuffle: bool = True,\n    stratify: Sequence | None = None\n) -&gt; dict[str, tuple[Sequence, ...]]\n</code></pre> <p>Split a set of items into multiple splits.</p> <pre><code>from amltk.sklearn.data import split\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\nsplits = split_data(x, y, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\ntrain_x, train_y = splits[\"train\"]\nval_x, val_y = splits[\"val\"]\ntest_x, test_y = splits[\"test\"]\n</code></pre> PARAMETER DESCRIPTION <code>items</code> <p>The items to split. Must be indexible, like a list, np.ndarray, pandas dataframe/series or a tuple, etc...</p> <p> TYPE: <code>Sequence</code> DEFAULT: <code>()</code> </p> <code>splits</code> <p>A dictionary of split names and their percentage of the data. The percentages must sum to 1.</p> <p> TYPE: <code>dict[str, float]</code> </p> <code>seed</code> <p>The seed to use for the random state.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data before splitting. Passed forward to sklearn.model_selection.train_test_split.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>stratify</code> <p>The stratification to use for the split. This will be passed forward to sklearn.model_selection.train_test_split. We account for using the stratification for all splits, ensuring we split of the stratification values themselves.</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, tuple[Sequence, ...]]</code> <p>A dictionary of split names and their split items.</p> Source code in <code>src/amltk/sklearn/data.py</code> <pre><code>def split_data(\n    *items: Sequence,\n    splits: dict[str, float],\n    seed: Seed | None = None,\n    shuffle: bool = True,\n    stratify: Sequence | None = None,\n) -&gt; dict[str, tuple[Sequence, ...]]:\n    \"\"\"Split a set of items into multiple splits.\n\n    ```python\n    from amltk.sklearn.data import split\n\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n    splits = split_data(x, y, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\n    train_x, train_y = splits[\"train\"]\n    val_x, val_y = splits[\"val\"]\n    test_x, test_y = splits[\"test\"]\n    ```\n\n    Args:\n        items: The items to split. Must be indexible, like a list, np.ndarray,\n            pandas dataframe/series or a tuple, etc...\n        splits: A dictionary of split names and their percentage of the data.\n            The percentages must sum to 1.\n        seed: The seed to use for the random state.\n        shuffle: Whether to shuffle the data before splitting. Passed forward\n            to [sklearn.model_selection.train_test_split][].\n        stratify: The stratification to use for the split. This will be passed\n            forward to [sklearn.model_selection.train_test_split][]. We account\n            for using the stratification for all splits, ensuring we split of\n            the stratification values themselves.\n\n    Returns:\n        A dictionary of split names and their split items.\n    \"\"\"\n    if not all(0 &lt; s &lt; 1 for s in splits.values()):\n        raise ValueError(f\"Splits ({splits=}) must be between 0 and 1\")\n\n    if sum(splits.values()) != 1:\n        raise ValueError(f\"Splits ({splits=}) must sum to 1\")\n\n    if len(splits) &lt; 2:  # noqa: PLR2004\n        raise ValueError(f\"Splits ({splits=}) must have at least 2 splits\")\n\n    rng = as_int(seed) if seed is not None else None\n\n    # Store the results of each split, indexed by the split number\n    split_results: dict[str, list[Sequence]] = {}\n    remaining: list[Sequence] = list(items)\n\n    remaining_percentage = 1.0\n\n    # Enumerate up to the last split\n    for name, split_percentage in list(splits.items())[0:-1]:\n        # If we stratify, make sure to also include it in the splitting so\n        # further splits can be stratified correctly.\n        to_split = remaining if stratify is None else [*remaining, stratify]\n\n        # Calculate the percentage of the remaining data to split\n        percentage = split_percentage / remaining_percentage\n\n        splitted = train_test_split(\n            *to_split,\n            train_size=percentage,\n            random_state=rng,\n            shuffle=shuffle,\n            stratify=stratify,\n        )\n\n        # Update the remaining percentage\n        remaining_percentage -= split_percentage\n\n        # Splitted returns pairs of (train, test) for each item in items\n        # so we need to split them up\n        lefts = splitted[::2]\n        rights = splitted[1::2]\n\n        # If we had stratify, we need to remove the last item from splits\n        # as it was the stratified array, setting the stratification for\n        # the next split\n        if stratify is not None:\n            stratify = rights[-1]  # type: ignore\n\n            lefts = lefts[:-1]\n            rights = rights[:-1]\n\n        # Lastly, we insert the lefts into the split_results\n        # and set the remaining to the rights\n        split_results[name] = lefts  # type: ignore\n        remaining = rights  # type: ignore\n\n    # Since we enumerated up to the last split, we need to add the last\n    # split manually\n    last_name = last(splits.keys())\n    split_results[last_name] = remaining\n\n    return {name: tuple(split) for name, split in split_results.items()}\n</code></pre>"},{"location":"api/amltk/sklearn/data/#amltk.sklearn.data.train_val_test_split","title":"train_val_test_split","text":"<pre><code>train_val_test_split(\n    *items: Sequence,\n    splits: tuple[float, float, float],\n    seed: Seed | None = None,\n    shuffle: bool = True,\n    stratify: Sequence | None = None\n) -&gt; tuple[Sequence, ...]\n</code></pre> <p>Split a set of items into train, val and test splits.</p> <pre><code>from amltk.sklearn.data import train_val_test_split\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\ntrain_x, train_y, val_x, val_y, test_x, test_y = train_val_test_split(\n    x, y, splits=(0.6, 0.2, 0.2),\n)\n</code></pre> PARAMETER DESCRIPTION <code>items</code> <p>The items to split. Must be indexible, like a list, np.ndarray, pandas dataframe/series or a tuple, etc...</p> <p> TYPE: <code>Sequence</code> DEFAULT: <code>()</code> </p> <code>splits</code> <p>A tuple of the percentage of the data to use for the train, val and test splits. The percentages must sum to 1.</p> <p> TYPE: <code>tuple[float, float, float]</code> </p> <code>seed</code> <p>The seed to use for the random state.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data before splitting. Passed forward to sklearn.model_selection.train_test_split.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>stratify</code> <p>The stratification to use for the split. This will be passed forward to sklearn.model_selection.train_test_split. We account for using the stratification for all splits, ensuring we split of the stratification values themselves.</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Sequence, ...]</code> <p>A tuple containing the train, val and test splits.</p> Source code in <code>src/amltk/sklearn/data.py</code> <pre><code>def train_val_test_split(\n    *items: Sequence,\n    splits: tuple[float, float, float],\n    seed: Seed | None = None,\n    shuffle: bool = True,\n    stratify: Sequence | None = None,\n) -&gt; tuple[Sequence, ...]:\n    \"\"\"Split a set of items into train, val and test splits.\n\n    ```python\n    from amltk.sklearn.data import train_val_test_split\n\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n    train_x, train_y, val_x, val_y, test_x, test_y = train_val_test_split(\n        x, y, splits=(0.6, 0.2, 0.2),\n    )\n    ```\n\n    Args:\n        items: The items to split. Must be indexible, like a list, np.ndarray,\n            pandas dataframe/series or a tuple, etc...\n        splits: A tuple of the percentage of the data to use for the train,\n            val and test splits. The percentages must sum to 1.\n        seed: The seed to use for the random state.\n        shuffle: Whether to shuffle the data before splitting. Passed forward\n            to [sklearn.model_selection.train_test_split][].\n        stratify: The stratification to use for the split. This will be passed\n            forward to [sklearn.model_selection.train_test_split][]. We account\n            for using the stratification for all splits, ensuring we split of\n            the stratification values themselves.\n\n    Returns:\n        A tuple containing the train, val and test splits.\n    \"\"\"\n    results = split_data(\n        *items,\n        splits={\"train\": splits[0], \"val\": splits[1], \"test\": splits[2]},\n        seed=seed,\n        shuffle=shuffle,\n        stratify=stratify,\n    )\n    return tuple(chain(results[\"train\"], results[\"val\"], results[\"test\"]))\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/","title":"Estimators","text":""},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators","title":"amltk.sklearn.estimators","text":"<p>Custom estimators for use with scikit-learn.</p>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier","title":"StoredPredictionClassifier","text":"<pre><code>StoredPredictionClassifier(\n    predictions: ndarray | None = None,\n    probabilities: ndarray | None = None,\n    classes: list[ndarray] | ndarray | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A class that just uses precomputed values for classification.</p> PARAMETER DESCRIPTION <code>predictions</code> <p>The precomputed predictions, if any.</p> <p> TYPE: <code>ndarray | None</code> DEFAULT: <code>None</code> </p> <code>probabilities</code> <p>The precomputed probabilities, if any.</p> <p> TYPE: <code>ndarray | None</code> DEFAULT: <code>None</code> </p> <code>classes</code> <p>The classes, if any.</p> <p> TYPE: <code>list[ndarray] | ndarray | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def __init__(\n    self,\n    predictions: np.ndarray | None = None,\n    probabilities: np.ndarray | None = None,\n    classes: list[np.ndarray] | np.ndarray | None = None,\n):\n    \"\"\"Initialize the estimator.\n\n    Args:\n        predictions: The precomputed predictions, if any.\n        probabilities: The precomputed probabilities, if any.\n        classes: The classes, if any.\n    \"\"\"\n    super().__init__()\n    self.predictions = predictions\n    self.probabilities = probabilities\n    self.classes = classes\n\n    # HACK: This is to enable sklearn-compatibility\n    # `clone` and other methods rely on this trailing underscore\n    # to indicate fitted attributes. We essentially declare it fitted\n    # at init for simplicity\n    self.classes_ = classes\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier.fit","title":"fit","text":"<pre><code>fit(*_: Any, **__: Any) -&gt; Self\n</code></pre> <p>Fit the estimator. Doesn't do anything.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def fit(self, *_: Any, **__: Any) -&gt; Self:\n    \"\"\"Fit the estimator. Doesn't do anything.\"\"\"\n    return self\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier.predict","title":"predict","text":"<pre><code>predict(X: Any, *_: Any, **__: Any) -&gt; ndarray\n</code></pre> <p>Predict the target values, returning the precomputed values.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def predict(self, X: Any, *_: Any, **__: Any) -&gt; np.ndarray:  # noqa: N803, ARG002\n    \"\"\"Predict the target values, returning the precomputed values.\"\"\"\n    if self.predictions is None:\n        if self.probabilities is None:\n            raise RuntimeError(\n                \"No predictions or probabilities were provided during\",\n                \" construction, so this estimator cannot be used for\",\n                \" `predict()`.\",\n            )\n        if self.classes_ is None:\n            raise RuntimeError(\n                \"No classes were provided during construction, so it can't\"\n                \" be used for `predict()` from probabilities.\",\n            )\n\n        predictions = probabilities_to_classes(\n            self.probabilities,\n            classes=self.classes_,\n        )\n    else:\n        predictions = self.predictions\n\n    return predictions\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(X: Any, *_: Any, **__: Any) -&gt; ndarray\n</code></pre> <p>Predict the probabilities, returning the precomputed values.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def predict_proba(\n    self,\n    X: Any,  # noqa: N803, ARG002\n    *_: Any,\n    **__: Any,\n) -&gt; np.ndarray:\n    \"\"\"Predict the probabilities, returning the precomputed values.\"\"\"\n    if self.probabilities is None:\n        raise RuntimeError(\n            \"No probabilities were provided during construction, so this\"\n            \" estimator cannot be used for `predict_proba()`.\",\n        )\n\n    return self.probabilities\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionRegressor","title":"StoredPredictionRegressor","text":"<pre><code>StoredPredictionRegressor(predictions: ndarray)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A class that just uses precomputed values for regression.</p> PARAMETER DESCRIPTION <code>predictions</code> <p>The precomputed predictions.</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def __init__(self, predictions: np.ndarray):\n    \"\"\"Initialize the estimator.\n\n    Args:\n        predictions: The precomputed predictions.\n    \"\"\"\n    super().__init__()\n    self.predictions = predictions\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionRegressor.fit","title":"fit","text":"<pre><code>fit(*_: Any, **__: Any) -&gt; Self\n</code></pre> <p>Fit the estimator. Doesn't do anything.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def fit(self, *_: Any, **__: Any) -&gt; Self:\n    \"\"\"Fit the estimator. Doesn't do anything.\"\"\"\n    return self\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionRegressor.predict","title":"predict","text":"<pre><code>predict(X: Any, *_: Any, **__: Any) -&gt; ndarray\n</code></pre> <p>Predict the target values, returning the precomputed values.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def predict(self, X: Any, *_: Any, **__: Any) -&gt; np.ndarray:  # noqa: N803, ARG002\n    \"\"\"Predict the target values, returning the precomputed values.\"\"\"\n    return self.predictions\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/","title":"Evaluation","text":""},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation","title":"amltk.sklearn.evaluation","text":"<p>This module contains the cross-validation evaluation protocol.</p> <p>This protocol will create a cross-validation task to be used in parallel and optimization. It represents a typical cross-validation evaluation for sklearn, handling some of the minor nuances of sklearn and it's interaction with optimization and parallelization.</p> <p>Please see <code>CVEvaluation</code> for more information on usage.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.PostSplitSignature","title":"PostSplitSignature  <code>module-attribute</code>","text":"<pre><code>PostSplitSignature: TypeAlias = Callable[\n    [Trial, int, \"CVEvaluation.PostSplitInfo\"],\n    \"CVEvaluation.PostSplitInfo\",\n]\n</code></pre> <p>A type alias for the post split callback signature.</p> <p>Please see <code>PostSplitInfo</code> for more information on the information available to this callback.</p> <pre><code>def my_post_split(\n    trial: Trial,\n    split_number: int,\n    eval: CVEvalauation.PostSplitInfo\n) -&gt; CVEvaluation.PostSplitInfo:\n    ...\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.TaskTypeName","title":"TaskTypeName  <code>module-attribute</code>","text":"<pre><code>TaskTypeName: TypeAlias = Literal[\n    \"binary\",\n    \"multiclass\",\n    \"multilabel-indicator\",\n    \"multiclass-multioutput\",\n    \"continuous\",\n    \"continuous-multioutput\",\n]\n</code></pre> <p>A type alias for the task type name as defined by sklearn.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.XLike","title":"XLike  <code>module-attribute</code>","text":"<pre><code>XLike: TypeAlias = DataFrame | ndarray\n</code></pre> <p>A type alias for X input data type as defined by sklearn.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.YLike","title":"YLike  <code>module-attribute</code>","text":"<pre><code>YLike: TypeAlias = Series | DataFrame | ndarray\n</code></pre> <p>A type alias for y input data type as defined by sklearn.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEarlyStoppingProtocol","title":"CVEarlyStoppingProtocol","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for early stopping in cross-validation.</p> <p>You class should implement the <code>update()</code> and <code>should_stop()</code> methods. You can optionally inherit from this class but it is not required.</p> <pre><code>class MyStopper:\n\n    def update(self, report: Trial.Report) -&gt; None:\n        if report.status is Trial.Status.SUCCESS:\n            # ... do some update logic\n\n    def should_stop(self, trial: Trial, split_infos: list[CVEvaluation.PostSplitInfo]) -&gt; bool | Exception:\n        mean_scores_up_to_current_split = np.mean([i.val_scores[\"accuracy\"] for i in split_infos])\n        if mean_scores_up_to_current_split &gt; 0.9:\n            return False  # Keep going\n        else:\n            return True  # Stop evaluating\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEarlyStoppingProtocol.should_stop","title":"should_stop","text":"<pre><code>should_stop(\n    trial: Trial, scores: SplitScores\n) -&gt; bool | Exception\n</code></pre> <p>Determines whether the cross-validation should stop early.</p> PARAMETER DESCRIPTION <code>trial</code> <p>The trial that is currently being evaluated.</p> <p> TYPE: <code>Trial</code> </p> <code>scores</code> <p>The scores from the evlauated splits.</p> <p> TYPE: <code>SplitScores</code> </p> RETURNS DESCRIPTION <code>bool | Exception</code> <p><code>True</code> if the cross-validation should stop, <code>False</code> if it should continue, or an <code>Exception</code> if it should stop and you'd like a custom error to be registered with the trial.</p> Source code in <code>src/amltk/sklearn/evaluation.py</code> <pre><code>def should_stop(\n    self,\n    trial: Trial,\n    scores: CVEvaluation.SplitScores,\n) -&gt; bool | Exception:\n    \"\"\"Determines whether the cross-validation should stop early.\n\n    Args:\n        trial: The trial that is currently being evaluated.\n        scores: The scores from the evlauated splits.\n\n    Returns:\n        `True` if the cross-validation should stop, `False` if it should\n        continue, or an `Exception` if it should stop and you'd like a custom\n        error to be registered with the trial.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEarlyStoppingProtocol.update","title":"update","text":"<pre><code>update(report: Report) -&gt; None\n</code></pre> <p>Update the protocol with a new report.</p> <p>This will be called when a trial has been completed, either successfully or failed. You can check for successful trials by using <code>report.status</code>.</p> PARAMETER DESCRIPTION <code>report</code> <p>The report from the trial.</p> <p> TYPE: <code>Report</code> </p> Source code in <code>src/amltk/sklearn/evaluation.py</code> <pre><code>def update(self, report: Trial.Report) -&gt; None:\n    \"\"\"Update the protocol with a new report.\n\n    This will be called when a trial has been completed, either successfully\n    or failed. You can check for successful trials by using\n    [`report.status`][amltk.optimization.Trial.Report.status].\n\n    Args:\n        report: The report from the trial.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation","title":"CVEvaluation","text":"<pre><code>CVEvaluation(\n    X: XLike,\n    y: YLike,\n    *,\n    X_test: XLike | None = None,\n    y_test: YLike | None = None,\n    splitter: (\n        Literal[\"holdout\", \"cv\"]\n        | BaseShuffleSplit\n        | BaseCrossValidator\n    ) = \"cv\",\n    n_splits: int = 5,\n    holdout_size: float = 0.33,\n    train_score: bool = False,\n    store_models: bool = False,\n    rebalance_if_required_for_stratified_splitting: (\n        bool | None\n    ) = None,\n    additional_scorers: Mapping[str, _Scorer] | None = None,\n    random_state: Seed | None = None,\n    params: Mapping[str, Any] | None = None,\n    task_hint: (\n        TaskTypeName\n        | Literal[\"classification\", \"regression\", \"auto\"]\n    ) = \"auto\",\n    working_dir: str | Path | PathBucket | None = None,\n    on_error: Literal[\"raise\", \"fail\"] = \"fail\",\n    post_split: PostSplitSignature | None = None,\n    post_processing: (\n        Callable[[Report, Node, CompleteEvalInfo], Report]\n        | None\n    ) = None,\n    post_processing_requires_models: bool = False\n)\n</code></pre> <p>               Bases: <code>Emitter</code></p> <p>Cross-validation evaluation protocol.</p> <p>This protocol will create a cross-validation task to be used in parallel and optimization. It represents a typical cross-validation evaluation for sklearn.</p> <p>Aside from the init parameters, it expects: * The pipeline you are optimizing can be made into a sklearn.pipeline.Pipeline calling <code>.build(\"sklearn\")</code>. * The seed for the trial will be passed as a param to <code>.configure()</code>. If you have a component that accepts a <code>random_state</code> parameter, you can use a <code>request()</code> so that it will be seeded correctly.</p> <pre><code>from amltk.sklearn import CVEvaluation\nfrom amltk.pipeline import Component, request\nfrom amltk.optimization import Metric\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import get_scorer\nfrom sklearn.datasets import load_iris\nfrom pathlib import Path\n\npipeline = Component(\n    RandomForestClassifier,\n    config={\"random_state\": request(\"random_state\")},\n    space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"entropy\"]},\n)\n\nworking_dir = Path(\"./some-path\")\nX, y = load_iris(return_X_y=True)\nevaluator = CVEvaluation(\n    X,\n    y,\n    n_splits=3,\n    splitter=\"cv\",\n    additional_scorers={\"roc_auc\": get_scorer(\"roc_auc_ovr\")},\n    store_models=False,\n    train_score=True,\n    working_dir=working_dir,\n)\n\nhistory = pipeline.optimize(\n    target=evaluator.fn,\n    metric=Metric(\"accuracy\", minimize=False, bounds=(0, 1)),\n    working_dir=working_dir,\n    max_trials=1,\n)\nprint(history.df())\nevaluator.bucket.rmdir()  # Cleanup\n</code></pre> <pre><code>                                                     status  ...  profile:cv:train_score:time:unit\nname                                                         ...                                  \nconfig_id=1_seed=1641201137_budget=None_instanc...  success  ...                           seconds\n\n[1 rows x 114 columns]\n</code></pre> <p>If you need to pass specific configuration items to your pipeline during configuration, you can do so using a <code>request()</code> in the config of your pipeline.</p> <p>In the below example, we allow the pipeline to be configured with <code>\"n_jobs\"</code> and pass it in to the <code>CVEvalautor</code> using the <code>params</code> argument.</p> <pre><code>from amltk.sklearn import CVEvaluation\nfrom amltk.pipeline import Component, request\nfrom amltk.optimization import Metric\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import get_scorer\nfrom sklearn.datasets import load_iris\nfrom pathlib import Path\n\nworking_dir = Path(\"./some-path\")\nX, y = load_iris(return_X_y=True)\n\npipeline = Component(\n    RandomForestClassifier,\n    config={\n        \"random_state\": request(\"random_state\"),\n        # Allow it to be configured with n_jobs\n        \"n_jobs\": request(\"n_jobs\", default=None)\n    },\n    space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"entropy\"]},\n)\n\nevaluator = CVEvaluation(\n    X,\n    y,\n    working_dir=working_dir,\n    # Use the `configure` keyword in params to pass to the `n_jobs`\n    # Anything in the pipeline requesting `n_jobs` will get the value\n    params={\"configure\": {\"n_jobs\": 2}}\n)\nhistory = pipeline.optimize(\n    target=evaluator.fn,\n    metric=Metric(\"accuracy\"),\n    working_dir=working_dir,\n    max_trials=1,\n)\nprint(history.df())\nevaluator.bucket.rmdir()  # Cleanup\n</code></pre> <pre><code>                                                     status  ...  profile:cv:split_4:time:unit\nname                                                         ...                              \nconfig_id=1_seed=1348220125_budget=None_instanc...  success  ...                       seconds\n\n[1 rows x 113 columns]\n</code></pre> <p>CV Early Stopping</p> <p>To see more about early stopping, please see <code>CVEvaluation.cv_early_stopping_plugin()</code>.</p> PARAMETER DESCRIPTION <code>X</code> <p>The features to use for training.</p> <p> TYPE: <code>XLike</code> </p> <code>y</code> <p>The target to use for training.</p> <p> TYPE: <code>YLike</code> </p> <code>X_test</code> <p>The features to use for testing. If provided, all scorers will be calculated on this data as well. Must be provided with <code>y_test=</code>.</p> <p>Scorer params for test scoring</p> <p>Due to nuances of sklearn's metadata routing, if you need to provide parameters to the scorer for the test data, you can prefix these with <code>\"test_\"</code>. For example, if you need to provide <code>pos_label</code> to the scorer for the test data, you must provide <code>test_pos_label</code> in the <code>params</code> argument.</p> <p> TYPE: <code>XLike | None</code> DEFAULT: <code>None</code> </p> <code>y_test</code> <p>The target to use for testing. If provided, all scorers will be calculated on this data as well. Must be provided with <code>X_test=</code>.</p> <p> TYPE: <code>YLike | None</code> DEFAULT: <code>None</code> </p> <code>splitter</code> <p>The cross-validation splitter to use. This can be either <code>\"holdout\"</code> or <code>\"cv\"</code>. Please see the related arguments below. If a scikit-learn cross-validator is provided, this will be used directly.</p> <p> TYPE: <code>Literal['holdout', 'cv'] | BaseShuffleSplit | BaseCrossValidator</code> DEFAULT: <code>'cv'</code> </p> <code>n_splits</code> <p>The number of cross-validation splits to use. This argument will be ignored if <code>splitter=\"holdout\"</code> or a custom splitter is provided for <code>splitter=</code>.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>holdout_size</code> <p>The size of the holdout set to use. This argument will be ignored if <code>splitter=\"cv\"</code> or a custom splitter is provided for <code>splitter=</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.33</code> </p> <code>train_score</code> <p>Whether to score on the training data as well. This will take extra time as predictions will be made on the training data as well.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>store_models</code> <p>Whether to store the trained models in the trial.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rebalance_if_required_for_stratified_splitting</code> <p>Whether the CVEvaluator should rebalance the training data to allow for stratified splitting. * If <code>True</code>, rebalancing will be done if required. That is when     the <code>splitter=</code> is <code>\"cv\"</code> or a <code>StratifiedKFold</code> and     there are fewer instances of a minority class than <code>n_splits=</code>. * If <code>None</code>, rebalancing will be done if required it. Same     as <code>True</code> but raises a warning if it occurs. * If <code>False</code>, rebalancing will never be done.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>additional_scorers</code> <p>Additional scorers to use.</p> <p> TYPE: <code>Mapping[str, _Scorer] | None</code> DEFAULT: <code>None</code> </p> <code>random_state</code> <p>The random state to use for the cross-validation <code>splitter=</code>. If a custom splitter is provided, this will be ignored.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>Parameters to pass to the estimator, splitter or scorers. See scikit-learn.org/stable/metadata_routing.html for more information.</p> <p>You may also additionally include the following as dictionarys:</p> <ul> <li><code>\"configure\"</code>: Parameters to pass to the pipeline     for <code>configure()</code>. Please     the example in the class docstring for more information.</li> <li> <p><code>\"build\"</code>: Parameters to pass to the pipeline for     <code>build()</code>.</p> <pre><code>from imblearn.pipeline import Pipeline as ImbalancedPipeline\nCVEvaluator(\n    ...,\n    params={\n        \"build\": {\n            \"builder\": \"sklearn\",\n            \"pipeline_type\": ImbalancedPipeline\n        }\n    }\n)\n</code></pre> </li> <li> <p><code>\"transform_context\"</code>: The transform context to use     for <code>configure()</code>.</p> </li> </ul> <p>Scorer params for test scoring</p> <p>Due to nuances of sklearn's metadata routing, if you need to provide parameters to the scorer for the test data, you must prefix these with <code>\"test_\"</code>. For example, if you need to provide <code>pos_label</code> to the scorer for the test data, you can provide <code>test_pos_label</code> in the <code>params</code> argument.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>task_hint</code> <p>A string indicating the task type matching those use by sklearn's <code>type_of_target</code>. This can be either <code>\"binary\"</code>, <code>\"multiclass\"</code>, <code>\"multilabel-indicator\"</code>, <code>\"continuous\"</code>, <code>\"continuous-multioutput\"</code> or <code>\"multiclass-multioutput\"</code>.</p> <p>You can also provide <code>\"classification\"</code> or <code>\"regression\"</code> for a more general hint.</p> <p>If not provided, this will be inferred from the target data. If you know this value, it is recommended to provide it as sometimes the target is ambiguous and sklearn may infer incorrectly.</p> <p> TYPE: <code>TaskTypeName | Literal['classification', 'regression', 'auto']</code> DEFAULT: <code>'auto'</code> </p> <code>working_dir</code> <p>The directory to use for storing data. If not provided, a temporary directory will be used. If provided as a string or a <code>Path</code>, it will be used as the path to the directory.</p> <p> TYPE: <code>str | Path | PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>on_error</code> <p>What to do if an error occurs in the task. This can be either <code>\"raise\"</code> or <code>\"fail\"</code>. If <code>\"raise\"</code>, the error will be raised and the task will fail. If <code>\"fail\"</code>, the error will be caught and the task will report a failure report with the error message stored inside. Set this to <code>\"fail\"</code> if you want to continue optimization even if some trials fail.</p> <p> TYPE: <code>Literal['raise', 'fail']</code> DEFAULT: <code>'fail'</code> </p> <code>post_split</code> <p>If provided, this callable will be called with a <code>PostSplitInfo</code>.</p> <p>For example, this could be useful if you'd like to save out-of-fold predictions for later use.</p> <pre><code>def my_post_split(\n    split_number: int,\n    info: CVEvaluator.PostSplitInfo,\n) -&gt; None:\n    X_val, y_val = info.val\n    oof_preds = fitted_model.predict(X_val)\n\n    split = info.current_split\n    info.trial.store({f\"oof_predictions_{split}.npy\": oof_preds})\n    return info\n</code></pre> <p>Run in the worker</p> <p>This callable will be pickled and sent to the worker that is executing an evaluation. This means that you should mitigate relying on any large objects if your callalbe is an object, as the object will get pickled and sent to the worker. This also means you can not rely on information obtained from other trials as when sending the callable to a worker, it is no longer updatable from the main process.</p> <p>You should also avoid holding on to references to either the model or large data that is passed in <code>PostSplitInfo</code> to the function.</p> <p>This parameter should primarily be used for callables that rely solely on the output of the current trial and wish to store/add additional information to the trial itself.</p> <p> TYPE: <code>PostSplitSignature | None</code> DEFAULT: <code>None</code> </p> <code>post_processing</code> <p>If provided, this callable will be called with all of the evaluated splits and the final report that will be returned. This can be used to do things such as augment the final scores if required, cleanup any resources or any other tasks that should be run after the evaluation has completed. This will be handed a <code>Report</code> and a <code>CompleteEvalInfo</code>, which contains all the information about the evaluation. If your function requires the individual models, you can set <code>post_processing_requires_models=True</code>. By default this is <code>False</code> as this requires having all models in memory at once.</p> <p>This can be useful when you'd like to report the score of a bagged model, i.e. an ensemble of all validation models. Another example is if you'd like to add to the summary, the score of what the model would be if refit on all the data.</p> <pre><code>from amltk.sklearn.voting import voting_with_prefitted_estimators\n\n# Compute the test score of all fold models bagged together\ndef my_post_processing(\n    report: Trial.Report,\n    pipeline: Node,\n    info: CVEvaluator.CompleteEvalInfo,\n) -&gt; Trial.Report:\n    bagged_model = voting_with_prefitted_estimators(info.models)\n    acc = info.scorers[\"accuracy\"]\n    bagged_score = acc(bagged_model, info.X_test, info.y_test)\n    report.summary[\"bagged_test_score\"] = bagged_score\n    return report\n</code></pre> <p>Run in the worker</p> <p>This callable will be pickled and sent to the worker that is executing an evaluation. This means that you should mitigate relying on any large objects if your callalbe is an object, as the object will get pickled and sent to the worker. This also means you can not rely on information obtained from other trials as when sending the callable to a worker, it is no longer updatable from the main process.</p> <p>This parameter should primarily be used for callables that will augment the report or what is stored with the trial. It should rely solely on the current trial to prevent unexpected issues.</p> <p> TYPE: <code>Callable[[Report, Node, CompleteEvalInfo], Report] | None</code> DEFAULT: <code>None</code> </p> <code>post_processing_requires_models</code> <p>Whether the <code>post_processing</code> function requires the models to be passed to it. If <code>True</code>, the models will be passed to the function in the <code>CompleteEvalInfo</code> object. If <code>False</code>, the models will not be passed to the function. By default this is <code>False</code> as this requires having all models in memory at once.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/sklearn/evaluation.py</code> <pre><code>def __init__(  # noqa: PLR0913, C901\n    self,\n    X: XLike,  # noqa: N803\n    y: YLike,\n    *,\n    X_test: XLike | None = None,  # noqa: N803\n    y_test: YLike | None = None,\n    splitter: (\n        Literal[\"holdout\", \"cv\"] | BaseShuffleSplit | BaseCrossValidator\n    ) = \"cv\",\n    n_splits: int = 5,  # sklearn default\n    holdout_size: float = 0.33,\n    train_score: bool = False,\n    store_models: bool = False,\n    rebalance_if_required_for_stratified_splitting: bool | None = None,\n    additional_scorers: Mapping[str, _Scorer] | None = None,\n    random_state: Seed | None = None,  # Only used if cv is an int/float\n    params: Mapping[str, Any] | None = None,\n    task_hint: (\n        TaskTypeName | Literal[\"classification\", \"regression\", \"auto\"]\n    ) = \"auto\",\n    working_dir: str | Path | PathBucket | None = None,\n    on_error: Literal[\"raise\", \"fail\"] = \"fail\",\n    post_split: PostSplitSignature | None = None,\n    post_processing: (\n        Callable[[Trial.Report, Node, CVEvaluation.CompleteEvalInfo], Trial.Report]\n        | None\n    ) = None,\n    post_processing_requires_models: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the evaluation protocol.\n\n    Args:\n        X: The features to use for training.\n        y: The target to use for training.\n        X_test: The features to use for testing. If provided, all\n            scorers will be calculated on this data as well.\n            Must be provided with `y_test=`.\n\n            !!! tip \"Scorer params for test scoring\"\n\n                Due to nuances of sklearn's metadata routing, if you need to provide\n                parameters to the scorer for the test data, you can prefix these\n                with `#!python \"test_\"`. For example, if you need to provide\n                `pos_label` to the scorer for the test data, you must provide\n                `test_pos_label` in the `params` argument.\n\n        y_test: The target to use for testing. If provided, all\n            scorers will be calculated on this data as well.\n            Must be provided with `X_test=`.\n        splitter: The cross-validation splitter to use. This can be either\n            `#!python \"holdout\"` or `#!python \"cv\"`. Please see the related\n            arguments below. If a scikit-learn cross-validator is provided,\n            this will be used directly.\n        n_splits: The number of cross-validation splits to use.\n            This argument will be ignored if `#!python splitter=\"holdout\"`\n            or a custom splitter is provided for `splitter=`.\n        holdout_size: The size of the holdout set to use. This argument\n            will be ignored if `#!python splitter=\"cv\"` or a custom splitter\n            is provided for `splitter=`.\n        train_score: Whether to score on the training data as well. This\n            will take extra time as predictions will be made on the\n            training data as well.\n        store_models: Whether to store the trained models in the trial.\n        rebalance_if_required_for_stratified_splitting: Whether the CVEvaluator\n            should rebalance the training data to allow for stratified splitting.\n            * If `True`, rebalancing will be done if required. That is when\n                the `splitter=` is `\"cv\"` or a `StratifiedKFold` and\n                there are fewer instances of a minority class than `n_splits=`.\n            * If `None`, rebalancing will be done if required it. Same\n                as `True` but raises a warning if it occurs.\n            * If `False`, rebalancing will never be done.\n        additional_scorers: Additional scorers to use.\n        random_state: The random state to use for the cross-validation\n            `splitter=`. If a custom splitter is provided, this will be\n            ignored.\n        params: Parameters to pass to the estimator, splitter or scorers.\n            See https://scikit-learn.org/stable/metadata_routing.html for\n            more information.\n\n            You may also additionally include the following as dictionarys:\n\n            * `#!python \"configure\"`: Parameters to pass to the pipeline\n                for [`configure()`][amltk.pipeline.Node.configure]. Please\n                the example in the class docstring for more information.\n            * `#!python \"build\"`: Parameters to pass to the pipeline for\n                [`build()`][amltk.pipeline.Node.build].\n\n                ```python\n                from imblearn.pipeline import Pipeline as ImbalancedPipeline\n                CVEvaluator(\n                    ...,\n                    params={\n                        \"build\": {\n                            \"builder\": \"sklearn\",\n                            \"pipeline_type\": ImbalancedPipeline\n                        }\n                    }\n                )\n                ```\n\n            * `#!python \"transform_context\"`: The transform context to use\n                for [`configure()`][amltk.pipeline.Node.configure].\n\n            !!! tip \"Scorer params for test scoring\"\n\n                Due to nuances of sklearn's metadata routing, if you need to provide\n                parameters to the scorer for the test data, you must prefix these\n                with `#!python \"test_\"`. For example, if you need to provide\n                `pos_label` to the scorer for the test data, you can provide\n                `test_pos_label` in the `params` argument.\n\n        task_hint: A string indicating the task type matching those\n            use by sklearn's `type_of_target`. This can be either\n            `#!python \"binary\"`, `#!python \"multiclass\"`,\n            `#!python \"multilabel-indicator\"`, `#!python \"continuous\"`,\n            `#!python \"continuous-multioutput\"` or\n            `#!python \"multiclass-multioutput\"`.\n\n            You can also provide `#!python \"classification\"` or\n            `#!python \"regression\"` for a more general hint.\n\n            If not provided, this will be inferred from the target data.\n            If you know this value, it is recommended to provide it as\n            sometimes the target is ambiguous and sklearn may infer\n            incorrectly.\n        working_dir: The directory to use for storing data. If not provided,\n            a temporary directory will be used. If provided as a string\n            or a `Path`, it will be used as the path to the directory.\n        on_error: What to do if an error occurs in the task. This can be\n            either `#!python \"raise\"` or `#!python \"fail\"`. If `#!python \"raise\"`,\n            the error will be raised and the task will fail. If `#!python \"fail\"`,\n            the error will be caught and the task will report a failure report\n            with the error message stored inside.\n            Set this to `#!python \"fail\"` if you want to continue optimization\n            even if some trials fail.\n        post_split: If provided, this callable will be called with a\n            [`PostSplitInfo`][amltk.sklearn.evaluation.CVEvaluation.PostSplitInfo].\n\n            For example, this could be useful if you'd like to save out-of-fold\n            predictions for later use.\n\n            ```python\n            def my_post_split(\n                split_number: int,\n                info: CVEvaluator.PostSplitInfo,\n            ) -&gt; None:\n                X_val, y_val = info.val\n                oof_preds = fitted_model.predict(X_val)\n\n                split = info.current_split\n                info.trial.store({f\"oof_predictions_{split}.npy\": oof_preds})\n                return info\n            ```\n\n            !!! warning \"Run in the worker\"\n\n                This callable will be pickled and sent to the worker that is\n                executing an evaluation. This means that you should mitigate\n                relying on any large objects if your callalbe is an object, as\n                the object will get pickled and sent to the worker. This also means\n                you can not rely on information obtained from other trials as when\n                sending the callable to a worker, it is no longer updatable from the\n                main process.\n\n                You should also avoid holding on to references to either the model\n                or large data that is passed in\n                [`PostSplitInfo`][amltk.sklearn.evaluation.CVEvaluation.PostSplitInfo]\n                to the function.\n\n                This parameter should primarily be used for callables that rely\n                solely on the output of the current trial and wish to store/add\n                additional information to the trial itself.\n\n        post_processing: If provided, this callable will be called with all of the\n            evaluated splits and the final report that will be returned.\n            This can be used to do things such as augment the final scores\n            if required, cleanup any resources or any other tasks that should be\n            run after the evaluation has completed. This will be handed a\n            [`Report`][amltk.optimization.trial.Trial.Report] and a\n            [`CompleteEvalInfo`][amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo],\n            which contains all the information about the evaluation. If your\n            function requires the individual models, you can set\n            `post_processing_requires_models=True`. By default this is `False`\n            as this requires having all models in memory at once.\n\n            This can be useful when you'd like to report the score of a bagged\n            model, i.e. an ensemble of all validation models. Another example\n            is if you'd like to add to the summary, the score of what the model\n            would be if refit on all the data.\n\n            ```python\n            from amltk.sklearn.voting import voting_with_prefitted_estimators\n\n            # Compute the test score of all fold models bagged together\n            def my_post_processing(\n                report: Trial.Report,\n                pipeline: Node,\n                info: CVEvaluator.CompleteEvalInfo,\n            ) -&gt; Trial.Report:\n                bagged_model = voting_with_prefitted_estimators(info.models)\n                acc = info.scorers[\"accuracy\"]\n                bagged_score = acc(bagged_model, info.X_test, info.y_test)\n                report.summary[\"bagged_test_score\"] = bagged_score\n                return report\n            ```\n\n            !!! warning \"Run in the worker\"\n\n                This callable will be pickled and sent to the worker that is\n                executing an evaluation. This means that you should mitigate\n                relying on any large objects if your callalbe is an object, as\n                the object will get pickled and sent to the worker. This also means\n                you can not rely on information obtained from other trials as when\n                sending the callable to a worker, it is no longer updatable from the\n                main process.\n\n                This parameter should primarily be used for callables that will\n                augment the report or what is stored with the trial. It should\n                rely solely on the current trial to prevent unexpected issues.\n\n        post_processing_requires_models: Whether the `post_processing` function\n            requires the models to be passed to it. If `True`, the models will\n            be passed to the function in the `CompleteEvalInfo` object. If `False`,\n            the models will not be passed to the function. By default this is\n            `False` as this requires having all models in memory at once.\n\n    \"\"\"\n    super().__init__()\n    if (X_test is not None and y_test is None) or (\n        y_test is not None and X_test is None\n    ):\n        raise ValueError(\n            \"Both `X_test`, `y_test` must be provided together if one is provided.\",\n        )\n\n    match working_dir:\n        case None:\n            tmpdir = Path(\n                tempfile.mkdtemp(\n                    prefix=self.TMP_DIR_PREFIX,\n                    suffix=datetime.now().isoformat(),\n                ),\n            )\n            bucket = PathBucket(tmpdir)\n        case str() | Path():\n            bucket = PathBucket(working_dir)\n        case PathBucket():\n            bucket = working_dir\n\n    match task_hint:\n        case \"classification\" | \"regression\" | \"auto\":\n            task_type = identify_task_type(y, task_hint=task_hint)\n        case (\n            \"binary\"\n            | \"multiclass\"\n            | \"multilabel-indicator\"\n            | \"continuous\"\n            | \"continuous-multioutput\"\n            | \"multiclass-multioutput\"  #\n        ):\n            task_type = task_hint\n        case _:\n            raise ValueError(\n                f\"Invalid {task_hint=} provided. Must be in {_valid_task_types}\"\n                f\"\\n{type(task_hint)=}\",\n            )\n\n    match splitter:\n        case \"cv\":\n            splitter = _default_cv_resampler(\n                task_type,\n                n_splits=n_splits,\n                random_state=random_state,\n            )\n\n        case \"holdout\":\n            splitter = _default_holdout(\n                task_type,\n                holdout_size=holdout_size,\n                random_state=random_state,\n            )\n        case _:\n            splitter = splitter  # noqa: PLW0127\n\n    # This whole block is to check whether we should resample for stratified\n    # sampling, in the case of a low minority class.\n    if (\n        isinstance(splitter, StratifiedKFold)\n        and rebalance_if_required_for_stratified_splitting is not False\n        and task_type in (\"binary\", \"multiclass\")\n    ):\n        if rebalance_if_required_for_stratified_splitting is None:\n            _warning = (\n                f\"Labels have fewer than `{n_splits=}` instances. Resampling data\"\n                \" to ensure it's possible to have one of each label in each fold.\"\n                \" Note that this may cause things to crash if you've provided extra\"\n                \" `params` as the `X` data will have gotten slightly larger. Please\"\n                \" set `rebalance_if_required_for_stratified_splitting=False` if you\"\n                \" do not wish this to be enabled automatically, in which case, you\"\n                \" may either perform resampling yourself or choose a smaller\"\n                \" `n_splits=`.\"\n            )\n        else:\n            _warning = None\n\n        x_is_frame = isinstance(X, pd.DataFrame)\n        y_is_frame = isinstance(y, pd.Series | pd.DataFrame)\n\n        X, y = resample_if_minority_class_too_few_for_n_splits(  # type: ignore\n            X if x_is_frame else pd.DataFrame(X),\n            y if y_is_frame else pd.Series(y),  # type: ignore\n            n_splits=n_splits,\n            seed=random_state,\n            _warning_if_occurs=_warning,\n        )\n\n        if not x_is_frame:\n            X = X.to_numpy()  # type: ignore\n        if not y_is_frame:\n            y = y.to_numpy()  # type: ignore\n\n    self.task_type = task_type\n    self.additional_scorers = additional_scorers\n    self.bucket = bucket\n    self.splitter = splitter\n    self.params = dict(params) if params is not None else {}\n    self.store_models = store_models\n    self.train_score = train_score\n\n    self.X_stored = self.bucket[self._X_FILENAME].put(X)\n    self.y_stored = self.bucket[self._Y_FILENAME].put(y)\n\n    self.X_test_stored = None\n    self.y_test_stored = None\n    if X_test is not None and y_test is not None:\n        self.X_test_stored = self.bucket[self._X_TEST_FILENAME].put(X_test)\n        self.y_test_stored = self.bucket[self._Y_TEST_FILENAME].put(y_test)\n\n    # We apply a heuristic that \"large\" parameters, such as sample_weights\n    # should be stored to disk as transferring them directly to subprocess as\n    # parameters is quite expensive (they must be non-optimally pickled and\n    # streamed to the receiving process). By saving it to a file, we can\n    # make use of things like numpy/pandas specific efficient pickling\n    # protocols and also avoid the need to stream it to the subprocess.\n    storable_params = {\n        k: v\n        for k, v in self.params.items()\n        if hasattr(v, \"__len__\") and len(v) &gt; self.LARGE_PARAM_HEURISTIC  # type: ignore\n    }\n    for k, v in storable_params.items():\n        match subclass_map(v, self.PARAM_EXTENSION_MAPPING, default=None):  # type: ignore\n            case (_, extension_to_save_as):\n                ext = extension_to_save_as\n            case _:\n                ext = \"pkl\"\n\n        self.params[k] = self.bucket[f\"{k}.{ext}\"].put(v)\n\n    # This is the actual function that will be called in the task\n    self.fn = partial(\n        cross_validate_task,\n        X=self.X_stored,\n        y=self.y_stored,\n        X_test=self.X_test_stored,\n        y_test=self.y_test_stored,\n        splitter=self.splitter,\n        additional_scorers=self.additional_scorers,\n        params=self.params,\n        store_models=self.store_models,\n        train_score=self.train_score,\n        on_error=on_error,\n        post_split=post_split,\n        post_processing=post_processing,\n        post_processing_requires_models=post_processing_requires_models,\n    )\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.LARGE_PARAM_HEURISTIC","title":"LARGE_PARAM_HEURISTIC  <code>class-attribute</code>","text":"<pre><code>LARGE_PARAM_HEURISTIC: int = 100\n</code></pre> <p>Any item in <code>params=</code> which is greater will be stored to disk when sent to the worker.</p> <p>When launching tasks, pickling and streaming large data to tasks can be expensive. This parameter checks if the object is large and if so, stores it to disk and gives it to the task as a <code>Stored</code> object instead.</p> <p>Please feel free to overwrite this class variable as needed.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.PARAM_EXTENSION_MAPPING","title":"PARAM_EXTENSION_MAPPING  <code>class-attribute</code>","text":"<pre><code>PARAM_EXTENSION_MAPPING: dict[type[Sized], str] = {\n    ndarray: \"npy\",\n    DataFrame: \"pdpickle\",\n    Series: \"pdpickle\",\n}\n</code></pre> <p>The mapping from types to extensions in <code>params</code>.</p> <p>If the parameter is an instance of one of these types, and is larger than <code>LARGE_PARAM_HEURISTIC</code>, then it will be stored to disk and loaded back up in the task.</p> <p>Please feel free to overwrite this class variable as needed.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.SPLIT_EVALUATED","title":"SPLIT_EVALUATED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SPLIT_EVALUATED: Event[\n    [Trial, SplitScores], bool | Exception\n] = Event(\"split-evaluated\")\n</code></pre> <p>Event that is emitted when a split has been evaluated.</p> <p>Only emitted if the evaluator plugin is being used.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.TMP_DIR_PREFIX","title":"TMP_DIR_PREFIX  <code>class-attribute</code>","text":"<pre><code>TMP_DIR_PREFIX: str = 'amltk-sklearn-cv-evaluation-data-'\n</code></pre> <p>Prefix for temporary directory names.</p> <p>This is only used when <code>working_dir</code> is not specified. If not specified you can control the tmp dir location by setting the <code>TMPDIR</code> environment variable. By default this is <code>/tmp</code>.</p> <p>When using a temporary directory, it will be deleted by default, controlled by the <code>delete_working_dir=</code> argument.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.X_stored","title":"X_stored  <code>instance-attribute</code>","text":"<pre><code>X_stored: Stored[XLike] = put(X)\n</code></pre> <p>The stored features.</p> <p>You can call <code>.load()</code> to load the data.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.additional_scorers","title":"additional_scorers  <code>instance-attribute</code>","text":"<pre><code>additional_scorers: Mapping[str, _Scorer] | None = (\n    additional_scorers\n)\n</code></pre> <p>Additional scorers that will be used.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.bucket","title":"bucket  <code>instance-attribute</code>","text":"<pre><code>bucket: PathBucket = bucket\n</code></pre> <p>The bucket to use for storing data.</p> <p>For cleanup, you can call <code>bucket.rmdir()</code>.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: Mapping[str, Any | Stored[Any]] = (\n    dict(params) if params is not None else {}\n)\n</code></pre> <p>Parameters to pass to the estimator, splitter or scorers.</p> <p>Please see scikit-learn.org/stable/metadata_routing.html for more.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.splitter","title":"splitter  <code>instance-attribute</code>","text":"<pre><code>splitter: BaseShuffleSplit | BaseCrossValidator = splitter\n</code></pre> <p>The splitter that will be used.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.store_models","title":"store_models  <code>instance-attribute</code>","text":"<pre><code>store_models: bool = store_models\n</code></pre> <p>Whether models will be stored in the trial.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.task_type","title":"task_type  <code>instance-attribute</code>","text":"<pre><code>task_type: TaskTypeName = task_type\n</code></pre> <p>The inferred task type.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.train_score","title":"train_score  <code>instance-attribute</code>","text":"<pre><code>train_score: bool = train_score\n</code></pre> <p>Whether scores will be calculated on the training data as well.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.y_stored","title":"y_stored  <code>instance-attribute</code>","text":"<pre><code>y_stored: Stored[YLike] = put(y)\n</code></pre> <p>The stored target.</p> <p>You can call <code>.load()</code> to load the data.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo","title":"CompleteEvalInfo  <code>dataclass</code>","text":"<pre><code>CompleteEvalInfo(\n    X: XLike,\n    y: YLike,\n    X_test: XLike | None,\n    y_test: YLike | None,\n    splitter: BaseShuffleSplit | BaseCrossValidator,\n    max_splits: int,\n    scores: SplitScores,\n    scorers: dict[str, _Scorer],\n    models: list[BaseEstimator] | None,\n    splitter_params: Mapping[str, Any],\n    fit_params: Mapping[str, Any],\n    scorer_params: Mapping[str, Any],\n    test_scorer_params: Mapping[str, Any],\n)\n</code></pre> <p>Information about the final evaluation of a cross-validation task.</p> <p>This class contains information about the final evaluation of a cross-validation that will be passed to the post-processing function.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.X","title":"X  <code>instance-attribute</code>","text":"<pre><code>X: XLike\n</code></pre> <p>The features to used for training.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.X_test","title":"X_test  <code>instance-attribute</code>","text":"<pre><code>X_test: XLike | None\n</code></pre> <p>The features used for testing.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.fit_params","title":"fit_params  <code>instance-attribute</code>","text":"<pre><code>fit_params: Mapping[str, Any]\n</code></pre> <p>The parameters that were used for fitting the estimator.</p> <p>Please use <code>select_params()</code> if you need to select the params specific to a split, i.e. for <code>sample_weights</code>.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.max_splits","title":"max_splits  <code>instance-attribute</code>","text":"<pre><code>max_splits: int\n</code></pre> <p>The maximum number of splits that were (or could have been) evaluated.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models: list[BaseEstimator] | None\n</code></pre> <p>The models that were trained in each split.</p> <p>This will be <code>None</code> if <code>post_processing_requires_models=False</code>.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.scorer_params","title":"scorer_params  <code>instance-attribute</code>","text":"<pre><code>scorer_params: Mapping[str, Any]\n</code></pre> <p>The parameters that were used for scoring the estimator.</p> <p>Please use <code>select_params()</code> if you need to select the params specific to a split, i.e. for <code>sample_weights</code>.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.scorers","title":"scorers  <code>instance-attribute</code>","text":"<pre><code>scorers: dict[str, _Scorer]\n</code></pre> <p>The scorers that were used.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.scores","title":"scores  <code>instance-attribute</code>","text":"<pre><code>scores: SplitScores\n</code></pre> <p>The scores for the splits that were evaluated.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.splitter","title":"splitter  <code>instance-attribute</code>","text":"<pre><code>splitter: BaseShuffleSplit | BaseCrossValidator\n</code></pre> <p>The splitter that was used.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.splitter_params","title":"splitter_params  <code>instance-attribute</code>","text":"<pre><code>splitter_params: Mapping[str, Any]\n</code></pre> <p>The parameters that were used for the splitter.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.test_scorer_params","title":"test_scorer_params  <code>instance-attribute</code>","text":"<pre><code>test_scorer_params: Mapping[str, Any]\n</code></pre> <p>The parameters that were used for scoring the test data.</p> <p>Please use <code>select_params()</code> if you need to select the params specific to a split, i.e. for <code>sample_weights</code>.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: YLike\n</code></pre> <p>The targets used for training.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.y_test","title":"y_test  <code>instance-attribute</code>","text":"<pre><code>y_test: YLike | None\n</code></pre> <p>The targets used for testing.</p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.CompleteEvalInfo.select_params","title":"select_params","text":"<pre><code>select_params(\n    params: Mapping[str, Any], indices: ndarray\n) -&gt; dict[str, Any]\n</code></pre> <p>Convinience method to select parameters for a specific split.</p> Source code in <code>src/amltk/sklearn/evaluation.py</code> <pre><code>def select_params(\n    self,\n    params: Mapping[str, Any],\n    indices: np.ndarray,\n) -&gt; dict[str, Any]:\n    \"\"\"Convinience method to select parameters for a specific split.\"\"\"\n    return _check_method_params(self.X, params, indices=indices)\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.PostSplitInfo","title":"PostSplitInfo","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information about the evaluation of a split.</p> ATTRIBUTE DESCRIPTION <code>X</code> <p>The features to used for training.</p> <p> TYPE: <code>XLike</code> </p> <code>y</code> <p>The targets used for training.</p> <p> TYPE: <code>YLike</code> </p> <code>X_test</code> <p>The features used for testing if it was passed in.</p> <p> TYPE: <code>XLike | None</code> </p> <code>y_test</code> <p>The targets used for testing if it was passed in.</p> <p> TYPE: <code>YLike | None</code> </p> <code>i_train</code> <p>The train indices for this split.</p> <p> TYPE: <code>ndarray</code> </p> <code>i_val</code> <p>The validation indices for this split.</p> <p> TYPE: <code>ndarray</code> </p> <code>model</code> <p>The model that was trained in this split.</p> <p> TYPE: <code>BaseEstimator</code> </p> <code>train_scores</code> <p>The training scores for this split if requested.</p> <p> TYPE: <code>Mapping[str, float] | None</code> </p> <code>val_scores</code> <p>The validation scores for this split.</p> <p> TYPE: <code>Mapping[str, float]</code> </p> <code>test_scores</code> <p>The test scores for this split if requested.</p> <p> TYPE: <code>Mapping[str, float] | None</code> </p> <code>fitting_params</code> <p>Any additional fitting parameters that were used.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>train_scorer_params</code> <p>Any additional scorer parameters used for evaluating scorers on training set.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>val_scorer_params</code> <p>Any additional scorer parameters used for evaluating scorers on training set.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>test_scorer_params</code> <p>Any additional scorer parameters used for evaluating scorers on training set.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.SplitScores","title":"SplitScores","text":"<p>               Bases: <code>NamedTuple</code></p> <p>The scores for a split.</p> ATTRIBUTE DESCRIPTION <code>val</code> <p>The validation scores for all evaluated split.</p> <p> TYPE: <code>Mapping[str, list[float]]</code> </p> <code>train</code> <p>The training scores for all evaluated splits if requested.</p> <p> TYPE: <code>Mapping[str, list[float]] | None</code> </p> <code>test</code> <p>The test scores for all evaluated splits if requested.</p> <p> TYPE: <code>Mapping[str, list[float]] | None</code> </p>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.CVEvaluation.cv_early_stopping_plugin","title":"cv_early_stopping_plugin","text":"<pre><code>cv_early_stopping_plugin(\n    strategy: CVEarlyStoppingProtocol | None = None,\n    *,\n    create_comms: (\n        Callable[[], tuple[Comm, Comm]] | None\n    ) = None\n) -&gt; _CVEarlyStoppingPlugin\n</code></pre> <p>Create a plugin for a task allow for early stopping.</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\n\nimport sklearn.datasets\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom amltk.sklearn import CVEvaluation\nfrom amltk.pipeline import Component\nfrom amltk.optimization import Metric, Trial\n\nworking_dir = Path(\"./some-path\")\npipeline = Component(DecisionTreeClassifier, space={\"max_depth\": (1, 10)})\nx, y = sklearn.datasets.load_iris(return_X_y=True)\nevaluator = CVEvaluation(x, y, n_splits=3, working_dir=working_dir)\n\n# Our early stopping strategy, with an `update()` and `should_stop()`\n# signature match what's expected.\n\n@dataclass\nclass CVEarlyStopper:\n    def update(self, report: Trial.Report) -&gt; None:\n        # Normally you would update w.r.t. a finished trial, such\n        # as updating a moving average of the scores.\n        pass\n\n    def should_stop(self, trial: Trial, scores: CVEvaluation.SplitScores) -&gt; bool | Exception:\n        # Return True to stop, False to continue. Alternatively, return a\n        # specific exception to attach to the report instead\n        return True\n\nhistory = pipeline.optimize(\n    target=evaluator.fn,\n    metric=Metric(\"accuracy\", minimize=False, bounds=(0, 1)),\n    max_trials=1,\n    working_dir=working_dir,\n\n    # Here we insert the plugin to the task that will get created\n    plugins=[evaluator.cv_early_stopping_plugin(strategy=CVEarlyStopper())],\n\n    # Notably, we set `on_trial_exception=\"continue\"` to not stop as\n    # we expect trials to fail given the early stopping strategy\n    on_trial_exception=\"continue\",\n)\n</code></pre> <pre>\n<code>\u256d\u2500\u2500\u2500\u2500 Report(config_id=1_seed=1509460901_budget=None_instance=None) - fail \u2500\u2500\u2500\u2500\u256e\n\u2502 Status(fail)                                                                 \u2502\n\u2502 MetricCollection(                                                            \u2502\n\u2502     metrics={                                                                \u2502\n\u2502         'accuracy': Metric(                                                  \u2502\n\u2502             name='accuracy',                                                 \u2502\n\u2502             minimize=False,                                                  \u2502\n\u2502             bounds=(0.0, 1.0),                                               \u2502\n\u2502             fn=None                                                          \u2502\n\u2502         )                                                                    \u2502\n\u2502     }                                                                        \u2502\n\u2502 )                                                                            \u2502\n\u2502 \u256d\u2500 Metrics \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 MetricCollection(                                                        \u2502 \u2502\n\u2502 \u2502     metrics={                                                            \u2502 \u2502\n\u2502 \u2502         'accuracy': Metric(                                              \u2502 \u2502\n\u2502 \u2502             name='accuracy',                                             \u2502 \u2502\n\u2502 \u2502             minimize=False,                                              \u2502 \u2502\n\u2502 \u2502             bounds=(0.0, 1.0),                                           \u2502 \u2502\n\u2502 \u2502             fn=None                                                      \u2502 \u2502\n\u2502 \u2502         )                                                                \u2502 \u2502\n\u2502 \u2502     }                                                                    \u2502 \u2502\n\u2502 \u2502 )                                                                        \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 config             {'DecisionTreeClassifier:max_depth': 7}                   \u2502\n\u2502 seed               1509460901                                                \u2502\n\u2502 bucket             PathBucket(PosixPath('some-path/config_id=1_seed=1509460\u2026 \u2502\n\u2502 summary            {'split_0:val_accuracy': 0.94}                            \u2502\n\u2502 storage            {'exception.txt'}                                         \u2502\n\u2502 profile:cv         Interval(                                                 \u2502\n\u2502                        memory=Interval(                                      \u2502\n\u2502                            start_vms=1229381632.0,                           \u2502\n\u2502                            start_rss=244924416.0,                            \u2502\n\u2502                            end_vms=1229381632,                               \u2502\n\u2502                            end_rss=247676928,                                \u2502\n\u2502                            unit=bytes                                        \u2502\n\u2502                        ),                                                    \u2502\n\u2502                        time=Interval(                                        \u2502\n\u2502                            start=1723534477.633168,                          \u2502\n\u2502                            end=1723534477.6756542,                           \u2502\n\u2502                            kind=wall,                                        \u2502\n\u2502                            unit=seconds                                      \u2502\n\u2502                        )                                                     \u2502\n\u2502                    )                                                         \u2502\n\u2502 profile:cv:fit     Interval(                                                 \u2502\n\u2502                        memory=Interval(                                      \u2502\n\u2502                            start_vms=1229381632.0,                           \u2502\n\u2502                            start_rss=245841920.0,                            \u2502\n\u2502                            end_vms=1229381632,                               \u2502\n\u2502                            end_rss=247545856,                                \u2502\n\u2502                            unit=bytes                                        \u2502\n\u2502                        ),                                                    \u2502\n\u2502                        time=Interval(                                        \u2502\n\u2502                            start=1723534477.645106,                          \u2502\n\u2502                            end=1723534477.649212,                            \u2502\n\u2502                            kind=wall,                                        \u2502\n\u2502                            unit=seconds                                      \u2502\n\u2502                        )                                                     \u2502\n\u2502                    )                                                         \u2502\n\u2502 profile:cv:score   Interval(                                                 \u2502\n\u2502                        memory=Interval(                                      \u2502\n\u2502                            start_vms=1229381632.0,                           \u2502\n\u2502                            start_rss=247545856.0,                            \u2502\n\u2502                            end_vms=1229381632,                               \u2502\n\u2502                            end_rss=247676928,                                \u2502\n\u2502                            unit=bytes                                        \u2502\n\u2502                        ),                                                    \u2502\n\u2502                        time=Interval(                                        \u2502\n\u2502                            start=1723534477.6497915,                         \u2502\n\u2502                            end=1723534477.651904,                            \u2502\n\u2502                            kind=wall,                                        \u2502\n\u2502                            unit=seconds                                      \u2502\n\u2502                        )                                                     \u2502\n\u2502                    )                                                         \u2502\n\u2502 profile:cv:split_0 Interval(                                                 \u2502\n\u2502                        memory=Interval(                                      \u2502\n\u2502                            start_vms=1229381632.0,                           \u2502\n\u2502                            start_rss=247676928.0,                            \u2502\n\u2502                            end_vms=1229381632,                               \u2502\n\u2502                            end_rss=247676928,                                \u2502\n\u2502                            unit=bytes                                        \u2502\n\u2502                        ),                                                    \u2502\n\u2502                        time=Interval(                                        \u2502\n\u2502                            start=1723534477.6522815,                         \u2502\n\u2502                            end=1723534477.675374,                            \u2502\n\u2502                            kind=wall,                                        \u2502\n\u2502                            unit=seconds                                      \u2502\n\u2502                        )                                                     \u2502\n\u2502                    )                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>!!! warning \"Recommended settings for <code>CVEvaluation</code></p> <pre><code>When a trial is early stopped, it will be counted as a failed trial.\nThis can conflict with the behaviour of `pipeline.optimize` which\nby default sets `on_trial_exception=\"raise\"`, causing the optimization\nto end. If using [`pipeline.optimize`][amltk.pipeline.Node.optimize],\nto set `on_trial_exception=\"continue\"` to continue optimization.\n</code></pre> <p>This will also add a new event to the task which you can subscribe to with <code>task.on(\"split-evaluated\")</code>. It will be passed a <code>CVEvaluation.PostSplitInfo</code> that you can use to make a decision on whether to continue or stop. The passed in <code>strategy=</code> simply sets up listening to these events for you. You can also do this manually.</p> <pre><code>scores = []\nevaluator = CVEvaluation(...)\ntask = scheduler.task(\n    evaluator.fn,\n    plugins=[evaluator.cv_early_stopping_plugin()]\n)\n\n@task.on(\"split-evaluated\")\ndef should_stop(trial: Trial, scores: CVEvaluation.SplitScores) -&gt; bool | Execption:\n    # Make a decision on whether to stop or continue\n    return info.scores[\"accuracy\"] &lt; np.mean(scores.val[\"accuracy\"])\n\n@task.on(\"result\")\ndef update_scores(_, report: Trial.Report) -&gt; bool | Execption:\n    if report.status is Trial.Status.SUCCESS:\n        return scores.append(report.values[\"accuracy\"])\n</code></pre> PARAMETER DESCRIPTION <code>strategy</code> <p>The strategy to use for early stopping. Must implement the <code>update()</code> and <code>should_stop()</code> methods of <code>CVEarlyStoppingProtocol</code>. Please follow the documentation link to find out more.</p> <p>By default, when no <code>strategy=</code> is passedj this is <code>None</code> and this will create a <code>Comm</code> object, allowing communication between the worker running the task and the main process. This adds a new event to the task that you can subscribe to with <code>task.on(\"split-evaluated\")</code>. This is how a passed in strategy will be called and updated.</p> <p> TYPE: <code>CVEarlyStoppingProtocol | None</code> DEFAULT: <code>None</code> </p> <code>create_comms</code> <p>A function that creates a pair of comms for the plugin to use. This is useful if you want to create a custom communication channel. If not provided, the default communication channel will be used.</p> <p>Default communication channel</p> <p>By default we use a simple <code>multiprocessing.Pipe</code> which works for parallel processses from <code>ProcessPoolExecutor</code>. This may not work if the tasks is being executed in a different filesystem or depending on the executor which executes the task.</p> <p> TYPE: <code>Callable[[], tuple[Comm, Comm]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>_CVEarlyStoppingPlugin</code> <p>The plugin to use for the task.</p> Source code in <code>src/amltk/sklearn/evaluation.py</code> <pre><code>def cv_early_stopping_plugin(\n    self,\n    strategy: CVEarlyStoppingProtocol\n    | None = None,  # TODO: Can provide some defaults...\n    *,\n    create_comms: Callable[[], tuple[Comm, Comm]] | None = None,\n) -&gt; CVEvaluation._CVEarlyStoppingPlugin:\n    \"\"\"Create a plugin for a task allow for early stopping.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" html=\"true\"\n    from dataclasses import dataclass\n    from pathlib import Path\n\n    import sklearn.datasets\n    from sklearn.tree import DecisionTreeClassifier\n\n    from amltk.sklearn import CVEvaluation\n    from amltk.pipeline import Component\n    from amltk.optimization import Metric, Trial\n\n    working_dir = Path(\"./some-path\")\n    pipeline = Component(DecisionTreeClassifier, space={\"max_depth\": (1, 10)})\n    x, y = sklearn.datasets.load_iris(return_X_y=True)\n    evaluator = CVEvaluation(x, y, n_splits=3, working_dir=working_dir)\n\n    # Our early stopping strategy, with an `update()` and `should_stop()`\n    # signature match what's expected.\n\n    @dataclass\n    class CVEarlyStopper:\n        def update(self, report: Trial.Report) -&gt; None:\n            # Normally you would update w.r.t. a finished trial, such\n            # as updating a moving average of the scores.\n            pass\n\n        def should_stop(self, trial: Trial, scores: CVEvaluation.SplitScores) -&gt; bool | Exception:\n            # Return True to stop, False to continue. Alternatively, return a\n            # specific exception to attach to the report instead\n            return True\n\n    history = pipeline.optimize(\n        target=evaluator.fn,\n        metric=Metric(\"accuracy\", minimize=False, bounds=(0, 1)),\n        max_trials=1,\n        working_dir=working_dir,\n\n        # Here we insert the plugin to the task that will get created\n        plugins=[evaluator.cv_early_stopping_plugin(strategy=CVEarlyStopper())],\n\n        # Notably, we set `on_trial_exception=\"continue\"` to not stop as\n        # we expect trials to fail given the early stopping strategy\n        on_trial_exception=\"continue\",\n    )\n    from amltk._doc import doc_print; doc_print(print, history[0])  # markdown-exec: hide\n    evaluator.bucket.rmdir()  # markdown-exec: hide\n    ```\n\n    !!! warning \"Recommended settings for `CVEvaluation`\n\n        When a trial is early stopped, it will be counted as a failed trial.\n        This can conflict with the behaviour of `pipeline.optimize` which\n        by default sets `on_trial_exception=\"raise\"`, causing the optimization\n        to end. If using [`pipeline.optimize`][amltk.pipeline.Node.optimize],\n        to set `on_trial_exception=\"continue\"` to continue optimization.\n\n    This will also add a new event to the task which you can subscribe to with\n    [`task.on(\"split-evaluated\")`][amltk.sklearn.evaluation.CVEvaluation.SPLIT_EVALUATED].\n    It will be passed a\n    [`CVEvaluation.PostSplitInfo`][amltk.sklearn.evaluation.CVEvaluation.PostSplitInfo]\n    that you can use to make a decision on whether to continue or stop. The\n    passed in `strategy=` simply sets up listening to these events for you.\n    You can also do this manually.\n\n    ```python\n    scores = []\n    evaluator = CVEvaluation(...)\n    task = scheduler.task(\n        evaluator.fn,\n        plugins=[evaluator.cv_early_stopping_plugin()]\n    )\n\n    @task.on(\"split-evaluated\")\n    def should_stop(trial: Trial, scores: CVEvaluation.SplitScores) -&gt; bool | Execption:\n        # Make a decision on whether to stop or continue\n        return info.scores[\"accuracy\"] &lt; np.mean(scores.val[\"accuracy\"])\n\n    @task.on(\"result\")\n    def update_scores(_, report: Trial.Report) -&gt; bool | Execption:\n        if report.status is Trial.Status.SUCCESS:\n            return scores.append(report.values[\"accuracy\"])\n    ```\n\n    Args:\n        strategy: The strategy to use for early stopping. Must implement the\n            `update()` and `should_stop()` methods of\n            [`CVEarlyStoppingProtocol`][amltk.sklearn.evaluation.CVEarlyStoppingProtocol].\n            Please follow the documentation link to find out more.\n\n            By default, when no `strategy=` is passedj this is `None` and\n            this will create a [`Comm`][amltk.scheduling.plugins.comm.Comm] object,\n            allowing communication between the worker running the task and the main\n            process. This adds a new event to the task that you can subscribe\n            to with\n            [`task.on(\"split-evaluated\")`][amltk.sklearn.evaluation.CVEvaluation.SPLIT_EVALUATED].\n            This is how a passed in strategy will be called and updated.\n        create_comms: A function that creates a pair of comms for the\n            plugin to use. This is useful if you want to create a\n            custom communication channel. If not provided, the default\n            communication channel will be used.\n\n            !!! note \"Default communication channel\"\n\n                By default we use a simple `multiprocessing.Pipe` which works\n                for parallel processses from\n                [`ProcessPoolExecutor`][concurrent.futures.ProcessPoolExecutor].\n                This may not work if the tasks is being executed in a different\n                filesystem or depending on the executor which executes the task.\n\n    Returns:\n        The plugin to use for the task.\n    \"\"\"  # noqa: E501\n    return CVEvaluation._CVEarlyStoppingPlugin(\n        self,\n        strategy=strategy,\n        create_comms=create_comms,\n    )\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.identify_task_type","title":"identify_task_type","text":"<pre><code>identify_task_type(\n    y: YLike,\n    *,\n    task_hint: Literal[\n        \"classification\", \"regression\", \"auto\"\n    ] = \"auto\"\n) -&gt; TaskTypeName\n</code></pre> <p>Identify the task type from the target data.</p> Source code in <code>src/amltk/sklearn/evaluation.py</code> <pre><code>def identify_task_type(  # noqa: PLR0911\n    y: YLike,\n    *,\n    task_hint: Literal[\"classification\", \"regression\", \"auto\"] = \"auto\",\n) -&gt; TaskTypeName:\n    \"\"\"Identify the task type from the target data.\"\"\"\n    inferred_type: TaskTypeName = type_of_target(y)\n    if task_hint == \"auto\":\n        warnings.warn(\n            f\"`{task_hint=}` was not provided. The task type was inferred from\"\n            f\" the target data to be '{inferred_type}'.\"\n            \" To silence this warning, please provide `task_hint`.\",\n            AutomaticTaskTypeInferredWarning,\n            stacklevel=2,\n        )\n        return inferred_type\n\n    match task_hint, inferred_type:\n        # First two cases are everything is fine\n        case (\n            \"classification\",\n            \"binary\"\n            | \"multiclass\"\n            | \"multilabel-indicator\"\n            | \"multiclass-multioutput\",\n        ):\n            return inferred_type\n        case (\"regression\", \"continuous\" | \"continuous-multioutput\"):\n            return inferred_type\n        # Hinted to be regression but we got a single column classification task\n        case (\"regression\", \"binary\" | \"multiclass\"):\n            warnings.warn(\n                f\"`{task_hint=}` but `{inferred_type=}`.\"\n                \" Set to `continuous` as there is only one target column.\",\n                MismatchedTaskTypeWarning,\n                stacklevel=2,\n            )\n            return \"continuous\"\n        # Hinted to be regression but we got multi-column classification task\n        case (\"regression\", \"multilabel-indicator\" | \"multiclass-multioutput\"):\n            warnings.warn(\n                f\"`{task_hint=}` but `{inferred_type=}`.\"\n                \" Set to `continuous-multiouput` as there are more than 1 target\"\n                \" columns.\",\n                MismatchedTaskTypeWarning,\n                stacklevel=2,\n            )\n            return \"continuous\"\n        # Hinted to be classification but we got a single column regression task\n        case (\"classification\", \"continuous\"):\n            match len(np.unique(y)):\n                case 1:\n                    raise ValueError(\n                        \"The target data has only one unique value. This is\"\n                        f\" not a valid classification task.\\n{y=}\",\n                    )\n                case 2:\n                    warnings.warn(\n                        f\"`{task_hint=}` but `{inferred_type=}`.\"\n                        \" Set to `binary` as only 2 unique values.\"\n                        \" To silence this, provide a specific task type to\"\n                        f\"`task_hint=` from {_valid_task_types}.\",\n                        MismatchedTaskTypeWarning,\n                        stacklevel=2,\n                    )\n                    return \"binary\"\n                case _:\n                    warnings.warn(\n                        f\"`{task_hint=}` but `{inferred_type=}`.\"\n                        \" Set to `multiclass` as &gt;2 unique values.\"\n                        \" To silence this, provide a specific task type to\"\n                        f\"`task_hint=` from {_valid_task_types}.\",\n                        MismatchedTaskTypeWarning,\n                        stacklevel=2,\n                    )\n                    return \"multiclass\"\n        # Hinted to be classification but we got multi-column regression task\n        case (\"classification\", \"continuous-multioutput\"):\n            # NOTE: this is a matrix wide .unique, I'm not sure how things\n            # work with multiclass-multioutput and whether it should be\n            # done by 2 unique per column\n            uniques_per_col = [np.unique(col) for col in y.T]\n            binary_columns = all(len(col) &lt;= 2 for col in uniques_per_col)  # noqa: PLR2004\n            if binary_columns:\n                warnings.warn(\n                    f\"`{task_hint=}` but `{inferred_type=}`.\"\n                    \" Set to `multilabel-indicator` as &lt;=2 unique values per column.\"\n                    \" To silence this, provide a specific task type to\"\n                    f\"`task_hint=` from {_valid_task_types}.\",\n                    MismatchedTaskTypeWarning,\n                    stacklevel=2,\n                )\n                return \"multilabel-indicator\"\n            else:  # noqa: RET505\n                warnings.warn(\n                    f\"`{task_hint=}` but `{inferred_type=}`.\"\n                    \" Set to `multiclass-multioutput` as at least one column has\"\n                    \" &gt;2 unique values.\"\n                    \" To silence this, provide a specific task type to\"\n                    f\"`task_hint=` from {_valid_task_types}.\",\n                    MismatchedTaskTypeWarning,\n                    stacklevel=2,\n                )\n                return \"multiclass-multioutput\"\n        case _:\n            raise RuntimeError(\n                f\"Unreachable, please report this bug. {task_hint=}, {inferred_type=}\",\n            )\n</code></pre>"},{"location":"api/amltk/sklearn/evaluation/#amltk.sklearn.evaluation.resample_if_minority_class_too_few_for_n_splits","title":"resample_if_minority_class_too_few_for_n_splits","text":"<pre><code>resample_if_minority_class_too_few_for_n_splits(\n    X_train: DataFrame,\n    y_train: Series,\n    *,\n    n_splits: int,\n    seed: Seed | None = None,\n    _warning_if_occurs: str | None = None\n) -&gt; tuple[DataFrame, DataFrame | Series]\n</code></pre> <p>Rebalance the training data to allow stratification.</p> <p>If your data only contains something such as 3 labels for a single class, and you wish to perform 5 fold cross-validation, you will need to rebalance the data to allow for stratification. This function will take the training data and labels and and resample the data to allow for stratification.</p> PARAMETER DESCRIPTION <code>X_train</code> <p>The training data.</p> <p> TYPE: <code>DataFrame</code> </p> <code>y_train</code> <p>The training labels.</p> <p> TYPE: <code>Series</code> </p> <code>n_splits</code> <p>The number of splits to perform.</p> <p> TYPE: <code>int</code> </p> <code>seed</code> <p>Used for deciding which instances to resample.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[DataFrame, DataFrame | Series]</code> <p>The rebalanced training data and labels.</p> Source code in <code>src/amltk/sklearn/evaluation.py</code> <pre><code>def resample_if_minority_class_too_few_for_n_splits(\n    X_train: pd.DataFrame,  # noqa: N803\n    y_train: pd.Series,\n    *,\n    n_splits: int,\n    seed: Seed | None = None,\n    _warning_if_occurs: str | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame | pd.Series]:\n    \"\"\"Rebalance the training data to allow stratification.\n\n    If your data only contains something such as 3 labels for a single class, and you\n    wish to perform 5 fold cross-validation, you will need to rebalance the data to\n    allow for stratification. This function will take the training data and labels and\n    and resample the data to allow for stratification.\n\n    Args:\n        X_train: The training data.\n        y_train: The training labels.\n        n_splits: The number of splits to perform.\n        seed: Used for deciding which instances to resample.\n\n    Returns:\n        The rebalanced training data and labels.\n    \"\"\"\n    if y_train.ndim != 1:\n        raise NotImplementedError(\n            \"Rebalancing for multi-output classification is not yet supported.\",\n        )\n\n    # If we are in binary/multilclass setting and there is not enough instances\n    # with a given label to perform stratified sampling with `n_splits`, we first\n    # find these labels, take the first N instances which have these labels and allows\n    # us to reach `n_splits` instances for each label.\n    indices_to_resample = None\n    label_counts = y_train.value_counts()\n    under_represented_labels = label_counts[label_counts &lt; n_splits]  # type: ignore\n\n    collected_indices = []\n    if any(under_represented_labels):\n        if _warning_if_occurs is not None:\n            warnings.warn(_warning_if_occurs, UserWarning, stacklevel=2)\n        under_rep_instances = y_train[y_train.isin(under_represented_labels.index)]  # type: ignore\n\n        grouped_by_label = under_rep_instances.to_frame(\"label\").groupby(  # type: ignore\n            \"label\",\n            observed=True,  # Handles categoricals\n        )\n        for _label, instances_with_label in grouped_by_label:\n            n_to_take = n_splits - len(instances_with_label)\n\n            need_to_sample_repeatedly = n_to_take &gt; len(instances_with_label)\n            resampled_instances = instances_with_label.sample(\n                n=n_to_take,\n                random_state=seed,  # type: ignore\n                # It could be that we have to repeat sample if there are not enough\n                # instances to hit `n_splits` for a given label.\n                replace=need_to_sample_repeatedly,\n            )\n            collected_indices.append(np.asarray(resampled_instances.index))\n\n        indices_to_resample = np.concatenate(collected_indices)\n\n    if indices_to_resample is not None:\n        # Give the new samples a new index to not overlap with the original data.\n        new_start_idx = X_train.index.max() + 1  # type: ignore\n        new_end_idx = new_start_idx + len(indices_to_resample)\n        new_idx = pd.RangeIndex(start=new_start_idx, stop=new_end_idx)\n        resampled_X = X_train.loc[indices_to_resample].set_index(new_idx)\n        resampled_y = y_train.loc[indices_to_resample].set_axis(new_idx)\n        X_train = pd.concat([X_train, resampled_X])\n        y_train = pd.concat([y_train, resampled_y])\n\n    return X_train, y_train\n</code></pre>"},{"location":"api/amltk/sklearn/voting/","title":"Voting","text":""},{"location":"api/amltk/sklearn/voting/#amltk.sklearn.voting","title":"amltk.sklearn.voting","text":"<p>Utilities for voting ensembles.</p>"},{"location":"api/amltk/sklearn/voting/#amltk.sklearn.voting.voting_with_preffited_estimators","title":"voting_with_preffited_estimators","text":"<pre><code>voting_with_preffited_estimators(\n    estimators: Iterable[BaseEstimator],\n    weights: Iterable[float] | None = None,\n    *,\n    voter: type[_Voter] | None = None,\n    **voting_kwargs: Any\n) -&gt; _Voter\n</code></pre> <p>Create a voting ensemble with pre-fitted estimators.</p> PARAMETER DESCRIPTION <code>estimators</code> <p>The estimators to use in the ensemble.</p> <p> TYPE: <code>Iterable[BaseEstimator]</code> </p> <code>weights</code> <p>The weights to use for the estimators. If None, will use uniform weights.</p> <p> TYPE: <code>Iterable[float] | None</code> DEFAULT: <code>None</code> </p> <code>voter</code> <p>The voting classifier or regressor to use. If None, will use the appropriate one based on the type of the first estimator.</p> <p> TYPE: <code>type[_Voter] | None</code> DEFAULT: <code>None</code> </p> <code>**voting_kwargs</code> <p>Additional arguments to pass to the voting classifier or regressor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>_Voter</code> <p>The voting classifier or regressor with the pre-fitted estimators.</p> Source code in <code>src/amltk/sklearn/voting.py</code> <pre><code>def voting_with_preffited_estimators(\n    estimators: Iterable[BaseEstimator],\n    weights: Iterable[float] | None = None,\n    *,\n    voter: type[_Voter] | None = None,\n    **voting_kwargs: Any,\n) -&gt; _Voter:\n    \"\"\"Create a voting ensemble with pre-fitted estimators.\n\n    Args:\n        estimators: The estimators to use in the ensemble.\n        weights: The weights to use for the estimators. If None,\n            will use uniform weights.\n        voter: The voting classifier or regressor to use.\n            If None, will use the appropriate one based on the type of the first\n            estimator.\n        **voting_kwargs: Additional arguments to pass to the voting classifier or\n            regressor.\n\n    Returns:\n        The voting classifier or regressor with the pre-fitted estimators.\n    \"\"\"\n    estimators = list(estimators)\n    est0 = estimators[0]\n    is_classification = voter is not None and issubclass(voter, VotingClassifier)\n\n    if voter is None:\n        if isinstance(est0, ClassifierMixin):\n            voter_cls = VotingClassifier\n            is_classification = True\n        elif isinstance(est0, ClassifierMixin):\n            voter_cls = VotingRegressor\n            is_classification = False\n        else:\n            raise ValueError(\n                f\"Could not infer voter type from estimator type: {type(est0)}.\"\n                \" Please specify the voter type explicitly.\",\n            )\n    else:\n        voter_cls = voter\n\n    if weights is None:\n        weights = np.ones(len(estimators)) / len(estimators)\n    else:\n        weights = list(weights)\n\n    named_estimators = [(str(i), e) for i, e in enumerate(estimators)]\n    _voter = voter_cls(named_estimators, weights=weights, **voting_kwargs)\n    _voter.estimators_ = [model for _, model in _voter.estimators]  # type: ignore\n\n    if is_classification:\n        est0_classes_ = est0.classes_  # type: ignore\n        _voter.classes_ = est0_classes_  # type: ignore\n        if np.ndim(est0_classes_) &gt; 1:\n            est0_classes_ = est0_classes_[0]\n            _voter.le_ = MultiLabelBinarizer().fit(est0_classes_)  # type: ignore\n        else:\n            _voter.le_ = LabelEncoder().fit(est0.classes_)  # type: ignore\n\n    _voter.named_estimators_ = Bunch()  # type: ignore\n\n    # Taken from Sklearn _BaseVoting.fit\n    # Uses 'drop' as placeholder for dropped estimators\n    est_iter = iter(_voter.estimators_)  # type: ignore\n    for name, est in _voter.estimators:  # type: ignore\n        current_est = est if est == \"drop\" else next(est_iter)\n        _voter.named_estimators_[name] = current_est  # type: ignore\n\n        if hasattr(current_est, \"feature_names_in_\"):\n            _voter.feature_names_in_ = current_est.feature_names_in_  # type: ignore\n\n    return _voter  # type: ignore\n</code></pre>"},{"location":"api/amltk/store/bucket/","title":"Bucket","text":""},{"location":"api/amltk/store/bucket/#amltk.store.bucket","title":"amltk.store.bucket","text":"<p>Module containing the base definition of a bucket.</p> <p>A bucket is a collection of resources that can be accessed by a key of a given type. This lets you easily store and retrieve objects of varying types in a single location.</p> Concrete examples <ul> <li><code>PathBucket</code>.</li> </ul>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket","title":"Bucket","text":"<p>               Bases: <code>ABC</code>, <code>MutableMapping[KeyT, Drop[LinkT]]</code>, <code>Generic[KeyT, LinkT]</code></p> <p>Definition of a bucket of resources, accessed by a Key.</p> <p>Indexing into a bucket returns a <code>Drop</code> that can be used to access the resource.</p> <p>The definition mostly follow that of MutableMapping, but with the change of <code>.keys()</code> and <code>.values()</code> to return iterators and <code>.items()</code> to return an iterator of tuples. The other change is that the <code>.values()</code> do not return the resources themselves, by rather a <code>Drop</code> which wraps the resource.</p>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__contains__","title":"__contains__","text":"<pre><code>__contains__(key: object) -&gt; bool\n</code></pre> <p>Check if a key is in the bucket.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to check for.</p> <p> TYPE: <code>object</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef __contains__(self, key: object) -&gt; bool:\n    \"\"\"Check if a key is in the bucket.\n\n    Args:\n        key: The key to check for.\n    \"\"\"\n    return key in self\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__delitem__","title":"__delitem__  <code>abstractmethod</code>","text":"<pre><code>__delitem__(key: KeyT) -&gt; None\n</code></pre> <p>Remove a resource from the bucket.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __delitem__(self, key: KeyT) -&gt; None:\n    \"\"\"Remove a resource from the bucket.\n\n    Args:\n        key: The key to the resource.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(key: KeyT) -&gt; Drop[LinkT]\n</code></pre> <p>Get a drop for a resource in the bucket.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __getitem__(self, key: KeyT) -&gt; Drop[LinkT]:\n    \"\"\"Get a drop for a resource in the bucket.\n\n    Args:\n        key: The key to the resource.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__iter__","title":"__iter__  <code>abstractmethod</code>","text":"<pre><code>__iter__() -&gt; Iterator[KeyT]\n</code></pre> <p>Iterate over the keys in the bucket.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __iter__(self) -&gt; Iterator[KeyT]:\n    \"\"\"Iterate over the keys in the bucket.\"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get the number of keys in the bucket.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef __len__(self) -&gt; int:\n    \"\"\"Get the number of keys in the bucket.\"\"\"\n    return ilen(iter(self))\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__setitem__","title":"__setitem__  <code>abstractmethod</code>","text":"<pre><code>__setitem__(key: KeyT, value: Any) -&gt; None\n</code></pre> <p>Store a value in the bucket.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> <code>value</code> <p>The value to store in the bucket.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __setitem__(self, key: KeyT, value: Any) -&gt; None:\n    \"\"\"Store a value in the bucket.\n\n    Args:\n        key: The key to the resource.\n        value: The value to store in the bucket.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.fetch","title":"fetch","text":"<pre><code>fetch(\n    *keys: KeyT,\n    default: None | Any | dict[KeyT, Any] = None\n) -&gt; dict[KeyT, Any]\n</code></pre> <p>Fetch a resource from the bucket.</p> PARAMETER DESCRIPTION <code>keys</code> <p>The keys to the resources.</p> <p> TYPE: <code>KeyT</code> DEFAULT: <code>()</code> </p> <code>default</code> <p>The default value to return if the key is not in the bucket. If a dict is passed, the default for each key will be the value in the dict for that key, using None if not present.</p> <p> TYPE: <code>None | Any | dict[KeyT, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[KeyT, Any]</code> <p>The resources stored in the bucket at the given keys.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def fetch(\n    self,\n    *keys: KeyT,\n    default: None | Any | dict[KeyT, Any] = None,\n) -&gt; dict[KeyT, Any]:\n    \"\"\"Fetch a resource from the bucket.\n\n    Args:\n        keys: The keys to the resources.\n        default: The default value to return if the key is not in the bucket.\n            If a dict is passed, the default for each key will be the value\n            in the dict for that key, using None if not present.\n\n    Returns:\n        The resources stored in the bucket at the given keys.\n    \"\"\"\n    default_dict = {} if not isinstance(default, dict) else default\n    return {\n        key: self[key].get(default=default_dict.get(key, default)) for key in keys\n    }\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.find","title":"find","text":"<pre><code>find(\n    pattern: str, *, multi_key: bool = False\n) -&gt; (\n    dict[str, Drop[LinkT]]\n    | dict[tuple[str, ...], Drop[LinkT]]\n    | None\n)\n</code></pre> <p>Find resources in the bucket.</p> <pre><code>found = bucket.find(r\"trial_(.+)_val_predictions.npy\")  # (1)!\nif found is None:\n    raise KeyError(\"No predictions found\")\n\nfor name, drop in found.items():\n    predictions = drop.get()\n    # Do something with the predictions\n    # ...\n</code></pre> <ol> <li>The <code>(.+)</code> is a capture group which will attempt to match anything <code>.</code>,     when there is one or more occurences <code>+</code>, and put it in a capure group <code>()</code>.     What is captured will be used as the key in the returned dict.</li> </ol> PARAMETER DESCRIPTION <code>pattern</code> <p>The pattern to search for.</p> <p> TYPE: <code>str</code> </p> <code>multi_key</code> <p>Whether you have multiple capture groups in the pattern.</p> <p>Multiple capture groups with <code>()</code></p> <p>If using multiple capture groups, the returned dict will have tuples as keys. If there is only one capture group, the tuple will be expanded to a single value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict[str, Drop[LinkT]] | dict[tuple[str, ...], Drop[LinkT]] | None</code> <p>A mapping of links to drops for the resources found.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def find(\n    self,\n    pattern: str,\n    *,\n    multi_key: bool = False,\n) -&gt; dict[str, Drop[LinkT]] | dict[tuple[str, ...], Drop[LinkT]] | None:\n    \"\"\"Find resources in the bucket.\n\n    ```python\n    found = bucket.find(r\"trial_(.+)_val_predictions.npy\")  # (1)!\n    if found is None:\n        raise KeyError(\"No predictions found\")\n\n    for name, drop in found.items():\n        predictions = drop.get()\n        # Do something with the predictions\n        # ...\n    ```\n\n    1. The `(.+)` is a **capture group** which will attempt to match anything `.`,\n        when there is one or more occurences `+`, and put it in a capure group `()`.\n        What is captured will be used as the key in the returned dict.\n\n    Args:\n        pattern: The pattern to search for.\n        multi_key: Whether you have multiple capture groups in the pattern.\n\n            !!! note \"Multiple capture groups with `()`\"\n\n                If using multiple capture groups, the returned dict will have\n                tuples as keys. If there is only one capture group, the tuple\n                will be expanded to a single value.\n\n    Returns:\n        A mapping of links to drops for the resources found.\n    \"\"\"\n    keys = [(key, match) for key in self if (match := re.search(pattern, str(key)))]\n    if not keys:\n        return None\n\n    matches = {match.groups(): self[key] for key, match in keys}\n\n    # If it's a tuple of length 1, we expand it\n    one_group = len(next(iter(matches.keys()))) == 1\n    if one_group:\n        if multi_key:\n            raise ValueError(\n                \"Use multi_key=True when the pattern has more than 1 capture group\",\n            )\n\n        return {key[0]: drop for key, drop in matches.items()}\n\n    # Here we have multi-groups =&gt; tuple keys\n    if not multi_key:\n        raise ValueError(\n            \"Use multi_key=False when the pattern has only 1 capture group\",\n        )\n\n    return matches\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.remove","title":"remove","text":"<pre><code>remove(keys: Iterable[KeyT]) -&gt; dict[KeyT, bool]\n</code></pre> <p>Remove resources from the bucket.</p> PARAMETER DESCRIPTION <code>keys</code> <p>The keys to the resources.</p> <p> TYPE: <code>Iterable[KeyT]</code> </p> RETURNS DESCRIPTION <code>dict[KeyT, bool]</code> <p>A mapping of keys to whether they were removed.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def remove(self, keys: Iterable[KeyT]) -&gt; dict[KeyT, bool]:\n    \"\"\"Remove resources from the bucket.\n\n    Args:\n        keys: The keys to the resources.\n\n    Returns:\n        A mapping of keys to whether they were removed.\n    \"\"\"\n    return {key: self[key].remove() for key in keys}\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.store","title":"store","text":"<pre><code>store(other: Mapping[KeyT, Any]) -&gt; None\n</code></pre> <p>Store items into the bucket with the given mapping.</p> PARAMETER DESCRIPTION <code>other</code> <p>The mapping of items to store in the bucket.</p> <p> TYPE: <code>Mapping[KeyT, Any]</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def store(self, other: Mapping[KeyT, Any]) -&gt; None:\n    \"\"\"Store items into the bucket with the given mapping.\n\n    Args:\n        other: The mapping of items to store in the bucket.\n    \"\"\"\n    for key, value in other.items():\n        self[key] = value\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.sub","title":"sub  <code>abstractmethod</code>","text":"<pre><code>sub(key: KeyT) -&gt; Self\n</code></pre> <p>Create a subbucket of this bucket.</p> PARAMETER DESCRIPTION <code>key</code> <p>The name of the sub bucket.</p> <p> TYPE: <code>KeyT</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new bucket with the same loaders as the current bucket.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@abstractmethod\ndef sub(self, key: KeyT) -&gt; Self:\n    \"\"\"Create a subbucket of this bucket.\n\n    Args:\n        key: The name of the sub bucket.\n\n    Returns:\n        A new bucket with the same loaders as the current bucket.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.update","title":"update","text":"<pre><code>update(items: Mapping[KeyT, Any]) -&gt; None\n</code></pre> <p>Update the bucket with the given mapping.</p> PARAMETER DESCRIPTION <code>items</code> <p>The mapping of items to store in the bucket.</p> <p> TYPE: <code>Mapping[KeyT, Any]</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef update(self, items: Mapping[KeyT, Any]) -&gt; None:  # type: ignore\n    \"\"\"Update the bucket with the given mapping.\n\n    Args:\n        items: The mapping of items to store in the bucket.\n    \"\"\"\n    for key, value in items.items():\n        self[key].put(value)\n</code></pre>"},{"location":"api/amltk/store/drop/","title":"Drop","text":""},{"location":"api/amltk/store/drop/#amltk.store.drop","title":"amltk.store.drop","text":"<p>A <code>Drop</code> in a <code>Bucket</code> is a reference to a resource.</p>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop","title":"Drop  <code>dataclass</code>","text":"<pre><code>Drop(\n    key: KeyT,\n    loaders: tuple[type[Loader[KeyT, Any]], ...],\n    _remove: Callable[[KeyT], bool],\n    _exists: Callable[[KeyT], bool],\n)\n</code></pre> <p>               Bases: <code>Generic[KeyT]</code></p> <p>A drop is a reference to a resource in a bucket.</p> <p>You likely do not need to create these yourself and are a class used by <code>Bucket</code> to wrap access to a resource located at a give <code>key</code>.</p> <p>The main use of this class is to attempt to use different <code>loaders</code> to load a resource at a given <code>key</code>, using the <code>key</code> to try infer which loader to use. Each drop has a list of default loaders that it will try to use to load the resource.</p> <p>To support well typed code, you can also specify a <code>check</code> type which will be used to checked when loading objects, to make sure it is of the correct type.</p> <p>The primary methods of interest are * <code>load</code> * <code>get</code> * <code>put</code> * <code>remove</code> * <code>exists</code> * <code>as_stored</code></p> PARAMETER DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> <code>loaders</code> <p>The loaders to use to load the resource.</p> <p> TYPE: <code>tuple[type[Loader[KeyT, Any]], ...]</code> </p>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.as_stored","title":"as_stored","text":"<pre><code>as_stored(\n    read: Callable[[KeyT], T] | None = None\n) -&gt; Stored[T]\n</code></pre> <p>Convert the drop to a <code>Stored</code>.</p> PARAMETER DESCRIPTION <code>read</code> <p>The method to use to load the resource. If <code>None</code> then the first loader that can load the resource will be used.</p> <p> TYPE: <code>Callable[[KeyT], T] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Stored[T]</code> <p>The drop as a <code>Stored</code>.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def as_stored(self, read: Callable[[KeyT], T] | None = None) -&gt; Stored[T]:\n    \"\"\"Convert the drop to a [`Stored`][amltk.store.Stored].\n\n    Args:\n        read: The method to use to load the resource. If `None` then\n            the first loader that can load the resource will be used.\n\n    Returns:\n        The drop as a [`Stored`][amltk.store.Stored].\n    \"\"\"\n    if read is None:\n        loader = first(\n            (_l for _l in self.loaders if _l.can_load(self.key)),\n            default=None,\n        )\n\n        if loader is None:\n            raise ValueError(f\"Can't load {self.key=} from {self.loaders=}\")\n\n        read = loader.load\n\n    return Stored(self.key, read=read)\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.exists","title":"exists","text":"<pre><code>exists() -&gt; bool\n</code></pre> <p>Check if the resource exists.</p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the resource exists, <code>False</code> otherwise.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check if the resource exists.\n\n    Returns:\n        `True` if the resource exists, `False` otherwise.\n    \"\"\"\n    return self._exists(self.key)\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.get","title":"get","text":"<pre><code>get(\n    default: Default | None = None,\n    *,\n    check: type[T] | None = None\n) -&gt; Default | T | None\n</code></pre> <p>Load the resource, or return the default if it can't be loaded.</p> <p>See <code>load</code> for more details.</p> Note <p>This function makes no distinction for the reason it fails to load, namely if it's of the incorrect type or the resource does not exist at the key.</p> PARAMETER DESCRIPTION <code>default</code> <p>The default value to return if the resource can't be loaded.</p> <p> TYPE: <code>Default | None</code> DEFAULT: <code>None</code> </p> <code>check</code> <p>By specifying a <code>type</code> we check the loaded object of that type, to enable correctly typed checked code. If the default value should be returned because the resource can't be loaded, then the default value is not checked.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Default | T | None</code> <p>The loaded resource or the default value if it cant be loaded.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def get(\n    self,\n    default: Default | None = None,\n    *,\n    check: type[T] | None = None,\n) -&gt; Default | T | None:\n    \"\"\"Load the resource, or return the default if it can't be loaded.\n\n    See [`load`][amltk.store.drop.Drop.load] for more details.\n\n    Note:\n        This function makes no distinction for the reason it fails to load,\n        namely if it's of the incorrect type or the resource does not exist\n        at the key.\n\n    Args:\n        default: The default value to return if the resource can't be loaded.\n        check: By specifying a `type` we check the loaded object of that type, to\n            enable correctly typed checked code. If the default value should\n            be returned because the resource can't be loaded, then the default\n            value is **not** checked.\n\n    Returns:\n        The loaded resource or the default value if it cant be loaded.\n    \"\"\"\n    try:\n        return self.load(check=check)\n    except TypeError as e:\n        raise e\n    except FileNotFoundError:\n        return default\n    except Exception as e:  # noqa: BLE001\n        logger.warning(\n            f\"Failed to load {self.key=} from {self.loaders=}: {e}\",\n            exc_info=True,\n        )\n\n    return None\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.load","title":"load","text":"<pre><code>load(*, check: type[T] | None = None) -&gt; T | Any\n</code></pre> <p>Load the resource.</p> PARAMETER DESCRIPTION <code>check</code> <p>By specifying a <code>type</code> we check the loaded object of that type, to enable correctly typed checked code.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>T | Any</code> <p>The loaded resource.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def load(self, *, check: type[T] | None = None) -&gt; T | Any:\n    \"\"\"Load the resource.\n\n    Args:\n        check: By specifying a `type` we check the loaded object of that type, to\n            enable correctly typed checked code.\n\n    Returns:\n        The loaded resource.\n    \"\"\"\n    loader = first(\n        (_l for _l in self.loaders if _l.can_load(self.key)),\n        default=None,\n    )\n    if loader is None:\n        raise ValueError(f\"Can't load {self.key=} from {self.loaders=}\")\n\n    value = loader.load(self.key)\n    loader_name = loader.name\n\n    if check is not None and not isinstance(value, check):\n        msg = (\n            f\"Value {value=} loaded by {loader_name=} is not of type {check=},\"\n            f\" but is of type {type(value)=}.\"\n        )\n        raise TypeError(msg)\n\n    return value\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.put","title":"put","text":"<pre><code>put(obj: T) -&gt; Stored[T]\n</code></pre> <p>Put an object into the bucket.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to put into the bucket.</p> <p> TYPE: <code>T</code> </p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def put(self, obj: T) -&gt; Stored[T]:\n    \"\"\"Put an object into the bucket.\n\n    Args:\n        obj: The object to put into the bucket.\n    \"\"\"\n    loader = first(\n        (_l for _l in self.loaders if _l.can_save(obj, self.key)),\n        default=None,\n    )\n    if not loader:\n        msg = (\n            f\"No default way to handle {type(obj)=} objects.\"\n            \" Please provide a `Loader` with your `Bucket` to specify\"\n            f\" how to handle this type of object with this extension: {self.key}.\"\n        )\n        raise ValueError(msg)\n\n    return loader.save(obj, self.key)\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.remove","title":"remove","text":"<pre><code>remove() -&gt; bool\n</code></pre> <p>Remove the resource from the bucket.</p> <p>Non-existent resources</p> <p>If the resource does not exist, then the function will return <code>True</code>.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def remove(self) -&gt; bool:\n    \"\"\"Remove the resource from the bucket.\n\n    !!! note \"Non-existent resources\"\n\n        If the resource does not exist, then the function will return `True`.\n    \"\"\"\n    return self._remove(self.key)\n</code></pre>"},{"location":"api/amltk/store/loader/","title":"Loader","text":""},{"location":"api/amltk/store/loader/#amltk.store.loader","title":"amltk.store.loader","text":"<p>Module containing the base protocol of a loader.</p> <p>For concrete implementations based on the <code>key</code> being a <code>Path</code> see the <code>path_loaders</code> module.</p>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader","title":"Loader","text":"<p>               Bases: <code>ABC</code>, <code>Generic[KeyT_contra, T]</code></p> <p>The base definition of a Loader.</p> <p>A Loader is a class that can save and load objects to and from a bucket. The Loader is responsible for knowing how to save and load objects of a particular type at a given key.</p>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.name","title":"name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.can_load","title":"can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: KeyT_contra, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>KeyT_contra</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_load(cls, key: KeyT_contra, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.can_save","title":"can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: KeyT_contra) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>KeyT_contra</code> </p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: KeyT_contra, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.load","title":"load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: KeyT_contra) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>KeyT_contra</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef load(cls, key: KeyT_contra, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.save","title":"save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: Any, key: KeyT_contra) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>KeyT_contra</code> </p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: KeyT_contra, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/stored/","title":"Stored","text":""},{"location":"api/amltk/store/stored/#amltk.store.stored","title":"amltk.store.stored","text":"<p>A value that is stored on disk and loaded lazily.</p> <p>This is useful for transmitting large objects between processes.</p> Stored<pre><code>from amltk.store import Stored\nimport pandas as pd\nfrom pathlib import Path\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\npath = Path(\"df.csv\")\n\ndf.to_csv(path)\nstored_df = Stored(path, read=pd.read_csv)\n\n# Somewhere in a processes\ndf = stored_df.load()\nprint(df)\n</code></pre> <pre><code>   Unnamed: 0  a  b\n0           0  1  4\n1           1  2  5\n2           2  3  6\n</code></pre> <p>You can quickly obtain these from buckets if you require using <code>put()</code>.</p> Stored from bucket<pre><code>from amltk import PathBucket\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nbucket = PathBucket(\"bucket_path\")\n\nstored_df = bucket[\"df.csv\"].put(df)\n\n# Somewhere in a processes\ndf = stored_df.load()\nprint(df)\n</code></pre> <pre><code>       a  b\nindex      \n0      1  4\n1      2  5\n2      3  6\n</code></pre>"},{"location":"api/amltk/store/stored/#amltk.store.stored.Stored","title":"Stored","text":"<pre><code>Stored(key: K, read: Callable[[K], V])\n</code></pre> <p>               Bases: <code>Generic[V]</code></p> <p>A value that is stored on disk and can be loaded when needed.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the value from.</p> <p> TYPE: <code>K</code> </p> <code>read</code> <p>A function that takes a key and returns the value.</p> <p> TYPE: <code>Callable[[K], V]</code> </p> Source code in <code>src/amltk/store/stored.py</code> <pre><code>def __init__(self, key: K, read: Callable[[K], V]):\n    \"\"\"Initialize the stored value.\n\n    Args:\n        key: The key to load the value from.\n        read: A function that takes a key and returns the value.\n    \"\"\"\n    super().__init__()\n    self.key = key\n    self.read = read\n</code></pre>"},{"location":"api/amltk/store/stored/#amltk.store.stored.Stored.load","title":"load","text":"<pre><code>load() -&gt; V\n</code></pre> <p>Get the value.</p> Source code in <code>src/amltk/store/stored.py</code> <pre><code>def load(self) -&gt; V:\n    \"\"\"Get the value.\"\"\"\n    return self.read(self.key)\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/","title":"Path bucket","text":""},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket","title":"amltk.store.paths.path_bucket","text":"<p>A module containing a concreate implementation of a <code>Bucket</code> that uses the Path API to store objects.</p>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket","title":"PathBucket","text":"<pre><code>PathBucket(\n    path: PathBucket | Path | str,\n    *,\n    loaders: Sequence[type[PathLoader]] | None = None,\n    create: bool = True,\n    clean: bool = False,\n    exists_ok: bool = True\n)\n</code></pre> <p>               Bases: <code>Bucket[str, Path]</code></p> <p>A bucket that uses the Path API to store objects.</p> <p>This bucket is a key-value lookup backed up by some filesystem. By assinging to the bucket, you store the object to the filesystem. However the values you get back are instead a <code>Drop</code> that can be used to perform operations on the stores object, such as <code>load</code>, <code>get</code> and <code>remove</code>.</p> Drop methods <ul> <li><code>Drop.load</code> - Load the object from the bucket.</li> <li><code>Drop.get</code> - Load the object from the bucket     with a default if something fails.</li> <li><code>Drop.put</code> - Store an object in the bucket.</li> <li><code>Drop.remove</code> - Remove the object from the     bucket.</li> <li><code>Drop.exists</code> - Check if the object exists     in the bucket.</li> </ul> <pre><code>from amltk.store.paths import PathBucket\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nbucket = PathBucket(\"path/to/bucket\")\n\narray = np.array([1, 2, 3])\ndataframe = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nmodel = LinearRegression()\n\n# Store things\nbucket[\"myarray.npy\"] = array # (1)!\nbucket[\"df.csv\"] = dataframe  # (2)!\nbucket[\"model.pkl\"].put(model)\n\nbucket[\"config.json\"] = {\"hello\": \"world\"}\nassert bucket[\"config.json\"].exists()\nbucket[\"config.json\"].remove()\n\n# Load things\narray = bucket[\"myarray.npy\"].load()\nmaybe_df = bucket[\"df.csv\"].get()  # (3)!\nmodel: LinearRegression = bucket[\"model.pkl\"].get(check=LinearRegression)  # (4)!\n\n# Create subdirectories\nmodel_bucket = bucket / \"my_model\" # (5)!\nmodel_bucket[\"model.pkl\"] = model\nmodel_bucket[\"predictions.npy\"] = model.predict(X)\n\n# Acts like a mapping\nassert \"myarray.npy\" in bucket\nassert len(bucket) == 3\nfor key, item in bucket.items():\n    print(key, item.load())\ndel bucket[\"model.pkl\"]\n</code></pre> <ol> <li>The <code>=</code> is a shortcut for <code>bucket[\"myarray.npy\"].put(array)</code></li> <li>The extension is used to determine which     <code>PathLoader</code> to use     and how to save it.</li> <li>The <code>get</code> method acts like the <code>dict.load</code> method.</li> <li>The <code>get</code> method can be used to check the type of the loaded object.     If the type does not match, a <code>TypeError</code> is raised.</li> <li>Uses the familiar <code>Path</code> API to create subdirectories.</li> </ol> PARAMETER DESCRIPTION <code>path</code> <p>The path to the bucket.</p> <p> TYPE: <code>PathBucket | Path | str</code> </p> <code>loaders</code> <p>A sequence of loaders to use when loading objects. These will be prepended to the default loaders and attempted to be used first.</p> <p> TYPE: <code>Sequence[type[PathLoader]] | None</code> DEFAULT: <code>None</code> </p> <code>create</code> <p>If True, the base path will be created if it does not exist.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>clean</code> <p>If True, the base path will be deleted if it exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>exists_ok</code> <p>If False, an error will be raised if the base path already exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def __init__(\n    self,\n    path: PathBucket | Path | str,\n    *,\n    loaders: Sequence[type[PathLoader]] | None = None,\n    create: bool = True,\n    clean: bool = False,\n    exists_ok: bool = True,\n) -&gt; None:\n    \"\"\"Create a new PathBucket.\n\n    Args:\n        path: The path to the bucket.\n        loaders: A sequence of loaders to use when loading objects.\n            These will be prepended to the default loaders and attempted\n            to be used first.\n        create: If True, the base path will be created if it does not\n            exist.\n        clean: If True, the base path will be deleted if it exists.\n        exists_ok: If False, an error will be raised if the base path\n            already exists.\n    \"\"\"\n    super().__init__()\n    _loaders = DEFAULT_LOADERS\n    if loaders is not None:\n        _loaders = tuple(chain(loaders, DEFAULT_LOADERS))\n\n    if isinstance(path, str):\n        path = Path(path)\n    elif isinstance(path, PathBucket):\n        # TODO: Should we inherit the loaders?\n        path = path.path\n\n    if clean and path.exists():\n        shutil.rmtree(path, ignore_errors=True)\n\n    if not exists_ok and path.exists():\n        raise FileExistsError(f\"File/Directory already exists at {path}\")\n\n    if create:\n        path.mkdir(parents=True, exist_ok=True)\n\n    self._create = create\n    self.path: Path = path\n    self.loaders = _loaders\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get the number of keys in the bucket.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef __len__(self) -&gt; int:\n    \"\"\"Get the number of keys in the bucket.\"\"\"\n    return ilen(iter(self))\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.add_loader","title":"add_loader","text":"<pre><code>add_loader(loader: type[PathLoader]) -&gt; None\n</code></pre> <p>Add a loader to the bucket.</p> PARAMETER DESCRIPTION <code>loader</code> <p>The loader to add.</p> <p> TYPE: <code>type[PathLoader]</code> </p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def add_loader(self, loader: type[PathLoader]) -&gt; None:\n    \"\"\"Add a loader to the bucket.\n\n    Args:\n        loader: The loader to add.\n    \"\"\"\n    self.loaders = (loader, *self.loaders)\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.fetch","title":"fetch","text":"<pre><code>fetch(\n    *keys: KeyT,\n    default: None | Any | dict[KeyT, Any] = None\n) -&gt; dict[KeyT, Any]\n</code></pre> <p>Fetch a resource from the bucket.</p> PARAMETER DESCRIPTION <code>keys</code> <p>The keys to the resources.</p> <p> TYPE: <code>KeyT</code> DEFAULT: <code>()</code> </p> <code>default</code> <p>The default value to return if the key is not in the bucket. If a dict is passed, the default for each key will be the value in the dict for that key, using None if not present.</p> <p> TYPE: <code>None | Any | dict[KeyT, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[KeyT, Any]</code> <p>The resources stored in the bucket at the given keys.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def fetch(\n    self,\n    *keys: KeyT,\n    default: None | Any | dict[KeyT, Any] = None,\n) -&gt; dict[KeyT, Any]:\n    \"\"\"Fetch a resource from the bucket.\n\n    Args:\n        keys: The keys to the resources.\n        default: The default value to return if the key is not in the bucket.\n            If a dict is passed, the default for each key will be the value\n            in the dict for that key, using None if not present.\n\n    Returns:\n        The resources stored in the bucket at the given keys.\n    \"\"\"\n    default_dict = {} if not isinstance(default, dict) else default\n    return {\n        key: self[key].get(default=default_dict.get(key, default)) for key in keys\n    }\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.find","title":"find","text":"<pre><code>find(\n    pattern: str, *, multi_key: bool = False\n) -&gt; (\n    dict[str, Drop[LinkT]]\n    | dict[tuple[str, ...], Drop[LinkT]]\n    | None\n)\n</code></pre> <p>Find resources in the bucket.</p> <pre><code>found = bucket.find(r\"trial_(.+)_val_predictions.npy\")  # (1)!\nif found is None:\n    raise KeyError(\"No predictions found\")\n\nfor name, drop in found.items():\n    predictions = drop.get()\n    # Do something with the predictions\n    # ...\n</code></pre> <ol> <li>The <code>(.+)</code> is a capture group which will attempt to match anything <code>.</code>,     when there is one or more occurences <code>+</code>, and put it in a capure group <code>()</code>.     What is captured will be used as the key in the returned dict.</li> </ol> PARAMETER DESCRIPTION <code>pattern</code> <p>The pattern to search for.</p> <p> TYPE: <code>str</code> </p> <code>multi_key</code> <p>Whether you have multiple capture groups in the pattern.</p> <p>Multiple capture groups with <code>()</code></p> <p>If using multiple capture groups, the returned dict will have tuples as keys. If there is only one capture group, the tuple will be expanded to a single value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict[str, Drop[LinkT]] | dict[tuple[str, ...], Drop[LinkT]] | None</code> <p>A mapping of links to drops for the resources found.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def find(\n    self,\n    pattern: str,\n    *,\n    multi_key: bool = False,\n) -&gt; dict[str, Drop[LinkT]] | dict[tuple[str, ...], Drop[LinkT]] | None:\n    \"\"\"Find resources in the bucket.\n\n    ```python\n    found = bucket.find(r\"trial_(.+)_val_predictions.npy\")  # (1)!\n    if found is None:\n        raise KeyError(\"No predictions found\")\n\n    for name, drop in found.items():\n        predictions = drop.get()\n        # Do something with the predictions\n        # ...\n    ```\n\n    1. The `(.+)` is a **capture group** which will attempt to match anything `.`,\n        when there is one or more occurences `+`, and put it in a capure group `()`.\n        What is captured will be used as the key in the returned dict.\n\n    Args:\n        pattern: The pattern to search for.\n        multi_key: Whether you have multiple capture groups in the pattern.\n\n            !!! note \"Multiple capture groups with `()`\"\n\n                If using multiple capture groups, the returned dict will have\n                tuples as keys. If there is only one capture group, the tuple\n                will be expanded to a single value.\n\n    Returns:\n        A mapping of links to drops for the resources found.\n    \"\"\"\n    keys = [(key, match) for key in self if (match := re.search(pattern, str(key)))]\n    if not keys:\n        return None\n\n    matches = {match.groups(): self[key] for key, match in keys}\n\n    # If it's a tuple of length 1, we expand it\n    one_group = len(next(iter(matches.keys()))) == 1\n    if one_group:\n        if multi_key:\n            raise ValueError(\n                \"Use multi_key=True when the pattern has more than 1 capture group\",\n            )\n\n        return {key[0]: drop for key, drop in matches.items()}\n\n    # Here we have multi-groups =&gt; tuple keys\n    if not multi_key:\n        raise ValueError(\n            \"Use multi_key=False when the pattern has only 1 capture group\",\n        )\n\n    return matches\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.remove","title":"remove","text":"<pre><code>remove(keys: Iterable[KeyT]) -&gt; dict[KeyT, bool]\n</code></pre> <p>Remove resources from the bucket.</p> PARAMETER DESCRIPTION <code>keys</code> <p>The keys to the resources.</p> <p> TYPE: <code>Iterable[KeyT]</code> </p> RETURNS DESCRIPTION <code>dict[KeyT, bool]</code> <p>A mapping of keys to whether they were removed.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def remove(self, keys: Iterable[KeyT]) -&gt; dict[KeyT, bool]:\n    \"\"\"Remove resources from the bucket.\n\n    Args:\n        keys: The keys to the resources.\n\n    Returns:\n        A mapping of keys to whether they were removed.\n    \"\"\"\n    return {key: self[key].remove() for key in keys}\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.rmdir","title":"rmdir","text":"<pre><code>rmdir() -&gt; None\n</code></pre> <p>Delete the bucket.</p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def rmdir(self) -&gt; None:\n    \"\"\"Delete the bucket.\"\"\"\n    shutil.rmtree(self.path)\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.sizes","title":"sizes","text":"<pre><code>sizes() -&gt; dict[str, int]\n</code></pre> <p>Get the sizes of all the files in the bucket.</p> <p>Files only</p> <p>This method only returns the sizes of the files in the bucket. It does not include directories, their sizes, or their contents.</p> RETURNS DESCRIPTION <code>dict[str, int]</code> <p>A dictionary mapping the keys to the sizes of the files.</p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def sizes(self) -&gt; dict[str, int]:\n    \"\"\"Get the sizes of all the files in the bucket.\n\n    !!! warning \"Files only\"\n\n        This method only returns the sizes of the files in the bucket.\n        It does not include directories, their sizes, or their contents.\n\n    Returns:\n        A dictionary mapping the keys to the sizes of the files.\n    \"\"\"\n    return {str(path.name): path.stat().st_size for path in self.path.iterdir()}\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.store","title":"store","text":"<pre><code>store(other: Mapping[KeyT, Any]) -&gt; None\n</code></pre> <p>Store items into the bucket with the given mapping.</p> PARAMETER DESCRIPTION <code>other</code> <p>The mapping of items to store in the bucket.</p> <p> TYPE: <code>Mapping[KeyT, Any]</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def store(self, other: Mapping[KeyT, Any]) -&gt; None:\n    \"\"\"Store items into the bucket with the given mapping.\n\n    Args:\n        other: The mapping of items to store in the bucket.\n    \"\"\"\n    for key, value in other.items():\n        self[key] = value\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.sub","title":"sub","text":"<pre><code>sub(key: str, *, create: bool | None = None) -&gt; Self\n</code></pre> <p>Create a subdirectory of the bucket.</p> PARAMETER DESCRIPTION <code>key</code> <p>The name of the subdirectory.</p> <p> TYPE: <code>str</code> </p> <code>create</code> <p>Whether the subdirectory will be created if it does not exist. If None, the default, the value of <code>create</code> passed to the constructor will be used.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new bucket with the same loaders as the current bucket.</p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>@override\ndef sub(self, key: str, *, create: bool | None = None) -&gt; Self:\n    \"\"\"Create a subdirectory of the bucket.\n\n    Args:\n        key: The name of the subdirectory.\n        create: Whether the subdirectory will be created if it does not\n            exist. If None, the default, the value of `create` passed to\n            the constructor will be used.\n\n    Returns:\n        A new bucket with the same loaders as the current bucket.\n    \"\"\"\n    return self.__class__(\n        self.path / key,\n        loaders=self.loaders,\n        create=self._create if create is None else create,\n        clean=False,\n    )\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.update","title":"update","text":"<pre><code>update(items: Mapping[KeyT, Any]) -&gt; None\n</code></pre> <p>Update the bucket with the given mapping.</p> PARAMETER DESCRIPTION <code>items</code> <p>The mapping of items to store in the bucket.</p> <p> TYPE: <code>Mapping[KeyT, Any]</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef update(self, items: Mapping[KeyT, Any]) -&gt; None:  # type: ignore\n    \"\"\"Update the bucket with the given mapping.\n\n    Args:\n        items: The mapping of items to store in the bucket.\n    \"\"\"\n    for key, value in items.items():\n        self[key].put(value)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/","title":"Path loaders","text":""},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders","title":"amltk.store.paths.path_loaders","text":"<p>Loaders for <code>PathBucket</code>s.</p> <p>The <code>Loader</code>s in this module are used to load and save objects identified by a unique <code>Path</code>. For saving objects, these loaders rely on checking the type of the object for <code>can_save</code> and <code>save</code> methods. For loading objects, these loaders rely on checking the file extension of the path for <code>can_load</code> and <code>load</code> methods.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader","title":"ByteLoader","text":"<p>               Bases: <code>PathLoader[bytes]</code></p> <p>A <code>Loader</code> for loading and saving <code>bytes</code> to binary files.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".bin\"</code></li> <li><code>\".bytes\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>bytes</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.can_load","title":"can_load  <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type | None = None\n) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".bin\", \".bytes\") and check in (bytes, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"amltk.store.paths.path_loaders.PathLoader.can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.can_save","title":"can_save  <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, dict | list) and key.suffix in (\".bin\", \".bytes\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"amltk.store.paths.path_loaders.PathLoader.can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; bytes\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; bytes:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"rb\") as f:\n        return f.read()\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"amltk.store.paths.path_loaders.PathLoader.load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.save","title":"save  <code>classmethod</code>","text":"<pre><code>save(obj: bytes, key: Path) -&gt; Stored[bytes]\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: bytes, key: Path, /) -&gt; Stored[bytes]:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"wb\") as f:\n        f.write(obj)\n    return Stored(key, cls.load)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"amltk.store.paths.path_loaders.PathLoader.save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader","title":"JSONLoader","text":"<p>               Bases: <code>PathLoader[_Json]</code></p> <p>A <code>Loader</code> for loading and saving <code>dict</code>s and <code>list</code>s to JSON.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".json\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>dict</code></li> <li><code>list</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'json'\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"amltk.store.paths.path_loaders.PathLoader.name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.can_load","title":"can_load  <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type | None = None\n) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix == \".json\" and check in (dict, list, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"amltk.store.paths.path_loaders.PathLoader.can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.can_save","title":"can_save  <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, dict | list) and key.suffix == \".json\"\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"amltk.store.paths.path_loaders.PathLoader.can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; _Json\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; _Json:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"r\") as f:\n        item = json.load(f)\n\n    if not isinstance(item, dict | list):\n        msg = f\"Expected `dict | list` from {key=} but got `{type(item).__name__}`\"\n        raise TypeError(msg)\n\n    return item  # type: ignore\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"amltk.store.paths.path_loaders.PathLoader.load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.save","title":"save  <code>classmethod</code>","text":"<pre><code>save(obj: _Json, key: Path) -&gt; Stored[_Json]\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: _Json, key: Path, /) -&gt; Stored[_Json]:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"w\") as f:\n        json.dump(obj, f)\n    return Stored(key, cls.load)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"amltk.store.paths.path_loaders.PathLoader.save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader","title":"NPYLoader","text":"<p>               Bases: <code>PathLoader[ndarray]</code></p> <p>A <code>Loader</code> for loading and saving <code>np.ndarray</code>s.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".npy\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>np.ndarray</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'np'\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"amltk.store.paths.path_loaders.PathLoader.name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.can_load","title":"can_load  <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type | None = None\n) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in {\".npy\"} and check in (np.ndarray, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"amltk.store.paths.path_loaders.PathLoader.can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.can_save","title":"can_save  <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, np.ndarray) and key.suffix in {\".npy\"}\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"amltk.store.paths.path_loaders.PathLoader.can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; ndarray\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; np.ndarray:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    item = np.load(key, allow_pickle=False)\n    if not isinstance(item, np.ndarray):\n        msg = f\"Expected `np.ndarray` from {key=} but got `{type(item).__name__}`.\"\n        raise TypeError(msg)\n\n    return item\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"amltk.store.paths.path_loaders.PathLoader.load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.save","title":"save  <code>classmethod</code>","text":"<pre><code>save(obj: ndarray, key: Path) -&gt; Stored[ndarray]\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: np.ndarray, key: Path, /) -&gt; Stored[np.ndarray]:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    np.save(key, obj, allow_pickle=False)\n    return Stored(key, cls.load)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"amltk.store.paths.path_loaders.PathLoader.save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader","title":"PDLoader","text":"<p>               Bases: <code>PathLoader[_DF]</code></p> <p>A <code>Loader</code> for loading and saving <code>pd.DataFrame</code>s.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".csv\"</code></li> <li><code>\".parquet\"</code></li> <li><code>\".pdpickle\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>pd.DataFrame</code></li> <li><code>pd.Series</code> - Only to <code>\".pdpickle\"</code> files</li> </ul> Multiindex support <p>There is currently no multi-index support as we explicitly use <code>index_col=0</code> when loading a <code>\".csv\"</code> file. This is because we assume that the first column is the index to prevent Unamed columns from being created.</p> Series support <p>There is currently limited support for pandas series as once written to csv/parquet, they are converted to a dataframe with a single column. See this issue</p> <p>Please consider using <code>\".pdpickle\"</code> instead.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'pd'\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"amltk.store.paths.path_loaders.PathLoader.name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.can_load","title":"can_load  <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type | None = None\n) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    if key.suffix in (\".pdpickle\", None):\n        return check in (pd.Series, pd.DataFrame, None)\n\n    if key.suffix in (\".csv\", \".parquet\"):\n        return check in (pd.DataFrame, None)\n\n    return False\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"amltk.store.paths.path_loaders.PathLoader.can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.can_save","title":"can_save  <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    if key.suffix == \".pdpickle\":\n        return isinstance(obj, pd.Series | pd.DataFrame)\n\n    if key.suffix == \".parquet\":\n        return isinstance(obj, pd.DataFrame)\n\n    if key.suffix == \".csv\":\n        # TODO: https://github.com/automl/amltk/issues/4\n        return isinstance(obj, pd.DataFrame) and obj.index.nlevels == 1\n\n    return False\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"amltk.store.paths.path_loaders.PathLoader.can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; _DF\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; _DF:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    if key.suffix == \".csv\":\n        return pd.read_csv(key, index_col=0)  # type: ignore\n\n    if key.suffix == \".parquet\":\n        return pd.read_parquet(key)  # type: ignore\n\n    if key.suffix == \".pdpickle\":\n        obj = pd.read_pickle(key)  # noqa: S301\n        if not isinstance(obj, pd.Series | pd.DataFrame):  # type: ignore\n            msg = (\n                f\"Expected `pd.Series | pd.DataFrame` from {key=}\"\n                f\" but got `{type(obj).__name__}`.\"\n            )\n            raise TypeError(msg)\n\n        return obj  # type: ignore\n\n    raise ValueError(f\"Unknown file extension {key.suffix}\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"amltk.store.paths.path_loaders.PathLoader.load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.save","title":"save  <code>classmethod</code>","text":"<pre><code>save(obj: _DF, key: Path) -&gt; Stored[_DF]\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: _DF, key: Path, /) -&gt; Stored[_DF]:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    # Most pandas methods only seem to support dataframes\n    logger.debug(f\"Saving {key=}\")\n\n    if key.suffix == \".pdpickle\":\n        obj.to_pickle(key)\n        return Stored(key, cls.load)\n\n    if key.suffix == \".csv\":\n        if obj.index.name is None and obj.index.nlevels == 1:\n            obj.index.name = \"index\"\n\n        obj.to_csv(key, index=True)\n        return Stored(key, cls.load)\n\n    if key.suffix == \".parquet\":\n        obj.to_parquet(key)\n        return Stored(key, cls.load)\n\n    raise ValueError(f\"Unknown extension {key.suffix=}\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"amltk.store.paths.path_loaders.PathLoader.save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader","title":"PathLoader","text":"<p>               Bases: <code>Loader[Path, T]</code></p> <p>A <code>Loader</code> for loading and saving objects indentified by a <code>Path</code>.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader","title":"PickleLoader","text":"<p>               Bases: <code>PathLoader[Any]</code></p> <p>A <code>Loader</code> for loading and saving any object to a pickle file.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".pkl\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li>object</li> </ul> Picklability <p>This loader uses Python's built-in <code>pickle</code> module to save and load objects. This means that the object must be picklable in order to be saved and loaded. If the object is not picklable, then an error will be raised when attempting to save or load the object.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'pickle'\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"amltk.store.paths.path_loaders.PathLoader.name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.can_load","title":"can_load  <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type | None = None\n) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(\n    cls,\n    key: Path,\n    /,\n    *,\n    check: type | None = None,\n) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".pkl\", \".pickle\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"amltk.store.paths.path_loaders.PathLoader.can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.can_save","title":"can_save  <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return True  # Anything can be attempted to be pickled\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"amltk.store.paths.path_loaders.PathLoader.can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; Any\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; Any:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"rb\") as f:\n        return pickle.load(f)  # noqa: S301\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"amltk.store.paths.path_loaders.PathLoader.load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.save","title":"save  <code>classmethod</code>","text":"<pre><code>save(obj: Any, key: Path) -&gt; Stored[Any]\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; Stored[Any]:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"wb\") as f:\n        pickle.dump(obj, f)\n    return Stored(key, cls.load)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"amltk.store.paths.path_loaders.PathLoader.save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader","title":"TxtLoader","text":"<p>               Bases: <code>PathLoader[str]</code></p> <p>A <code>Loader</code> for loading and saving <code>str</code>s to text files.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".text\"</code></li> <li><code>\".txt\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>str</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'text'\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"amltk.store.paths.path_loaders.PathLoader.name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.can_load","title":"can_load  <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type | None = None\n) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".text\", \".txt\") and check in (str, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"amltk.store.paths.path_loaders.PathLoader.can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.can_save","title":"can_save  <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, str) and key.suffix in (\".text\", \".txt\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"amltk.store.paths.path_loaders.PathLoader.can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; str\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; str:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"r\") as f:\n        return f.read()\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"amltk.store.paths.path_loaders.PathLoader.load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.save","title":"save  <code>classmethod</code>","text":"<pre><code>save(obj: str, key: Path) -&gt; Stored[str]\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: str, key: Path, /) -&gt; Stored[str]:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"w\") as f:\n        f.write(obj)\n    return Stored(key, cls.load)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"amltk.store.paths.path_loaders.PathLoader.save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader","title":"YAMLLoader","text":"<p>               Bases: <code>PathLoader[_Yaml]</code></p> <p>A <code>Loader</code> for loading and saving <code>dict</code>s and <code>list</code>s to YAML.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".yaml\"</code></li> <li><code>\".yml\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>dict</code></li> <li><code>list</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: ClassVar = 'yaml'\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"amltk.store.paths.path_loaders.PathLoader.name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.can_load","title":"can_load  <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type | None = None\n) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".yaml\", \".yml\") and check in (dict, list, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"amltk.store.paths.path_loaders.PathLoader.can_load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_load(\n    key: Path, /, *, check: type[T] | None = None\n) -&gt; bool\n</code></pre> <p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.can_save","title":"can_save  <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, dict | list) and key.suffix in (\".yaml\", \".yml\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"amltk.store.paths.path_loaders.PathLoader.can_save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_save(obj: Any, key: Path) -&gt; bool\n</code></pre> <p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; _Yaml\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; _Yaml:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    if yaml is None:\n        raise ModuleNotFoundError(\"PyYAML is not installed\")\n\n    with key.open(\"r\") as f:\n        item = yaml.safe_load(f)\n\n    if not isinstance(item, dict | list):\n        msg = f\"Expected `dict | list` from {key=} but got `{type(item).__name__}`\"\n        raise TypeError(msg)\n\n    return item  # type: ignore\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"amltk.store.paths.path_loaders.PathLoader.load  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>load(key: Path) -&gt; T\n</code></pre> <p>Load an object from the given key.</p> PARAMETER DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.save","title":"save  <code>classmethod</code>","text":"<pre><code>save(obj: _Yaml, key: Path) -&gt; Stored[_Yaml]\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: _Yaml, key: Path, /) -&gt; Stored[_Yaml]:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    if yaml is None:\n        raise ModuleNotFoundError(\"PyYAML is not installed\")\n\n    with key.open(\"w\") as f:\n        yaml.dump(obj, f)\n\n    return Stored(key, cls.load)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"amltk.store.paths.path_loaders.PathLoader.save  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>save(obj: T, key: Path) -&gt; Stored[T]\n</code></pre> <p>Save an object to under the given key.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>T</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: T, key: Path, /) -&gt; Stored[T]:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"examples/","title":"Index","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>This houses the examples for the project. Use the navigation bar to the left to view more.</p>"},{"location":"examples/dask-jobqueue/","title":"Using the Scheduler with SLURM (dask-jobqueue)","text":"Expand to copy <code>examples/dask-jobqueue.py</code>  (top right) <pre><code>import traceback\nfrom typing import Any\n\nimport openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Component, Node, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn import split_data\nfrom amltk.store import PathBucket\n\nN_WORKERS = 32\nscheduler = Scheduler.with_slurm(\n    n_workers=N_WORKERS,  # Number of workers to launch\n    queue=\"the-name-of-the-partition/queue\",  # Name of the queue to submit to\n    cores=1,  # Number of cores per worker\n    memory=\"4 GB\",  # Memory per worker\n    walltime=\"00:20:00\",  # Walltime per worker\n    # submit_command=\"sbatch --extra-arguments\",  # Sometimes you need extra arguments to the launch command\n)\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                OneHotEncoder(drop=\"first\"),\n            ],\n            \"numerical\": Component(\n                SimpleImputer,\n                space={\"strategy\": [\"mean\", \"median\"]},\n            ),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\n\ndef target_function(trial: Trial, _pipeline: Node) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            trial.bucket[\"X_train.csv\"].load(),\n            trial.bucket[\"X_val.csv\"].load(),\n            trial.bucket[\"X_test.csv\"].load(),\n            trial.bucket[\"y_train.npy\"].load(),\n            trial.bucket[\"y_val.npy\"].load(),\n            trial.bucket[\"y_test.npy\"].load(),\n        )\n\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    try:\n        with trial.profile(\"fit\"):\n            sklearn_pipeline.fit(X_train, y_train)\n    except Exception as e:\n        tb = traceback.format_exc()\n        trial.store({\"exception.txt\": f\"{e}\\n {tb}\"})\n        return trial.fail(e, tb)\n\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_acc)\n\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\nbucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  # &lt;!&gt; (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\ntask = scheduler.task(target_function)\n\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline)\n\n\ntrial_history = History()\n\n\n@task.on_result\ndef process_result_and_launc(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n    optimizer.tell(report)\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline)\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    raise RuntimeError(\"Scheduler cancelled a worker!\")\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=60)\n\n    history_df = trial_history.df()\n    print(history_df)\n    print(len(history_df))\n</code></pre>"},{"location":"examples/dask-jobqueue/#description","title":"Description","text":"<p>The point of this example is to show how to set up <code>dask-jobqueue</code> with a realistic workload.</p> <p>Dependencies</p> <p>Requires the following integrations and dependencies:</p> <ul> <li><code>pip install openml amltk[smac, sklearn, dask-jobqueue]</code></li> </ul> <p>This example shows how to use <code>dask-jobqueue</code> to run HPO on a <code>RandomForestClassifier</code> with SMAC. This workload is borrowed from the HPO example.</p> <p>SMAC can not handle fast updates and seems to be quite efficient for this workload with ~32 cores.</p> <pre><code>import traceback\nfrom typing import Any\n\nimport openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Component, Node, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn import split_data\nfrom amltk.store import PathBucket\n\nN_WORKERS = 32\nscheduler = Scheduler.with_slurm(\n    n_workers=N_WORKERS,  # Number of workers to launch\n    queue=\"the-name-of-the-partition/queue\",  # Name of the queue to submit to\n    cores=1,  # Number of cores per worker\n    memory=\"4 GB\",  # Memory per worker\n    walltime=\"00:20:00\",  # Walltime per worker\n    # submit_command=\"sbatch --extra-arguments\",  # Sometimes you need extra arguments to the launch command\n)\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                OneHotEncoder(drop=\"first\"),\n            ],\n            \"numerical\": Component(\n                SimpleImputer,\n                space={\"strategy\": [\"mean\", \"median\"]},\n            ),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\n\ndef target_function(trial: Trial, _pipeline: Node) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            trial.bucket[\"X_train.csv\"].load(),\n            trial.bucket[\"X_val.csv\"].load(),\n            trial.bucket[\"X_test.csv\"].load(),\n            trial.bucket[\"y_train.npy\"].load(),\n            trial.bucket[\"y_val.npy\"].load(),\n            trial.bucket[\"y_test.npy\"].load(),\n        )\n\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    try:\n        with trial.profile(\"fit\"):\n            sklearn_pipeline.fit(X_train, y_train)\n    except Exception as e:\n        tb = traceback.format_exc()\n        trial.store({\"exception.txt\": f\"{e}\\n {tb}\"})\n        return trial.fail(e, tb)\n\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_acc)\n\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\nbucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  #  (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\ntask = scheduler.task(target_function)\n\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline)\n\n\ntrial_history = History()\n\n\n@task.on_result\ndef process_result_and_launc(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n    optimizer.tell(report)\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline)\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    raise RuntimeError(\"Scheduler cancelled a worker!\")\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=60)\n\n    history_df = trial_history.df()\n    print(history_df)\n    print(len(history_df))\n</code></pre>"},{"location":"examples/hpo/","title":"HPO","text":"Expand to copy <code>examples/hpo.py</code>  (top right) <pre><code>from typing import Any\n\nimport openml\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom amltk.sklearn import split_data\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom amltk.pipeline import Component, Node, Sequential, Split\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                OneHotEncoder(drop=\"first\"),\n            ],\n            \"numerical\": Component(\n                SimpleImputer,\n                space={\"strategy\": [\"mean\", \"median\"]},\n            ),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n\nfrom sklearn.metrics import accuracy_score\n\nfrom amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\n\ndef target_function(\n    trial: Trial,\n    _pipeline: Node,\n    data_bucket: PathBucket,\n) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    # Load in data\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            data_bucket[\"X_train.csv\"].load(),\n            data_bucket[\"X_val.csv\"].load(),\n            data_bucket[\"X_test.csv\"].load(),\n            data_bucket[\"y_train.npy\"].load(),\n            data_bucket[\"y_val.npy\"].load(),\n            data_bucket[\"y_test.npy\"].load(),\n        )\n\n    # Configure the pipeline with the trial config before building it.\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    # Fit the pipeline, indicating when you want to start the trial timing\n    try:\n        with trial.profile(\"fit\"):\n            sklearn_pipeline.fit(X_train, y_train)\n    except Exception as e:\n        return trial.fail(e)\n\n    # Make our predictions with the model\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    # Save the scores to the summary of the trial\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    # Save all of this to the file system\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    # Finally report the success\n    return trial.success(accuracy=val_acc)\n\n\nfrom amltk.store import PathBucket\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\ndata_bucket = bucket / \"data\"\ndata_bucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nprint(bucket)\nprint(dict(bucket))\nprint(dict(data_bucket))\nfrom amltk.optimization import Metric\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  # &lt;!&gt; (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\n\n# 1. You can also explicitly pass in the space of hyperparameters to optimize.\n#   ```python\n#   space = pipeline.search_space(\"configspace\")\n#   # or\n#   space = pipeline.search_space(SMACOptimizer.preffered_parser())\n#   ```\ntask = scheduler.task(target_function)\n\nprint(task)\n\n\n@scheduler.on_start\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline, data_bucket=data_bucket)\n\n\n\n\n@task.on_result\ndef tell_optimizer(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n\n\nfrom amltk.optimization import History\n\ntrial_history = History()\n\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n\n\n\n\n@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline, data_bucket=data_bucket)\n\n\n\n\n@task.on_exception\ndef stop_scheduler_on_exception(*_: Any) -&gt; None:\n    scheduler.stop()\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    scheduler.stop()\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=5)\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n</code></pre>"},{"location":"examples/hpo/#description","title":"Description","text":"<p>Dependencies</p> <p>Requires the following integrations and dependencies:</p> <ul> <li><code>pip install openml amltk[smac, sklearn]</code></li> </ul> <p>This example shows the basic of setting up a simple HPO loop around a <code>RandomForestClassifier</code>. We will use the OpenML to get a dataset and also use some static preprocessing as part of our pipeline definition.</p> <p>You can fine the pipeline guide here and the optimization guide here to learn more.</p> <p>You can skip the imports sections and go straight to the pipeline definition.</p>"},{"location":"examples/hpo/#dataset","title":"Dataset","text":"<p>Below is just a small function to help us get the dataset from OpenML and encode the labels.</p> CodeRun <pre><code>from typing import Any\n\nimport openml\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom amltk.sklearn import split_data\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n</code></pre> <p></p>"},{"location":"examples/hpo/#pipeline-definition","title":"Pipeline Definition","text":"<p>Here we define a pipeline which splits categoricals and numericals down two different paths, and then combines them back together before passing them to the <code>RandomForestClassifier</code>.</p> <p>For more on definitions of pipelines, see the Pipeline guide.</p> CodeRun <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom amltk.pipeline import Component, Node, Sequential, Split\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                OneHotEncoder(drop=\"first\"),\n            ],\n            \"numerical\": Component(\n                SimpleImputer,\n                space={\"strategy\": [\"mean\", \"median\"]},\n            ),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n</code></pre> <pre><code>Sequential(name='Pipeline', item=None, nodes=(Split(name='feature_preprocessing', item=None, nodes=(Sequential(name='categorical', item=None, nodes=(Fixed(name='SimpleImputer', item=SimpleImputer(fill_value='missing', strategy='constant'), nodes=(), config=None, space=None, fidelities=None, config_transform=None, meta=None), Fixed(name='OneHotEncoder', item=OneHotEncoder(drop='first'), nodes=(), config=None, space=None, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None), Sequential(name='numerical', item=None, nodes=(Component(name='SimpleImputer', item=&lt;class 'sklearn.impute._base.SimpleImputer'&gt;, nodes=(), config=None, space={'strategy': ['mean', 'median']}, fidelities=None, config_transform=None, meta=None),), config=None, space=None, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None), Component(name='RandomForestClassifier', item=&lt;class 'sklearn.ensemble._forest.RandomForestClassifier'&gt;, nodes=(), config=None, space={'n_estimators': (10, 100), 'max_features': (0.0, 1.0), 'criterion': ['gini', 'entropy', 'log_loss']}, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None)\nConfiguration space object:\n  Hyperparameters:\n    Pipeline:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, entropy, log_loss}, Default: gini\n    Pipeline:RandomForestClassifier:max_features, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n    Pipeline:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:feature_preprocessing:numerical:SimpleImputer:strategy, Type: Categorical, Choices: {mean, median}, Default: mean\n</code></pre>"},{"location":"examples/hpo/#target-function","title":"Target Function","text":"<p>The function we will optimize must take in a <code>Trial</code> and return a <code>Trial.Report</code>. We also pass in a <code>PathBucket</code> which is a dict-like view of the file system, where we have our dataset stored.</p> <p>We also pass in our pipeline, which we will use to build our sklearn pipeline with a specific <code>trial.config</code> suggested by the <code>Optimizer</code>.</p> CodeRun <pre><code>from sklearn.metrics import accuracy_score\n\nfrom amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\n\ndef target_function(\n    trial: Trial,\n    _pipeline: Node,\n    data_bucket: PathBucket,\n) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    # Load in data\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            data_bucket[\"X_train.csv\"].load(),\n            data_bucket[\"X_val.csv\"].load(),\n            data_bucket[\"X_test.csv\"].load(),\n            data_bucket[\"y_train.npy\"].load(),\n            data_bucket[\"y_val.npy\"].load(),\n            data_bucket[\"y_test.npy\"].load(),\n        )\n\n    # Configure the pipeline with the trial config before building it.\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    # Fit the pipeline, indicating when you want to start the trial timing\n    try:\n        with trial.profile(\"fit\"):\n            sklearn_pipeline.fit(X_train, y_train)\n    except Exception as e:\n        return trial.fail(e)\n\n    # Make our predictions with the model\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    # Save the scores to the summary of the trial\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    # Save all of this to the file system\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    # Finally report the success\n    return trial.success(accuracy=val_acc)\n</code></pre> <p></p>"},{"location":"examples/hpo/#running-the-whole-thing","title":"Running the Whole Thing","text":"<p>Now we can run the whole thing. We will use the <code>Scheduler</code> to run the optimization, and the <code>SMACOptimizer</code> to optimize the pipeline.</p>"},{"location":"examples/hpo/#getting-and-storing-data","title":"Getting and storing data","text":"<p>We use a <code>PathBucket</code> to store the data. This is a dict-like view of the file system.</p> CodeRun <pre><code>from amltk.store import PathBucket\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\ndata_bucket = bucket / \"data\"\ndata_bucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nprint(bucket)\nprint(dict(bucket))\nprint(dict(data_bucket))\n</code></pre> <pre><code>PathBucket(PosixPath('example-hpo'))\n{'data': Drop(key=PosixPath('example-hpo/data'))}\n{'y_test.npy': Drop(key=PosixPath('example-hpo/data/y_test.npy')), 'X_val.csv': Drop(key=PosixPath('example-hpo/data/X_val.csv')), 'y_val.npy': Drop(key=PosixPath('example-hpo/data/y_val.npy')), 'X_train.csv': Drop(key=PosixPath('example-hpo/data/X_train.csv')), 'y_train.npy': Drop(key=PosixPath('example-hpo/data/y_train.npy')), 'X_test.csv': Drop(key=PosixPath('example-hpo/data/X_test.csv'))}\n</code></pre>"},{"location":"examples/hpo/#setting-up-the-scheduler-task-and-optimizer","title":"Setting up the Scheduler, Task and Optimizer","text":"<p>We use the <code>Scheduler.with_processes</code> method to create a <code>Scheduler</code> that will run the optimization.</p> <p>Please check out the full guides to learn more!</p> <p>We then create an <code>SMACOptimizer</code> which will optimize the pipeline. We pass in pipeline, and SMAC the optimizer will parser out the space of hyperparameters to optimize.</p> CodeRun <pre><code>from amltk.optimization import Metric\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  #  (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\n</code></pre> <ol> <li>You can also explicitly pass in the space of hyperparameters to optimize.   <pre><code>space = pipeline.search_space(\"configspace\")\n# or\nspace = pipeline.search_space(SMACOptimizer.preffered_parser())\n</code></pre></li> </ol> <p></p> <p>Next we create a <code>Task</code>, passing in the function we want to run and the scheduler we will run it in.</p> CodeRun <pre><code>task = scheduler.task(target_function)\n\nprint(task)\n</code></pre> <pre><code>Task(unique_ref=Task-target_function-oe1r2TsT, plugins=[])\n</code></pre> <p>We use the callback decorators of the <code>Scheduler</code> and the <code>Task</code> to add callbacks that get called during events that happen during the running of the scheduler. Using this, we can control the flow of how things run. Check out the task guide for more.</p> <p>This one here asks the optimizer for a new trial when the scheduler starts and launches the task we created earlier with this trial.</p> CodeRun <pre><code>@scheduler.on_start\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline, data_bucket=data_bucket)\n</code></pre> <p></p> <p>When a <code>Task</code> returns and we get a report, i.e. with <code>task.success()</code> or <code>task.fail()</code>, the <code>task</code> will fire off the callbacks registered with <code>@result</code>. We can use these to add callbacks that get called when these events happen.</p> <p>Here we use it to update the optimizer with the report we got.</p> CodeRun <pre><code>@task.on_result\ndef tell_optimizer(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n</code></pre> <p></p> <p>We can use the <code>History</code> class to store the reports we get from the <code>Task</code>. We can then use this to analyze the results of the optimization afterwords.</p> CodeRun <pre><code>from amltk.optimization import History\n\ntrial_history = History()\n\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n</code></pre> <p></p> <p>We launch a new task when the scheduler is empty, i.e. when all the tasks have finished. This will keep going until we hit the timeout we set on the scheduler.</p> <p>If you want to run the optimization in parallel, you can use the <code>@task.on_result</code> callback to launch a new task when you get a report. This will launch a new task as soon as one finishes.</p> CodeRun <pre><code>@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline, data_bucket=data_bucket)\n</code></pre> <p></p> <p>If something goes wrong, we likely want to stop the scheduler.</p> CodeRun <pre><code>@task.on_exception\ndef stop_scheduler_on_exception(*_: Any) -&gt; None:\n    scheduler.stop()\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    scheduler.stop()\n</code></pre> <p></p>"},{"location":"examples/hpo/#setting-the-system-to-run","title":"Setting the system to run","text":"<p>Lastly we use <code>Scheduler.run</code> to run the scheduler. We pass in a timeout of 20 seconds.</p> CodeRun <pre><code>if __name__ == \"__main__\":\n    scheduler.run(timeout=5)\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n</code></pre> <p></p>"},{"location":"examples/hpo_with_ensembling/","title":"Performing HPO with Post-Hoc Ensembling.","text":"Expand to copy <code>examples/hpo_with_ensembling.py</code>  (top right) <pre><code>from __future__ import annotations\n\nimport shutil\nimport traceback\nfrom asyncio import Future\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport openml\nfrom sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    MinMaxScaler,\n    OneHotEncoder,\n    RobustScaler,\n    StandardScaler,\n)\nfrom sklearn.svm import SVC\n\nfrom amltk.data.conversions import probabilities_to_classes\nfrom amltk.ensembling.weighted_ensemble_caruana import weighted_ensemble_caruana\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Choice, Component, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn.data import split_data\nfrom amltk.store import PathBucket\n\n\n\ndef get_dataset(seed: int) -&gt; tuple[np.ndarray, ...]:\n    dataset = openml.datasets.get_dataset(\n        31,\n        download_qualities=False,\n        download_features_meta_data=False,\n        download_data=True,\n    )\n    X, y, _, _ = dataset.get_data(\n        dataset_format=\"dataframe\",\n        target=dataset.default_target_attribute,\n    )\n    _y = LabelEncoder().fit_transform(y)\n    splits = split_data(  # &lt;!&gt; (1)!\n        X,  # &lt;!&gt;\n        _y,  # &lt;!&gt;\n        splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2},  # &lt;!&gt;\n        seed=seed,  # &lt;!&gt;\n    )  # &lt;!&gt;\n\n    x_train, y_train = splits[\"train\"]\n    x_val, y_val = splits[\"val\"]\n    x_test, y_test = splits[\"test\"]\n    return x_train, x_val, x_test, y_train, y_val, y_test  # type: ignore\n\n\n# 1. We use the [`split_data()`][amltk.sklearn.data.split_data] function from the\n#   to split the data into a custom amount of splits, in this case\n#   `#!python \"train\", \"val\", \"test\"`. You could also use the\n#   dedicated [`train_val_test_split()`][amltk.sklearn.data.train_val_test_split]\n#   function instead.\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categories\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                Component(\n                    OneHotEncoder,\n                    space={\n                        \"min_frequency\": (0.01, 0.1),\n                        \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n                    },\n                    config={\"drop\": \"first\"},\n                ),\n            ],\n            \"numbers\": [\n                Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n                Component(VarianceThreshold, space={\"threshold\": (0.0, 0.2)}),\n                Choice(StandardScaler, MinMaxScaler, RobustScaler, name=\"scaler\"),\n            ],\n        },\n        name=\"feature_preprocessing\",\n        config={\n            \"categories\": make_column_selector(dtype_include=object),\n            \"numbers\": make_column_selector(dtype_include=np.number),\n        },\n    )\n    &gt;&gt; Choice(  # &lt;!&gt; (1)!\n        Component(SVC, space={\"C\": (0.1, 10.0)}, config={\"probability\": True}),\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n        ),\n        Component(\n            MLPClassifier,\n            space={\n                \"activation\": [\"identity\", \"logistic\", \"relu\"],\n                \"alpha\": (0.0001, 0.1),\n                \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n            },\n        ),\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n\n# 1. Here we define a choice of algorithms to use where each entry is a possible\n#   algorithm to use. Each algorithm is defined by a step, which is a\n#   configuration of a sklearn estimator. The space parameter is a dictionary\n#   of hyperparameters to optimize over, and the config parameter is a\n#   dictionary of fixed parameters to set on the estimator.\n# 2. Here we gropu the numerical preprocessing steps to use. Each step is a\n#  scaler to use. Each scaler is defined by a step, which is a configuration\n#  of the preprocessor. The space parameter is a dictionary of\n#  hyperparameters to optimize over, and the config parameter is a dictionary\n#  of fixed parameters to set on the preprocessing step.\n# 3. Here we group the categorical preprocessing steps to use.\n#   Each step is given a space, which is a dictionary of hyperparameters to\n#   optimize over, and a config, which is a dictionary of fixed parameters to\n#   set on the preprocessing step.\n\n\ndef target_function(\n    trial: Trial,\n    bucket: PathBucket,\n    pipeline: Sequential,\n) -&gt; Trial.Report:\n    X_train, X_val, X_test, y_train, y_val, y_test = (  # (1)!\n        bucket[\"X_train.csv\"].load(),\n        bucket[\"X_val.csv\"].load(),\n        bucket[\"X_test.csv\"].load(),\n        bucket[\"y_train.npy\"].load(),\n        bucket[\"y_val.npy\"].load(),\n        bucket[\"y_test.npy\"].load(),\n    )\n    pipeline = pipeline.configure(trial.config)  # &lt;!&gt; (2)!\n    sklearn_pipeline = pipeline.build(\"sklearn\")  # &lt;!&gt;\n\n    try:\n        with trial.profile(\"fit\"):  # &lt;!&gt; (3)!\n            sklearn_pipeline.fit(X_train, y_train)\n    except Exception as e:\n        tb = traceback.format_exc()\n        trial.store(\n            {\n                \"exception.txt\": str(e),\n                \"config.json\": dict(trial.config),\n                \"traceback.txt\": str(tb),\n            },\n        )\n        return trial.fail()  # &lt;!&gt; (4)!\n\n    # Make our predictions with the model\n    train_predictions = sklearn_pipeline.predict(X_train)\n    val_predictions = sklearn_pipeline.predict(X_val)\n    test_predictions = sklearn_pipeline.predict(X_test)\n\n    val_probabilites = sklearn_pipeline.predict_proba(X_val)\n    val_accuracy = float(accuracy_score(val_predictions, y_val))\n\n    # Save the scores to the summary of the trial\n    trial.summary[\"train_accuracy\"] = float(accuracy_score(train_predictions, y_train))\n    trial.summary[\"validation_accuracy\"] = val_accuracy\n    trial.summary[\"test_accuracy\"] = float(accuracy_score(test_predictions, y_test))\n\n    # Save all of this to the file system\n    trial.store(  # (5)!\n        {\n            \"config.json\": dict(trial.config),\n            \"scores.json\": trial.summary,\n            \"model.pkl\": sklearn_pipeline,\n            \"val_predictions.npy\": val_predictions,\n            \"val_probabilities.npy\": val_probabilites,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_accuracy)  # &lt;!&gt; (6)!\n\n\n# 1. We can easily load data from a [`PathBucket`][amltk.store.PathBucket]\n#   using the `load` method.\n# 2. We configure the pipeline with a specific set of hyperparameters suggested\n#  by the optimizer through the [`Trial`][amltk.optimization.Trial] object.\n# 3. We begin the trial by timing the execution of the target function and capturing\n#  any potential exceptions.\n# 4. If the trial failed, we return a failed report with a cost of infinity.\n# 5. We save the results of the trial using\n#   [`Trial.store`][amltk.optimization.Trial.store], creating a subdirectory\n#   for this trial.\n# 6. We return a successful report with the cost of the trial, which is the\n# inverse of the validation accuracy.\n\n\n@dataclass\nclass Ensemble:\n    weights: dict[str, float]\n    trajectory: list[tuple[str, float]]\n    configs: dict[str, dict[str, Any]]\n\n\ndef create_ensemble(\n    history: History,\n    bucket: PathBucket,\n    /,\n    size: int = 5,\n    seed: int = 42,\n) -&gt; Ensemble:\n    if len(history) == 0:\n        return Ensemble({}, [], {})\n\n    validation_predictions = {\n        report.name: report.retrieve(\"val_probabilities.npy\") for report in history\n    }\n    targets = bucket[\"y_val.npy\"].load()\n\n    accuracy: Callable[[np.ndarray, np.ndarray], float] = accuracy_score  # type: ignore\n\n    def _score(_targets: np.ndarray, ensembled_probabilities: np.ndarray) -&gt; float:\n        predictions = probabilities_to_classes(ensembled_probabilities, classes=[0, 1])\n        return accuracy(_targets, predictions)\n\n    weights, trajectory, final_probabilities = weighted_ensemble_caruana(  # &lt;!&gt;\n        model_predictions=validation_predictions,  # &lt;!&gt;\n        targets=targets,  # &lt;!&gt;\n        size=size,  # &lt;!&gt;\n        metric=_score,  # &lt;!&gt;\n        select=max,  # &lt;!&gt;\n        seed=seed,  # &lt;!&gt;\n    )  # &lt;!&gt;\n\n    configs = {name: history[name].retrieve(\"config.json\") for name in weights}\n    return Ensemble(weights=weights, trajectory=trajectory, configs=configs)\n\n\nseed = 42\n\nX_train, X_val, X_test, y_train, y_val, y_test = get_dataset(seed)  # (1)!\n\npath = Path(\"example-hpo-with-ensembling\")\nif path.exists():\n    shutil.rmtree(path)\n\nbucket = PathBucket(path)\nbucket.store(  # (2)!\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nscheduler = Scheduler.with_processes()  # (3)!\noptimizer = SMACOptimizer.create(\n    space=pipeline,\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0, 1)),\n    bucket=path,\n    seed=seed,\n)  # (4)!\n\ntask = scheduler.task(target_function)  # (5)!\nensemble_task = scheduler.task(create_ensemble)  # (6)!\n\ntrial_history = History()\nensembles: list[Ensemble] = []\n\n\n@scheduler.on_start  # (7)!\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@task.on_result\ndef tell_optimizer(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n\n\n@task.on_result\ndef add_to_history(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n\n\n@task.on_result\ndef launch_ensemble_task(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When a task successfully completes, launch an ensemble task.\"\"\"\n    if report.status is Trial.Status.SUCCESS:\n        ensemble_task.submit(trial_history, bucket)\n\n\n@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@ensemble_task.on_result\ndef save_ensemble(future: Future, ensemble: Ensemble) -&gt; None:\n    \"\"\"When an ensemble task returns, save it.\"\"\"\n    ensembles.append(ensemble)\n\n\n@ensemble_task.on_exception\ndef print_ensemble_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@task.on_exception\ndef print_task_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@scheduler.on_timeout\ndef run_last_ensemble_task() -&gt; None:\n    \"\"\"When the scheduler is empty, run the last ensemble task.\"\"\"\n    ensemble_task.submit(trial_history, bucket)\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=5, wait=True)  # (8)!\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n\n    best_ensemble = max(ensembles, key=lambda e: e.trajectory[-1])\n\n    print(\"Best ensemble:\")\n    print(best_ensemble)\n# 1. We use `#!python get_dataset()` defined earlier to load the\n#  dataset.\n# 2. We use [`store()`][amltk.store.Bucket.store] to store the data in the bucket, with\n#  each key being the name of the file and the value being the data.\n# 3. We use [`Scheduler.with_processes()`][amltk.scheduling.Scheduler.with_processes]\n#  create a [`Scheduler`][amltk.scheduling.Scheduler] that runs everything\n#  in a different process. You can of course use a different backend if you want.\n# 4. We use [`SMACOptimizer.create()`][amltk.optimization.optimizers.smac.SMACOptimizer.create] to create a\n#  [`SMACOptimizer`][amltk.optimization.optimizers.smac.SMACOptimizer] given the space from the pipeline\n#  to optimize over.\n# 5. We create a [`Task`][amltk.scheduling.Task] that will run our objective, passing\n#  in the function to run and the scheduler for where to run it\n# 6. We use [`task()`][amltk.scheduling.Task] to create a\n#  [`Task`][amltk.scheduling.Task] for the `create_ensemble` method above.\n#  This will also run in parallel with the hpo trials if using a non-sequential scheduling mode.\n# 7. We use `@scheduler.on_start()` hook to register a\n#  callback that will be called when the scheduler starts. We can use the\n#  `repeat` argument to make sure it's called many times if we want.\n# 8. We use [`Scheduler.run()`][amltk.scheduling.Scheduler.run] to run the scheduler.\n#  Here we set it to run briefly for 5 seconds and wait for remaining tasks to finish\n#  before continuing.\n</code></pre>"},{"location":"examples/hpo_with_ensembling/#description","title":"Description","text":"<p>Dependencies</p> <p>Requires the following integrations and dependencies:</p> <ul> <li><code>pip install openml amltk[smac, sklearn]</code></li> </ul> <p>This example performs Hyperparameter optimization on a fairly default data-preprocessing + model sklearn pipeline, using a dataset pulled from OpenML.</p> <p>After the HPO is complete, we use the validation predictions from each trial to create an ensemble using the Weighted Ensemble algorithm from Caruana et al. (2004).</p> Reference: Ensemble selection from libraries of models <p>Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew and Alex Ksikes</p> <p>ICML 2004</p> <p>dl.acm.org/doi/10.1145/1015330.1015432</p> <p>www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf</p> <p>This makes heavy use of the pipelines and the optimization faculties of amltk. You can fine the pipeline guide here and the optimization guide here to learn more.</p> <p>You can skip the imports sections and go straight to the pipeline definition.</p>"},{"location":"examples/hpo_with_ensembling/#imports","title":"Imports","text":"CodeRun <pre><code>from __future__ import annotations\n\nimport shutil\nimport traceback\nfrom asyncio import Future\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport openml\nfrom sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    MinMaxScaler,\n    OneHotEncoder,\n    RobustScaler,\n    StandardScaler,\n)\nfrom sklearn.svm import SVC\n\nfrom amltk.data.conversions import probabilities_to_classes\nfrom amltk.ensembling.weighted_ensemble_caruana import weighted_ensemble_caruana\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Choice, Component, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn.data import split_data\nfrom amltk.store import PathBucket\n</code></pre> <p>Below is just a small function to help us get the dataset from OpenML and encode the labels.</p> CodeRun <pre><code>def get_dataset(seed: int) -&gt; tuple[np.ndarray, ...]:\n    dataset = openml.datasets.get_dataset(\n        31,\n        download_qualities=False,\n        download_features_meta_data=False,\n        download_data=True,\n    )\n    X, y, _, _ = dataset.get_data(\n        dataset_format=\"dataframe\",\n        target=dataset.default_target_attribute,\n    )\n    _y = LabelEncoder().fit_transform(y)\n    splits = split_data(  #  (1)!\n        X,  \n        _y,  \n        splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2},  \n        seed=seed,  \n    )  \n\n    x_train, y_train = splits[\"train\"]\n    x_val, y_val = splits[\"val\"]\n    x_test, y_test = splits[\"test\"]\n    return x_train, x_val, x_test, y_train, y_val, y_test  # type: ignore\n</code></pre> <ol> <li>We use the <code>split_data()</code> function from the   to split the data into a custom amount of splits, in this case   <code>\"train\", \"val\", \"test\"</code>. You could also use the   dedicated <code>train_val_test_split()</code>   function instead.</li> </ol> <p></p>"},{"location":"examples/hpo_with_ensembling/#pipeline-definition","title":"Pipeline Definition","text":"<p>Here we define a pipeline which splits categoricals and numericals down two different paths, and then combines them back together before passing them to a choice of classifier between a Random Forest, Support Vector Machine, and Multi-Layer Perceptron.</p> <p>For more on definitions of pipelines, see the Pipeline guide.</p> CodeRun <pre><code>pipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categories\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                Component(\n                    OneHotEncoder,\n                    space={\n                        \"min_frequency\": (0.01, 0.1),\n                        \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n                    },\n                    config={\"drop\": \"first\"},\n                ),\n            ],\n            \"numbers\": [\n                Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n                Component(VarianceThreshold, space={\"threshold\": (0.0, 0.2)}),\n                Choice(StandardScaler, MinMaxScaler, RobustScaler, name=\"scaler\"),\n            ],\n        },\n        name=\"feature_preprocessing\",\n        config={\n            \"categories\": make_column_selector(dtype_include=object),\n            \"numbers\": make_column_selector(dtype_include=np.number),\n        },\n    )\n    &gt;&gt; Choice(  #  (1)!\n        Component(SVC, space={\"C\": (0.1, 10.0)}, config={\"probability\": True}),\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n        ),\n        Component(\n            MLPClassifier,\n            space={\n                \"activation\": [\"identity\", \"logistic\", \"relu\"],\n                \"alpha\": (0.0001, 0.1),\n                \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n            },\n        ),\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n</code></pre> <ol> <li>Here we define a choice of algorithms to use where each entry is a possible   algorithm to use. Each algorithm is defined by a step, which is a   configuration of a sklearn estimator. The space parameter is a dictionary   of hyperparameters to optimize over, and the config parameter is a   dictionary of fixed parameters to set on the estimator.</li> <li>Here we gropu the numerical preprocessing steps to use. Each step is a  scaler to use. Each scaler is defined by a step, which is a configuration  of the preprocessor. The space parameter is a dictionary of  hyperparameters to optimize over, and the config parameter is a dictionary  of fixed parameters to set on the preprocessing step.</li> <li>Here we group the categorical preprocessing steps to use.   Each step is given a space, which is a dictionary of hyperparameters to   optimize over, and a config, which is a dictionary of fixed parameters to   set on the preprocessing step.</li> </ol> <pre><code>Sequential(name='Pipeline', item=None, nodes=(Split(name='feature_preprocessing', item=None, nodes=(Sequential(name='categories', item=None, nodes=(Fixed(name='SimpleImputer', item=SimpleImputer(fill_value='missing', strategy='constant'), nodes=(), config=None, space=None, fidelities=None, config_transform=None, meta=None), Component(name='OneHotEncoder', item=&lt;class 'sklearn.preprocessing._encoders.OneHotEncoder'&gt;, nodes=(), config={'drop': 'first'}, space={'min_frequency': (0.01, 0.1), 'handle_unknown': ['ignore', 'infrequent_if_exist']}, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None), Sequential(name='numbers', item=None, nodes=(Component(name='SimpleImputer', item=&lt;class 'sklearn.impute._base.SimpleImputer'&gt;, nodes=(), config=None, space={'strategy': ['mean', 'median']}, fidelities=None, config_transform=None, meta=None), Component(name='VarianceThreshold', item=&lt;class 'sklearn.feature_selection._variance_threshold.VarianceThreshold'&gt;, nodes=(), config=None, space={'threshold': (0.0, 0.2)}, fidelities=None, config_transform=None, meta=None), Choice(name='scaler', item=None, nodes=(Component(name='MinMaxScaler', item=&lt;class 'sklearn.preprocessing._data.MinMaxScaler'&gt;, nodes=(), config=None, space=None, fidelities=None, config_transform=None, meta=None), Component(name='RobustScaler', item=&lt;class 'sklearn.preprocessing._data.RobustScaler'&gt;, nodes=(), config=None, space=None, fidelities=None, config_transform=None, meta=None), Component(name='StandardScaler', item=&lt;class 'sklearn.preprocessing._data.StandardScaler'&gt;, nodes=(), config=None, space=None, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None)), config={'categories': &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd98e26bc0&gt;, 'numbers': &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd98e26f20&gt;}, space=None, fidelities=None, config_transform=None, meta=None), Choice(name='Choice-Q6hAl1zM', item=None, nodes=(Component(name='MLPClassifier', item=&lt;class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'&gt;, nodes=(), config=None, space={'activation': ['identity', 'logistic', 'relu'], 'alpha': (0.0001, 0.1), 'learning_rate': ['constant', 'invscaling', 'adaptive']}, fidelities=None, config_transform=None, meta=None), Component(name='RandomForestClassifier', item=&lt;class 'sklearn.ensemble._forest.RandomForestClassifier'&gt;, nodes=(), config=None, space={'n_estimators': (10, 100), 'criterion': ['gini', 'log_loss']}, fidelities=None, config_transform=None, meta=None), Component(name='SVC', item=&lt;class 'sklearn.svm._classes.SVC'&gt;, nodes=(), config={'probability': True}, space={'C': (0.1, 10.0)}, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None)), config=None, space=None, fidelities=None, config_transform=None, meta=None)\nConfiguration space object:\n  Hyperparameters:\n    Pipeline:Choice-Q6hAl1zM:MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    Pipeline:Choice-Q6hAl1zM:MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    Pipeline:Choice-Q6hAl1zM:MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    Pipeline:Choice-Q6hAl1zM:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    Pipeline:Choice-Q6hAl1zM:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:Choice-Q6hAl1zM:SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    Pipeline:Choice-Q6hAl1zM:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n    Pipeline:feature_preprocessing:categories:OneHotEncoder:handle_unknown, Type: Categorical, Choices: {ignore, infrequent_if_exist}, Default: ignore\n    Pipeline:feature_preprocessing:categories:OneHotEncoder:min_frequency, Type: UniformFloat, Range: [0.01, 0.1], Default: 0.055\n    Pipeline:feature_preprocessing:numbers:SimpleImputer:strategy, Type: Categorical, Choices: {mean, median}, Default: mean\n    Pipeline:feature_preprocessing:numbers:VarianceThreshold:threshold, Type: UniformFloat, Range: [0.0, 0.2], Default: 0.1\n    Pipeline:feature_preprocessing:numbers:scaler:__choice__, Type: Categorical, Choices: {MinMaxScaler, RobustScaler, StandardScaler}, Default: MinMaxScaler\n  Conditions:\n    Pipeline:Choice-Q6hAl1zM:MLPClassifier:activation | Pipeline:Choice-Q6hAl1zM:__choice__ == 'MLPClassifier'\n    Pipeline:Choice-Q6hAl1zM:MLPClassifier:alpha | Pipeline:Choice-Q6hAl1zM:__choice__ == 'MLPClassifier'\n    Pipeline:Choice-Q6hAl1zM:MLPClassifier:learning_rate | Pipeline:Choice-Q6hAl1zM:__choice__ == 'MLPClassifier'\n    Pipeline:Choice-Q6hAl1zM:RandomForestClassifier:criterion | Pipeline:Choice-Q6hAl1zM:__choice__ == 'RandomForestClassifier'\n    Pipeline:Choice-Q6hAl1zM:RandomForestClassifier:n_estimators | Pipeline:Choice-Q6hAl1zM:__choice__ == 'RandomForestClassifier'\n    Pipeline:Choice-Q6hAl1zM:SVC:C | Pipeline:Choice-Q6hAl1zM:__choice__ == 'SVC'\n</code></pre>"},{"location":"examples/hpo_with_ensembling/#target-function","title":"Target Function","text":"<p>Next we establish the actual target function we wish to evaluate, that is, the function we wish to optimize. In this case, we are optimizing the accuracy of the model on the validation set.</p> <p>The target function takes a <code>Trial</code> object, which has the configuration of the pipeline to evaluate and provides utility to time, and return the results of the evaluation, whether it be a success or failure.</p> <p>We make use of a <code>PathBucket</code> to store and load the data, and the <code>Pipeline</code> we defined above to configure the pipeline with the hyperparameters we are optimizing over.</p> <p>For more details, please check out the Optimization guide for more details.</p> CodeRun <pre><code>def target_function(\n    trial: Trial,\n    bucket: PathBucket,\n    pipeline: Sequential,\n) -&gt; Trial.Report:\n    X_train, X_val, X_test, y_train, y_val, y_test = (  # (1)!\n        bucket[\"X_train.csv\"].load(),\n        bucket[\"X_val.csv\"].load(),\n        bucket[\"X_test.csv\"].load(),\n        bucket[\"y_train.npy\"].load(),\n        bucket[\"y_val.npy\"].load(),\n        bucket[\"y_test.npy\"].load(),\n    )\n    pipeline = pipeline.configure(trial.config)  #  (2)!\n    sklearn_pipeline = pipeline.build(\"sklearn\")  \n\n    try:\n        with trial.profile(\"fit\"):  #  (3)!\n            sklearn_pipeline.fit(X_train, y_train)\n    except Exception as e:\n        tb = traceback.format_exc()\n        trial.store(\n            {\n                \"exception.txt\": str(e),\n                \"config.json\": dict(trial.config),\n                \"traceback.txt\": str(tb),\n            },\n        )\n        return trial.fail()  #  (4)!\n\n    # Make our predictions with the model\n    train_predictions = sklearn_pipeline.predict(X_train)\n    val_predictions = sklearn_pipeline.predict(X_val)\n    test_predictions = sklearn_pipeline.predict(X_test)\n\n    val_probabilites = sklearn_pipeline.predict_proba(X_val)\n    val_accuracy = float(accuracy_score(val_predictions, y_val))\n\n    # Save the scores to the summary of the trial\n    trial.summary[\"train_accuracy\"] = float(accuracy_score(train_predictions, y_train))\n    trial.summary[\"validation_accuracy\"] = val_accuracy\n    trial.summary[\"test_accuracy\"] = float(accuracy_score(test_predictions, y_test))\n\n    # Save all of this to the file system\n    trial.store(  # (5)!\n        {\n            \"config.json\": dict(trial.config),\n            \"scores.json\": trial.summary,\n            \"model.pkl\": sklearn_pipeline,\n            \"val_predictions.npy\": val_predictions,\n            \"val_probabilities.npy\": val_probabilites,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_accuracy)  #  (6)!\n</code></pre> <ol> <li>We can easily load data from a <code>PathBucket</code>   using the <code>load</code> method.</li> <li>We configure the pipeline with a specific set of hyperparameters suggested  by the optimizer through the <code>Trial</code> object.</li> <li>We begin the trial by timing the execution of the target function and capturing  any potential exceptions.</li> <li>If the trial failed, we return a failed report with a cost of infinity.</li> <li>We save the results of the trial using   <code>Trial.store</code>, creating a subdirectory   for this trial.</li> <li>We return a successful report with the cost of the trial, which is the inverse of the validation accuracy.</li> </ol> <p></p> <p>Next we define a simple <code>@dataclass</code> to store the our definition of an Esemble, which is simply a collection of the models trial names to their weight in the ensemble. We also store the trajectory of the ensemble, which is a list of tuples of the trial name and the weight of the trial at that point in the trajectory. Finally, we store the configuration of each trial in the ensemble.</p> <p>We could of course add extra functionality to the Ensemble, give it references to the <code>PathBucket</code> and the pipeline objects, and even add methods to train the ensemble, but for the sake of simplicity we will leave it as is.</p> CodeRun <pre><code>@dataclass\nclass Ensemble:\n    weights: dict[str, float]\n    trajectory: list[tuple[str, float]]\n    configs: dict[str, dict[str, Any]]\n\n\ndef create_ensemble(\n    history: History,\n    bucket: PathBucket,\n    /,\n    size: int = 5,\n    seed: int = 42,\n) -&gt; Ensemble:\n    if len(history) == 0:\n        return Ensemble({}, [], {})\n\n    validation_predictions = {\n        report.name: report.retrieve(\"val_probabilities.npy\") for report in history\n    }\n    targets = bucket[\"y_val.npy\"].load()\n\n    accuracy: Callable[[np.ndarray, np.ndarray], float] = accuracy_score  # type: ignore\n\n    def _score(_targets: np.ndarray, ensembled_probabilities: np.ndarray) -&gt; float:\n        predictions = probabilities_to_classes(ensembled_probabilities, classes=[0, 1])\n        return accuracy(_targets, predictions)\n\n    weights, trajectory, final_probabilities = weighted_ensemble_caruana(  \n        model_predictions=validation_predictions,  \n        targets=targets,  \n        size=size,  \n        metric=_score,  \n        select=max,  \n        seed=seed,  \n    )  \n\n    configs = {name: history[name].retrieve(\"config.json\") for name in weights}\n    return Ensemble(weights=weights, trajectory=trajectory, configs=configs)\n</code></pre> <p></p>"},{"location":"examples/hpo_with_ensembling/#main","title":"Main","text":"<p>Finally we come to the main script that runs everything.</p> CodeRun <pre><code>seed = 42\n\nX_train, X_val, X_test, y_train, y_val, y_test = get_dataset(seed)  # (1)!\n\npath = Path(\"example-hpo-with-ensembling\")\nif path.exists():\n    shutil.rmtree(path)\n\nbucket = PathBucket(path)\nbucket.store(  # (2)!\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nscheduler = Scheduler.with_processes()  # (3)!\noptimizer = SMACOptimizer.create(\n    space=pipeline,\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0, 1)),\n    bucket=path,\n    seed=seed,\n)  # (4)!\n\ntask = scheduler.task(target_function)  # (5)!\nensemble_task = scheduler.task(create_ensemble)  # (6)!\n\ntrial_history = History()\nensembles: list[Ensemble] = []\n\n\n@scheduler.on_start  # (7)!\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@task.on_result\ndef tell_optimizer(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n\n\n@task.on_result\ndef add_to_history(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n\n\n@task.on_result\ndef launch_ensemble_task(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When a task successfully completes, launch an ensemble task.\"\"\"\n    if report.status is Trial.Status.SUCCESS:\n        ensemble_task.submit(trial_history, bucket)\n\n\n@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@ensemble_task.on_result\ndef save_ensemble(future: Future, ensemble: Ensemble) -&gt; None:\n    \"\"\"When an ensemble task returns, save it.\"\"\"\n    ensembles.append(ensemble)\n\n\n@ensemble_task.on_exception\ndef print_ensemble_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@task.on_exception\ndef print_task_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@scheduler.on_timeout\ndef run_last_ensemble_task() -&gt; None:\n    \"\"\"When the scheduler is empty, run the last ensemble task.\"\"\"\n    ensemble_task.submit(trial_history, bucket)\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=5, wait=True)  # (8)!\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n\n    best_ensemble = max(ensembles, key=lambda e: e.trajectory[-1])\n\n    print(\"Best ensemble:\")\n    print(best_ensemble)\n</code></pre> <ol> <li>We use <code>get_dataset()</code> defined earlier to load the  dataset.</li> <li>We use <code>store()</code> to store the data in the bucket, with  each key being the name of the file and the value being the data.</li> <li>We use <code>Scheduler.with_processes()</code>  create a <code>Scheduler</code> that runs everything  in a different process. You can of course use a different backend if you want.</li> <li>We use <code>SMACOptimizer.create()</code> to create a  <code>SMACOptimizer</code> given the space from the pipeline  to optimize over.</li> <li>We create a <code>Task</code> that will run our objective, passing  in the function to run and the scheduler for where to run it</li> <li>We use <code>task()</code> to create a  <code>Task</code> for the <code>create_ensemble</code> method above.  This will also run in parallel with the hpo trials if using a non-sequential scheduling mode.</li> <li>We use <code>@scheduler.on_start()</code> hook to register a  callback that will be called when the scheduler starts. We can use the  <code>repeat</code> argument to make sure it's called many times if we want.</li> <li>We use <code>Scheduler.run()</code> to run the scheduler.  Here we set it to run briefly for 5 seconds and wait for remaining tasks to finish  before continuing.</li> </ol> <p></p>"},{"location":"examples/pytorch-example/","title":"Script for building and evaluating an example of PyTorch MLP model on MNIST dataset.","text":"Expand to copy <code>examples/pytorch-example.py</code>  (top right) <pre><code>from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport torch\nimport torch.nn.functional as f\nfrom torch import nn, optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import datasets, transforms\n\nfrom amltk import Choice, Component, Metric, Sequential\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pytorch import (\n    MatchChosenDimensions,\n    MatchDimensions,\n    build_model_from_pipeline,\n)\n\nif TYPE_CHECKING:\n    from amltk import Node, Trial\n\nfrom rich import print\n\n\ndef test(\n    model: nn.Module,\n    device: torch.device,\n    test_loader: torch.utils.data.DataLoader,\n) -&gt; tuple[float, float]:\n    \"\"\"Evaluate the performance of the model on the test dataset.\n\n    Args:\n        model (nn.Module): The model to be evaluated.\n        device (torch.device): The device to use for evaluation.\n        test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n\n    Returns:\n        tuple[float, float]: Test loss and accuracy.\n    \"\"\"\n    model.eval()\n    test_loss = 0.0\n    correct = 0.0\n    with torch.no_grad():\n        for _test_data, _test_target in test_loader:\n            test_data, test_target = _test_data.to(device), _test_target.to(device)\n            output = model(test_data)\n            test_loss += f.nll_loss(output, test_target, reduction=\"sum\").item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(test_target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = 100.0 * correct / len(test_loader.dataset)\n    return float(test_loss), float(accuracy)\n\n\ndef eval_configuration(\n    trial: Trial,\n    pipeline: Node,\n    device: str = \"cpu\",  # Change if you have a GPU\n    epochs: int = 1,  # Fixed for now\n    lr: float = 0.1,  # Fixed for now\n    gamma: float = 0.7,  # Fixed for now\n    batch_size: int = 64,  # Fixed for now\n    log_interval: int = 10,  # Fixed for now\n) -&gt; Trial.Report:\n    \"\"\"Evaluates a configuration within the given trial.\n\n    This function trains a model based on the provided pipeline and hyperparameters,\n    evaluates its performance, and returns a report containing the evaluation results.\n\n    Args:\n        trial: The trial object for storing trial-specific information.\n        pipeline: The pipeline defining the model architecture.\n        device: The device to use for training and evaluation (default is \"cpu\").\n        epochs: The number of training epochs (default is 1).\n        lr: The learning rate for the optimizer (default is 0.1).\n        gamma: The gamma value for the learning rate scheduler (default is 0.7).\n        batch_size: The batch size for training and evaluation (default is 64).\n        log_interval: The interval for logging training progress (default is 10).\n\n    Returns:\n        Trial.Report: A report containing the evaluation results.\n    \"\"\"\n    trial.store({\"config.json\": pipeline.config})\n    torch.manual_seed(trial.seed)\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            \"../data\",\n            train=True,\n            download=False,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))],\n            ),\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            \"../data\",\n            train=False,\n            download=False,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))],\n            ),\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n\n    _device = torch.device(device)\n    print(\"Using device\", _device)\n\n    model = (\n        pipeline.configure(trial.config)\n        .build(builder=build_model_from_pipeline)\n        .to(_device)\n    )\n\n    with trial.profile(\"training\"):\n        optimizer = optim.Adadelta(model.parameters(), lr=lr)\n        lr_scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n\n        for epoch in range(epochs):\n            for batch_idx, (_data, _target) in enumerate(train_loader):\n                optimizer.zero_grad()\n                data, target = _data.to(_device), _target.to(_device)\n\n                output = model(data)\n                loss = f.nll_loss(output, target)\n\n                loss.backward()\n                optimizer.step()\n\n                if batch_idx % log_interval == 0:\n                    print(\n                        \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                            epoch,\n                            batch_idx * len(data),\n                            len(train_loader.dataset),\n                            100.0 * batch_idx / len(train_loader),\n                            loss.item(),\n                        ),\n                    )\n                    lr_scheduler.step()\n\n    final_train_loss, final_train_acc = test(model, _device, train_loader)\n    final_test_loss, final_test_acc = test(model, _device, test_loader)\n    trial.summary[\"final_test_loss\"] = final_test_loss\n    trial.summary[\"final_test_accuracy\"] = final_test_acc\n    trial.summary[\"final_train_loss\"] = final_train_loss\n    trial.summary[\"final_train_accuracy\"] = final_train_acc\n\n    return trial.success(accuracy=final_test_acc)\n\n\ndef main() -&gt; None:\n    \"\"\"Main function to orchestrate the model training and evaluation process.\n\n    This function sets up the training environment, defines the search space\n    for hyperparameter optimization, and iteratively evaluates different\n    configurations using the SMAC optimizer.\n\n    Returns:\n        None\n    \"\"\"\n    # Training settings\n    _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    torch.device(_device)\n\n    # Download the dataset\n    datasets.MNIST(\"../data\", train=True, download=False)\n    datasets.MNIST(\"../data\", train=False, download=False)\n\n    # Define the pipeline with search space for hyperparameter optimization\n    pipeline: Sequential = Sequential(\n        Choice(\n            Sequential(\n                nn.Flatten(start_dim=1),\n                Component(\n                    nn.Linear,\n                    config={\"in_features\": 784, \"out_features\": 100},\n                    name=\"choice1-fc1\",\n                ),\n                name=\"choice1\",\n            ),\n            Sequential(\n                Component(\n                    nn.Conv2d,\n                    config={\n                        \"in_channels\": 1,  # MNIST images are grayscale\n                        \"out_channels\": 32,  # Number of output channels (filters)\n                        \"kernel_size\": (3, 3),  # Size of the convolutional kernel\n                        \"stride\": (1, 1),  # Stride of the convolution\n                        \"padding\": (1, 1),  # Padding to add to the input\n                    },\n                    name=\"choice2\",\n                ),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=(2, 2)),\n                nn.Flatten(start_dim=1),\n                name=\"choice2\",\n            ),\n            name=\"layer1\",\n        ),\n        Component(\n            nn.Linear,\n            config={\n                \"in_features\": MatchChosenDimensions(\n                    choice_name=\"layer1\",\n                    choices={\"choice1\": 100, \"choice2\": 32 * 14 * 14},\n                ),\n                \"out_features\": MatchDimensions(\"fc2\", param=\"in_features\"),\n            },\n            name=\"fc1\",\n        ),\n        Choice(nn.ReLU(), nn.Sigmoid(), name=\"activation\"),\n        Component(\n            nn.Linear,\n            space={\"in_features\": (10, 50), \"out_features\": (10, 30)},\n            name=\"fc2\",\n        ),\n        Component(\n            nn.Linear,\n            config={\n                \"in_features\": MatchDimensions(\"fc2\", param=\"out_features\"),\n                \"out_features\": 10,\n            },\n            name=\"fc3\",\n        ),\n        Component(nn.LogSoftmax, config={\"dim\": 1}),\n        name=\"my-mlp-pipeline\",\n    )\n\n    # Define the metric for optimization\n    metric: Metric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\n\n    # Initialize the SMAC optimizer\n    optimizer = SMACOptimizer.create(\n        space=pipeline,\n        metrics=metric,\n        seed=1,\n        bucket=\"pytorch-experiments\",\n    )\n\n    # Iteratively evaluate different configurations using the optimizer\n    trial = optimizer.ask()\n    report = eval_configuration(trial, pipeline, device=_device)\n    optimizer.tell(report)\n    print(report)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/pytorch-example/#description","title":"Description","text":"<p>The script defines functions for constructing a neural network model from a pipeline, training the model, and evaluating its performance.</p> <p>References: - PyTorch MNIST example: github.com/pytorch/examples/blob/main/mnist/main.py</p> <pre><code>from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nimport torch\nimport torch.nn.functional as f\nfrom torch import nn, optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import datasets, transforms\n\nfrom amltk import Choice, Component, Metric, Sequential\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pytorch import (\n    MatchChosenDimensions,\n    MatchDimensions,\n    build_model_from_pipeline,\n)\n\nif TYPE_CHECKING:\n    from amltk import Node, Trial\n\nfrom rich import print\n\n\ndef test(\n    model: nn.Module,\n    device: torch.device,\n    test_loader: torch.utils.data.DataLoader,\n) -&gt; tuple[float, float]:\n    \"\"\"Evaluate the performance of the model on the test dataset.\n\n    Args:\n        model (nn.Module): The model to be evaluated.\n        device (torch.device): The device to use for evaluation.\n        test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n\n    Returns:\n        tuple[float, float]: Test loss and accuracy.\n    \"\"\"\n    model.eval()\n    test_loss = 0.0\n    correct = 0.0\n    with torch.no_grad():\n        for _test_data, _test_target in test_loader:\n            test_data, test_target = _test_data.to(device), _test_target.to(device)\n            output = model(test_data)\n            test_loss += f.nll_loss(output, test_target, reduction=\"sum\").item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(test_target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = 100.0 * correct / len(test_loader.dataset)\n    return float(test_loss), float(accuracy)\n\n\ndef eval_configuration(\n    trial: Trial,\n    pipeline: Node,\n    device: str = \"cpu\",  # Change if you have a GPU\n    epochs: int = 1,  # Fixed for now\n    lr: float = 0.1,  # Fixed for now\n    gamma: float = 0.7,  # Fixed for now\n    batch_size: int = 64,  # Fixed for now\n    log_interval: int = 10,  # Fixed for now\n) -&gt; Trial.Report:\n    \"\"\"Evaluates a configuration within the given trial.\n\n    This function trains a model based on the provided pipeline and hyperparameters,\n    evaluates its performance, and returns a report containing the evaluation results.\n\n    Args:\n        trial: The trial object for storing trial-specific information.\n        pipeline: The pipeline defining the model architecture.\n        device: The device to use for training and evaluation (default is \"cpu\").\n        epochs: The number of training epochs (default is 1).\n        lr: The learning rate for the optimizer (default is 0.1).\n        gamma: The gamma value for the learning rate scheduler (default is 0.7).\n        batch_size: The batch size for training and evaluation (default is 64).\n        log_interval: The interval for logging training progress (default is 10).\n\n    Returns:\n        Trial.Report: A report containing the evaluation results.\n    \"\"\"\n    trial.store({\"config.json\": pipeline.config})\n    torch.manual_seed(trial.seed)\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            \"../data\",\n            train=True,\n            download=False,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))],\n            ),\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            \"../data\",\n            train=False,\n            download=False,\n            transform=transforms.Compose(\n                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))],\n            ),\n        ),\n        batch_size=batch_size,\n        shuffle=True,\n    )\n\n    _device = torch.device(device)\n    print(\"Using device\", _device)\n\n    model = (\n        pipeline.configure(trial.config)\n        .build(builder=build_model_from_pipeline)\n        .to(_device)\n    )\n\n    with trial.profile(\"training\"):\n        optimizer = optim.Adadelta(model.parameters(), lr=lr)\n        lr_scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n\n        for epoch in range(epochs):\n            for batch_idx, (_data, _target) in enumerate(train_loader):\n                optimizer.zero_grad()\n                data, target = _data.to(_device), _target.to(_device)\n\n                output = model(data)\n                loss = f.nll_loss(output, target)\n\n                loss.backward()\n                optimizer.step()\n\n                if batch_idx % log_interval == 0:\n                    print(\n                        \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                            epoch,\n                            batch_idx * len(data),\n                            len(train_loader.dataset),\n                            100.0 * batch_idx / len(train_loader),\n                            loss.item(),\n                        ),\n                    )\n                    lr_scheduler.step()\n\n    final_train_loss, final_train_acc = test(model, _device, train_loader)\n    final_test_loss, final_test_acc = test(model, _device, test_loader)\n    trial.summary[\"final_test_loss\"] = final_test_loss\n    trial.summary[\"final_test_accuracy\"] = final_test_acc\n    trial.summary[\"final_train_loss\"] = final_train_loss\n    trial.summary[\"final_train_accuracy\"] = final_train_acc\n\n    return trial.success(accuracy=final_test_acc)\n\n\ndef main() -&gt; None:\n    \"\"\"Main function to orchestrate the model training and evaluation process.\n\n    This function sets up the training environment, defines the search space\n    for hyperparameter optimization, and iteratively evaluates different\n    configurations using the SMAC optimizer.\n\n    Returns:\n        None\n    \"\"\"\n    # Training settings\n    _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    torch.device(_device)\n\n    # Download the dataset\n    datasets.MNIST(\"../data\", train=True, download=False)\n    datasets.MNIST(\"../data\", train=False, download=False)\n\n    # Define the pipeline with search space for hyperparameter optimization\n    pipeline: Sequential = Sequential(\n        Choice(\n            Sequential(\n                nn.Flatten(start_dim=1),\n                Component(\n                    nn.Linear,\n                    config={\"in_features\": 784, \"out_features\": 100},\n                    name=\"choice1-fc1\",\n                ),\n                name=\"choice1\",\n            ),\n            Sequential(\n                Component(\n                    nn.Conv2d,\n                    config={\n                        \"in_channels\": 1,  # MNIST images are grayscale\n                        \"out_channels\": 32,  # Number of output channels (filters)\n                        \"kernel_size\": (3, 3),  # Size of the convolutional kernel\n                        \"stride\": (1, 1),  # Stride of the convolution\n                        \"padding\": (1, 1),  # Padding to add to the input\n                    },\n                    name=\"choice2\",\n                ),\n                nn.ReLU(),\n                nn.MaxPool2d(kernel_size=(2, 2)),\n                nn.Flatten(start_dim=1),\n                name=\"choice2\",\n            ),\n            name=\"layer1\",\n        ),\n        Component(\n            nn.Linear,\n            config={\n                \"in_features\": MatchChosenDimensions(\n                    choice_name=\"layer1\",\n                    choices={\"choice1\": 100, \"choice2\": 32 * 14 * 14},\n                ),\n                \"out_features\": MatchDimensions(\"fc2\", param=\"in_features\"),\n            },\n            name=\"fc1\",\n        ),\n        Choice(nn.ReLU(), nn.Sigmoid(), name=\"activation\"),\n        Component(\n            nn.Linear,\n            space={\"in_features\": (10, 50), \"out_features\": (10, 30)},\n            name=\"fc2\",\n        ),\n        Component(\n            nn.Linear,\n            config={\n                \"in_features\": MatchDimensions(\"fc2\", param=\"out_features\"),\n                \"out_features\": 10,\n            },\n            name=\"fc3\",\n        ),\n        Component(nn.LogSoftmax, config={\"dim\": 1}),\n        name=\"my-mlp-pipeline\",\n    )\n\n    # Define the metric for optimization\n    metric: Metric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\n\n    # Initialize the SMAC optimizer\n    optimizer = SMACOptimizer.create(\n        space=pipeline,\n        metrics=metric,\n        seed=1,\n        bucket=\"pytorch-experiments\",\n    )\n\n    # Iteratively evaluate different configurations using the optimizer\n    trial = optimizer.ask()\n    report = eval_configuration(trial, pipeline, device=_device)\n    optimizer.tell(report)\n    print(report)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/sklearn-hpo-cv/","title":"Random Search with CVEvaluation.","text":"Expand to copy <code>examples/sklearn-hpo-cv.py</code>  (top right) <pre><code>from collections.abc import Mapping\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport openml\nimport pandas as pd\nfrom ConfigSpace import Categorical, Integer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import get_scorer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nfrom amltk.optimization.optimizers.random_search import RandomSearch\nfrom amltk.optimization.trial import Metric, Trial\nfrom amltk.pipeline import Choice, Component, Node, Sequential, Split, request\nfrom amltk.sklearn import CVEvaluation\n\n\ndef get_fold(\n    openml_task_id: int,\n    fold: int,\n) -&gt; tuple[\n    pd.DataFrame,\n    pd.DataFrame,\n    pd.DataFrame | pd.Series,\n    pd.DataFrame | pd.Series,\n]:\n    \"\"\"Get the data for a specific fold of an OpenML task.\n\n    Args:\n        openml_task_id: The OpenML task id.\n        fold: The fold number.\n        n_splits: The number of splits that will be applied. This is used\n            to resample training data such that enough at least instance for each class is present for\n            every stratified split.\n        seed: The random seed to use for reproducibility of resampling if necessary.\n    \"\"\"\n    task = openml.tasks.get_task(\n        openml_task_id,\n        download_splits=True,\n        download_data=True,\n        download_qualities=True,\n        download_features_meta_data=True,\n    )\n    train_idx, test_idx = task.get_train_test_split_indices(fold=fold)\n    X, y = task.get_X_and_y(dataset_format=\"dataframe\")  # type: ignore\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n    return X_train, X_test, y_train, y_test\n\n\npreprocessing = Split(\n    {\n        \"numerical\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n        \"categorical\": [\n            Component(\n                OrdinalEncoder,\n                config={\n                    \"categories\": \"auto\",\n                    \"handle_unknown\": \"use_encoded_value\",\n                    \"unknown_value\": -1,\n                    \"encoded_missing_value\": -2,\n                },\n            ),\n            Choice(\n                \"passthrough\",\n                Component(\n                    OneHotEncoder,\n                    space={\"max_categories\": (2, 20)},\n                    config={\n                        \"categories\": \"auto\",\n                        \"drop\": None,\n                        \"sparse_output\": False,\n                        \"handle_unknown\": \"infrequent_if_exist\",\n                    },\n                ),\n                name=\"one_hot\",\n            ),\n        ],\n    },\n    name=\"preprocessing\",\n)\n\n\ndef rf_config_transform(config: Mapping[str, Any], _: Any) -&gt; dict[str, Any]:\n    new_config = dict(config)\n    if new_config[\"class_weight\"] == \"None\":\n        new_config[\"class_weight\"] = None\n    return new_config\n\n\n# NOTE: This space should not be used for evaluating how good this RF is\n# vs other algorithms\nrf_classifier = Component(\n    item=RandomForestClassifier,\n    config_transform=rf_config_transform,\n    space={\n        \"criterion\": [\"gini\", \"entropy\"],\n        \"max_features\": Categorical(\n            \"max_features\",\n            list(np.logspace(0.1, 1, base=10, num=10) / 10),\n            ordered=True,\n        ),\n        \"min_samples_split\": Integer(\"min_samples_split\", bounds=(2, 20), default=2),\n        \"min_samples_leaf\": Integer(\"min_samples_leaf\", bounds=(1, 20), default=1),\n        \"bootstrap\": Categorical(\"bootstrap\", [True, False], default=True),\n        \"class_weight\": [\"balanced\", \"balanced_subsample\", \"None\"],\n        \"min_impurity_decrease\": (1e-9, 1e-1),\n    },\n    config={\n        \"random_state\": request(\n            \"random_state\",\n            default=None,\n        ),  # Will be provided later by the `Trial`\n        \"n_estimators\": 512,\n        \"max_depth\": None,\n        \"min_weight_fraction_leaf\": 0.0,\n        \"max_leaf_nodes\": None,\n        \"warm_start\": False,  # False due to no iterative fit used here\n        \"n_jobs\": 1,\n    },\n)\n\nrf_pipeline = Sequential(preprocessing, rf_classifier, name=\"rf_pipeline\")\n\n\ndef do_something_after_a_split_was_evaluated(\n    trial: Trial,\n    fold: int,\n    info: CVEvaluation.PostSplitInfo,\n) -&gt; CVEvaluation.PostSplitInfo:\n    return info\n\n\ndef do_something_after_a_complete_trial_was_evaluated(\n    report: Trial.Report,\n    pipeline: Node,\n    info: CVEvaluation.CompleteEvalInfo,\n) -&gt; Trial.Report:\n    return report\n\n\ndef main() -&gt; None:\n    random_seed = 42\n    openml_task_id = 31  # Adult dataset, classification\n    task_hint = \"classification\"\n    outer_fold_number = (\n        0  # Only run the first outer fold, wrap this in a loop if needs be, with a unique history file\n        # for each one\n    )\n    optimizer_cls = RandomSearch\n    working_dir = Path(\"example-sklearn-hpo-cv\").absolute()\n    results_to = working_dir / \"results.parquet\"\n    inner_fold_seed = random_seed + outer_fold_number\n    metric_definition = Metric(\n        \"accuracy\",\n        minimize=False,\n        bounds=(0, 1),\n        fn=get_scorer(\"accuracy\"),\n    )\n\n    per_process_memory_limit = None  # (4, \"GB\")  # NOTE: May have issues on Mac\n    per_process_walltime_limit = None  # (60, \"s\")\n\n    debugging = False\n    if debugging:\n        max_trials = 1\n        max_time = 30\n        n_workers = 1\n        # raise an error with traceback, something went wrong\n        on_trial_exception = \"raise\"\n        display = True\n        wait_for_all_workers_to_finish = True\n    else:\n        max_trials = 10\n        max_time = 300\n        n_workers = 4\n        # Just mark the trial as fail and move on to the next one\n        on_trial_exception = \"continue\"\n        display = True\n        wait_for_all_workers_to_finish = False\n\n    X, X_test, y, y_test = get_fold(\n        openml_task_id=openml_task_id,\n        fold=outer_fold_number,\n    )\n\n    # This object below is a highly customizable class to create a function that we can use for\n    # evaluating pipelines.\n    evaluator = CVEvaluation(\n        # Provide data, number of times to split, cross-validation and a hint of the task type\n        X,\n        y,\n        splitter=\"cv\",\n        n_splits=8,\n        task_hint=task_hint,\n        # Seeding for reproducibility\n        random_state=inner_fold_seed,\n        # Provide test data to get test scores\n        X_test=X_test,\n        y_test=y_test,\n        # Record training scores\n        train_score=True,\n        # Where to store things\n        working_dir=working_dir,\n        # What to do when something goes wrong.\n        on_error=\"raise\" if on_trial_exception == \"raise\" else \"fail\",\n        # Whether you want models to be store on disk under working_dir\n        store_models=False,\n        # A callback to be called at the end of each split\n        post_split=do_something_after_a_split_was_evaluated,\n        # Some callback that is called at the end of all fold evaluations\n        post_processing=do_something_after_a_complete_trial_was_evaluated,\n        # Whether the post_processing callback requires models will required models, i.e.\n        # to compute some bagged average over all fold models. If `False` will discard models eagerly\n        # to sasve sapce.\n        post_processing_requires_models=False,\n        # This handles edge cases related to stratified splitting when there are too\n        # few instances of a specific class. May wish to disable if your passing extra fit params\n        rebalance_if_required_for_stratified_splitting=True,\n        # Extra parameters requested by sklearn models/group splitters or metrics,\n        # such as `sample_weight`\n        params=None,\n    )\n\n    # Here we just use the `optimize` method to setup and run an optimization loop\n    # with `n_workers`. Please either look at the source code for `optimize` or\n    # refer to the `Scheduler` and `Optimizer` guide if you need more fine grained control.\n    # If you need to evaluate a certain configuraiton, you can create your own `Trial` object.\n    #\n    # trial = Trial.create(name=...., info=None, config=..., bucket=..., seed=..., metrics=metric_def)\n    # report = evaluator.evaluate(trial, rf_pipeline)\n    # print(report)\n    #\n    history = rf_pipeline.optimize(\n        target=evaluator.fn,\n        metric=metric_definition,\n        optimizer=optimizer_cls,\n        seed=inner_fold_seed,\n        process_memory_limit=per_process_memory_limit,\n        process_walltime_limit=per_process_walltime_limit,\n        working_dir=working_dir,\n        max_trials=max_trials,\n        timeout=max_time,\n        display=display,\n        wait=wait_for_all_workers_to_finish,\n        n_workers=n_workers,\n        on_trial_exception=on_trial_exception,\n    )\n\n    df = history.df()\n\n    # Assign some new information to the dataframe\n    df.assign(\n        outer_fold=outer_fold_number,\n        inner_fold_seed=inner_fold_seed,\n        task_id=openml_task_id,\n        max_trials=max_trials,\n        max_time=max_time,\n        optimizer=optimizer_cls.__name__,\n        n_workers=n_workers,\n    )\n    print(df)\n    print(f\"Saving dataframe of results to path: {results_to}\")\n    df.to_parquet(results_to)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/sklearn-hpo-cv/#description","title":"Description","text":"<p>This example demonstrates the <code>CVEvaluation</code> class, which builds a custom cross-validation task that can be used to evaluate <code>pipelines</code> with cross-validation, using <code>RandomSearch</code>.</p> <pre><code>from collections.abc import Mapping\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport openml\nimport pandas as pd\nfrom ConfigSpace import Categorical, Integer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import get_scorer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nfrom amltk.optimization.optimizers.random_search import RandomSearch\nfrom amltk.optimization.trial import Metric, Trial\nfrom amltk.pipeline import Choice, Component, Node, Sequential, Split, request\nfrom amltk.sklearn import CVEvaluation\n\n\ndef get_fold(\n    openml_task_id: int,\n    fold: int,\n) -&gt; tuple[\n    pd.DataFrame,\n    pd.DataFrame,\n    pd.DataFrame | pd.Series,\n    pd.DataFrame | pd.Series,\n]:\n    \"\"\"Get the data for a specific fold of an OpenML task.\n\n    Args:\n        openml_task_id: The OpenML task id.\n        fold: The fold number.\n        n_splits: The number of splits that will be applied. This is used\n            to resample training data such that enough at least instance for each class is present for\n            every stratified split.\n        seed: The random seed to use for reproducibility of resampling if necessary.\n    \"\"\"\n    task = openml.tasks.get_task(\n        openml_task_id,\n        download_splits=True,\n        download_data=True,\n        download_qualities=True,\n        download_features_meta_data=True,\n    )\n    train_idx, test_idx = task.get_train_test_split_indices(fold=fold)\n    X, y = task.get_X_and_y(dataset_format=\"dataframe\")  # type: ignore\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n    return X_train, X_test, y_train, y_test\n\n\npreprocessing = Split(\n    {\n        \"numerical\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n        \"categorical\": [\n            Component(\n                OrdinalEncoder,\n                config={\n                    \"categories\": \"auto\",\n                    \"handle_unknown\": \"use_encoded_value\",\n                    \"unknown_value\": -1,\n                    \"encoded_missing_value\": -2,\n                },\n            ),\n            Choice(\n                \"passthrough\",\n                Component(\n                    OneHotEncoder,\n                    space={\"max_categories\": (2, 20)},\n                    config={\n                        \"categories\": \"auto\",\n                        \"drop\": None,\n                        \"sparse_output\": False,\n                        \"handle_unknown\": \"infrequent_if_exist\",\n                    },\n                ),\n                name=\"one_hot\",\n            ),\n        ],\n    },\n    name=\"preprocessing\",\n)\n\n\ndef rf_config_transform(config: Mapping[str, Any], _: Any) -&gt; dict[str, Any]:\n    new_config = dict(config)\n    if new_config[\"class_weight\"] == \"None\":\n        new_config[\"class_weight\"] = None\n    return new_config\n\n\n# NOTE: This space should not be used for evaluating how good this RF is\n# vs other algorithms\nrf_classifier = Component(\n    item=RandomForestClassifier,\n    config_transform=rf_config_transform,\n    space={\n        \"criterion\": [\"gini\", \"entropy\"],\n        \"max_features\": Categorical(\n            \"max_features\",\n            list(np.logspace(0.1, 1, base=10, num=10) / 10),\n            ordered=True,\n        ),\n        \"min_samples_split\": Integer(\"min_samples_split\", bounds=(2, 20), default=2),\n        \"min_samples_leaf\": Integer(\"min_samples_leaf\", bounds=(1, 20), default=1),\n        \"bootstrap\": Categorical(\"bootstrap\", [True, False], default=True),\n        \"class_weight\": [\"balanced\", \"balanced_subsample\", \"None\"],\n        \"min_impurity_decrease\": (1e-9, 1e-1),\n    },\n    config={\n        \"random_state\": request(\n            \"random_state\",\n            default=None,\n        ),  # Will be provided later by the `Trial`\n        \"n_estimators\": 512,\n        \"max_depth\": None,\n        \"min_weight_fraction_leaf\": 0.0,\n        \"max_leaf_nodes\": None,\n        \"warm_start\": False,  # False due to no iterative fit used here\n        \"n_jobs\": 1,\n    },\n)\n\nrf_pipeline = Sequential(preprocessing, rf_classifier, name=\"rf_pipeline\")\n\n\ndef do_something_after_a_split_was_evaluated(\n    trial: Trial,\n    fold: int,\n    info: CVEvaluation.PostSplitInfo,\n) -&gt; CVEvaluation.PostSplitInfo:\n    return info\n\n\ndef do_something_after_a_complete_trial_was_evaluated(\n    report: Trial.Report,\n    pipeline: Node,\n    info: CVEvaluation.CompleteEvalInfo,\n) -&gt; Trial.Report:\n    return report\n\n\ndef main() -&gt; None:\n    random_seed = 42\n    openml_task_id = 31  # Adult dataset, classification\n    task_hint = \"classification\"\n    outer_fold_number = (\n        0  # Only run the first outer fold, wrap this in a loop if needs be, with a unique history file\n        # for each one\n    )\n    optimizer_cls = RandomSearch\n    working_dir = Path(\"example-sklearn-hpo-cv\").absolute()\n    results_to = working_dir / \"results.parquet\"\n    inner_fold_seed = random_seed + outer_fold_number\n    metric_definition = Metric(\n        \"accuracy\",\n        minimize=False,\n        bounds=(0, 1),\n        fn=get_scorer(\"accuracy\"),\n    )\n\n    per_process_memory_limit = None  # (4, \"GB\")  # NOTE: May have issues on Mac\n    per_process_walltime_limit = None  # (60, \"s\")\n\n    debugging = False\n    if debugging:\n        max_trials = 1\n        max_time = 30\n        n_workers = 1\n        # raise an error with traceback, something went wrong\n        on_trial_exception = \"raise\"\n        display = True\n        wait_for_all_workers_to_finish = True\n    else:\n        max_trials = 10\n        max_time = 300\n        n_workers = 4\n        # Just mark the trial as fail and move on to the next one\n        on_trial_exception = \"continue\"\n        display = True\n        wait_for_all_workers_to_finish = False\n\n    X, X_test, y, y_test = get_fold(\n        openml_task_id=openml_task_id,\n        fold=outer_fold_number,\n    )\n\n    # This object below is a highly customizable class to create a function that we can use for\n    # evaluating pipelines.\n    evaluator = CVEvaluation(\n        # Provide data, number of times to split, cross-validation and a hint of the task type\n        X,\n        y,\n        splitter=\"cv\",\n        n_splits=8,\n        task_hint=task_hint,\n        # Seeding for reproducibility\n        random_state=inner_fold_seed,\n        # Provide test data to get test scores\n        X_test=X_test,\n        y_test=y_test,\n        # Record training scores\n        train_score=True,\n        # Where to store things\n        working_dir=working_dir,\n        # What to do when something goes wrong.\n        on_error=\"raise\" if on_trial_exception == \"raise\" else \"fail\",\n        # Whether you want models to be store on disk under working_dir\n        store_models=False,\n        # A callback to be called at the end of each split\n        post_split=do_something_after_a_split_was_evaluated,\n        # Some callback that is called at the end of all fold evaluations\n        post_processing=do_something_after_a_complete_trial_was_evaluated,\n        # Whether the post_processing callback requires models will required models, i.e.\n        # to compute some bagged average over all fold models. If `False` will discard models eagerly\n        # to sasve sapce.\n        post_processing_requires_models=False,\n        # This handles edge cases related to stratified splitting when there are too\n        # few instances of a specific class. May wish to disable if your passing extra fit params\n        rebalance_if_required_for_stratified_splitting=True,\n        # Extra parameters requested by sklearn models/group splitters or metrics,\n        # such as `sample_weight`\n        params=None,\n    )\n\n    # Here we just use the `optimize` method to setup and run an optimization loop\n    # with `n_workers`. Please either look at the source code for `optimize` or\n    # refer to the `Scheduler` and `Optimizer` guide if you need more fine grained control.\n    # If you need to evaluate a certain configuraiton, you can create your own `Trial` object.\n\n    # trial = Trial.create(name=...., info=None, config=..., bucket=..., seed=..., metrics=metric_def)\n    # report = evaluator.evaluate(trial, rf_pipeline)\n    # print(report)\n\n    history = rf_pipeline.optimize(\n        target=evaluator.fn,\n        metric=metric_definition,\n        optimizer=optimizer_cls,\n        seed=inner_fold_seed,\n        process_memory_limit=per_process_memory_limit,\n        process_walltime_limit=per_process_walltime_limit,\n        working_dir=working_dir,\n        max_trials=max_trials,\n        timeout=max_time,\n        display=display,\n        wait=wait_for_all_workers_to_finish,\n        n_workers=n_workers,\n        on_trial_exception=on_trial_exception,\n    )\n\n    df = history.df()\n\n    # Assign some new information to the dataframe\n    df.assign(\n        outer_fold=outer_fold_number,\n        inner_fold_seed=inner_fold_seed,\n        task_id=openml_task_id,\n        max_trials=max_trials,\n        max_time=max_time,\n        optimizer=optimizer_cls.__name__,\n        n_workers=n_workers,\n    )\n    print(df)\n    print(f\"Saving dataframe of results to path: {results_to}\")\n    df.to_parquet(results_to)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guides/","title":"Index","text":"<p>The guides here serve as a well-structured introduction to the capabilities of AutoML-Toolkit. Notably, we have three core concepts at the heart of AutoML-Toolkit, with supporting types and auxiliary functionality to enable these concepts.</p> <p>These take the form of a scheduling, a pipeline construction and optimization. By combining these concepts, we provide an extensive framework from which to do AutoML research, utilize AutoML for your task or build brand new AutoML systems.</p> <ul> <li> <p>Scheduling</p> <p>Dealing with multiple processes and simultaneous compute, can be both difficult in terms of understanding and utilization. Often a prototype script just doesn't work when you need to run larger experiments.</p> <p>We provide an event-driven system with a flexible backend, to help you write code that scales from just a few more cores on your machine to utilizing an entire cluster.</p> <p>This guide introduces <code>Task</code>s and the <code>Scheduler</code> in which they run, as well as <code>@events</code> which you can subscribe callbacks to. Define what should run, when it should run and simply define a callback to say what should happen when it's done.</p> <p>This framework allows you to write code that simply scales, with as little code change required as possible. Go from a single local process to an entire cluster with the same script and 5 lines of code.</p> <p>Checkout the Scheduling guide! for the full guide. We also cover some of these topics in brief detail in the reference pages.</p> <p>Notable Features</p> <ul> <li>A system that allows incremental and encapsulated feature addition.</li> <li>An <code>@event</code> system with easy to use callbacks.</li> <li>Place constraints and modify your <code>Task</code>     with <code>Plugins</code></li> <li>Integrations for different backends for where to run your tasks.</li> <li>A wide set of events to plug into.</li> <li>An easy way to extend the functionality provided with your own set of domain or task     specific events.</li> </ul> </li> </ul> <ul> <li> <p>Pipelines</p> <p>Optimizer require some search space to optimize, yet provide no utility to actually define these search space. When scaling beyond a simple single model, these search space become harder to define, difficult to extend and are often disjoint from the actual pipeline creation. When you want to create search spaces that can have choices between models, parametrized pre-processing and a method to quickly change these setups, it can often feel tedious and error-prone</p> <p>By piecing together <code>Node</code>s of a pipeline, utilizing a set of different building blocks such as a <code>Component</code>, <code>Sequential</code>, <code>Choice</code>es and more, you can abstractly define your entire pipeline. Once you're done, we'll stitch together the entire <code>search_space()</code>, allow you to easily <code>configure()</code> it and finally <code>build()</code> it into a concrete object you can use, all in the same place.</p> <p>Checkout the Pipeline guide! We also cover some of these topics in brief detail in the reference pages.</p> <p>Notable Features</p> <ul> <li>An easy, declarative pipeline structure, allowing for rapid addition, deletion and   modification during experimentation.</li> <li>A flexible pipeline capable of handling complex structures and subpipelines.</li> <li>Multiple component types to help you define your pipeline.</li> <li>Exporting of pipelines into concrete implementations like an sklearn.pipeline.Pipeline   for use in your downstream tasks.</li> <li>Extensible to add your own component types and <code>builder=</code>s to use.</li> </ul> </li> </ul> <ul> <li> <p>Optimization</p> <p>An optimizer is the backbone behind many AutoML systems and the quickest way to improve the performance of your current pipelines. However, optimizers vary in terms of how they expect you to write code, they vary in how much control they take of your code and can be quite difficult to interact with other than their <code>run()</code> function.</p> <p>By setting a simple expectation on an <code>Optimizer</code>, e.g. that it should have an <code>ask()</code> and <code>tell()</code>, you are placed get back in terms of defining the loop, define what happens, when and you can store what you'd like to record and put it where you'd like to put it.</p> <p>By unifying their suggestions as a <code>Trial</code> and a convenient <code>Report</code> to hand back to them, you can switch between optimizers with minimal changes required. We have added a load of utility to the <code>Trial</code>'s, such that you can easily profile sections, add extra summary information, store artifacts and export DataFrames.</p> <p>Checkout the Optimization guide. We recommend reading the previous two guides to fully understand the possibilities with optimization. We also cover some of these topics in brief detail in the reference pages.</p> <p>Notable Features</p> <ul> <li>An assortment of different optimizers for you to swap in an out with relative ease through a unified interface.</li> <li>A suite of utilities to help you record that data you want from your HPO experiments.</li> <li>Full control of how you interact with it, allowing for easy warm-starting, complex swapping mechanisms or custom stopping criteria.</li> <li>A simple interface to integrate in your own optimizer.</li> </ul> </li> </ul>"},{"location":"guides/optimization/","title":"Optimization Guide","text":"<p>One of the core tasks of any AutoML system is to optimize some objective, whether it be some pipeline, a black-box or even a toy function. In the context of AMLTK, this means defining some <code>Metric(s)</code> to optimize and creating an <code>Optimizer</code> to optimize them.</p> <p>You can check out the integrated optimizers in our optimizer reference.</p> <p>This guide relies lightly on topics covered in the Pipeline Guide for creating a pipeline but also the Scheduling guide for creating a <code>Scheduler</code> and a <code>Task</code>. These aren't required but if something is not clear or you'd like to know how something works, please refer to these guides or the reference!</p>"},{"location":"guides/optimization/#optimizing-a-1-d-function","title":"Optimizing a 1-D function","text":"<p>We'll start with a simple example of maximizing a polynomial function The first thing to do is define the function we want to optimize.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef poly(x):\n    return (x**2 + 4*x + 3) / x\n\nfig, ax = plt.subplots()\nx = np.linspace(-10, 10, 100)\nax.plot(x, poly(x))\n</code></pre> 2024-08-13T07:34:43.932482 image/svg+xml Matplotlib v3.9.2, https://matplotlib.org/ <p>Our next step is to define the search range over which we want to optimize, in this case, the range of values <code>x</code> can take. Here we use a simple <code>Searchable</code>, however we can represent entire machine learning pipelines, with conditionality and much more complex ranges. (Pipeline guide)</p> <p>Vocab...</p> <p>When dealing with such functions, one might call the <code>x</code> just a parameter. However in the context of Machine Learning, if this <code>poly()</code> function was more like <code>train_model()</code>, then we would refer to <code>x</code> as a hyperparameter with it's range as it's search space.</p> <pre><code>from amltk.pipeline import Searchable\n\ndef poly(x: float) -&gt; float:\n    return (x**2 + 4*x + 3) / x\n\ns = Searchable(\n    {\"x\": (-10.0, 10.0)},\n    name=\"my-searchable\"\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Searchable(my-searchable) \u2500\u256e\n\u2502 space {'x': (-10.0, 10.0)}  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"guides/optimization/#creating-an-optimizer","title":"Creating an Optimizer","text":"<p>We'll utilize SMAC here for optimization as an example but you can find other available optimizers here.</p> Requirements <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[smac]\n\n# Or directly\npip install smac\n</code></pre> <p>The first thing we'll need to do is create a <code>Metric</code>: a definition of some value we want to optimize.</p> <pre><code>from amltk.optimization import Metric\n\nmetric = Metric(\"score\", minimize=False)\nprint(metric)\n</code></pre> <pre><code>score (maximize)\n</code></pre> <p>The next step is to actually create an optimizer, you'll have to refer to their reference documentation. However, for most integrated optimizers, we expose a helpful <code>create()</code>.</p> <pre><code>from amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.optimization import Metric\nfrom amltk.pipeline import Searchable\n\ndef poly(x: float) -&gt; float:\n    return (x**2 + 4*x + 3) / x\n\nmetric = Metric(\"score\", minimize=False)\nspace = Searchable(space={\"x\": (-10.0, 10.0)}, name=\"my-searchable\")\n\noptimizer = SMACOptimizer.create(space=space, metrics=metric, seed=42)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"guides/optimization/#running-an-optimizer","title":"Running an Optimizer","text":"<p>At this point, we can begin optimizing our function, using the <code>ask</code> to get <code>Trial</code>s and <code>tell</code> methods with <code>Trial.Report</code>s.</p> <pre><code>from amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.optimization import Metric, History, Trial\nfrom amltk.pipeline import Searchable\n\ndef poly(x: float) -&gt; float:\n    return (x**2 + 4*x + 3) / x\n\nmetric = Metric(\"score\", minimize=False)\nspace = Searchable(space={\"x\": (-10.0, 10.0)}, name=\"my-searchable\")\n\noptimizer = SMACOptimizer.create(space=space, metrics=metric, seed=42)\n\nhistory = History()\nfor _ in range(10):\n    # Get a trial from an Optimizer\n    trial: Trial = optimizer.ask()\n    print(f\"Evaluating trial {trial.name} with config {trial.config}\")\n\n    # Access the the trial's config\n    x = trial.config[\"my-searchable:x\"]\n\n    try:\n        score = poly(x)\n    except ZeroDivisionError as e:\n        # Generate a failed report (i.e. poly(x) raised divide by zero exception with x=0)\n        report = trial.fail(e)\n    else:\n        # Generate a success report\n        report = trial.success(score=score)\n\n    # Store artifacts with the trial, using file extensions to infer how to store it\n    trial.store({ \"config.json\": trial.config, \"array.npy\": [1, 2, 3] })\n\n    # Tell the Optimizer about the report\n    optimizer.tell(report)\n\n    # Add the report to the history\n    history.add(report)\n</code></pre> <pre><code>Evaluating trial config_id=1_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 5.9014238975942}\nEvaluating trial config_id=2_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -2.0745517686009}\nEvaluating trial config_id=3_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -8.2577728666365}\nEvaluating trial config_id=4_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 4.4309198483825}\nEvaluating trial config_id=5_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 0.2431046403944}\nEvaluating trial config_id=6_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -6.4137935638428}\nEvaluating trial config_id=7_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -2.5898005627096}\nEvaluating trial config_id=8_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 8.7605084478855}\nEvaluating trial config_id=9_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 8.4289555996656}\nEvaluating trial config_id=10_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -4.5996635966003}\n</code></pre> <p>And we can use the <code>History</code> to see the history of the optimization process</p> <pre><code>df = history.df()\nprint(df)\n</code></pre> <pre><code>                                                     status  ...  config:my-searchable:x\nname                                                         ...                        \nconfig_id=1_seed=1608637542_budget=None_instanc...  success  ...                5.901424\nconfig_id=2_seed=1608637542_budget=None_instanc...  success  ...               -2.074552\nconfig_id=3_seed=1608637542_budget=None_instanc...  success  ...               -8.257773\nconfig_id=4_seed=1608637542_budget=None_instanc...  success  ...                 4.43092\nconfig_id=5_seed=1608637542_budget=None_instanc...  success  ...                0.243105\nconfig_id=6_seed=1608637542_budget=None_instanc...  success  ...               -6.413794\nconfig_id=7_seed=1608637542_budget=None_instanc...  success  ...               -2.589801\nconfig_id=8_seed=1608637542_budget=None_instanc...  success  ...                8.760508\nconfig_id=9_seed=1608637542_budget=None_instanc...  success  ...                8.428956\nconfig_id=10_seed=1608637542_budget=None_instan...  success  ...               -4.599664\n\n[10 rows x 9 columns]\n</code></pre> <p>Okay so there are a few things introduced all at once here, let's go over them bit by bit.</p>"},{"location":"guides/optimization/#the-trial-object","title":"The <code>Trial</code> object","text":"<p>The <code>Trial</code> object is the main object that you'll be interacting with when optimizing. It contains a load of useful properties and functionality to help you during optimization.</p> <p>The <code>.config</code> will contain name spaced parameters, in this case, <code>my-searchable:x</code>, based on the pipeline/search space you specified.</p> <p>It's also quite typical to store artifacts with the trial, a common feature of things like TensorBoard, MLFlow, etc. We provide a primitive way to store artifacts with the trial using <code>.store()</code> which takes a dictionary of file names to file contents. The file extension is used to infer how to store the file, for example, <code>.json</code> files will be stored as JSON, <code>.npy</code> files will be stored as numpy arrays. You are of course still free to use your other favourite logging tools in conjunction with AMLTK!</p> <p>Lastly, we use <code>trial.success()</code> or <code>trial.fail()</code> which generates a <code>Trial.Report</code> for us, that we can give back to the optimizer.</p> <p>Feel free to explore the full API.</p>"},{"location":"guides/optimization/#the-history-object","title":"The <code>History</code> object","text":"<p>You may have noticed that we also created a <code>History</code> object to store our reports in. This is a simple container to store the reports together and get a dataframe out of. We may extend this with future utility such as plotting or other export formats but for now, we can use it primarily for getting our results together in one place.</p> <p>We'll create a simple example where we create our own trials and record some results on them, getting out a dataframe at the end.</p> <pre><code>from amltk.optimization import History, Trial, Metric\nfrom amltk.store import PathBucket\n\nmetric = Metric(\"score\", minimize=False, bounds=(0, 5))\nhistory = History()\n\ntrials = [\n    Trial.create(name=\"trial-1\", config={\"x\": 1.0}, metrics=[metric]),\n    Trial.create(name=\"trial-2\", config={\"x\": 2.0}, metrics=[metric]),\n    Trial.create(name=\"trial-3\", config={\"x\": 3.0}, metrics=[metric]),\n]\n\nfor trial in trials:\n    x = trial.config[\"x\"]\n    if x &gt;= 2:\n        report = trial.fail()\n    else:\n        report = trial.success(score=x)\n\n    history.add(report)\n\ndf = history.df()\nprint(df)\n\nbest = history.best()\nprint(best)\n</code></pre> <pre><code>          status  trial_seed  ... metric:score [0.0, 5.0] (maximize) config:x\nname                          ...                                            \ntrial-1  success        &lt;NA&gt;  ...                                  1        1\ntrial-2     fail        &lt;NA&gt;  ...                               &lt;NA&gt;        2\ntrial-3     fail        &lt;NA&gt;  ...                               &lt;NA&gt;        3\n\n[3 rows x 9 columns]\nTrial.Report(trial=Trial(name='trial-1', config={'x': 1.0}, bucket=PathBucket(PosixPath('trial-trial-1-2024-08-13T07:34:44.111411')), metrics=MetricCollection(metrics={'score': Metric(name='score', minimize=False, bounds=(0.0, 5.0), fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 44, 111406), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 44, 111941), exception=None, values={'score': 1.0})\n</code></pre> <p>You can use the <code>History.df()</code> method to get a dataframe of the history and use your favourite dataframe tools to analyze the results.</p>"},{"location":"guides/optimization/#optimizing-an-sklearn-pipeline","title":"Optimizing an Sklearn-Pipeline","text":"<p>To give a more concrete example, we will optimize a simple sklearn pipeline. You'll likely want to refer to the pipeline guide for more information on pipelines, but the example should be clear enough without it.</p> <p>We start with defining our pipeline.</p> <pre><code>from typing import Any\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.neural_network import MLPClassifier\n\nfrom amltk.pipeline import Sequential, Choice, Component\n\ndef dims_to_hidden_layer(config: dict[str, Any], _):\n    config = dict(config)\n    config[\"hidden_layer_sizes\"] = (config.pop(\"dim1\"), config.pop(\"dim2\"))\n    return config\n\n# A pipeline with a choice of scalers and a parametrized MLP\nmy_pipeline = (\n    Sequential(name=\"my-pipeline\")\n    &gt;&gt; Choice(\n        StandardScaler,\n        MinMaxScaler,\n        Component(RobustScaler, space={\"with_scaling\": [True, False], \"unit_variance\": [True, False]}),\n        name=\"scaler\",\n    )\n    &gt;&gt; Component(\n        MLPClassifier,\n        space={\n            \"dim1\": (1, 10),\n            \"dim2\": (1, 10),\n            \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n        },\n        config_transform=dims_to_hidden_layer,\n    )\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Sequential(my-pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Choice(scaler) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(MinMaxSc\u2500\u256e \u256d\u2500 Component(RobustSc\u2500\u256e \u256d\u2500 Component(StandardS\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item class          \u2502 \u2502 item  class         \u2502 \u2502 item class           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      MinMaxScaler(\u2026 \u2502 \u2502       RobustScaler\u2026 \u2502 \u2502      StandardScaler\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 space {             \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502                         \u2502           'with_sc\u2026 \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502       [             \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               True, \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               False \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502           ],        \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502           'unit_va\u2026 \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502       [             \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               True, \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               False \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502           ]         \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502       }             \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                      \u2193                                       \u2502\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                 \u2502\n\u2502 \u2502 item      class MLPClassifier(...)                       \u2502                 \u2502\n\u2502 \u2502 space     {                                              \u2502                 \u2502\n\u2502 \u2502               'dim1': (1, 10),                           \u2502                 \u2502\n\u2502 \u2502               'dim2': (1, 10),                           \u2502                 \u2502\n\u2502 \u2502               'activation': [                            \u2502                 \u2502\n\u2502 \u2502                   'relu',                                \u2502                 \u2502\n\u2502 \u2502                   'tanh',                                \u2502                 \u2502\n\u2502 \u2502                   'logistic'                             \u2502                 \u2502\n\u2502 \u2502               ]                                          \u2502                 \u2502\n\u2502 \u2502           }                                              \u2502                 \u2502\n\u2502 \u2502 transform def dims_to_hidden_layer(...)                  \u2502                 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>Next up, we need to define a simple target function we want to evaluate on.</p> <pre><code>from sklearn.model_selection import cross_validate\nfrom amltk.optimization import Trial\nfrom amltk.store import Stored\nimport numpy as np\n\ndef evaluate(\n    trial: Trial,\n    pipeline: Sequential,\n    X: Stored[np.ndarray],\n    y: Stored[np.ndarray],\n) -&gt; Trial.Report:\n    # Configure our pipeline and build it\n    sklearn_pipeline = (\n        pipeline\n        .configure(trial.config)\n        .build(\"sklearn\")\n    )\n\n    # Load in our data\n    X = X.load()\n    y = y.load()\n\n    # Use sklearns.cross_validate as our evaluator\n    with trial.profile(\"cross-validate\"):\n        results = cross_validate(sklearn_pipeline, X, y, scoring=\"accuracy\", cv=3, return_estimator=True)\n\n    test_scores = results[\"test_score\"]\n    estimators = results[\"estimator\"]  # You can store these if you like (you'll likely want to use the `.pkl` suffix for the filename)\n\n    # Report the mean test score\n    mean_test_score = np.mean(test_scores)\n    return trial.success(acc=mean_test_score)\n</code></pre> <pre><code>\n</code></pre> <p>With that, we'll also store our data, so that on each evaluate call, we load it in. This doesn't make much sense for a single in-process call but when scaling up to using multiple processes or remote compute, this is a good practice to follow.</p> <p>For this we use a <code>PathBucket</code> and get a <code>Stored</code> from it, a reference to some object we can <code>load()</code> back in later.</p> <pre><code>from sklearn.datasets import load_iris\nfrom amltk.store import PathBucket\n\n# Load in our data\n_X, _y = load_iris(return_X_y=True)\n\n# Store our data in a bucket\nbucket = PathBucket(\"my-bucket\")\nstored_X = bucket[\"X.npy\"].put(_X)\nstored_y = bucket[\"y.npy\"].put(_y)\n</code></pre> <pre><code>\n</code></pre> <p>Lastly, we'll create our optimizer and run it. In this example, we'll use the <code>SMACOptimizer</code> but you can refer to the optimizer reference for other optimizers. For basic use cases, you should be able to swap in and out the optimizer and it should work without any changes.</p> <pre><code>from amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.optimization import Metric, History\n\nmetric = Metric(\"acc\", minimize=False, bounds=(0, 1))\noptimizer = SMACOptimizer.create(\n    space=my_pipeline,  # Let it know what to optimize\n    metrics=metric,  # And let it know what to expect\n    bucket=bucket,  # And where to store artifacts for trials and optimizer output\n)\n\nhistory = History()\n\nfor _ in range(10):\n    # Get a trial from the optimizer\n    trial = optimizer.ask()\n\n    # Evaluate the trial\n    report = evaluate(trial=trial, pipeline=my_pipeline, X=stored_X, y=stored_y)\n\n    # Tell the optimizer about the report\n    optimizer.tell(report)\n\n    # Add the report to the history\n    history.add(report)\n\ndf = history.df()\n</code></pre> <pre><code>                                                     status  ...  config:my-pipeline:scaler:RobustScaler:with_scaling\nname                                                         ...                                                     \nconfig_id=1_seed=869965598_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=2_seed=869965598_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=3_seed=869965598_budget=None_instance...  success  ...                                               True  \nconfig_id=4_seed=869965598_budget=None_instance...  success  ...                                              False  \nconfig_id=5_seed=869965598_budget=None_instance...  success  ...                                              False  \nconfig_id=6_seed=869965598_budget=None_instance...  success  ...                                               True  \nconfig_id=7_seed=869965598_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=8_seed=869965598_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=9_seed=869965598_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=10_seed=869965598_budget=None_instanc...  success  ...                                               &lt;NA&gt;  \n\n[10 rows x 26 columns]\n</code></pre>"},{"location":"guides/pipelines/","title":"Pipelines Guide","text":"<p>AutoML-toolkit was built to support future development of AutoML systems and a central part of an AutoML system is its pipeline. The purpose of this guide is to help you understand all the utility AutoML-toolkit can provide to help you define your pipeline. We will do this by introducing concepts from the ground up, rather than top down. Please see the reference if you just want to quickly look something up.</p>"},{"location":"guides/pipelines/#introduction","title":"Introduction","text":"<p>The kinds of pipelines that exist in an AutoML system come in many different forms. For example, one might be an sklearn.pipeline.Pipeline, others might be some deep-learning pipeline, while some might even stand for some real life machinery process and the settings of these machines.</p> <p>To accommodate this, what AutoML-Toolkit provides is an abstract representation of a pipeline, to help you define its search space and also to build concrete objects in code if possible (see builders).</p> <p>We categorize this into 4 steps:</p> <ol> <li> <p>Parametrize your pipeline using the various components,     including the kinds of items in the pipeline, the search spaces and any additional configuration.     Each of the various types of components gives a syntactic meaning when performing the next steps.</p> </li> <li> <p><code>pipeline.search_space(parser=...)</code>,     Get a useable search space out of the pipeline. This can then be passed to an     <code>Optimizer</code>.</p> </li> <li> <p><code>pipeline.configure(config=...)</code>,     Configure your pipeline, either manually or using a configuration suggested by     an optimizer.</p> </li> <li> <p><code>pipeline.build(builder=...)</code>,     Build your configured pipeline definition into something useable, i.e.     an <code>sklearn.pipeline.Pipeline</code> or a     <code>torch.nn.Module</code>.</p> </li> </ol> <p>At the core of these definitions is the many <code>Nodes</code> it consists of. By combining these together, you can define a directed acyclic graph (DAG), that represents the structure of your pipeline. Here is one such sklearn example that we will build up towards.</p> ResultSource <p> <pre>\n<code>\u256d\u2500 Sequential(Classy Pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Split(preprocessing) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 config {                                                                 \u2502 \u2502\n\u2502 \u2502            'categoricals':                                               \u2502 \u2502\n\u2502 \u2502        &lt;sklearn.compose._column_transformer.make_column_selector object  \u2502 \u2502\n\u2502 \u2502        at 0x7efd5a328550&gt;,                                               \u2502 \u2502\n\u2502 \u2502            'numerics':                                                   \u2502 \u2502\n\u2502 \u2502        &lt;sklearn.compose._column_transformer.make_column_selector object  \u2502 \u2502\n\u2502 \u2502        at 0x7efd5a32a590&gt;                                                \u2502 \u2502\n\u2502 \u2502        }                                                                 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Sequential(categoricals) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerics) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u256e \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 item SimpleImputer(fill_va\u2026 \u2502 \u2502 \u2502 \u2502 item  class                  \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502      strategy='constant')   \u2502 \u2502 \u2502 \u2502       SimpleImputer(...)     \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502 space {                      \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                \u2193                \u2502 \u2502 \u2502           'strategy': [      \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'mean',        \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 item OneHotEncoder(drop='f\u2026 \u2502 \u2502 \u2502 \u2502               'median'       \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           ]                  \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502       }                      \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502                                     \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502\n\u2502 \u2502                                     \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                      \u2193                                       \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e     \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...)                              \u2502     \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100), 'criterion': ['gini', 'log_loss']} \u2502     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> Pipeline<pre><code>from sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nfrom amltk.pipeline import Component, Split, Sequential\n\nfeature_preprocessing = Split(\n    {\n        \"categoricals\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")],\n        \"numerics\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n    },\n    config={\n        \"categoricals\": make_column_selector(dtype_include=object),\n        \"numerics\": make_column_selector(dtype_include=np.number),\n    },\n    name=\"preprocessing\",\n)\n\npipeline = Sequential(\n    feature_preprocessing,\n    Component(RandomForestClassifier, space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]}),\n    name=\"Classy Pipeline\",\n)\n</code></pre> <code>rich</code> printing <p>To get the same output locally (terminal or Notebook), you can either call <code>thing.__rich()__</code>, use <code>from rich import print; print(thing)</code> or in a Notebook, simply leave it as the last object of a cell.</p> <p>Once we have our pipeline definition, extracting a search space, configuring it and building it into something useful can be done with the methods.</p> <p>Guide Requirements</p> <p>For this guide, we will be using <code>ConfigSpace</code> and <code>scikit-learn</code>, you can install them manually or as so:</p> <pre><code>pip install \"amltk[sklearn, configspace]\"\n</code></pre>"},{"location":"guides/pipelines/#component","title":"Component","text":"<p>A pipeline consists of building blocks which we can combine together to create a DAG. We will start by introducing the <code>Component</code>, the common operations, and then show how to combine them together.</p> <p>A <code>Component</code> is the most common kind of node in a pipeline. Like all parts of the pipeline, they subclass <code>Node</code>, but a <code>Component</code> signifies this is some concrete object, with a possible <code>.space</code> and <code>.config</code>.</p>"},{"location":"guides/pipelines/#definition","title":"Definition","text":"Naming Nodes <p>By default, a <code>Component</code> (or any <code>Node</code> for that matter), will use the function/classname for the <code>.name</code> of the <code>Node</code>. You can explicitly pass a <code>name=</code> as a keyword argument when constructing these.</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk.pipeline import Component\n\n@dataclass\nclass MyModel:\n    f: float\n    i: int\n    c: str\n\nmy_component = Component(\n    MyModel,\n    space={\"f\": (0.0, 1.0), \"i\": (0, 10), \"c\": [\"red\", \"green\", \"blue\"]},\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class MyModel(...)                                             \u2502\n\u2502 space {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>You can also use a function instead of a class if that is preferred.</p> <pre><code>def myfunc(f: float, i: int, c: str) -&gt; MyModel:\n    if f &lt; 0.5:\n        c = \"red\"\n    return MyModel(f=f, i=i, c=c)\n\ncomponent_with_function = Component(\n    myfunc,\n    space={\"f\": (0.0, 1.0), \"i\": (0, 10), \"c\": [\"red\", \"green\", \"blue\"]},\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Component(function) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  def myfunc(...)                                                \u2502\n\u2502 space {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"guides/pipelines/#search-space","title":"Search Space","text":"<p>If interacting with an <code>Optimizer</code>, you'll often require some search space object to pass to it. To extract a search space from a <code>Component</code>, we can call  <code>search_space(parser=...)</code>, passing in the kind of search space you'd like to get out of it.</p> <pre><code>space = my_component.search_space(\"configspace\")\nprint(space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MyModel:c, Type: Categorical, Choices: {red, green, blue}, Default: red\n    MyModel:f, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n    MyModel:i, Type: UniformInteger, Range: [0, 10], Default: 5\n</code></pre> <p>Available Search Spaces</p> <p>Please see the spaces reference</p> <p>Depending on what you pass as the <code>parser=</code> to <code>search_space(parser=...)</code>, we'll attempt to give you a valid search space. In this case, we specified <code>\"configspace\"</code> and  so we get a <code>ConfigSpace</code> implementation.</p> <p>You may also define your own <code>parser=</code> and use that if desired.</p>"},{"location":"guides/pipelines/#configure","title":"Configure","text":"<p>Pretty straight forward, but what do we do with this <code>config</code>? Well we can <code>configure(config=...)</code> the component with it.</p> <pre><code>config = space.sample_configuration()\nconfigured_component = my_component.configure(config)\n</code></pre> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)                                             \u2502\n\u2502 config {'c': 'red', 'f': 0.8512433941288, 'i': 0}                     \u2502\n\u2502 space  {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>You'll notice that each variable in the space has been set to some value. We could also manually define a config and pass that in. You are not obliged to fully specify this either.</p> <pre><code>manually_configured_component = my_component.configure({\"f\": 0.5, \"i\": 1})\n</code></pre> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)                                             \u2502\n\u2502 config {'f': 0.5, 'i': 1}                                             \u2502\n\u2502 space  {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>Immutable methods!</p> <p>One thing you may have noticed is that we assigned the result of <code>configure(config=...)</code> to a new variable. This is because we do not mutate the original <code>my_component</code> and instead return a copy with all of the <code>config</code> variables set.</p>"},{"location":"guides/pipelines/#build","title":"Build","text":"<p>To build the individual item of a <code>Component</code> we can use <code>build_item()</code> and it simply calls the <code>.item</code> with the config we have set.</p> <pre><code># Same as if we did `configured_component.item(**configured_component.config)`\nthe_built_model = configured_component.build_item()\nprint(the_built_model)\n</code></pre> <pre><code>MyModel(f=0.8512433941288, i=0, c='red')\n</code></pre> <p>However, as we'll see later, we often have multiple steps of a pipeline joined together and so we need some way to get a full object out of it that takes into account all of these items joined together. We can do this with <code>build(builder=...)</code>.</p> <pre><code>the_built_model = configured_component.build(builder=\"sklearn\")\nprint(the_built_model)\n</code></pre> <pre><code>Pipeline(steps=[('MyModel', MyModel(f=0.8512433941288, i=0, c='red'))])\n</code></pre> <p>For a look at the available arguments to pass to <code>builder=</code>, see the builder reference</p>"},{"location":"guides/pipelines/#fixed","title":"Fixed","text":"<p>Sometimes we just have some part of the pipeline with no search space and no configuration required, i.e. just some prebuilt thing. We can use the <code>Fixed</code> node type to signify this.</p> <pre><code>from amltk.pipeline import Fixed\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrozen_rf = Fixed(RandomForestClassifier(n_estimators=5))\n</code></pre> <pre><code>&lt;pre style=\"font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace;font-size:0.75rem\"&gt;\n&lt;code style=\"font-family:inherit\"&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u256d\u2500 &lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e; font-weight: bold\"&gt;Fixed&lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;(&lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e; font-style: italic\"&gt;RandomForestClassifier&lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e&lt;/span&gt;\n&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u2502&lt;/span&gt; &lt;span style=\"color: #000000; text-decoration-color: #000000\"&gt;item &lt;/span&gt;&lt;span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"&gt;RandomForestClassifier&lt;/span&gt;&lt;span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"&gt;(&lt;/span&gt;&lt;span style=\"color: #808000; text-decoration-color: #808000\"&gt;n_estimators&lt;/span&gt;&lt;span style=\"color: #000000; text-decoration-color: #000000\"&gt;=&lt;/span&gt;&lt;span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"&gt;5&lt;/span&gt;&lt;span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"&gt;)&lt;/span&gt; &lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u2502&lt;/span&gt;\n&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f&lt;/span&gt;\n&lt;/code&gt;\n&lt;/pre&gt;\n</code></pre>"},{"location":"guides/pipelines/#parameter-requests","title":"Parameter Requests","text":"<p>Sometimes you may wish to explicitly specify some value should be added to the <code>.config</code> during <code>configure()</code> which would be difficult to include in the <code>config</code> directly, for example the <code>random_state</code> of an sklearn estimator. You can pass these extra parameters into <code>configure(params={...})</code>, which do not require any namespace prefixing.</p> <p>For this reason, we introduce the concept of a <code>request()</code>, allowing you to specify that a certain parameter should be added to the config during <code>configure()</code>.</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk import Component, request\n\n@dataclass\nclass MyModel:\n    f: float\n    random_state: int\n\nmy_component = Component(\n    MyModel,\n    space={\"f\": (0.0, 1.0)},\n    config={\"random_state\": request(\"seed\", default=42)}\n)\n\n# Without passing the params\nconfigured_component_no_seed = my_component.configure({\"f\": 0.5})\n\n# With passing the params\nconfigured_component_with_seed = my_component.configure({\"f\": 0.5}, params={\"seed\": 1337})\n</code></pre> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)             \u2502\n\u2502 config {'random_state': 42, 'f': 0.5} \u2502\n\u2502 space  {'f': (0.0, 1.0)}              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)               \u2502\n\u2502 config {'random_state': 1337, 'f': 0.5} \u2502\n\u2502 space  {'f': (0.0, 1.0)}                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>If you explicitly require a parameter to be set, just do not set a <code>default=</code>.</p> <pre><code>my_component = Component(\n    MyModel,\n    space={\"f\": (0.0, 1.0)},\n    config={\"random_state\": request(\"seed\")}\n)\n\nmy_component.configure({\"f\": 0.5}, params={\"seed\": 5})  # All good\n\ntry:\n    my_component.configure({\"f\": 0.5})  # Missing required parameter\nexcept ValueError as e:\n    print(e)\n</code></pre> <pre><code>Missing request=ParamRequest(_has_default=False, key='seed', default=&lt;NotSet&gt;) for Component(name='MyModel', item=&lt;class '_code_block_session_Pipeline_Parameter_Request_n1_.MyModel'&gt;, nodes=(), config={'random_state': ParamRequest(_has_default=False, key='seed', default=&lt;NotSet&gt;)}, space={'f': (0.0, 1.0)}, fidelities=None, config_transform=None, meta=None).\nparams=None\n</code></pre>"},{"location":"guides/pipelines/#config-transform","title":"Config Transform","text":"<p>Some search space and optimizers may have limitations in terms of the kinds of parameters they can support, one notable example is tuple parameters. To get around this, we can pass a <code>config_transform=</code> to <code>component</code> which will transform the config before it is passed to the <code>.item</code> during <code>build()</code>.</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk import Component\n\n@dataclass\nclass MyModel:\n    dimensions: tuple[int, int]\n\ndef config_transform(config: dict, _) -&gt; dict:\n    \"\"\"Convert \"dim1\" and \"dim2\" into a tuple.\"\"\"\n    dim1 = config.pop(\"dim1\")\n    dim2 = config.pop(\"dim2\")\n    config[\"dimensions\"] = (dim1, dim2)\n    return config\n\nmy_component = Component(\n    MyModel,\n    space={\"dim1\": (1, 10), \"dim2\": (1, 10)},\n    config_transform=config_transform,\n)\n\nconfigured_component = my_component.configure({\"dim1\": 5, \"dim2\": 5})\n</code></pre> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item      class MyModel(...)                 \u2502\n\u2502 config    {'dimensions': (5, 5)}             \u2502\n\u2502 space     {'dim1': (1, 10), 'dim2': (1, 10)} \u2502\n\u2502 transform def config_transform(...)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>Transform Context</p> <p>There may be times where you have some additional context, which you may only know at configuration time.  In this case, it is possible to pass this additional context to <code>configure(..., transform_context=...)</code>,  which will be forwarded as the second argument to your <code>.config_transform</code>.</p>"},{"location":"guides/pipelines/#sequential","title":"Sequential","text":"<p>A single component might be enough for some basic definitions but generally we need to combine multiple components together. AutoML-Toolkit is designed for large and more complex structures which can be made from simple atomic <code>Node</code>s.</p>"},{"location":"guides/pipelines/#chaining-together-nodes","title":"Chaining Together Nodes","text":"<p>We'll begin by creating two components that wrap scikit-learn estimators.</p> <pre><code>from sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom amltk.pipeline import Component\n\nimputer = Component(SimpleImputer, space={\"strategy\": [\"median\", \"mean\"]})\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n</code></pre> <pre>\n<code>\u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class SimpleImputer(...)         \u2502\n\u2502 space {'strategy': ['median', 'mean']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class RandomForestClassifier(...) \u2502\n\u2502 space {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>Infix <code>&gt;&gt;</code></p> <p>To join these two components together, we can either use the infix notation using <code>&gt;&gt;</code>, or passing them directly to a <code>Sequential</code>. However, a random name will be given when using the infix notation.</p> <pre><code>joined_components = imputer &gt;&gt; rf\n</code></pre> <pre><code>from amltk.pipeline import Sequential\npipeline = Sequential(imputer, rf, name=\"My Pipeline\")\n</code></pre> <pre>\n<code>\u256d\u2500 Sequential(My Pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e  \u2502\n\u2502 \u2502 item  class SimpleImputer(...)         \u2502  \u2502\n\u2502 \u2502 space {'strategy': ['median', 'mean']} \u2502  \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"guides/pipelines/#operations","title":"Operations","text":"<p>You can perform much of the same operations as we did for the individual node but now taking into account everything in the pipeline.</p> <pre><code>space = pipeline.search_space(\"configspace\")\nconfig = space.sample_configuration()\nconfigured_pipeline = pipeline.configure(config)\n</code></pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    My Pipeline:RandomForestClassifier:n_estimators, Type: UniformInteger, \nRange: [10, 100], Default: 55\n    My Pipeline:SimpleImputer:strategy, Type: Categorical, Choices: {median, \nmean}, Default: median\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'My Pipeline:RandomForestClassifier:n_estimators': 86,\n  'My Pipeline:SimpleImputer:strategy': 'median',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Sequential(My Pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e  \u2502\n\u2502 \u2502 item   class SimpleImputer(...)         \u2502  \u2502\n\u2502 \u2502 config {'strategy': 'median'}           \u2502  \u2502\n\u2502 \u2502 space  {'strategy': ['median', 'mean']} \u2502  \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502\n\u2502                      \u2193                       \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item   class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 config {'n_estimators': 86}              \u2502 \u2502\n\u2502 \u2502 space  {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>To build a pipeline of nodes, we simply call <code>build(builder=...)</code>. We explicitly pass the builder we want to use, which informs <code>build()</code> how to go from the abstract pipeline definition you've defined to something concrete you can use. You can find the available builders here.</p> <pre><code>from sklearn.pipeline import Pipeline as SklearnPipeline\n\nbuilt_pipeline = configured_pipeline.build(\"sklearn\")\nassert isinstance(built_pipeline, SklearnPipeline)\n</code></pre> <pre>Pipeline(steps=[('SimpleImputer', SimpleImputer(strategy='median')),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=86))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('SimpleImputer', SimpleImputer(strategy='median')),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=86))])</pre> \u00a0SimpleImputer?Documentation for SimpleImputer<pre>SimpleImputer(strategy='median')</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=86)</pre>"},{"location":"guides/pipelines/#other-building-blocks","title":"Other Building blocks","text":"<p>We saw the basic building block of a <code>Component</code>, but AutoML-Toolkit also provides support for some other kinds of building blocks. These building blocks can be attached and joined together just like a <code>Component</code> can and allow for much more complex pipeline structures.</p>"},{"location":"guides/pipelines/#choice","title":"Choice","text":"<p>A <code>Choice</code> is a way to define a choice between multiple components. This is useful when you want to search over multiple algorithms, which may each have their own hyperparameters.</p> <p>We'll start again by creating two nodes:</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk.pipeline import Component\n\n@dataclass\nclass ModelA:\n    i: int\n\n@dataclass\nclass ModelB:\n    c: str\n\nmodel_a = Component(ModelA, space={\"i\": (0, 100)})\nmodel_b = Component(ModelB, space={\"c\": [\"red\", \"blue\"]})\n</code></pre> <pre>\n<code>\u256d\u2500 Component(ModelA) \u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class ModelA(...) \u2502\n\u2502 space {'i': (0, 100)}   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(ModelB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class ModelB(...)      \u2502\n\u2502 space {'c': ['red', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>Now combining them into a choice is rather straight forward:</p> <pre><code>from amltk.pipeline import Choice\n\nmodel_choice = Choice(model_a, model_b, name=\"estimator\")\n</code></pre> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(ModelA) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(ModelB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class ModelA(...) \u2502 \u2502 item  class ModelB(...)      \u2502 \u2502\n\u2502 \u2502 space {'i': (0, 100)}   \u2502 \u2502 space {'c': ['red', 'blue']} \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> Conditionals and Search Spaces <p>Not all search space implementations support conditionals and so some <code>parser=</code> may not be able to handle this. In this case, there won't be any conditionality in the search space.</p> <p>Check out the parser reference for more information.</p> <p>Just as we did with a <code>Component</code>, we can also get a <code>search_space()</code> from the choice.</p> <pre><code>space = model_choice.search_space(\"configspace\")\n</code></pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    estimator:ModelA:i, Type: UniformInteger, Range: [0, 100], Default: 50\n    estimator:ModelB:c, Type: Categorical, Choices: {red, blue}, Default: red\n    estimator:__choice__, Type: Categorical, Choices: {ModelA, ModelB}, Default:\nModelA\n  Conditions:\n    estimator:ModelA:i | estimator:__choice__ == 'ModelA'\n    estimator:ModelB:c | estimator:__choice__ == 'ModelB'\n\n</code>\n</pre> <p>When we <code>configure()</code> a choice, we will collapse it down to a single component. This is done according to what is set in the config.</p> <p><pre><code>config = space.sample_configuration()\nconfigured_choice = model_choice.configure(config)\n</code></pre> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {'__choice__': 'ModelB'}                               \u2502\n\u2502 \u256d\u2500 Component(ModelA) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(ModelB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class ModelA(...) \u2502 \u2502 item   class ModelB(...)      \u2502 \u2502\n\u2502 \u2502 space {'i': (0, 100)}   \u2502 \u2502 config {'c': 'blue'}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 space  {'c': ['red', 'blue']} \u2502 \u2502\n\u2502                             \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>  You'll notice that it set the <code>.config</code> of the <code>Choice</code> to <code>{\"__choice__\": \"model_a\"}</code> or <code>{\"__choice__\": \"model_b\"}</code>. This lets a builder know which of these two to build.</p>"},{"location":"guides/pipelines/#split","title":"Split","text":"<p>A <code>Split</code> is a way to signify a split in the dataflow of a pipeline. This <code>Split</code> by itself will not do anything but it informs the builder about what to do. Each builder will have its own specific strategy for dealing with one.</p> <p>Let's go ahead with a scikit-learn example, where we'll split the data into categorical and numerical features and then perform some preprocessing on each of them.</p> <pre><code>from sklearn.compose import make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nfrom amltk.pipeline import Component, Split\n\nselect_categories = make_column_selector(dtype_include=object)\nselect_numerical = make_column_selector(dtype_include=np.number)\n\npreprocessor = Split(\n    {\n        \"categories\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")],\n        \"numerics\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n    },\n    config={\"categories\": select_categories, \"numerics\": select_numerical},\n    name=\"feature_preprocessing\",\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Split(feature_preprocessing) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7efd5a32b5b0&gt;,                                                      \u2502\n\u2502            'numerics':                                                       \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7efd5a329f00&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerics) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502 space {                        \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           'strategy': [        \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502               'mean',          \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'median'         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502           ]                    \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502       }                        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>An important thing to note here is that first, we passed a <code>dict</code> to <code>Split</code>, such that we can name the individual paths. This is important because we need some name to refer to them when configuring the <code>Split</code>. It does this by simply wrapping each of the paths in a <code>Sequential</code>.</p> <p>The second thing is that the parameters set for the <code>.config</code> matches those of the paths. This let's the <code>Split</code> know which data should be sent where. Each <code>builder=</code> will have its own way of how to set up a <code>Split</code> and you should refer to the builders reference for more information.</p> <p>Our last step is just to convert this into a useable object and so once again we use <code>build()</code>.</p> <pre><code>built_pipeline = preprocessor.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('feature_preprocessing',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a32b5b0&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a329f00&gt;)]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('feature_preprocessing',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a32b5b0&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a329f00&gt;)]))])</pre> \u00a0feature_preprocessing: ColumnTransformer?Documentation for feature_preprocessing: ColumnTransformer<pre>ColumnTransformer(transformers=[('categories',\n                                 Pipeline(steps=[('SimpleImputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('OneHotEncoder',\n                                                  OneHotEncoder(drop='first'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a32b5b0&gt;),\n                                ('SimpleImputer', SimpleImputer(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a329f00&gt;)])</pre> categories<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a32b5b0&gt;</pre> \u00a0SimpleImputer?Documentation for SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(drop='first')</pre> SimpleImputer<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7efd5a329f00&gt;</pre> \u00a0SimpleImputer?Documentation for SimpleImputer<pre>SimpleImputer()</pre>"},{"location":"guides/pipelines/#join","title":"Join","text":"<p>TODO</p> <p>TODO</p>"},{"location":"guides/pipelines/#searchable","title":"Searchable","text":"<p>TODO</p> <p>TODO</p>"},{"location":"guides/pipelines/#option","title":"Option","text":"<p>TODO</p> <p>Please feel free to provide a contribution!</p>"},{"location":"guides/scheduling/","title":"Scheduling","text":"<p>AutoML-toolkit was designed to make offloading computation away from the main process easy, to foster increased ability for interact-ability, deployment and control. At the same time, we wanted to have an event based system to manage the complexity that comes with AutoML systems, all while making the API intuitive and extensible.</p> <p>By the end of this guide, we hope that the following code, its options and its inner working become easy to understand.</p> SourceResult Scheduler<pre><code>from amltk import Scheduler\n\n# Some function to offload to compute\ndef collatz(n: int) -&gt; int:\n    is_even = (n % 2 == 0)\n    return int(n / 2) if is_even else int(3 * n + 1)\n\n# Setup the scheduler and create a \"task\"\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(collatz)\n\nanswers = []\n\n# Tell the scheduler what to do when it starts\n@scheduler.on_start\ndef start_computing() -&gt; None:\n    answers.append(12)\n    task.submit(12)  # Launch the task with the argument 12\n\n# Tell the scheduler what to do when the task returns\n@task.on_result\ndef compute_next(_, next_n: int) -&gt; None:\n    answers.append(next_n)\n    if scheduler.running() and next_n != 1:\n        task.submit(next_n)\n\n# Run the scheduler\nscheduler.run(timeout=1)  # One second timeout\nprint(answers)\n</code></pre> <p>[12, 6, 3, 10, 5, 16, 8, 4, 2, 1]  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def start_computing() -&gt; None (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 9\n\u2503   @on_future_done 9\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 9\n\u2517\u2501\u2501 \u256d\u2500 Task collatz(n: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-collatz-HWl6vJIn \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>We start by introducing the engine, the <code>Scheduler</code> and how this interacts with python's built-in <code>Executor</code> interface to offload compute to processes, cluster nodes, or even cloud resources.</p> <p>However, the <code>Scheduler</code> is rather useless without some fuel. For this, we present <code>Tasks</code>, the computational task to perform with the <code>Scheduler</code> and start the system's gears turning.</p> <code>rich</code> printing <p>To get the same output locally (terminal or Notebook), you can either call <code>thing.__rich()__</code>, use <code>from rich import print; print(thing)</code> or in a Notebook, simply leave it as the last object of a cell.</p> <p>You'll have to install with <code>amltk[jupyter]</code> or <code>pip install rich[jupyter]</code> manually.</p>"},{"location":"guides/scheduling/#scheduler","title":"Scheduler","text":"<p>The core engine of the AutoML-Toolkit is the <code>Scheduler</code>. Its purpose is to allow you to create workflows in an event driven manner. It does this by allowing you to <code>submit()</code> functions with arguments to be computed in the background, while the main process can continue to do other work. Once this computation has completed, you can react with various callbacks, most likely to submit more computations.</p> Sounds like <code>asyncio</code>? <p>If you're familiar with pythons <code>await/async</code> syntax, then this description might sound similar. The <code>Scheduler</code> is powered by an asynchronous event loop but hides this complexity in its API. We do have an asynchronous API which we will discuss later.</p>"},{"location":"guides/scheduling/#backend","title":"Backend","text":"<p>The first thing to do is define where this computation should happen. A <code>Scheduler</code> builds upon an <code>Executor</code>, an interface provided by python's <code>concurrent.futures</code> module. This interface is used to abstract away the details of how the computation is actually performed. This allows us to easily switch between different backends, such as threads, processes, clusters, cloud resources, or even custom backends.</p> <p>Available Executors</p> <p>You can find a list of these in our executor reference.</p> <p>The simplest one is a <code>ProcessPoolExecutor</code>, which will create a pool of processes to run the compute in parallel. We provide a convenience function for this as <code>Scheduler.with_processes()</code>.</p> <pre><code>from concurrent.futures import ProcessPoolExecutor\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_start\n    @on_finishing\n    @on_finished\n    @on_stop\n    @on_timeout\n    @on_empty\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre>"},{"location":"guides/scheduling/#running-the-scheduler","title":"Running the Scheduler","text":"<p>You may have noticed from the above example that there are many events the scheduler will emit, such as <code>@start</code> or <code>@future-done</code>. One particularly important one is <code>@start</code>, an event to signal the scheduler has started and is ready to accept tasks.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello() -&gt; None:\n    print(\"hello\")\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_start\n    \u2514\u2500\u2500 def print_hello() -&gt; None\n    @on_finishing\n    @on_finished\n    @on_stop\n    @on_timeout\n    @on_empty\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>From the output, we can see that the <code>print_hello()</code> function was registered to the event <code>@start</code>, but it was never called and no <code>\"hello\"</code> was printed.</p> <p>For this to happen, we actually have to <code>run()</code> the scheduler.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.run()\n</code></pre>  hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>Now the output will show a little yellow number next to the <code>@start</code> and the <code>print_hello()</code>, indicating that event was triggered and the callback was called.</p> <p>You can subscribe multiple callbacks to the same event and they will each be called in the order they were registered.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello_1() -&gt; None:\n    print(\"hello 1\")\n\ndef print_hello_2() -&gt; None:\n    print(\"hello 2\")\n\nscheduler.on_start(print_hello_2)  # You can also register without a decorator\n\nscheduler.run()\n</code></pre>  hello 1 hello 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u251c\u2500\u2500 def print_hello_1() -&gt; None (1)\n    \u2514\u2500\u2500 def print_hello_2() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>Determinism</p> <p>It's worth noting that even though we are using an event based system, we are still guaranteed deterministic execution of the callbacks for any given event. The source of indeterminism is the order in which events are emitted, this is determined entirely by your compute functions themselves.</p>"},{"location":"guides/scheduling/#submitting-compute","title":"Submitting Compute","text":"<p>The <code>Scheduler</code> exposes a simple <code>submit()</code> method which allows you to submit compute to be performed while the scheduler is running.</p> <p>While we will later visit the <code>Task</code> class for defining these units of compute, it is beneficial to see how the <code>Scheduler</code> operates directly with <code>submit()</code>, without abstractions.</p> <p>In the below example, we will use the <code>@future-result</code> event to submit more compute once the previous computation has returned a result.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\ndef expensive_function(x: int) -&gt; int:\n    return 2 ** x\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, 2)  # Submit compute\n\n# Called when the submitted function is done\n@scheduler.on_future_result\ndef print_result(_, result: int) -&gt; None:\n    print(result)\n    if result &lt; 10:\n        scheduler.submit(expensive_function, result)\n\nscheduler.run()\n</code></pre>  4 16  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 2\n    @on_future_done 2\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 2\n    \u2514\u2500\u2500 def print_result(_, result: int) -&gt; None (2)\n</code>\n</pre> What's a <code>Future</code>? <p>A <code>Future</code> is a special object which represents the result of an asynchronous computation. It's an object that can be queried for its result/exception of some computation which may not have completed yet.</p>"},{"location":"guides/scheduling/#scheduler-events","title":"Scheduler Events","text":"<p>Here are some of the possible <code>@events</code> a <code>Scheduler</code> can emit, but please visit the scheduler reference for a complete list.</p> <p><code>@events</code></p> <code>@start</code><code>@future-result</code><code>@future-exception</code><code>@future-submitted</code><code>@future-done</code><code>@future-cancelled</code><code>@timeout</code><code>@stop</code><code>@finishing</code><code>@finished</code><code>@empty</code> <p>We can access all the counts of all events through the <code>scheduler.event_counts</code> property. This is a <code>dict</code> which has the events as keys and the amount of times it was emitted as the values.</p>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_start","title":"amltk.scheduling.Scheduler.on_start  <code>instance-attribute</code>","text":"<pre><code>on_start: Subscriber[[], Any] = subscriber(STARTED)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler starts. This is the first event emitted by the scheduler and one of the only ways to submit the initial compute to the scheduler.</p> <pre><code>@scheduler.on_start\ndef my_callback():\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_future_result","title":"amltk.scheduling.Scheduler.on_future_result  <code>instance-attribute</code>","text":"<pre><code>on_future_result: Subscriber[[Future, Any], Any] = (\n    subscriber(FUTURE_RESULT)\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when a future returned with a result, no exception raise.</p> <pre><code>@scheduler.on_future_result\ndef my_callback(future: Future, result: Any):\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_future_exception","title":"amltk.scheduling.Scheduler.on_future_exception  <code>instance-attribute</code>","text":"<pre><code>on_future_exception: Subscriber[\n    [Future, BaseException], Any\n] = subscriber(FUTURE_EXCEPTION)\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute raised an uncaught exception.</p> <pre><code>@scheduler.on_future_exception\ndef my_callback(future: Future, exception: BaseException):\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_future_submitted","title":"amltk.scheduling.Scheduler.on_future_submitted  <code>instance-attribute</code>","text":"<pre><code>on_future_submitted: Subscriber[[Future], Any] = subscriber(\n    FUTURE_SUBMITTED\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is submitted.</p> <pre><code>@scheduler.on_future_submitted\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_future_done","title":"amltk.scheduling.Scheduler.on_future_done  <code>instance-attribute</code>","text":"<pre><code>on_future_done: Subscriber[[Future], Any] = subscriber(\n    FUTURE_DONE\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is done, regardless of whether it was successful or not.</p> <pre><code>@scheduler.on_future_done\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_future_cancelled","title":"amltk.scheduling.Scheduler.on_future_cancelled  <code>instance-attribute</code>","text":"<pre><code>on_future_cancelled: Subscriber[[Future], Any] = subscriber(\n    FUTURE_CANCELLED\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when a future is cancelled. This usually occurs due to the underlying Scheduler, and is not something we do directly, other than when shutting down the scheduler.</p> <pre><code>@scheduler.on_future_cancelled\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_timeout","title":"amltk.scheduling.Scheduler.on_timeout  <code>instance-attribute</code>","text":"<pre><code>on_timeout: Subscriber[[], Any] = subscriber(TIMEOUT)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler reaches the timeout.</p> <pre><code>@scheduler.on_timeout\ndef my_callback():\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_stop","title":"amltk.scheduling.Scheduler.on_stop  <code>instance-attribute</code>","text":"<pre><code>on_stop: Subscriber[[str, BaseException | None], Any] = (\n    subscriber(STOP)\n)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is has been stopped due to the <code>stop()</code> method being called.</p> <pre><code>@scheduler.on_stop\ndef my_callback(stop_msg: str, exception: BaseException | None):\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_finishing","title":"amltk.scheduling.Scheduler.on_finishing  <code>instance-attribute</code>","text":"<pre><code>on_finishing: Subscriber[[], Any] = subscriber(FINISHING)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finishing up. This occurs right before the scheduler shuts down the executor.</p> <pre><code>@scheduler.on_finishing\ndef my_callback():\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_finished","title":"amltk.scheduling.Scheduler.on_finished  <code>instance-attribute</code>","text":"<pre><code>on_finished: Subscriber[[], Any] = subscriber(FINISHED)\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finished, has shutdown the executor and possibly terminated any remaining compute.</p> <pre><code>@scheduler.on_finished\ndef my_callback():\n    ...\n</code></pre>"},{"location":"guides/scheduling/#amltk.scheduling.Scheduler.on_empty","title":"amltk.scheduling.Scheduler.on_empty  <code>instance-attribute</code>","text":"<pre><code>on_empty: Subscriber[[], Any] = subscriber(EMPTY)\n</code></pre> <p>A <code>Subscriber</code> which is called when the queue is empty. This can be useful to re-fill the queue and prevent the scheduler from exiting.</p> <pre><code>@scheduler.on_empty\ndef my_callback():\n    ...\n</code></pre>"},{"location":"guides/scheduling/#controlling-callbacks","title":"Controlling Callbacks","text":"<p>There's a few parameters you can pass to any event subscriber such as <code>@start</code> or <code>@future-result</code>. These control the behavior of what happens when its event is fired and can be used to control the flow of your system.</p> <p>These are covered more extensively in our events reference.</p> <code>repeat=</code><code>max_calls=</code><code>when=</code><code>every=</code> <p>Repeat the callback a certain number of times, every time the event is emitted.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n# Print \"hello\" 3 times when the scheduler starts\n@scheduler.on_start(repeat=3)\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.run()\n</code></pre>  hello hello hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>Limit the number of times a callback can be called, after which the callback will be ignored.</p> <pre><code>from asyncio import Future\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\ndef expensive_function(x: int) -&gt; int:\n    return x ** 2\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\n@scheduler.on_future_result(max_calls=3)\ndef print_result(future, result) -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\nscheduler.run()\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 4\n    @on_future_done 4\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 4\n    \u2514\u2500\u2500 def print_result(future, result) -&gt; None (3)\n</code>\n</pre> <p>A callable which takes no arguments and returns a <code>bool</code>. The callback will only be called when the <code>when</code> callable returns <code>True</code>.</p> <p>Below is a rather contrived example, but it shows how we can use the <code>when</code> parameter to control when the callback is called.</p> <pre><code>import random\nfrom amltk.scheduling import Scheduler\n\nLOCALE = random.choice([\"English\", \"German\"])\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start(when=lambda: LOCALE == \"English\")\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n@scheduler.on_start(when=lambda: LOCALE == \"German\")\ndef print_guten_tag() -&gt; None:\n    print(\"guten tag\")\n\nscheduler.run()\n</code></pre>  hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u251c\u2500\u2500 def print_hello() -&gt; None (1)\n    \u2514\u2500\u2500 def print_guten_tag() -&gt; None\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>Only call the callback every <code>every</code> times the event is emitted. This includes the first time it's called.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n# Print \"hello\" only every 2 times the scheduler starts.\n@scheduler.on_start(every=2)\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n# Run the scheduler 5 times\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\n</code></pre>  hello hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 5\n    @on_start 5\n    \u2514\u2500\u2500 def print_hello() -&gt; None (2)\n    @on_finishing 5\n    @on_finished 5\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre>"},{"location":"guides/scheduling/#stopping-the-scheduler","title":"Stopping the Scheduler","text":"<p>There are a few ways the <code>Scheduler</code> will stop. The one we have implicitly been using this whole time is when the <code>Scheduler</code> has run out of events to process with no compute left to perform. This is the default behavior but can be controlled with <code>run(end_on_empty=False)</code>.</p> <p>However, there are more explicit methods.</p> <code>scheduler.stop()</code><code>scheduler.run(timeout=...)</code> <p>You can explicitly call <code>stop()</code> from aywhere on the <code>Scheduler</code> to stop it. By default this will wait for any currently running compute to finish but you can inform the scheduler to stop immediately with <code>run(wait=False)</code>.</p> <p>You'll notice this in the event count of the Scheduler where the event <code>@future-cancelled</code> was fired.</p> <pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function(sleep_for: int) -&gt; None:\n    time.sleep(sleep_for)\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, sleep_for=10)\n\n@scheduler.on_future_submitted\ndef stop_the_scheduler(_) -&gt; None:\n    scheduler.stop()\n\nscheduler.run(wait=False)\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop 1\n    @on_timeout\n    @on_future_submitted 1\n    \u2514\u2500\u2500 def stop_the_scheduler(_) -&gt; None (1)\n    @on_future_done\n    @on_future_cancelled 1\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>You can also tell the <code>Scheduler</code> to stop after a certain amount of time with the <code>timeout=</code> argument to <code>run()</code>.</p> <p>This will also trigger the <code>@timeout</code> event as seen in the <code>Scheduler</code> output.</p> <pre><code>import time\nfrom asyncio import Future\n\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; None:\n    time.sleep(0.1)\n    return 42\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\n# This will endlessly loop the scheduler\n@scheduler.on_future_done\ndef submit_again(future: Future) -&gt; None:\n    if scheduler.running():\n        scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1)  # End after 1 second\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 10\n    @on_future_done 10\n    \u2514\u2500\u2500 def submit_again(future: _asyncio.Future) -&gt; None (10)\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 10\n</code>\n</pre>"},{"location":"guides/scheduling/#exceptions","title":"Exceptions","text":"<p>Dealing with exceptions is an important part of any AutoML system. It is important to clarify that there are two kinds of exceptions that can occur within the Scheduler.</p> <p>The 1st kind that can happen is within some function submitted with <code>submit()</code>. When this happens, the <code>@future-exception</code> will be emitted, passing the exception to the callback.</p> <p>By default, the <code>Scheduler</code> will then raise the exception that occurred up to your program and end its computations. This is done by setting <code>run(on_exception=\"raise\")</code>, the default, but it also takes three other possibilities:</p> <ul> <li><code>\"continue\"</code> - Just emit the exception and keep running.</li> <li><code>\"end\"</code> - Emit the exception and then stop the scheduler but don't raise it.</li> <li><code>{MyException: \"continue\", OtherException: \"raise\"}</code> - Decide what to do     for each exception type. Note that this checked in order using <code>isinstance(...)</code></li> </ul> <p>One example is to just <code>stop()</code> the scheduler when some exception occurs.</p> <pre><code>from asyncio import Future\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef failing_compute_function(err_msg: str) -&gt; None:\n    raise ValueError(err_msg)\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(failing_compute_function, \"Failed!\")\n\n@scheduler.on_future_exception\ndef stop_the_scheduler(future: Future, exception: Exception) -&gt; None:\n    print(f\"Got exception {exception}\")\n    scheduler.stop()  # You can optionally pass `exception=` for logging purposes.\n\nscheduler.run(on_exception=\"continue\")  # Scheduler will not stop because of the error\n</code></pre>  Got exception Failed!  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop 1\n    @on_timeout\n    @on_future_submitted 1\n    @on_future_done 1\n    @on_future_cancelled\n    @on_future_exception 1\n    \u2514\u2500\u2500 def stop_the_scheduler(future: _asyncio.Future, exception: Exception) -&gt;\n        None (1)\n    @on_future_result\n</code>\n</pre> <p>The second kind of exception that can happen is one that happens in the main process. For example, this could happen in one of your callbacks or in the <code>Scheduler</code> itself (please raise an issue if this occurs!). By default when you call <code>run()</code> it will set <code>run(on_exception=\"raise\")</code> and raise the exception that occurred, with its traceback. This is to help you debug your program.</p> <p>You may also use <code>run(on_exception=\"end\")</code>, which will just end the <code>Scheduler</code> and raise no exception, or use <code>run(on_exception=\"continue\")</code>, in which case the <code>Scheduler</code> will continue on with whatever events are next to process.</p>"},{"location":"guides/scheduling/#tasks","title":"Tasks","text":"<p>Now that we have seen how the <code>Scheduler</code> works, we can look at the <code>Task</code>, a wrapper around a function that you'll want to submit to the <code>Scheduler</code>. The preferred way to create one of these <code>Tasks</code> is to use <code>scheduler.task(function)</code>.</p>"},{"location":"guides/scheduling/#running-a-task","title":"Running a task","text":"<p>In the following example, we will create a task for the scheduler and attempt to call it. This task will be run by the backend specified.</p> <pre><code>from amltk import Scheduler\n\n# Some function to offload to compute\ndef collatz(n: int) -&gt; int:\n    is_even = (n % 2 == 0)\n    return int(n / 2) if is_even else int(3 * n + 1)\n\nscheduler = Scheduler.with_processes(1)\n\n# Creating a \"task\"\ncollatz_task = scheduler.task(collatz)\n\ntry:\n    collatz_task.submit(5)\nexcept Exception as e:\n    print(f\"{type(e)}: {e}\")\n</code></pre> <pre><code>&lt;class 'amltk.exceptions.SchedulerNotRunningError'&gt;: Scheduler is not running, cannot submit task &lt;function collatz at 0x7efd59f41630&gt; with args=(5,), kwargs={}\n</code></pre> <p>As you can see, we can not submit tasks before the scheduler is running. This is because the backend that it's running on usually has to be setup and teardown when <code>scheduler.run()</code> is called.</p> <p>The proper approach would be to do the following:</p> <pre><code>from amltk import Scheduler\n\n# Some function to offload to compute\ndef collatz(n: int) -&gt; int:\n    is_even = (n % 2 == 0)\n    return int(n / 2) if is_even else int(3 * n + 1)\n\n# Setup the scheduler and create a \"task\"\nscheduler = Scheduler.with_processes(1)\ncollatz_task = scheduler.task(collatz)\n\n@scheduler.on_start\ndef launch_initial_task() -&gt; None:\n    collatz_task.submit(5)\n\nscheduler.run()\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def launch_initial_task() -&gt; None (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 1\n\u2503   @on_future_done 1\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 1\n\u2517\u2501\u2501 \u256d\u2500 Task collatz(n: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-collatz-5yu2a82u \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"guides/scheduling/#task-specific-events","title":"Task Specific Events","text":"<p>As you may have noticed, we can see the <code>Task</code> itself in the <code>Scheduler</code> as well as the events it defines. This allows us to react to certain tasks themselves, and not generally everything that may pass through the <code>Scheduler</code>.</p> <p>In the below example, we'll do two things. First, we'll create a <code>Task</code> and react to its events, but also use the <code>Scheduler</code> directly and use <code>submit()</code>. Then we'll see how the callbacks reacted to different events.</p> <pre><code>from amltk import Scheduler\n\ndef echo(msg: str) -&gt; str:\n    return msg\n\nscheduler = Scheduler.with_processes(1)\necho_task = scheduler.task(echo)\n\n# Launch the task and do a raw `submit()` with the Scheduler\n@scheduler.on_start\ndef launch_initial_task() -&gt; None:\n    echo_task.submit(\"hello\")\n    scheduler.submit(echo, \"hi\")\n\n# Callback for anything resulting from the scheduler\n@scheduler.on_future_result\ndef from_scheduler(_, msg: str) -&gt; None:\n    print(f\"result_from_scheduler {msg}\")\n\n# Callback for specifically results from the `echo_task`\n@echo_task.on_result\ndef from_task(_, msg: str) -&gt; None:\n    print(f\"result_from_task {msg}\")\n\nscheduler.run()\n</code></pre>  result_from_scheduler hello result_from_task hello result_from_scheduler hi  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def launch_initial_task() -&gt; None (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 2\n\u2503   @on_future_done 2\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 2\n\u2503   \u2514\u2500\u2500 def from_scheduler(_, msg: str) -&gt; None (2)\n\u2517\u2501\u2501 \u256d\u2500 Task echo(msg: str) -&gt; str \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-echo-FP5NRPb4 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>We can see in the output of the above code that the <code>@scheduler.on_future_result</code> was called twice, meaning our callback <code>def from_scheduler()</code> was called twice, once for the result of <code>echo_task.submit(\"hello\")</code> and the other time from <code>scheduler.submit(echo, \"hi\")</code>. On the other hand, the event <code>@task.on_result</code> was only called once, meaning our callback <code>def from_task()</code> was only called once.</p> <p>In practice, you will likely need to define a variety of tasks for your AutoML System and having dedicated code to respond to individual tasks is of critical importance. This can even allow you to chain the results of one task into another, and define more complex workflows.</p> <p>The below example shows how you can define two tasks with the scheduler and have certain callbacks for different tasks, or even share callbacks between them!</p> <pre><code>from amltk import Scheduler\n\ndef expensive_thing_1(x: int) -&gt; int:\n    return x * 2\n\ndef expensive_thing_2(x: int) -&gt; int:\n    return x ** 2\n\n# Create a scheduler and 2 tasks\nscheduler = Scheduler.with_processes(1)\ntask_1 = scheduler.task(expensive_thing_1)\ntask_2 = scheduler.task(expensive_thing_2)\n\n# A list of things we want to compute\nitems = iter([1, 2, 3])\n\n@scheduler.on_start\ndef submit_initial() -&gt; None:\n    next_item = next(items)\n    task_1.submit(next_item)\n\n@task_1.on_result\ndef submit_task_2_with_results_of_task_1(_, result: int) -&gt; None:\n    \"\"\"When task_1 returns, send the result to task_2\"\"\"\n    task_2.submit(result)\n\n@task_1.on_result\ndef submit_task_1_with_next_item(_, result: int) -&gt; None:\n    \"\"\"When task_1 returns, launch it again with the next items\"\"\"\n    next_item = next(items, None)\n    if next_item is not None:\n        task_1.submit(next_item)\n        return\n\n    print(\"Done!\")\n\n# You may share callbacks for the two tasks\n@task_1.on_exception\n@task_2.on_exception\ndef handle_task_exception(_, exception: BaseException) -&gt; None:\n    print(f\"A task errored! {exception}\")\n\nscheduler.run()\n</code></pre>  Done!  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def submit_initial() -&gt; None (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 6\n\u2503   @on_future_done 6\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 6\n\u2523\u2501\u2501 \u256d\u2500 Task expensive_thing_1(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2503   \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-expensive_thing_1-5e3V0LFE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 \u256d\u2500 Task expensive_thing_2(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-expensive_thing_2-hDPx098A \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"guides/scheduling/#task-plugins","title":"Task Plugins","text":"<p>Another benefit of <code>Task</code> objects is that we can attach a <code>Plugin</code> to them. These plugins can automate control behaviour of tasks, either through preventing their execution, modifying the function and its arguments or even attaching plugin specific events!</p> <p>For a complete reference, please see the plugin reference page.</p>"},{"location":"guides/scheduling/#call-limiter","title":"Call Limiter","text":"<p>Perhaps one of the more useful plugins, at least when designing an AutoML System, is the <code>Limiter</code> plugin. This can help you control both its concurrency or the absolute limit of how many times a certain task can be successfully submitted.</p> <p>In the following contrived example, we will setup a <code>Scheduler</code> with 2 workers and attempt to submit a <code>Task</code> 4 times in rapid succession. However, we have the constraint that we only ever want 2 of these tasks running at a given time. Let's see how we could achieve that.</p> <pre><code>from amltk.scheduling import Scheduler, Limiter\n\ndef my_func(x: int) -&gt; int:\n    return x\n\nscheduler = Scheduler.with_processes(2)\n\n# Specify a concurrency limit of 2\ntask = scheduler.task(my_func, plugins=Limiter(max_concurrent=2))\n\n# A list of 10 things we want to compute\nitems = iter(range(10))\nresults = []\n\n@scheduler.on_start(repeat=4)  # Repeat callback 4 times\ndef submit() -&gt; None:\n    next_item = next(items)\n    task.submit(next_item)\n\n@task.on_result\ndef record_result(_, result: int) -&gt; None:\n    results.append(result)\n\n@task.on_result\ndef launch_another(_, result: int) -&gt; None:\n    next_item = next(items, None)\n    if next_item is not None:\n        task.submit(next_item)\n\nscheduler.run()\nprint(results)\n</code></pre>  [1, 0, 4, 5, 6, 7, 8, 9]  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def submit() -&gt; None (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 8\n\u2503   @on_future_done 8\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 8\n\u2517\u2501\u2501 \u256d\u2500 Task my_func(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n    \u2502 \u2502 Concurrent 0/2                                                       \u2502 \u2502\n    \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-my_func-9zUq9QHP \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>You can notice that this limiting worked, given the numbers <code>2</code> and <code>3</code> were skipped and not printed. As expected, we successfully launched the task with both  <code>0</code> and <code>1</code>, but as these tasks were not done processing, the <code>Limiter</code> kicks in and prevents the other two.</p> <p>A natural extension to ask is then, \"how do we requeue these?\". Well, let's take a look at the above output. The plugin has added three new events to <code>Task</code>, namely <code>@call-limit-reached</code>, <code>@concurrent-limit-reached</code> and <code>@disabled-due-to-running-task</code>.</p> <p>To subscribe to these extra events (or any for that matter), we can use the <code>task.on()</code> method. Below is the same example except here we respond to <code>@call-limit-reached</code> and requeue the submissions that failed.</p> <pre><code>from amltk.scheduling import Scheduler, Limiter, Task\nfrom amltk.types import Requeue\n\ndef my_func(x: int) -&gt; int:\n    return x\n\nscheduler = Scheduler.with_processes(2)\ntask = scheduler.task(my_func, plugins=Limiter(max_concurrent=2))\n\n# A list of 10 things we want to compute\nitems = Requeue(range(10))  # A convenience type that you can requeue/append to\nresults = []\n\n@scheduler.on_start(repeat=4)  # Repeat callback 4 times\ndef submit() -&gt; None:\n    next_item = next(items)\n    task.submit(next_item)\n\n@task.on(\"concurrent-limit-reached\")\ndef add_back_to_queue(task: Task, x: int) -&gt; None:\n    items.requeue(x)  # Put x back at the start of the queue\n\n@task.on_result\ndef record_result(_, result: int) -&gt; None:\n    results.append(result)\n\n@task.on_result\ndef launch_another(_, result: int) -&gt; None:\n    next_item = next(items, None)\n    if next_item is not None:\n        task.submit(next_item)\n\nscheduler.run()\nprint(results)\n</code></pre>  [1, 0, 2, 3, 5, 4, 6, 7, 8, 9]  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def submit() -&gt; None (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 10\n\u2503   @on_future_done 10\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 10\n\u2517\u2501\u2501 \u256d\u2500 Task my_func(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n    \u2502 \u2502 Concurrent 0/2                                                       \u2502 \u2502\n    \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-my_func-BWdi3t6r \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"guides/scheduling/#under-construction","title":"Under Construction","text":"<p>Please see the following reference pages in the meantime:</p> <ul> <li>scheduler reference - A slightly     more condensed version of how to use the <code>Scheduler</code>.</li> <li>task reference - A more comprehensive     explanation of <code>Task</code>s and their <code>@events</code>.</li> <li>plugin reference - An intro to plugins     and how to create your own.</li> <li>executors reference - A list of     executors and how to use them.</li> <li>events reference - A more comprehensive     look at the event system in AutoML-Toolkit and how to work with them or extend them.</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Here you'll find a non-exhaustive but quick reference to many of the core types and utilities available to AMLTK. Please use the Table of Contents on the left to browse them.</p> <p>If you're looking for a more in depth understanding of how automl-toolkit works, please take a look at the guides section.</p> <p>If you're looking for signatures and specific function documentation, check out the API docs.</p>"},{"location":"reference/data/","title":"Data","text":"<p>AutoML-Toolkit provides some utility for manipulating data containers, specifically <code>pd.DataFrame</code>, <code>pd.Series</code>, <code>np.ndarray</code>.</p>"},{"location":"reference/data/#reducing-the-size-of-your-data-in-memory","title":"Reducing the size of your data in memory","text":"<p>Often times, the defaults of <code>numpy</code> and <code>pandas</code> is to use large dtypes that are suited for most tasks. However sometimes this can be prohibitive, especially in low memory compute regimes.</p> <p>To measure the memory consumption of a data container, we can use <code>byte_size()</code>. While independant methods exist for each of these structures, we wrap them together in a single function for convenience.</p> ref-data-bytesize<pre><code>from amltk.data import byte_size\n\nimport pandas as pd\nimport numpy as np\n\nx = np.arange(100)\ny = pd.Series(np.linspace(1, 100, 100))\nz = pd.DataFrame({\"a\": np.arange(100), \"b\": pd.Series(np.linspace(1, 100, 100))})\n\nprint(\"x: \", byte_size(x))\nprint(\"y: \", byte_size(y))\nprint(\"z: \", byte_size(z))\n\nprint(\"combined: \", byte_size([x, y, z]))\n</code></pre> <pre><code>x:  800\ny:  928\nz:  1728\ncombined:  3456\n</code></pre> <p>Now that we can measure the size of our data, we can use the <code>reduce_dtypes()</code> function to reduce the memory of our data by:</p> <ul> <li>Find the smallest <code>int</code> dtype that can represent integer data</li> <li>Reduce the percision of floating point data by one step. i.e. <code>float64</code> -&gt; <code>float32</code></li> </ul> ref-data-reducedtypes<pre><code>from amltk.data import reduce_dtypes, byte_size\n\nimport pandas as pd\nimport numpy as np\n\nx = np.arange(100)\ny = pd.Series(np.linspace(1, 100, 100))\nz = pd.DataFrame({\"a\": np.arange(100), \"b\": pd.Series(np.linspace(1, 100, 100))})\n\nprint(f\"x: {x.dtype}\")\nprint(f\"y: {y.dtype}\")\nprint(f\"z: {z.dtypes}\")\n\nprint(\"combined memory: \", byte_size([x, y, z]))\n\nx, y, z = [reduce_dtypes(d) for d in [x, y, z]]\n\nprint(f\"x: {x.dtype}\")\nprint(f\"y: {y.dtype}\")\nprint(f\"z: {z.dtypes}\")\n\nprint(\"combined memory: \", byte_size([x, y, z]))\n</code></pre> <pre><code>x: int64\ny: float64\nz: a      int64\nb    float64\ndtype: object\ncombined memory:  3456\nx: uint8\ny: UInt8\nz: a    UInt8\nb    UInt8\ndtype: object\ncombined memory:  956\n</code></pre>"},{"location":"reference/data/buckets/","title":"Buckets","text":"<p>A bucket is a collection of dict-like view of resources that can be accessed by a key of a given type. This lets you easily store and retrieve objects of varying types in a single location.</p> <p>The main implementation we provide is the <code>PathBucket</code>, which is a dict-like view over a directory to quickly store many files of different types and also retrieve them.</p> <pre><code>from amltk.store.paths import PathBucket\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nbucket = PathBucket(\"./path/to/bucket\")\n\narray = np.array([1, 2, 3])\ndataframe = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nmodel = LinearRegression()\n\n# Store things\nbucket[\"myarray.npy\"] = array # (1)!\nbucket[\"df.csv\"] = dataframe  # (2)!\nbucket[\"model.pkl\"].put(model)\n\nbucket[\"config.json\"] = {\"hello\": \"world\"}\nassert bucket[\"config.json\"].exists()\nbucket[\"config.json\"].remove()\n\n# Store multiple at once\nbucket.store(\n    {\n        \"myarray.npy\": array,\n        \"df.csv\": dataframe,\n        \"model.pkl\": model,\n        \"config.json\": {\"hello\": \"world\"}\n    }\n)\n\n# Load things\narray = bucket[\"myarray.npy\"].load()\nmaybe_df = bucket[\"df.csv\"].get()  # (3)!\nmodel: LinearRegression = bucket[\"model.pkl\"].get(check=LinearRegression)  # (4)!\n\n# Load multiple at once\nitems = bucket.fetch(\"myarray.npy\", \"df.csv\", \"model.pkl\", \"config.json\")\narray = items[\"myarray.npy\"]\ndf = items[\"df.csv\"]\nmodel = items[\"model.pkl\"]\nconfig = items[\"config.json\"]\n\n# Create subdirectories\nmodel_bucket = bucket / \"my_model\" # (5)!\nmodel_bucket[\"model.pkl\"] = model\nmodel_bucket[\"predictions.npy\"] = model.predict(X)\n\n# Acts like a mapping\nassert \"myarray.npy\" in bucket\nassert len(bucket) == 3\nfor key, item in bucket.items():\n    print(key, item.load())\ndel bucket[\"model.pkl\"]\n</code></pre> <ol> <li>The <code>=</code> is a shortcut for <code>bucket[\"myarray.npy\"].put(array)</code></li> <li>The extension is used to determine which     <code>PathLoader</code> to use     and how to save it.</li> <li>The <code>get</code> method acts like the <code>dict.load</code> method.</li> <li>The <code>get</code> method can be used to check the type of the loaded object.     If the type does not match, a <code>TypeError</code> is raised.</li> <li>Uses the familiar <code>Path</code> API to create subdirectories.</li> </ol>"},{"location":"reference/metalearning/","title":"Metalearning","text":"<p>An important part of AutoML systems is to perform well on new unseen data. There are a variety of methods to do so but we provide some building blocks to help implement these methods.</p> <p>API</p> <p>The meta-learning features have not been extensively used yet and such no solid API has been developed yet. We will deprecate any API subject to change before changing them.</p>"},{"location":"reference/metalearning/#metafeatures","title":"MetaFeatures","text":"<p>A <code>MetaFeature</code> is some statistic about a dataset/task, that can be used to make datasets or tasks more comparable, thus enabling meta-learning methods.</p> <p>Calculating meta-features of a dataset is quite straight foward.</p> Metafeatures<pre><code>import openml\nfrom amltk.metalearning import compute_metafeatures\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nmfs = compute_metafeatures(X, y)\n\nprint(mfs)\n</code></pre> <pre><code>instance_count                                           1000.000000\nlog_instance_count                                          6.907755\nnumber_of_classes                                           2.000000\nnumber_of_features                                         20.000000\nlog_number_of_features                                      2.995732\npercentage_missing_values                                   0.000000\npercentage_of_instances_with_missing_values                 0.000000\npercentage_of_features_with_missing_values                  0.000000\npercentage_of_categorical_columns_with_missing_values       0.000000\npercentage_of_categorical_values_with_missing_values        0.000000\npercentage_of_numeric_columns_with_missing_values           0.000000\npercentage_of_numeric_values_with_missing_values            0.000000\nnumber_of_numeric_features                                  7.000000\nnumber_of_categorical_features                             13.000000\nratio_numerical_features                                    0.350000\nratio_categorical_features                                  0.650000\nratio_features_to_instances                                 0.020000\nminority_class_imbalance                                    0.200000\nmajority_class_imbalance                                    0.200000\nclass_imbalance                                             0.400000\nmean_categorical_imbalance                                  0.500500\nstd_categorical_imbalance                                   0.234994\nskewness_mean                                               0.920379\nskewness_std                                                0.904952\nskewness_min                                               -0.531348\nskewness_max                                                1.949628\nkurtosis_mean                                               0.924278\nkurtosis_std                                                1.785467\nkurtosis_min                                               -1.381449\nkurtosis_max                                                4.292590\ndtype: float64\n</code></pre> <p>By default <code>compute_metafeatures()</code> will calculate all the <code>MetaFeature</code> implemented, iterating through their subclasses to do so. You can pass an explicit list as well to <code>compute_metafeatures(X, y, features=[...])</code>.</p> <p>To implement your own is also quite straight forward:</p> Create Metafeature<pre><code>from amltk.metalearning import MetaFeature, compute_metafeatures\nimport openml\nimport pandas as pd\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nclass TotalValues(MetaFeature):\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; int:\n        return int(x.shape[0] * x.shape[1])\n\nmfs = compute_metafeatures(X, y, features=[TotalValues])\nprint(mfs)\n</code></pre> <pre><code>total_values    20000\ndtype: int64\n</code></pre> <p>As many metafeatures rely on pre-computed dataset statistics, and they do not need to be calculated more than once, you can specify the dependancies of a meta feature. When a metafeature would return something other than a single value, i.e. a <code>dict</code> or a <code>pd.DataFrame</code>, we instead call those a <code>DatasetStatistic</code>. These will not be included in the result of <code>compute_metafeatures()</code>. These <code>DatasetStatistic</code>s will only be calculated once on a call to <code>compute_metafeatures()</code> so they can be re-used across all <code>MetaFeature</code>s that require that dependancy.</p> Metafeature Dependancy<pre><code>from amltk.metalearning import MetaFeature, DatasetStatistic, compute_metafeatures\nimport openml\nimport pandas as pd\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nclass NAValues(DatasetStatistic):\n    \"\"\"A mask of all NA values in a dataset\"\"\"\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; pd.DataFrame:\n        return x.isna()\n\n\nclass PercentageNA(MetaFeature):\n    \"\"\"The percentage of values missing\"\"\"\n\n    dependencies = (NAValues,)\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; int:\n        na_values = dependancy_values[NAValues]\n        n_na = na_values.sum().sum()\n        n_values = int(x.shape[0] * x.shape[1])\n        return float(n_na / n_values)\n\nmfs = compute_metafeatures(X, y, features=[PercentageNA])\nprint(mfs)\n</code></pre> <pre><code>percentage_n_a    0.0\ndtype: float64\n</code></pre> <p>To view the description of a particular <code>MetaFeature</code>, you can call <code>.description()</code> on it. Otherwise you can access all of them in the following way:</p> SourceResult Metafeature Descriptions<pre><code>from pprint import pprint\nfrom amltk.metalearning import metafeature_descriptions\n\ndescriptions = metafeature_descriptions()\nfor name, description in descriptions.items():\n    print(\"---\")\n    print(name)\n    print(\"---\")\n    print(\" * \" + description)\n</code></pre> <pre><code>---\ninstance_count\n---\n * Number of instances in the dataset.\n---\nlog_instance_count\n---\n * Logarithm of the number of instances in the dataset.\n---\nnumber_of_classes\n---\n * Number of classes in the dataset.\n---\nnumber_of_features\n---\n * Number of features in the dataset.\n---\nlog_number_of_features\n---\n * Logarithm of the number of features in the dataset.\n---\npercentage_missing_values\n---\n * Percentage of missing values in the dataset.\n---\npercentage_of_instances_with_missing_values\n---\n * Percentage of instances with missing values.\n---\npercentage_of_features_with_missing_values\n---\n * Percentage of features with missing values.\n---\npercentage_of_categorical_columns_with_missing_values\n---\n * Percentage of categorical columns with missing values.\n---\npercentage_of_categorical_values_with_missing_values\n---\n * Percentage of categorical values with missing values.\n---\npercentage_of_numeric_columns_with_missing_values\n---\n * Percentage of numeric columns with missing values.\n---\npercentage_of_numeric_values_with_missing_values\n---\n * Percentage of numeric values with missing values.\n---\nnumber_of_numeric_features\n---\n * Number of numeric features in the dataset.\n---\nnumber_of_categorical_features\n---\n * Number of categorical features in the dataset.\n---\nratio_numerical_features\n---\n * Ratio of numerical features to total features in the dataset.\n---\nratio_categorical_features\n---\n * Ratio of categoricals features to total features in the dataset.\n---\nratio_features_to_instances\n---\n * Ratio of features to instances in the dataset.\n---\nminority_class_imbalance\n---\n * Imbalance of the minority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.\n---\nmajority_class_imbalance\n---\n * Imbalance of the majority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.\n---\nclass_imbalance\n---\n * Mean Target Imbalance of the classes in general.\n\n    0 =&gt; Balanced. 1 Imbalanced.\n\n---\nmean_categorical_imbalance\n---\n * The mean imbalance of categorical features.\n---\nstd_categorical_imbalance\n---\n * The std imbalance of categorical features.\n---\nskewness_mean\n---\n * The mean skewness of numerical features.\n---\nskewness_std\n---\n * The std skewness of numerical features.\n---\nskewness_min\n---\n * The min skewness of numerical features.\n---\nskewness_max\n---\n * The max skewness of numerical features.\n---\nkurtosis_mean\n---\n * The mean kurtosis of numerical features.\n---\nkurtosis_std\n---\n * The std kurtosis of numerical features.\n---\nkurtosis_min\n---\n * The min kurtosis of numerical features.\n---\nkurtosis_max\n---\n * The max kurtosis of numerical features.\n---\ntotal_values\n---\n * \n---\npercentage_n_a\n---\n * The percentage of values missing\n</code></pre>"},{"location":"reference/metalearning/#dataset-distances","title":"Dataset Distances","text":"<p>One common way to define how similar two datasets are is to compute some \"similarity\" between them. This notion of \"similarity\" requires computing some features of a dataset (metafeatures) first, such that we can numerically compute some distance function.</p> <p>Let's see how we can quickly compute the distance between some datasets with <code>dataset_distance()</code>!</p> Dataset Distances P.1<pre><code>import pandas as pd\nimport openml\n\nfrom amltk.metalearning import compute_metafeatures\n\ndef get_dataset(dataset_id: int) -&gt; tuple[pd.DataFrame, pd.Series]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n    X, y, _, _ = dataset.get_data(\n        dataset_format=\"dataframe\",\n        target=dataset.default_target_attribute,\n    )\n    return X, y\n\nd31 = get_dataset(31)\nd3 = get_dataset(3)\nd4 = get_dataset(4)\n\nmetafeatures_dict = {\n    \"dataset_31\": compute_metafeatures(*d31),\n    \"dataset_3\": compute_metafeatures(*d3),\n    \"dataset_4\": compute_metafeatures(*d4),\n}\n\nmetafeatures = pd.DataFrame(metafeatures_dict)\nprint(metafeatures)\n</code></pre> <pre><code>                                                     dataset_31  ...  dataset_4\ninstance_count                                      1000.000000  ...  57.000000\nlog_instance_count                                     6.907755  ...   4.043051\nnumber_of_classes                                      2.000000  ...   2.000000\nnumber_of_features                                    20.000000  ...  16.000000\nlog_number_of_features                                 2.995732  ...   2.772589\npercentage_missing_values                              0.000000  ...   0.357456\npercentage_of_instances_with_missing_values            0.000000  ...   0.982456\npercentage_of_features_with_missing_values             0.000000  ...   1.000000\npercentage_of_categorical_columns_with_missing_...     0.000000  ...   1.000000\npercentage_of_categorical_values_with_missing_v...     0.000000  ...   0.410088\npercentage_of_numeric_columns_with_missing_values      0.000000  ...   1.000000\npercentage_of_numeric_values_with_missing_values       0.000000  ...   0.304825\nnumber_of_numeric_features                             7.000000  ...   8.000000\nnumber_of_categorical_features                        13.000000  ...   8.000000\nratio_numerical_features                               0.350000  ...   0.500000\nratio_categorical_features                             0.650000  ...   0.500000\nratio_features_to_instances                            0.020000  ...   0.280702\nminority_class_imbalance                               0.200000  ...   0.149123\nmajority_class_imbalance                               0.200000  ...   0.149123\nclass_imbalance                                        0.400000  ...   0.298246\nmean_categorical_imbalance                             0.500500  ...   0.308063\nstd_categorical_imbalance                              0.234994  ...   0.228906\nskewness_mean                                          0.920379  ...   0.255076\nskewness_std                                           0.904952  ...   1.420729\nskewness_min                                          -0.531348  ...  -2.007217\nskewness_max                                           1.949628  ...   3.318064\nkurtosis_mean                                          0.924278  ...   2.046258\nkurtosis_std                                           1.785467  ...   4.890029\nkurtosis_min                                          -1.381449  ...  -2.035406\nkurtosis_max                                           4.292590  ...  13.193069\n\n[30 rows x 3 columns]\n</code></pre> <p>Now we want to know which one of <code>\"dataset_3\"</code> or <code>\"dataset_4\"</code> is more similar to <code>\"dataset_31\"</code>.</p> Dataset Distances P.2<pre><code>from amltk.metalearning import dataset_distance\n\ntarget = metafeatures_dict.pop(\"dataset_31\")\nothers = metafeatures_dict\n\ndistances = dataset_distance(target, others, distance_metric=\"l2\")\nprint(distances)\n</code></pre> <pre><code>dataset_4     943.079572\ndataset_3    2196.197231\nName: l2, dtype: float64\n</code></pre> <p>Seems like <code>\"dataset_3\"</code> is some notion of closer to <code>\"dataset_31\"</code> than <code>\"dataset_4\"</code>. However the scale of the metafeatures are not exactly all close. For example, many lie between <code>(0, 1)</code> but some like <code>instance_count</code> can completely dominate the show.</p> <p>Lets repeat the computation but specify that we should apply a <code>\"minmax\"</code> scaling across the rows.</p> Dataset Distances P.3<pre><code>distances = dataset_distance(\n    target,\n    others,\n    distance_metric=\"l2\",\n    scaler=\"minmax\"\n)\nprint(distances)\n</code></pre> <pre><code>dataset_3    3.293831\ndataset_4    3.480296\nName: l2, dtype: float64\n</code></pre> <p>Now <code>\"dataset_3\"</code> is considered more similar but the difference between the two is a lot less dramatic. In general, applying some scaling to values of different scales is required for metalearning.</p> <p>You can also use an sklearn.preprocessing.MinMaxScaler or anything other scaler from scikit-learn for that matter.</p> Dataset Distances P.3<pre><code>from sklearn.preprocessing import MinMaxScaler\n\ndistances = dataset_distance(\n    target,\n    others,\n    distance_metric=\"l2\",\n    scaler=MinMaxScaler()\n)\nprint(distances)\n</code></pre> <pre><code>dataset_3    3.293831\ndataset_4    3.480296\nName: l2, dtype: float64\n</code></pre>"},{"location":"reference/metalearning/#portfolio-selection","title":"Portfolio Selection","text":"<p>A portfolio in meta-learning is to a set (ordered or not) of configurations that maximize some notion of coverage across datasets or tasks. The intuition here is that this also means that any new dataset is also covered!</p> <p>Suppose we have the given performances of some configurations across some datasets. Initial Portfolio<pre><code>import pandas as pd\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\nprint(portfolio)\n</code></pre> <pre><code>           c1  c2  c3  c4\ndataset_1  90  20  10  90\ndataset_2  60  10  20  10\ndataset_3  20  90  40  10\ndataset_4  10  20  90  10\n</code></pre> </p> <p>If we could only choose <code>k=3</code> of these configurations on some new given dataset, which ones would you choose and in what priority? Here is where we can apply <code>portfolio_selection()</code>!</p> <p>The idea is that we pick a subset of these algorithms that maximise some value of utility for the portfolio. We do this by adding a single configuration from the entire set, 1-by-1 until we reach <code>k</code>, beginning with the empty portfolio.</p> <p>Let's see this in action!</p> Portfolio Selection<pre><code>import pandas as pd\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\"\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre> <p>The trajectory tells us which configuration was added at each time stamp along with the utility of the portfolio with that configuration added. However we havn't specified how exactly we defined the utility of a given portfolio. We could define our own function to do so:</p> Portfolio Selection Custom<pre><code>import pandas as pd\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\ndef my_function(p: pd.DataFrame) -&gt; float:\n    # Take the maximum score for each dataset and then take the mean across them.\n    return p.max(axis=1).mean()\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\",\n    portfolio_value=my_function,\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre> <p>This notion of reducing across all configurations for a dataset and then aggregating these is common enough that we can also directly just define these operations and we will perform the rest.</p> Portfolio Selection With Reduction<pre><code>import pandas as pd\nimport numpy as np\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\",\n    row_reducer=np.max,  # This is actually the default\n    aggregator=np.mean,  # This is actually the default\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre>"},{"location":"reference/optimization/history/","title":"History","text":"API links <ul> <li><code>History</code></li> <li><code>Report</code></li> </ul>"},{"location":"reference/optimization/history/#basic-usage","title":"Basic Usage","text":"<p>The <code>History</code> class is used to store <code>Report</code>s from <code>Trial</code>s.</p> <p>In it's most simple usage, you can simply <code>add()</code> a <code>Report</code> as you recieve them and then use the <code>df()</code> method to get a <code>pandas.DataFrame</code> of the history.</p> Reference History<pre><code>from amltk.optimization import Trial, History, Metric\n\nloss = Metric(\"loss\", minimize=True)\n\ndef quadratic(x):\n    return x**2\n\nhistory = History()\ntrials = [\n    Trial.create(name=f\"trial_{count}\", config={\"x\": i}, metrics=[loss])\n    for count, i in enumerate(range(-5, 5))\n]\n\nreports = []\nfor trial in trials:\n    x = trial.config[\"x\"]\n    report = trial.success(loss=quadratic(x))\n    history.add(report)\n\nprint(history.df())\n</code></pre> <pre><code>          status  trial_seed  ... metric:loss (minimize) config:x\nname                          ...                                \ntrial_0  success        &lt;NA&gt;  ...                     25       -5\ntrial_1  success        &lt;NA&gt;  ...                     16       -4\ntrial_2  success        &lt;NA&gt;  ...                      9       -3\ntrial_3  success        &lt;NA&gt;  ...                      4       -2\ntrial_4  success        &lt;NA&gt;  ...                      1       -1\ntrial_5  success        &lt;NA&gt;  ...                      0        0\ntrial_6  success        &lt;NA&gt;  ...                      1        1\ntrial_7  success        &lt;NA&gt;  ...                      4        2\ntrial_8  success        &lt;NA&gt;  ...                      9        3\ntrial_9  success        &lt;NA&gt;  ...                     16        4\n\n[10 rows x 9 columns]\n</code></pre> <p>Typically, to use this inside of an optimization run, you would add the reports inside of a callback from your <code>Task</code>s. Please see the optimization guide for more details.</p> With an Optimizer and Scheduler <pre><code>from amltk.optimization import Trial, History, Metric\nfrom amltk.scheduling import Scheduler\nfrom amltk.pipeline import Searchable\n\nsearchable = Searchable(\"quad\", space={\"x\": (-5, 5)})\nn_workers = 2\n\ndef quadratic(x):\n    return x**2\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    cost = quadratic(x)\n    return trial.success(cost=cost)\n\noptimizer = SMACOptimizer(space=searchable, metrics=Metric(\"cost\", minimize=True), seed=42)\n\nscheduler = Scheduler.with_processes(2)\ntask = scheduler.task(quadratic)\n\n@scheduler.on_start(repeat=n_workers)\ndef launch_trial():\n    trial = optimizer.ask()\n    task(trial)\n\n@task.on_result\ndef add_to_history(report):\n    history.add(report)\n\n@task.on_done\ndef launch_another(_):\n    trial = optimizer.ask()\n    task(trial)\n\nscheduler.run(timeout=3)\n</code></pre>"},{"location":"reference/optimization/history/#querying","title":"Querying","text":"<p>The <code>History</code> can be queried by either an index or by the trial name.</p> History Querying [str]<pre><code>last_report = history[-1]\nprint(last_report)\nprint(history[last_report.name])\n</code></pre> <pre><code>Trial.Report(trial=Trial(name='trial_9', config={'x': 4}, bucket=PathBucket(PosixPath('trial-trial_9-2024-08-13T07:34:54.644221')), metrics=MetricCollection(metrics={'loss': Metric(name='loss', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 54, 644220), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 54, 644401), exception=None, values={'loss': 16})\nTrial.Report(trial=Trial(name='trial_9', config={'x': 4}, bucket=PathBucket(PosixPath('trial-trial_9-2024-08-13T07:34:54.644221')), metrics=MetricCollection(metrics={'loss': Metric(name='loss', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 54, 644220), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 54, 644401), exception=None, values={'loss': 16})\n</code></pre> <pre><code>for report in history:\n    print(report.name, f\"loss = {report.values['loss']}\")\n</code></pre> <pre><code>trial_0 loss = 25\ntrial_1 loss = 16\ntrial_2 loss = 9\ntrial_3 loss = 4\ntrial_4 loss = 1\ntrial_5 loss = 0\ntrial_6 loss = 1\ntrial_7 loss = 4\ntrial_8 loss = 9\ntrial_9 loss = 16\n</code></pre> <pre><code>sorted_history = history.sortby(\"loss\")\nprint(sorted_history[0])\n</code></pre> <pre><code>Trial.Report(trial=Trial(name='trial_5', config={'x': 0}, bucket=PathBucket(PosixPath('trial-trial_5-2024-08-13T07:34:54.644019')), metrics=MetricCollection(metrics={'loss': Metric(name='loss', minimize=True, bounds=None, fn=None)}), created_at=datetime.datetime(2024, 8, 13, 7, 34, 54, 644018), seed=None, fidelities={}, summary={}, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, reported_at=datetime.datetime(2024, 8, 13, 7, 34, 54, 644370), exception=None, values={'loss': 0})\n</code></pre>"},{"location":"reference/optimization/history/#filtering","title":"Filtering","text":"<p>You can filter the history by using the <code>filter()</code> method. This method takes a <code>Callable[[Trial.Report], bool]</code> and returns a new <code>History</code> with only the <code>Report</code>s that return <code>True</code> from the given function.</p> Filtering<pre><code>def is_even(report):\n    return report.config[\"x\"] % 2 == 0\n\neven_history = history.filter(is_even)\neven_history_df = even_history.df(profiles=False)\nprint(even_history_df)\n</code></pre> <pre><code>          status  trial_seed  ... metric:loss (minimize) config:x\nname                          ...                                \ntrial_1  success        &lt;NA&gt;  ...                     16       -4\ntrial_3  success        &lt;NA&gt;  ...                      4       -2\ntrial_5  success        &lt;NA&gt;  ...                      0        0\ntrial_7  success        &lt;NA&gt;  ...                      4        2\ntrial_9  success        &lt;NA&gt;  ...                     16        4\n\n[5 rows x 9 columns]\n</code></pre>"},{"location":"reference/optimization/metrics/","title":"Metrics","text":""},{"location":"reference/optimization/metrics/#metric","title":"Metric","text":"<p>A <code>Metric</code> to let optimizers know how to handle numeric values properly.</p> <p>A <code>Metric</code> is defined by a <code>.name: str</code> and whether it is better to <code>.minimize: bool</code> the metric. Further, you can specify <code>.bounds: tuple[lower, upper]</code> which can help optimizers and other code know how to treat metrics.</p> <p>To easily convert between <code>loss</code> and <code>score</code> of some value you can use the <code>loss()</code> and <code>score()</code> methods.</p> <p>If the metric is bounded, you can also make use of the <code>distance_to_optimal()</code> function which is the distance to the optimal value.</p> <p>In the case of optimization, we provide a <code>normalized_loss()</code> which normalized the value to be a minimization loss, that is also bounded if the metric itself is bounded.</p> <pre><code>from amltk.optimization import Metric\n\nacc = Metric(\"accuracy\", minimize=False, bounds=(0, 100))\n\nprint(f\"Distance: {acc.distance_to_optimal(90)}\")  # Distance to optimal.\nprint(f\"Loss: {acc.loss(90)}\")  # Something that can be minimized\nprint(f\"Score: {acc.score(90)}\")  # Something that can be maximized\nprint(f\"Normalized loss: {acc.normalized_loss(90)}\")  # Normalized loss\n</code></pre> <pre><code>Distance: 10.0\nLoss: -90.0\nScore: 90.0\nNormalized loss: 0.09999999999999998\n</code></pre>"},{"location":"reference/optimization/optimizers/","title":"Optimizers","text":""},{"location":"reference/optimization/optimizers/#optimizers","title":"Optimizers","text":"<p>An <code>Optimizer</code>'s goal is to achieve the optimal value for a given <code>Metric</code> or <code>Metrics</code> using repeated <code>Trials</code>.</p> <p>What differentiates AMLTK from other optimization libraries is that we rely solely on optimizers that support an \"Ask-and-Tell\" interface. This means we can \"Ask\" and optimizer for its next suggested <code>Trial</code>, and we can \"Tell\" it a <code>Report</code> when we have one. In fact, here's the required interface.</p> <pre><code>class Optimizer:\n\n    def tell(self, report: Trial.Report) -&gt; None: ...\n\n    def ask(self) -&gt; Trial: ...\n</code></pre> <p>Now we do require optimizers to implement these <code>ask()</code> and <code>tell()</code> methods, correctly filling in a <code>Trial</code> with appropriate parsing out results from the <code>Report</code>, as this will be different for every optimizer.</p> Why only Ask and Tell Optimizers? <ol> <li> <p>Easy Parallelization: Many optimizers handle running the function to optimize and hence     roll out their own parallelization schemes and store data in all various different ways. By taking     this repsonsiblity away from an optimzer and giving it to the user, we can easily parallelize how     we wish</p> </li> <li> <p>API maintenance: Many optimziers are research code and hence a bit unstable with resepct to their     API so wrapping around them can be difficult. By requiring this \"Ask-and-Tell\" interface,     we reduce the complexity of what is required of both the \"Optimizer\" and wrapping it.</p> </li> <li> <p>Full Integration: We can fully hook into the life cycle of a running optimizer. We are not relying     on the optimizer to support callbacks at every step of their hot-loop and as such, we     can fully leverage all the other systems of AutoML-toolkit</p> </li> <li> <p>Easy Integration: it makes developing and integrating new optimizers easy. You only have     to worry that the internal state of the optimizer is updated accordingly to these     two \"Ask\" and \"Tell\" events and that's it.</p> </li> </ol> <p>For a reference on implementing an optimizer you can refer to any of the following API Docs: * SMAC * NePs * Optuna * Random Search</p>"},{"location":"reference/optimization/optimizers/#integrating-your-own","title":"Integrating your own","text":"<p>The base <code>Optimizer</code> class, defines the API we require optimizers to implement.</p> <ul> <li><code>ask()</code> - Ask the optimizer for a     new <code>Trial</code> to evaluate.</li> <li><code>tell()</code> - Tell the optimizer     the result of the sampled config. This comes in the form of a     <code>Trial.Report</code>.</li> </ul> <p>Additionally, to aid users from switching between optimizers, the <code>preferred_parser()</code> method should return either a <code>parser</code> function or a string that can be used with <code>node.search_space(parser=..._)</code> to extract the search space for the optimizer.</p> <p>Please refer to the code of Random Search on github for an example of how to implement a new optimizer.</p>"},{"location":"reference/optimization/profiling/","title":"Profiling","text":""},{"location":"reference/optimization/profiling/#profiling","title":"Profiling","text":"<p>Whether for debugging, building an AutoML system or for optimization purposes, we provide a powerful <code>Profiler</code>, which can generate a <code>Profile</code> of different sections of code. This is particularly useful with <code>Trial</code>s, so much so that we attach one to every <code>Trial</code> made as <code>trial.profiler</code>.</p> <p>When done profiling, you can export all generated profiles as a dataframe using <code>profiler.df()</code>.</p> <pre><code>from amltk.profiling import Profiler\nimport numpy as np\n\nprofiler = Profiler()\n\nwith profiler(\"loading-data\"):\n    X = np.random.rand(1000, 1000)\n\nwith profiler(\"training-model\"):\n    model = np.linalg.inv(X)\n\nwith profiler(\"predicting\"):\n    y = model @ X\n\nprint(profiler.df())\n</code></pre> <pre><code>                memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nloading-data        1.973531e+09      1981534208  ...       wall    seconds\ntraining-model      1.981534e+09      2009014272  ...       wall    seconds\npredicting          2.009014e+09      2016751616  ...       wall    seconds\n\n[3 rows x 12 columns]\n</code></pre> <p>You'll find these profiles as keys in the <code>Profiler</code>, e.g. <code>python profiler[\"loading-data\"]</code>.</p> <p>This will measure both the time it took within the block but also the memory consumed before and after the block finishes, allowing you to get an estimate of the memory consumed.</p> Memory, vms vs rms <p>While not entirely accurate, this should be enough for info for most use cases.</p> <p>Given the main process uses 2GB of memory and the process then spawns a new process in which you are profiling, as you might do from a <code>Task</code>. In this new process you use another 2GB on top of that, then:</p> <ul> <li> <p>The virtual memory size (vms) will show 4GB as the new process will share the 2GB with the main process and have it's own 2GB.</p> </li> <li> <p>The resident set size (rss) will show 2GB as the new process will only have 2GB of it's own memory.</p> </li> </ul> <p>If you need to profile some iterator, like a for loop, you can use <code>Profiler.each()</code> which will measure the entire loop but also each individual iteration. This can be useful for iterating batches of a deep-learning model, splits of a cross-validator or really any loop with work you want to profile.</p> <pre><code>from amltk.profiling import Profiler\nimport numpy as np\n\nprofiler = Profiler()\n\nfor i in profiler.each(range(3), name=\"for-loop\"):\n    X = np.random.rand(1000, 1000)\n\nprint(profiler.df())\n</code></pre> <pre><code>            memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nfor-loop        2.000744e+09      2008219648  ...       wall    seconds\nfor-loop:0      2.000744e+09      2000744448  ...       wall    seconds\nfor-loop:1      2.000744e+09      2008219648  ...       wall    seconds\nfor-loop:2      2.008220e+09      2008219648  ...       wall    seconds\n\n[4 rows x 12 columns]\n</code></pre> <p>Lastly, to disable profiling without editing much code, you can always use <code>Profiler.disable()</code> and <code>Profiler.enable()</code> to toggle profiling on and off.</p>"},{"location":"reference/optimization/trials/","title":"Trials","text":""},{"location":"reference/optimization/trials/#trial-and-report","title":"Trial and Report","text":"<p><code>Trial</code> - typically the output of <code>Optimizer.ask()</code>, indicating what the optimizer would like to evaluate next.  e provide a host of convenience methods attached to the <code>Trial</code> to make it easy to save results, store artifacts, and more.</p> <p><code>Trial.Report</code> -  the output of a <code>trial.success(cost=...)</code> or <code>trial.fail(cost=...)</code> call. Provides an easy way to report back to the optimizer's <code>tell()</code>.</p>"},{"location":"reference/optimization/trials/#trial","title":"Trial","text":"<p>A <code>Trial</code> encapsulates some configuration that needs to be evaluated. Typically, this is what is generated by an <code>Optimizer.ask()</code> call.</p> <ul> <li> <p><code>trial.success()</code> to generate a success <code>Report</code>, typically passing what your chosen optimizer expects, e.g., <code>\"loss\"</code> or <code>\"cost\"</code>.</p> </li> <li> <p><code>trial.fail()</code> to generate a failure <code>Report</code>. If an exception is passed to <code>fail()</code>, it will be attached to the report along with any traceback it can deduce. Each <code>Optimizer</code> will take care of what to do from here.</p> </li> </ul> <pre><code>from amltk.optimization import Trial, Metric\nfrom amltk.store import PathBucket\n\ncost = Metric(\"cost\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n\n    with trial.profile(\"expensive-calculation\"):\n        cost = x**2 - y\n\n    return trial.success(cost=cost)\n\n# ... usually obtained from an optimizer\ntrial = Trial.create(\n    name=\"some-unique-name\",\n    config={\"x\": 1, \"y\": 2},\n    metrics=[cost]\n)\n\nreport = target_function(trial)\nprint(report.df())\n</code></pre> <pre><code>                   status  ...  profile:expensive-calculation:time:unit\nname                       ...                                         \nsome-unique-name  success  ...                                  seconds\n\n[1 rows x 22 columns]\n</code></pre> <p>What you can return with <code>trial.success()</code> or <code>trial.fail()</code> depends on the <code>metrics</code> of the trial. Typically, an optimizer will provide the trial with the list of metrics</p> <p>Some important properties are that they have a unique <code>.name</code> given the optimization run, a candidate <code>.config</code> to evaluate, a possible <code>.seed</code> to use, and an <code>.info</code> object, which is the optimizer specific information, if required by you.</p> <p>Reporting success (or failure)</p> <p>When using the <code>success()</code> method, make sure to provide values for all metrics specified in the <code>.metrics</code> attribute. Usually these are set by the optimizer generating the <code>Trial</code>.</p> <p>If you instead report using <code>fail()</code>, any metric not specified will be set to the <code>.worst</code> value of the metric.</p> <p>Each metric has a unique name, and it's crucial to use the correct names when reporting success, otherwise an error will occur.</p> Reporting success for metrics <p>For example:</p> <pre><code>from amltk.optimization import Trial, Metric\n\n# Gotten from some optimizer usually, i.e. via `optimizer.ask()`\ntrial = Trial.create(\n    name=\"example_trial\",\n    config={\"param\": 42},\n    metrics=[Metric(name=\"accuracy\", minimize=False)]\n)\n\n# Incorrect usage (will raise an error)\ntry:\n    report = trial.success(invalid_metric=0.95)\nexcept ValueError as error:\n    print(error)\n\n# Correct usage\nreport = trial.success(accuracy=0.95)\n</code></pre> <pre><code> Please provide a value for the metric 'accuracy' as  this is one of the metrics of the trial. \n Try `trial.success(accuracy=value, ...)`.\n</code></pre> <p>If using <code>Plugins</code>, they may insert some extra objects in the <code>.extra</code> dict.</p> <p>To profile your trial, you can wrap the logic you'd like to check with <code>trial.profile()</code>, which will automatically profile the block of code for memory before and after as well as time taken.</p> <p>If you've <code>profile()</code>'ed any intervals, you can access them by name through <code>trial.profiles</code>. Please see the <code>Profiler</code> for more.</p> Profiling with a trial. profile<pre><code>from amltk.optimization import Trial\n\ntrial = Trial.create(name=\"some-unique-name\", config={})\n\n# ... somewhere where you've begun your trial.\nwith trial.profile(\"some_interval\"):\n    for work in range(100):\n        pass\n\nprint(trial.profiler.df())\n</code></pre> <pre><code>               memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nsome_interval      2.008220e+09      2008219648  ...       wall    seconds\n\n[1 rows x 12 columns]\n</code></pre> <p>You can also record anything you'd like into the <code>.summary</code>, a plain <code>dict</code> or use <code>trial.store()</code> to store artifacts related to the trial.</p> What to put in <code>.summary</code>? <p>For large items, e.g. predictions or models, these are highly advised to <code>.store()</code> to disk, especially if using a <code>Task</code> for multiprocessing.</p> <p>Further, if serializing the report using the <code>report.df()</code>, returning a single row, or a <code>History</code> with <code>history.df()</code> for a dataframe consisting of many of the reports, then you'd likely only want to store things that are scalar and can be serialised to disk by a pandas DataFrame.</p>"},{"location":"reference/optimization/trials/#report","title":"Report","text":"<p>The <code>Trial.Report</code> encapsulates a <code>Trial</code>, its status and any metrics/exceptions that may have occured.</p> <p>Typically you will not create these yourself, but instead use <code>trial.success()</code> or <code>trial.fail()</code> to generate them.</p> <pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True)\n\ntrial = Trial.create(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\nwith trial.profile(\"fitting\"):\n    # Do some work\n    # ...\n    report = trial.success(loss=1)\n\nprint(report.df())\n</code></pre> <pre><code>        status  trial_seed  ... profile:fitting:time:kind profile:fitting:time:unit\nname                        ...                                                    \ntrial  success        &lt;NA&gt;  ...                      wall                   seconds\n\n[1 rows x 21 columns]\n</code></pre> <p>These reports are used to report back metrics to an <code>Optimizer</code> with <code>Optimizer.tell()</code> but can also be stored for your own uses.</p> <p>You can access the original trial with the <code>.trial</code> attribute, and the <code>Status</code> of the trial with the <code>.status</code> attribute.</p> <p>You may also want to check out the <code>History</code> class for storing a collection of <code>Report</code>s, allowing for an easier time to convert them to a dataframe or perform some common Hyperparameter optimization parsing of metrics.</p>"},{"location":"reference/pipelines/builders/","title":"Builders","text":""},{"location":"reference/pipelines/builders/#builders","title":"Builders","text":"<p>A pipeline of <code>Node</code>s is just an abstract representation of some implementation of a pipeline that will actually do things, for example an sklearn <code>Pipeline</code> or a Pytorch <code>Sequential</code>.</p> <p>To facilitate custom builders and to allow you to customize building, there is a explicit argument <code>builder=</code> required when calling <code>.build(builder=...)</code> on your pipeline.</p> <p>Each builder gives the various kinds of components an actual meaning, for example the <code>Split</code> with the sklearn <code>builder()</code>, translates to a <code>ColumnTransformer</code> and a <code>Sequential</code> translates to an sklearn <code>Pipeline</code>.</p>"},{"location":"reference/pipelines/builders/#scikit-learn","title":"Scikit-learn","text":""},{"location":"reference/pipelines/builders/#amltk.pipeline.builders.sklearn","title":"amltk.pipeline.builders.sklearn","text":"<p>The sklearn <code>builder()</code>, converts a pipeline made of <code>Node</code>s into a sklearn <code>Pipeline</code>.</p> <p>Requirements</p> <p>This requires <code>sklearn</code> which can be installed with:</p> <pre><code>pip install \"amltk[scikit-learn]\"\n\n# Or directly\npip install scikit-learn\n</code></pre> <p>Each kind of node corresponds to a different part of the end pipeline:</p> <code>Fixed</code><code>Component</code><code>Sequential</code><code>Split</code><code>Join</code><code>Choice</code> <p><code>Fixed</code> - The estimator will simply be cloned, allowing you to directly configure some object in a pipeline.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Fixed\n\nest = Fixed(RandomForestClassifier(n_estimators=25))\nbuilt_pipeline = est.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> <p><code>Component</code> - The estimator will be built from the component's config. This is mostly useful to allow a space to be defined for the component.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Component\n\nest = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\n# ... Likely get the configuration through an optimizer or sampling\nconfigured_est = est.configure({\"n_estimators\": 25})\n\nbuilt_pipeline = configured_est.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> <p><code>Sequential</code> - The sequential will be converted into a <code>Pipeline</code>, building whatever nodes are contained within in.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom amltk.pipeline import Component, Sequential\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, config={\"n_estimators\": 25})\n)\nbuilt_pipeline = pipeline.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre> \u00a0PCA?Documentation for PCA<pre>PCA(n_components=3)</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> <p><code>Split</code> - The split will be converted into a <code>ColumnTransformer</code>, where each path and the data that should go through it is specified by the split's config. You can provide a <code>ColumnTransformer</code> directly as the item to the <code>Split</code>, or otherwise if left blank, it will default to the standard sklearn one.</p> <p>You can use a <code>Fixed</code> with the special keyword <code>\"passthrough\"</code> as you might normally do with a <code>ColumnTransformer</code>.</p> <p>By default, we provide two special keywords you can provide to a <code>Split</code>, namely <code>\"categorical\"</code> and <code>\"numerical\"</code>, which will automatically configure a <code>ColumnTransorfmer</code> to pass the appropraite columns of a data-frame to the given paths.</p> <pre><code>from amltk.pipeline import Split, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    Component(\n        OneHotEncoder,\n        space={\n            \"min_frequency\": (0.01, 0.1),\n            \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n        },\n        config={\"drop\": \"first\"},\n    ),\n]\nnumerical_pipeline = [SimpleImputer(strategy=\"median\"), StandardScaler()]\n\nsplit = Split(\n    {\n        \"categorical\": categorical_pipeline,\n        \"numerical\": numerical_pipeline\n    }\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Split(Split-OtdhQu0d) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(categorical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item SimpleImputer(strategy='\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502                 \u2193                  \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u256d\u2500 Fixed(StandardScaler) \u2500\u256e        \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Component(OneHotEncoder) \u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 item StandardScaler()   \u2502        \u2502 \u2502\n\u2502 \u2502 \u2502 item   class                  \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f        \u2502 \u2502\n\u2502 \u2502 \u2502        OneHotEncoder(...)     \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u2502 \u2502 config {'drop': 'first'}      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502 space  {                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'min_frequency': ( \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.01,          \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.1            \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ),                 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'handle_unknown':  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        [                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'ignore',      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'infrequent_i\u2026 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ]                  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        }                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502                                        \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>You can manually specify the column selectors if you prefer.</p> <pre><code>split = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numbers\": numerical_pipeline,\n    },\n    config={\n        \"categories\": make_column_selector(dtype_include=object),\n        \"numbers\": make_column_selector(dtype_include=np.number),\n    },\n)\n</code></pre> <p><code>Join</code> - The join will be converted into a <code>FeatureUnion</code>.</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\njoin = Join(PCA(n_components=2), SelectKBest(k=3), name=\"my_feature_union\")\n\npipeline = join.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre> \u00a0my_feature_union: FeatureUnion?Documentation for my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                               ('SelectKBest', SelectKBest(k=3))])</pre> PCA\u00a0PCA?Documentation for PCA<pre>PCA(n_components=2)</pre> SelectKBest\u00a0SelectKBest?Documentation for SelectKBest<pre>SelectKBest(k=3)</pre> <p><code>Choice</code> - The estimator will be built from the chosen component's config. This is very similar to <code>Component</code>.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom amltk.pipeline import Choice\n\n# The choice here is usually provided during the `.configure()` step.\nestimator_choice = Choice(\n    RandomForestClassifier(),\n    MLPClassifier(),\n    config={\"__choice__\": \"RandomForestClassifier\"}\n)\n\nbuilt_pipeline = estimator_choice.build(\"sklearn\")\n</code></pre> <pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre> \u00a0RandomForestClassifier?Documentation for RandomForestClassifier<pre>RandomForestClassifier()</pre>"},{"location":"reference/pipelines/builders/#pytorch","title":"PyTorch","text":"Planned <p>If anyone has good knowledge of building pytorch networks in a more functional manner and would like to contribute, please feel free to reach out!</p> <p>At the moment, we do not provide any native support for <code>torch</code>. You can however make use of <code>skorch</code> to convert your networks to a scikit-learn interface, using the scikit-learn builder instead.</p>"},{"location":"reference/pipelines/pipeline/","title":"Pipeline","text":"<p>A pipeline is a collection of <code>Node</code>s that are connected together to form a directed acylic graph, where the nodes follow a parent-child relation ship. The purpose of these is to form some abstract representation of what you want to search over/optimize and then build into a concrete object.</p>"},{"location":"reference/pipelines/pipeline/#key-operations","title":"Key Operations","text":"<p>Once a pipeline is created, you can perform 3 very critical operations on it:</p> <ul> <li><code>search_space(parser=...)</code> - This will return the   search space of the pipeline, as defined by it's nodes. You can find the reference to   the available parsers and search spaces here.</li> <li><code>configure(config=...)</code> - This will return a   new pipeline where each node is configured correctly.</li> <li><code>build(builder=...)</code> - This will return some     concrete object from a configured pipeline. You can find the reference to     the available builders here.</li> </ul>"},{"location":"reference/pipelines/pipeline/#node","title":"Node","text":"<p>A <code>Node</code> is the basic building block of a pipeline. It contains various attributes, such as a</p> <ul> <li><code>.name</code> - The name of the node, which is used     to identify it in the pipeline.</li> <li><code>.item</code> - The concrete object or some function to construct one</li> <li><code>.space</code> - A search space to consider for this node</li> <li><code>.config</code> - The specific configuration to use for this     node once <code>build</code> is called.</li> <li><code>.nodes</code> - Other nodes that this node links to.</li> </ul> <p>To give syntactic meaning to these nodes, we have various subclasses. For example, <code>Sequential</code> is a node where the order of the <code>nodes</code> it contains matter, while a <code>Component</code> is a node that can be used to parametrize and construct a concrete object, but does not lead to anything else.</p> <p>Each node type here is either a leaf or a branch, where a branch has children, while while a leaf does not.</p> <p>There various components are listed here:</p>"},{"location":"reference/pipelines/pipeline/#component-leaf","title":"<code>Component</code> - <code>leaf</code>","text":"<p>A parametrizable node type with some way to build an object, given a configuration.</p> <pre><code>from amltk.pipeline import Component\nfrom dataclasses import dataclass\n\n@dataclass\nclass Model:\n    x: float\n\nc = Component(Model, space={\"x\": (0.0, 1.0)}, name=\"model\")\n</code></pre> <pre>\n<code>\u256d\u2500 Component(model) \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class Model(...)  \u2502\n\u2502 space {'x': (0.0, 1.0)} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/pipelines/pipeline/#searchable-leaf","title":"<code>Searchable</code> - <code>leaf</code>","text":"<p>A parametrizable node type that contains a search space that should be searched over, but does not provide a concrete object.</p> <pre><code>from amltk.pipeline import Searchable\n\ndef run_script(mode, n):\n    # ... run some actual script\n    pass\n\nscript_space = Searchable({\"mode\": [\"orange\", \"blue\", \"red\"], \"n\": (10, 100)})\n</code></pre> <pre>\n<code>\u256d\u2500 Searchable(Searchable-XL8TnRdi) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 space {'mode': ['orange', 'blue', 'red'], 'n': (10, 100)} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/pipelines/pipeline/#fixed-leaf","title":"<code>Fixed</code> - <code>leaf</code>","text":"<p>A non-parametrizable node type that contains an object that should be used as is.</p> <pre><code>from amltk.pipeline import Component, Fixed, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\n\nestimator = RandomForestClassifier()\n# ... pretend it was fit\nfitted_estimator = Fixed(estimator)\n</code></pre> <pre>\n<code>\u256d\u2500 Fixed(RandomForestClassifier) \u2500\u256e\n\u2502 item RandomForestClassifier()   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/pipelines/pipeline/#sequential-branch","title":"<code>Sequential</code> - <code>branch</code>","text":"<p>A node type which signifies an order between its children, such as a sequential set of preprocessing and estimator through which the data should flow.</p> <pre><code>from amltk.pipeline import Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)}),\n    name=\"my_pipeline\"\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/pipelines/pipeline/#choice-branch","title":"<code>Choice</code> - <code>branch</code>","text":"<p>A node type that signifies a choice between multiple children, usually chosen during configuration.</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = Choice(rf, mlp, name=\"estimator\")\n</code></pre> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/pipelines/pipeline/#split-branch","title":"<code>Split</code> - <code>branch</code>","text":"<p>A node where the output of the previous node is split amongst its children, according to it's configuration.</p> <pre><code>from amltk.pipeline import Component, Split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(drop=\"first\"),\n]\nnumerical_pipeline = Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})\n\npreprocessor = Split(\n    {\"categories\": categorical_pipeline, \"numerical\": numerical_pipeline},\n    name=\"my_split\"\n)\n</code></pre> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502 space {                        \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           'strategy': [        \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502               'mean',          \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'median'         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502           ]                    \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502       }                        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/pipelines/pipeline/#join-branch","title":"<code>Join</code> - <code>branch</code>","text":"<p>A node where the output of the previous node is sent all of its children.</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\njoin = Join(pca, kbest, name=\"my_feature_union\")\n</code></pre> <pre>\n<code>\u256d\u2500 Join(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/pipelines/pipeline/#syntax-sugar","title":"Syntax Sugar","text":"<p>You can connect these nodes together using either the constructors explicitly, as shown in the examples. We also provide some index operators:</p> <ul> <li><code>&gt;&gt;</code> - Connect nodes together to form a <code>Sequential</code></li> <li><code>&amp;</code> - Connect nodes together to form a <code>Join</code></li> <li><code>|</code> - Connect nodes together to form a <code>Choice</code></li> </ul> <p>There is also another short-hand that you may find useful to know:</p> <ul> <li><code>{comp1, comp2, comp3}</code> - This will automatically be converted into a     <code>Choice</code> between the given components.</li> <li><code>(comp1, comp2, comp3)</code> - This will automatically be converted into a     <code>Join</code> between the given components.</li> <li><code>[comp1, comp2, comp3]</code> - This will automatically be converted into a     <code>Sequential</code> between the given components.</li> </ul>"},{"location":"reference/pipelines/spaces/","title":"Spaces","text":""},{"location":"reference/pipelines/spaces/#spaces","title":"Spaces","text":"<p>A common requirement when performing optimization of some pipeline is to be able to parametrize it. To do so we often think about parametrize each component separately, with the structure of the pipeline adding additional constraints.</p> <p>To facilitate this, we allow the construction of piplines, where each part of the pipeline can contains a <code>.space</code>. When we wish to extract out the entire search space from the pipeline, we can call <code>search_space(parser=...)</code> on the root node of our pipeline, returning some sort of space object.</p> <p>Now there are unfortunately quite a few search space implementations out there. Some support concepts such as forbidden combinations, conditionals and functional constraints, while others are fully constrained just numerical parameters. Other reasons to choose a particular space representation is dependant upon some <code>Optimizer</code> you may wish to use, where typically they will only have one preferred search space representation.</p> <p>To generalize over this, AMLTK itself will not care what is in a <code>.space</code> of each part of the pipeline, i.e.</p> <pre><code>from amltk.pipeline import Component\n\nc = Component(object, space=\"hmmm, a str space?\")\n</code></pre> <pre>\n<code>\u256d\u2500 Component(object) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class object(...)    \u2502\n\u2502 space 'hmmm, a str space?' \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>What follow's below is a list of supported parsers you could pass <code>parser=</code> to extract a search space representation.</p> <ul> <li><code>ConfigSpace</code> - A parser for the   ConfigSpace library.</li> <li><code>Optuna</code> - A parser specifically for optuna.</li> </ul>"},{"location":"reference/scheduling/events/","title":"Events","text":""},{"location":"reference/scheduling/events/#events","title":"Events","text":"<p>One of the primary ways to respond to <code>@events</code> emitted with by a <code>Task</code> the <code>Scheduler</code> is through use of a callback.</p> <p>The reason for this is to enable an easier time for API's to utilize multiprocessing and remote compute from the <code>Scheduler</code>, without having to burden users with knowing the details of how to use multiprocessing.</p> <p>A callback subscribes to some event using a decorator but can also be done in a functional style if preferred. The below example is based on the event <code>@scheduler.on_start</code> but the same applies to all events.</p> DecoratorsFunctional <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.run()\n</code></pre>  hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.on_start(print_hello)\nscheduler.run()\n</code></pre>  hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>There are a number of ways to customize the behaviour of these callbacks, notably to control how often they get called and when they get called.</p> Callback customization <code>on('event', repeat=...)</code><code>on('event', max_calls=...)</code><code>on('event', when=...)</code><code>on('event', every=...)</code> <p>This will cause the callback to be called <code>repeat</code> times successively. This is most useful in combination with <code>@scheduler.on_start</code> to launch a number of tasks at the start of the scheduler.</p> <pre><code>from amltk import Scheduler\n\nN_WORKERS = 2\n\ndef f(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(f)\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    task.submit(1)\n\nscheduler.run()\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def on_start() (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 2\n\u2503   @on_future_done 2\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 2\n\u2517\u2501\u2501 \u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-Bzy5HHkJ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>Limit the number of times a callback can be called, after which, the callback will be ignored.</p> <pre><code>from asyncio import Future\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\ndef expensive_function(x: int) -&gt; int:\n    return x ** 2\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\n@scheduler.on_future_result(max_calls=3)\ndef print_result(future, result) -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\nscheduler.run()\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 4\n    @on_future_done 4\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 4\n    \u2514\u2500\u2500 def print_result(future, result) -&gt; None (3)\n</code>\n</pre> <p>A callable which takes no arguments and returns a <code>bool</code>. The callback will only be called when the <code>when</code> callable returns <code>True</code>.</p> <p>Below is a rather contrived example, but it shows how we can use the <code>when</code> parameter to control when the callback is called.</p> <pre><code>import random\nfrom amltk.scheduling import Scheduler\n\nLOCALE = random.choice([\"English\", \"German\"])\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start(when=lambda: LOCALE == \"English\")\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n@scheduler.on_start(when=lambda: LOCALE == \"German\")\ndef print_guten_tag() -&gt; None:\n    print(\"guten tag\")\n\nscheduler.run()\n</code></pre>  hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u251c\u2500\u2500 def print_hello() -&gt; None (1)\n    \u2514\u2500\u2500 def print_guten_tag() -&gt; None\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> <p>Only call the callback every <code>every</code> times the event is emitted. This includes the first time it's called.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n# Print \"hello\" only every 2 times the scheduler starts.\n@scheduler.on_start(every=2)\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n# Run the scheduler 5 times\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\n</code></pre>  hello hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 5\n    @on_start 5\n    \u2514\u2500\u2500 def print_hello() -&gt; None (2)\n    @on_finishing 5\n    @on_finished 5\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre>"},{"location":"reference/scheduling/events/#emitter-subscribers-and-events","title":"Emitter, Subscribers and Events","text":"<p>This part of the documentation is not necessary to understand or use for AMLTK. People wishing to build tools upon AMLTK may still find this a useful component to add to their arsenal.</p> <p>The core of making this functionality work is the <code>Emitter</code>. Its purpose is to have <code>@events</code> that can be emitted and subscribed to. Classes like the <code>Scheduler</code> and <code>Task</code> carry around with them an <code>Emitter</code> to enable all of this functionality.</p> <p>Creating an <code>Emitter</code> is rather straight-forward, but we must also create <code>Events</code> that people can subscribe to.</p> <pre><code>from amltk.scheduling import Emitter, Event\nemitter = Emitter(\"my-emitter\")\n\nevent: Event[int] = Event(\"my-event\") # (1)!\n\n@emitter.on(event)\ndef my_callback(x: int) -&gt; None:\n    print(f\"Got {x}!\")\n\nemitter.emit(event, 42) # (2)!\n</code></pre> <ol> <li>The typing <code>Event[int]</code> is used to indicate that the event will be emitting     an integer. This is not necessary, but it is useful for type-checking and     documentation.</li> <li>The <code>emitter.emit(event, 42)</code> is used to emit the event. This will call     all the callbacks registered for the event, i.e. <code>my_callback()</code>.</li> </ol> <p>Independent Events</p> <p>Given a single <code>Emitter</code> and a single instance of an <code>Event</code>, there is no way to have different <code>@events</code> for callbacks. There are two options, both used extensively in AMLTK.</p> <p>The first is to have different <code>Events</code> quite naturally, i.e. you distinguish between different things that can happen. However, you often want to have different objects emit the same <code>Event</code> but have different callbacks for each object.</p> <p>This makes most sense in the context of a <code>Task</code> the <code>Event</code> instances are shared as class variables in the <code>Task</code> class, however a user likely want's to subscribe to the <code>Event</code> for a specific instance of the <code>Task</code>.</p> <p>This is where the second option comes in, in which each object carries around its own <code>Emitter</code> instance. This is how a user can subscribe to the same kind of <code>Event</code> but individually for each <code>Task</code>.</p> <p>However, to shield users from this and to create named access points for users to subscribe to, we can use the <code>Subscriber</code> class, conveniently created by the <code>Emitter.subscriber()</code> method.</p> <pre><code>from amltk.scheduling import Emitter, Event\nemitter = Emitter(\"my-emitter\")\n\nclass GPT:\n\n    event: Event[str] = Event(\"my-event\")\n\n    def __init__(self) -&gt; None:\n        self.on_answer: Subscriber[str] = emitter.subscriber(self.event)\n\n    def ask(self, question: str) -&gt; None:\n        emitter.emit(self.event, \"hello world!\")\n\ngpt = GPT()\n\n@gpt.on_answer\ndef print_answer(answer: str) -&gt; None:\n    print(answer)\n\ngpt.ask(\"What is the conical way for an AI to greet someone?\")\n</code></pre> <p>Typically these event based systems make little sense in a synchronous context, however with the <code>Scheduler</code> and <code>Task</code> classes, they are used to enable a simple way to use multiprocessing and remote compute.</p>"},{"location":"reference/scheduling/executors/","title":"Executors","text":""},{"location":"reference/scheduling/executors/#executors","title":"Executors","text":"<p>The <code>Scheduler</code> uses an <code>Executor</code>, a builtin python native to <code>submit(f, *args, **kwargs)</code> to be computed else where, whether it be locally or remotely.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler(executor=...)\n</code></pre> <p>Some parallelism libraries natively support this interface while we can wrap others. You can also wrap you own custom backend by using the <code>Executor</code> interface, which is relatively simple to implement.</p> <p>If there's any executor background you wish to integrate, we would be happy to consider it and greatly appreciate a PR!</p>"},{"location":"reference/scheduling/executors/#python","title":"<code>Python</code>","text":"<p>Python supports the <code>Executor</code> interface natively with the <code>concurrent.futures</code> module for processes with the <code>ProcessPoolExecutor</code> and <code>ThreadPoolExecutor</code> for threads.</p> Usage Process Pool ExecutorThread Pool Executor <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)  # (1)!\n</code></pre> <ol> <li>Explicitly use the <code>with_processes</code> method to create a <code>Scheduler</code> with    a <code>ProcessPoolExecutor</code> with 2 workers.    <pre><code> from concurrent.futures import ProcessPoolExecutor\n from amltk.scheduling import Scheduler\n\n executor = ProcessPoolExecutor(max_workers=2)\n scheduler = Scheduler(executor=executor)\n</code></pre></li> </ol> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_threads(2)  # (1)!\n</code></pre> <ol> <li>Explicitly use the <code>with_threads</code> method to create a <code>Scheduler</code> with    a <code>ThreadPoolExecutor</code> with 2 workers.    <pre><code> from concurrent.futures import ThreadPoolExecutor\n from amltk.scheduling import Scheduler\n\n executor = ThreadPoolExecutor(max_workers=2)\n scheduler = Scheduler(executor=executor)\n</code></pre></li> </ol> <p>Why to not use threads</p> <p>Python also defines a <code>ThreadPoolExecutor</code> but there are some known drawbacks to offloading heavy compute to threads. Notably, there's no way in python to terminate a thread from the outside while it's running.</p>"},{"location":"reference/scheduling/executors/#dask","title":"<code>dask</code>","text":"<p>Dask and the supporting extension <code>dask.distributed</code> provide a robust and flexible framework for scheduling compute across workers.</p> <p>Example</p> <pre><code>from dask.distributed import Client\nfrom amltk.scheduling import Scheduler\n\nclient = Client(...)\nexecutor = client.get_executor()\nscheduler = Scheduler(executor=executor)\n\n# Important to do if the program will continue!\nclient.close()\n</code></pre>"},{"location":"reference/scheduling/executors/#dask-jobqueue","title":"<code>dask-jobqueue</code>","text":"<p><code>dask-jobqueue</code> is a package for scheduling jobs across common clusters setups such as PBS, Slurm, MOAB, SGE, LSF, and HTCondor.</p> <p>Please see the <code>dask-jobqueue</code> documentation In particular, we only control the parameter <code>n_workers=</code> and use <code>adaptive=</code> to control where to use <code>adapt()</code> or <code>scale()</code> method, every other keyword is forwarded to the relative cluster implementation.</p> <p>In general, you should specify the requirements of each individual worker and tune your load with the <code>n_workers=</code> parameter.</p> <p>If you have any tips, tricks, working setups, gotchas, please feel free to leave a PR or simply an issue!</p> Usage SlurmOthers <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_slurm(\n    n_workers=10,  # (1)!\n    adaptive=True,\n    queue=...,\n    cores=4,\n    memory=\"6 GB\",\n    walltime=\"00:10:00\"\n)\n</code></pre> <ol> <li>The <code>n_workers</code> parameter is used to set the number of workers    to start with.    The <code>adapt()</code>    method will be called on the cluster to dynamically scale up to <code>n_workers=</code> based on    the load.    The <code>with_slurm</code> method will create a <code>SLURMCluster</code>    and pass it to the <code>Scheduler</code> constructor.    <pre><code>from dask_jobqueue import SLURMCluster\nfrom amltk.scheduling import Scheduler\n\ncluster = SLURMCluster(\n    queue=...,\n    cores=4,\n    memory=\"6 GB\",\n    walltime=\"00:10:00\"\n)\ncluster.adapt(max_workers=10)\nexecutor = cluster.get_client().get_executor()\nscheduler = Scheduler(executor=executor)\n</code></pre></li> </ol> <p>Running outside the login node</p> <p>If you're running the scheduler itself in a job, this may not work on some cluster setups. The scheduler itself is lightweight and can run on the login node without issue. However you should make sure to offload heavy computations to a worker.</p> <p>If you get it to work, for example in an interactive job, please let us know!</p> <p>Modifying the launch command</p> <p>On some cluster commands, you'll need to modify the launch command. You can use the following to do so:</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_slurm(n_workers=..., submit_command=\"sbatch --extra\"\n</code></pre> <p>Please see the <code>dask-jobqueue</code> documentation and the following methods:</p> <ul> <li><code>Scheduler.with_pbs()</code></li> <li><code>Scheduler.with_lsf()</code></li> <li><code>Scheduler.with_moab()</code></li> <li><code>Scheduler.with_sge()</code></li> <li><code>Scheduler.with_htcondor()</code></li> </ul>"},{"location":"reference/scheduling/executors/#loky","title":"<code>loky</code>","text":"<p>Loky is the default backend executor behind <code>joblib</code>, the parallelism that powers scikit-learn.</p> Usage SimpleExplicit <pre><code>from amltk import Scheduler\n\n# Pass any arguments you would pass to `loky.get_reusable_executor`\nscheduler = Scheduler.with_loky(...)\n</code></pre> <pre><code>import loky\nfrom amltk import Scheduler\n\nscheduler = Scheduler(executor=loky.get_reusable_executor(...))\n</code></pre> BLAS numeric backend <p>The loky executor seems to pick up on a different BLAS library (from scipy) which is different than those used by jobs from something like a <code>ProcessPoolExecutor</code>.</p> <p>This is likely not to matter for a majority of use-cases.</p>"},{"location":"reference/scheduling/executors/#ray","title":"<code>ray</code>","text":"<p>Ray is an open-source unified compute framework that makes it easy to scale AI and Python workloads \u2014 from reinforcement learning to deep learning to tuning, and model serving.</p> <p>In progress</p> <p>Ray is currently in the works of supporting the Python <code>Executor</code> interface. See this PR for more info.</p>"},{"location":"reference/scheduling/executors/#airflow","title":"<code>airflow</code>","text":"<p>Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. Their list of integrations to platforms is endless but features compute platforms such as Kubernetes, AWS, Microsoft Azure and GCP.</p> <p>In progress</p> <p>We plan to support <code>airflow</code> in the future. If you'd like to help out, please reach out to us!</p>"},{"location":"reference/scheduling/executors/#debugging","title":"Debugging","text":"<p>Sometimes you'll need to debug what's going on and remove the noise of processes and parallelism. For this, we have implemented a very basic <code>SequentialExecutor</code> to run everything in a sequential manner!</p> EasyExplicit <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_sequential()\n</code></pre> <pre><code>from amltk.scheduling import Scheduler, SequetialExecutor\n\nscheduler = Scheduler(executor=SequentialExecutor())\n</code></pre> <p>Recursion</p> <p>If you use The <code>SequentialExecutor</code>, be careful that the stack of function calls can get quite large, quite quick. If you are using this for debugging, keep the number of submitted tasks from callbacks small and focus in on debugging. If using this for sequential ordering of operations, prefer to use <code>with_processes(1)</code> as this will still maintain order but not have these stack issues.</p>"},{"location":"reference/scheduling/plugins/","title":"Plugins","text":""},{"location":"reference/scheduling/plugins/#plugins","title":"Plugins","text":"<p>Plugins are a way to modify a <code>Task</code>, to add new functionality or change the behaviour of what goes on in the function that is dispatched to the <code>Scheduler</code>.</p> <p>Some plugins will also add new <code>@event</code>s to a task, which can be used to respond accordingly to something that may have occured with your task.</p> <p>You can add a plugin to a <code>Task</code> as so:</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef some_function(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(1)\n\n# When creating a task with the scheduler\ntask = scheduler.task(some_function, plugins=[Limiter(max_calls=10)])\n\n\n# or directly to a Task\ntask = Task(some_function, scheduler=scheduler, plugins=[Limiter(max_calls=10)])\n</code></pre> <pre>\n<code>\u256d\u2500 Task some_function(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/10                                                               \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-some_function-ZCzxQpJY \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#limiter","title":"Limiter","text":""},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.limiter","title":"amltk.scheduling.plugins.limiter","text":"<p>The <code>Limiter</code> can limit the number of times a function is called, how many concurrent instances of it can be running, or whether it can run while another task is running.</p> <p>The functionality of the <code>Limiter</code> could also be implemented without a plugin but it gives some nice utility.</p> Usage <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-dFE6Yw2e \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <code>@events</code> <code>@call-limit-reached</code><code>@concurrent-limit-reached</code><code>@disabled-due-to-running-task</code>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.Limiter.CALL_LIMIT_REACHED","title":"amltk.scheduling.plugins.Limiter.CALL_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CALL_LIMIT_REACHED: Event[..., Any] = Event(\n    \"call-limit-reached\"\n)\n</code></pre> <p>The event emitted when the task has reached its call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-l6391QGS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.Limiter.CONCURRENT_LIMIT_REACHED","title":"amltk.scheduling.plugins.Limiter.CONCURRENT_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CONCURRENT_LIMIT_REACHED: Event[..., Any] = Event(\n    \"concurrent-limit-reached\"\n)\n</code></pre> <p>The event emitted when the task has reached its concurrent call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_concurrent=2)])\n\n@task.on(\"concurrent-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Concurrent 0/2                                                           \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-7d0TnlVN \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.Limiter.DISABLED_DUE_TO_RUNNING_TASK","title":"amltk.scheduling.plugins.Limiter.DISABLED_DUE_TO_RUNNING_TASK  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED_DUE_TO_RUNNING_TASK: Event[..., Any] = Event(\n    \"disabled-due-to-running-task\"\n)\n</code></pre> <p>The event emitter when the task was not submitted due to some other running task.</p> <p>Will call any subscribers with the task as first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler, Task\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\nother_task = scheduler.task(fn)\ntask = scheduler.task(fn, plugins=[Limiter(not_while_running=other_task)])\n\n@task.on(\"disabled-due-to-running-task\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Not While def fn(...) Ref: Task-fn-uxHmfub3                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-gS2x55te \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#pynisher","title":"Pynisher","text":""},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.pynisher","title":"amltk.scheduling.plugins.pynisher","text":"<p>The <code>PynisherPlugin</code> uses pynisher to place memory, walltime and cputime constraints on processes, crashing them if these limits are reached. These default units are <code>bytes (\"B\")</code> and <code>seconds (\"s\")</code> but you can also use other units, please see the relevant API doc.</p> <p>It's best use is when used with <code>Scheduler.with_processes()</code> to have work performed in processes.</p> <p>Requirements</p> <p>This required <code>pynisher</code> which can be installed with:</p> <pre><code>pip install amltk[pynisher]\n\n# Or directly\npip install pynisher\n</code></pre> Usage <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-wPjLi9hQ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <code>@events</code> <code>@pynisher-timeout</code><code>@pynisher-memory-limit</code><code>@pynisher-cputime-limit</code><code>@pynisher-walltime-limit</code> Scheduler Executor <p>This will place process limits on the task as soon as it starts running, whever it may be running. If you are using <code>Scheduler.with_sequential()</code> then this will place limits on the main process, likely not what you want. This also does not work with a <code>ThreadPoolExecutor</code>.</p> <p>If using this with something like [<code>dask-jobqueue</code>], then this will place limits on the workers it spawns. It would be better to place limits directly through dask job-queue then.</p> Platform Limitations (Mac, Windows) <p>Pynisher has some limitations with memory on Mac and Windows: automl/pynisher#features</p> <p>You can check this with <code>PynisherPlugin.supports(\"memory\")</code>, <code>PynisherPlugin.supports(\"cpu_time\")</code> and <code>PynisherPlugin.supports(\"wall_time\")</code>. See <code>PynisherPlugin.supports()</code></p>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.pynisher.PynisherPlugin.TIMEOUT","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.TIMEOUT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TIMEOUT: Event[[TimeoutException], Any] = Event(\n    \"pynisher-timeout\"\n)\n</code></pre> <p>A Task timed out, either due to the wall time or cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-hZ03FLTJ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.pynisher.PynisherPlugin.MEMORY_LIMIT_REACHED","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.MEMORY_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MEMORY_LIMIT_REACHED: Event[[MemoryLimitException], Any] = (\n    Event(\"pynisher-memory-limit\")\n)\n</code></pre> <p>A Task was submitted but reached it's memory limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport numpy as np\n\ndef f(x: int) -&gt; int:\n    x = np.arange(100000000)\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(memory_limit=(1, \"KB\")))\n\n@task.on(\"pynisher-memory-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory     Wall Time  CPU Time                                          \u2502 \u2502\n\u2502 \u2502  (1, 'KB')  None       None                                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-WI0zTjUy \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.pynisher.PynisherPlugin.CPU_TIME_LIMIT_REACHED","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.CPU_TIME_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CPU_TIME_LIMIT_REACHED: Event[\n    [CpuTimeoutException], Any\n] = Event(\"pynisher-cputime-limit\")\n</code></pre> <p>A Task was submitted but reached it's cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    i = 0\n    while True:\n        # Keep busying computing the answer to everything\n        i += 1\n\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(cputime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-cputime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    None       (1, 's')                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-mbWR8gX9 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.pynisher.PynisherPlugin.WALL_TIME_LIMIT_REACHED","title":"amltk.scheduling.plugins.pynisher.PynisherPlugin.WALL_TIME_LIMIT_REACHED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WALL_TIME_LIMIT_REACHED: Event[\n    [WallTimeoutException], Any\n] = Event(\"pynisher-walltime-limit\")\n</code></pre> <p>A Task was submitted but reached it's wall time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-walltime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <pre>\n<code>\u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-i8OWPrE1 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#comm","title":"Comm","text":""},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.comm","title":"amltk.scheduling.plugins.comm","text":"<p>The <code>Comm.Plugin</code> enables two way-communication with running <code>Task</code>.</p> <p>The <code>Comm</code> provides an easy interface to communicate while the <code>Comm.Msg</code> encapsulates messages between the main process and the <code>Task</code>.</p> Usage <p>To setup a <code>Task</code> to work with a <code>Comm</code>, the <code>Task</code> must accept a <code>comm</code> as a keyword argument. This is to prevent it conflicting with any args passed through during the call to <code>submit()</code>.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef powers_of_two(start: int, n: int, *, comm: Comm) -&gt; None:\n    with comm.open():\n        for i in range(n):\n            comm.send(start ** (i+1))\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(powers_of_two, plugins=Comm.Plugin())\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(2, 5)\n\n@task.on(\"comm-open\")\ndef on_open(msg: Comm.Msg):\n    print(f\"Task has opened | {msg}\")\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>Task has opened | Comm.Msg(kind=&lt;Kind.OPEN: 'open'&gt;, data=None)\n[2, 4, 8, 16, 32]\n</code></pre> <p>You can also block a worker, waiting for a response from the main process, allowing for the worker to <code>request()</code> data from the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef my_worker(comm: Comm, n_tasks: int) -&gt; None:\n    with comm.open():\n        for task_number in range(n_tasks):\n            task = comm.request(task_number)\n            comm.send(f\"Task recieved {task} for {task_number}\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(my_worker, plugins=Comm.Plugin())\n\nitems = [\"A\", \"B\", \"C\"]\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(n_tasks=3)\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    task_number = msg.data\n    msg.respond(items[task_number])\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>['Task recieved A for 0', 'Task recieved B for 1', 'Task recieved C for 2']\n</code></pre> <code>@events</code> <code>@comm-message</code><code>@comm-request</code><code>@comm-open</code><code>@comm-close</code>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.comm.Comm.MESSAGE","title":"amltk.scheduling.plugins.comm.Comm.MESSAGE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MESSAGE: Event[[Msg], Any] = Event('comm-message')\n</code></pre> <p>A Task has sent a message to the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(x: int, comm: Comm | None = None) -&gt; int:\n    assert comm is not None\n    with comm.open():\n        comm.send(x + 1)\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@task.on(\"comm-message\")\ndef callback(msg: Comm.Msg):\n    print(msg.data)\n</code></pre> <pre>\n<code>\u256d\u2500 Task fn(x: int, comm: amltk.scheduling.plugins.comm.Comm | None = None) -&gt; \u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-dRCPpRTk \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.comm.Comm.REQUEST","title":"amltk.scheduling.plugins.comm.Comm.REQUEST  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REQUEST: Event[[Msg], Any] = Event('comm-request')\n</code></pre> <p>A Task has sent a request.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef greeter(greeting: str, comm: Comm | None = None) -&gt; None:\n    assert comm is not None\n    with comm.open():\n        name = comm.request()\n        comm.send(f\"{greeting} {name}!\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(greeter, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit(\"Hello\")\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    msg.respond(\"Alice\")\n\n@task.on(\"comm-message\")\ndef on_msg(msg: Comm.Msg):\n    print(msg.data)\n\nscheduler.run()\n</code></pre>  Hello Alice!  <pre>\n<code>\u256d\u2500 Task greeter(greeting: str, comm: amltk.scheduling.plugins.comm.Comm | None\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-greeter-QEUC6xdm \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.comm.Comm.OPEN","title":"amltk.scheduling.plugins.comm.Comm.OPEN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OPEN: Event[[Msg], Any] = Event('comm-open')\n</code></pre> <p>The task has signalled it's open.</p> <p>```python exec=\"true\" source=\"material-block\" html=\"true\" hl_lines=\"5 15-17\"  from amltk.scheduling import Scheduler  from amltk.scheduling.plugins import Comm</p> <p>def fn(comm: Comm) -&gt; None:      with comm.open():          pass from amltk._doc import make_picklable; make_picklable(fn)  # markdown-exec: hide</p> <p>scheduler = Scheduler.with_processes(1)  task = scheduler.task(fn, plugins=Comm.Plugin())</p> <p>@scheduler.on_start  def on_start():      task.submit()</p> <p>@task.on(\"comm-open\")  def callback(msg: Comm.Msg):      print(\"Comm has just used comm.open()\")</p> <p>scheduler.run()  from amltk._doc import doc_print; doc_print(print, task)  # markdown-exec: hide  ```</p>"},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.comm.Comm.CLOSE","title":"amltk.scheduling.plugins.comm.Comm.CLOSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CLOSE: Event[[Msg], Any] = Event('comm-close')\n</code></pre> <p>The task has signalled it's close.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm) -&gt; None:\n    with comm.open():\n        pass\n        # Will send a close signal to the main process as it exists this block\n\n    print(\"Done\")\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit()\n\n@task.on(\"comm-close\")\ndef on_close(msg: Comm.Msg):\n    print(f\"Worker close with {msg}\")\n\nscheduler.run()\n</code></pre>  Worker close with Comm.Msg(kind=, data=None)  <pre>\n<code>\u256d\u2500 Task fn(comm: amltk.scheduling.plugins.comm.Comm) -&gt; None \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-rV5lGSkk \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> Supported Backends <p>The current implementation relies on <code>Pipe</code> which only works between processes on the same system/cluster. There is also limited support with <code>dask</code> backends.</p> <p>This could be extended to allow for web sockets or other forms of connections but requires time. Please let us know in the Github issues if this is something you are interested in!</p>"},{"location":"reference/scheduling/plugins/#threadpoolctl","title":"ThreadPoolCTL","text":""},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.threadpoolctl","title":"amltk.scheduling.plugins.threadpoolctl","text":"<p>The <code>ThreadPoolCTLPlugin</code> if useful for parallel training of models. Without limiting with threadpoolctl, the number of threads used by a given model may oversubscribe to resources and cause significant slowdowns.</p> <p>This is the mechanism employed by scikit-learn to limit the number of threads used by a given model.</p> <p>See threadpoolctl documentation.</p> <p>Requirements</p> <p>This requires <code>threadpoolctl</code> which can be installed with:</p> <pre><code>pip install amltk[threadpoolctl]\n\n# Or directly\npip install threadpoolctl\n</code></pre> Usage <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\nscheduler = Scheduler.with_processes(1)\n\ndef f() -&gt; None:\n    # ... some task that respects the limits set by threadpoolctl\n    pass\n\ntask = scheduler.task(f, plugins=ThreadPoolCTLPlugin(max_threads=1))\n</code></pre> <pre>\n<code>\u256d\u2500 Task f() -&gt; None \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin threadpoolctl-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Max Threads  User-API                                                   \u2502 \u2502\n\u2502 \u2502  1            None                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-9WVsJgNn \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#warning-filter","title":"Warning Filter","text":""},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.warning_filter","title":"amltk.scheduling.plugins.warning_filter","text":"<p>The <code>WarningFilter</code> if used to automatically filter out warnings from a <code>Task</code> as it runs.</p> <p>This wraps your function in context manager <code>warnings.catch_warnings()</code> and applies your arguments to <code>warnings.filterwarnings()</code>, as you would normally filter warnings in Python.</p> Usage <pre><code>import warnings\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import WarningFilter\n\ndef f() -&gt; None:\n    warnings.warn(\"This is a warning\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(f, plugins=WarningFilter(\"ignore\"))\n</code></pre> <pre>\n<code>\u256d\u2500 Task f() -&gt; None \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin warning-filter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Args         Kwargs                                                     \u2502 \u2502\n\u2502 \u2502  ('ignore',)  {}                                                         \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-4ZryZEh6 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"reference/scheduling/plugins/#creating-your-own-plugin","title":"Creating Your Own Plugin","text":""},{"location":"reference/scheduling/plugins/#amltk.scheduling.plugins.plugin","title":"amltk.scheduling.plugins.plugin","text":"<p>A plugin that can be attached to a Task.</p> <p>By inheriting from a <code>Plugin</code>, you can hook into a <code>Task</code>. A plugin can affect, modify and extend its behaviours. Please see the documentation of the methods for more information. Creating a plugin is only necesary if you need to modify actual behaviour of the task. For siply hooking into the lifecycle of a task, you can use the <code>@events</code> that a <code>Task</code> emits.</p> Creating a Plugin <p>For a full example of a simple plugin, see the <code>Limiter</code> plugin which prevents the task being submitted if for example, it has already been submitted too many times.</p> <p>The below example shows how to create a plugin that prints the task name before submitting it. It also emits an event when the task is submitted.</p> <pre><code>from __future__ import annotations\nfrom typing import Callable\n\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Plugin\nfrom amltk.scheduling.events import Event\n\n# A simple plugin that prints the task name before submitting\nclass Printer(Plugin):\n    name = \"my-plugin\"\n\n    # Define an event the plugin will emit\n    # Event[Task] indicates the callback for the event will be called with the task\n    PRINTED: Event[str] = Event(\"printer-msg\")\n\n    def __init__(self, greeting: str):\n        self.greeting = greeting\n        self.n_greetings = 0\n\n    def attach_task(self, task) -&gt; None:\n        self.task = task\n        # Register an event with the task, this lets the task know valid events\n        # people can subscribe to and helps it show up in visuals\n        task.add_event(self.PRINTED)\n        task.on_submitted(self._print_submitted, hidden=True)  # You can hide this callback from visuals\n\n    def pre_submit(self, fn, *args, **kwargs) -&gt; tuple[Callable, tuple, dict]:\n        print(f\"{self.greeting} for {self.task} {args} {kwargs}\")\n        self.n_greetings += 1\n        return fn, args, kwargs\n\n    def _print_submitted(self, future, *args, **kwargs) -&gt; None:\n        msg = f\"Task was submitted {self.task} {args} {kwargs}\"\n        self.task.emit(self.PRINTED, msg)  # Emit the event with a msg\n\n    def __rich__(self):\n        # Custome how the plugin is displayed in rich (Optional)\n        # rich is an optional dependancy of amltk so we move the imports into here\n        from rich.panel import Panel\n\n        return Panel(\n            f\"Greeting: {self.greeting} ({self.n_greetings})\",\n            title=f\"Plugin {self.name}\"\n        )\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=[Printer(\"Hello\")])\n\n@scheduler.on_start\ndef on_start():\n    task.submit(15)\n\n@task.on(\"printer-msg\")\ndef callback(msg: str):\n    print(\"\\nmsg\")\n\nscheduler.run()\n</code></pre>  Hello for Task(unique_ref=Task-fn-TyEI9BtB, plugins=[&lt;_code_block_n152_.Printer object at 0x7efd593eaa40&gt;]) (15,) {}  msg  <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin my-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Greeting: Hello (1)                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-fn-TyEI9BtB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>All methods are optional, and you can choose to implement only the ones you need. Most plugins will likely need to implement the <code>attach_task()</code> method, which is called when the plugin is attached to a task. In this method, you can for example subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> <p>Plugins are also encouraged to utilize the events of a <code>Task</code> to further hook into the lifecycle of the task. For exampe, by saving a reference to the task in the <code>attach_task()</code> method, you can use the <code>emit()</code> method of the task to emit your own specialized events.</p>"},{"location":"reference/scheduling/queue_monitor/","title":"Queue monitor","text":""},{"location":"reference/scheduling/queue_monitor/#queue-monitor","title":"Queue Monitor","text":"<p>A <code>QueueMonitor</code> is a monitor for the scheduler queue.</p> <p>This module contains a monitor for the scheduler queue. The monitor tracks the queue state at every event emitted by the scheduler. The data can be converted to a pandas DataFrame or plotted as a stacked barchart.</p> <p>Monitoring Frequency</p> <p>To prevent repeated polling, we sample the scheduler queue at every scheduler event. This is because the queue is only modified upon one of these events. This means we don't need to poll the queue at a fixed interval. However, if you need more fine grained updates, you can add extra events/timings at which the monitor should <code>update()</code>.</p> <p>Performance impact</p> <p>If your tasks and callbacks are very fast (~sub 10ms), then the monitor has a non-nelgible impact however for most use cases, this should not be a problem. As anything, you should profile how much work the scheduler can get done, with and without the monitor, to see if it is a problem for your use case.</p> <p>In the below example, we have a very fast running function that runs on repeat, sometimes too fast for the scheduler to keep up, letting some futures buildup needing to be processed.</p> <pre><code>import time\nimport matplotlib.pyplot as plt\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.queue_monitor import QueueMonitor\n\ndef fast_function(x: int) -&gt; int:\n    return x + 1\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\nmonitor = QueueMonitor(scheduler)\ntask = scheduler.task(fast_function)\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef start():\n    task.submit(1)\n\n@task.on_result\ndef result(_, x: int):\n    if scheduler.running():\n        task.submit(x)\n\nscheduler.run(timeout=1)\ndf = monitor.df()\nprint(df)\n</code></pre> <pre><code>                               queue_size  queued  finished  cancelled  idle\ntime                                                                        \n2024-08-13 07:34:55.853111673           0       0         0          0     2\n2024-08-13 07:34:55.872440325           1       1         0          0     1\n2024-08-13 07:34:55.872990541           2       2         0          0     0\n2024-08-13 07:34:55.877353082           1       0         1          0     1\n2024-08-13 07:34:55.877504454           2       1         1          0     0\n...                                   ...     ...       ...        ...   ...\n2024-08-13 07:34:56.873615563           2       2         0          0     0\n2024-08-13 07:34:56.873897389           2       2         0          0     0\n2024-08-13 07:34:56.885472047           2       2         0          0     0\n2024-08-13 07:34:56.885893213           1       0         1          0     1\n2024-08-13 07:34:56.885936895           0       0         0          0     2\n\n[4859 rows x 5 columns]\n</code></pre> <p>We can also <code>plot()</code> the data as a stacked barchart with a set interval.</p> <pre><code>fig, ax = plt.subplots()\nmonitor.plot(interval=(50, \"ms\"))\n</code></pre> 2024-08-13T07:34:57.052303 image/svg+xml Matplotlib v3.9.2, https://matplotlib.org/"},{"location":"reference/scheduling/scheduler/","title":"Scheduler","text":""},{"location":"reference/scheduling/scheduler/#scheduler","title":"Scheduler","text":"<p>The <code>Scheduler</code> uses an <code>Executor</code>, a builtin python native with a <code>submit(f, *args, **kwargs)</code> function to submit compute to be compute else where, whether it be locally or remotely.</p> <p>The <code>Scheduler</code> is primarily used to dispatch compute to an <code>Executor</code> and emit <code>@events</code>, which can trigger user callbacks.</p> <p>Typically you should not use the <code>Scheduler</code> directly for dispatching and responding to computed functions, but rather use a <code>Task</code></p> Running in a Jupyter Notebook/Colab <p>If you are using a Jupyter Notebook, you likley need to use the following at the top of your notebook:</p> <pre><code>import nest_asyncio  # Only necessary in Notebooks\nnest_asyncio.apply()\n\nscheduler.run(...)\n</code></pre> <p>This is due to the fact a notebook runs in an async context. If you do not wish to use the above snippet, you can instead use:</p> <pre><code>await scheduler.async_run(...)\n</code></pre> Basic Usage <p>In this example, we create a scheduler that uses local processes as workers. We then create a task that will run a function <code>fn</code> and submit it to the scheduler. Lastly, a callback is registered to <code>@future-result</code> to print the result when the compute is done.</p> <pre><code>from amltk.scheduling import Scheduler\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef launch_the_compute():\n    scheduler.submit(fn, 1)\n\n@scheduler.on_future_result\ndef callback(future, result):\n    print(f\"Result: {result}\")\n\nscheduler.run()\n</code></pre>  Result: 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def launch_the_compute() (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 1\n    @on_future_done 1\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 1\n    \u2514\u2500\u2500 def callback(future, result) (1)\n</code>\n</pre> <p>The last line in the previous example called <code>scheduler.run()</code> is what starts the scheduler running, in which it will first emit the <code>@start</code> event. This triggered the callback <code>launch_the_compute()</code> which submitted the function <code>fn</code> with the arguments <code>1</code>.</p> <p>The scheduler then ran the compute and waited for it to complete, emitting the <code>@future-result</code> event when it was done successfully. This triggered the callback <code>callback()</code> which printed the result.</p> <p>At this point, there is no more compute happening and no more events to respond to so the scheduler will halt.</p> <code>@events</code> Scheduler Status EventsSubmitted Compute Events <p>When the scheduler enters some important state, it will emit an event to let you know.</p> <code>@start</code><code>@finishing</code><code>@finished</code><code>@stop</code><code>@timeout</code><code>@empty</code> <p>A <code>Subscriber</code> which is called when the scheduler starts. This is the first event emitted by the scheduler and one of the only ways to submit the initial compute to the scheduler.</p> <pre><code>@scheduler.on_start\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finishing up. This occurs right before the scheduler shuts down the executor.</p> <pre><code>@scheduler.on_finishing\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finished, has shutdown the executor and possibly terminated any remaining compute.</p> <pre><code>@scheduler.on_finished\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is has been stopped due to the <code>stop()</code> method being called.</p> <pre><code>@scheduler.on_stop\ndef my_callback(stop_msg: str, exception: BaseException | None):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler reaches the timeout.</p> <pre><code>@scheduler.on_timeout\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the queue is empty. This can be useful to re-fill the queue and prevent the scheduler from exiting.</p> <pre><code>@scheduler.on_empty\ndef my_callback():\n    ...\n</code></pre> <p>When any compute goes through the <code>Scheduler</code>, it will emit an event to let you know. You should however prefer to use a <code>Task</code> as it will emit specific events for the task at hand, and not all compute.</p> <code>@future-submitted</code><code>@future-result</code><code>@future-exception</code><code>@future-done</code><code>@future-cancelled</code> <p>A <code>Subscriber</code> which is called when some compute is submitted.</p> <pre><code>@scheduler.on_future_submitted\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future returned with a result, no exception raise.</p> <pre><code>@scheduler.on_future_result\ndef my_callback(future: Future, result: Any):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute raised an uncaught exception.</p> <pre><code>@scheduler.on_future_exception\ndef my_callback(future: Future, exception: BaseException):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is done, regardless of whether it was successful or not.</p> <pre><code>@scheduler.on_future_done\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future is cancelled. This usually occurs due to the underlying Scheduler, and is not something we do directly, other than when shutting down the scheduler.</p> <pre><code>@scheduler.on_future_cancelled\ndef my_callback(future: Future):\n    ...\n</code></pre> Common usages of <code>run()</code> <p>There are various ways to <code>run()</code> the scheduler, notably how long it should run with <code>timeout=</code> and also how it should react to any exception that may have occurred within the <code>Scheduler</code> itself or your callbacks.</p> <p>Please see the <code>run()</code> API doc for more details and features, however we show two common use cases of using the <code>timeout=</code> parameter.</p> <p>You can render a live display using <code>run(display=...)</code>. This require <code>rich</code> to be installed. You can install this with <code>pip install rich</code> or <code>pip install amltk[rich]</code>.</p> <code>run(timeout=...)</code><code>run(timeout=..., wait=False)</code> <p>You can tell the <code>Scheduler</code> to stop after a certain amount of time with the <code>timeout=</code> argument to <code>run()</code>.</p> <p>This will also trigger the <code>@timeout</code> event as seen in the <code>Scheduler</code> output.</p> <pre><code>import time\nfrom asyncio import Future\n\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; int:\n    time.sleep(0.1)\n    return 42\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\n# This will endlessly loop the scheduler\n@scheduler.on_future_done\ndef submit_again(future: Future) -&gt; None:\n    if scheduler.running():\n        scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1)  # End after 1 second\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 10\n    @on_future_done 10\n    \u2514\u2500\u2500 def submit_again(future: _asyncio.Future) -&gt; None (10)\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 10\n</code>\n</pre> <p>By specifying that the <code>Scheduler</code> should not wait for ongoing tasks to finish, the <code>Scheduler</code> will attempt to cancel and possibly terminate any running tasks.</p> <pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; None:\n    time.sleep(10)\n\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1, wait=False)  # End after 1 second\n</code></pre> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; None (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 1\n    @on_future_done\n    @on_future_cancelled 1\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> Forcibly Terminating Workers <p>As an <code>Executor</code> does not provide an interface to forcibly terminate workers, we provide <code>Scheduler(terminate=...)</code> as a custom strategy for cleaning up a provided executor. It is not possible to terminate running thread based workers, for example using <code>ThreadPoolExecutor</code> and any Executor using threads to spawn tasks will have to wait until all running tasks are finish before python can close.</p> <p>It's likely <code>terminate</code> will trigger the <code>EXCEPTION</code> event for any tasks that are running during the shutdown, not* a cancelled event. This is because we use a <code>Future</code> under the hood and these can not be cancelled once running. However there is no guarantee of this and is up to how the <code>Executor</code> handles this.</p> Scheduling something to be run later <p>You can schedule some function to be run later using the <code>scheduler.call_later()</code> method.</p> <p>Note</p> <p>This does not run the function in the background, it just schedules some function to be called later, where you could perhaps then use submit to scheduler a <code>Task</code> to run the function in the background.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef fn() -&gt; int:\n    print(\"Ending now!\")\n    scheduler.stop()\n\n@scheduler.on_start\ndef schedule_fn() -&gt; None:\n    scheduler.call_later(1, fn)\n\nscheduler.run(end_on_empty=False)\n</code></pre> <pre><code>Ending now!\n</code></pre>"},{"location":"reference/scheduling/task/","title":"Task","text":""},{"location":"reference/scheduling/task/#tasks","title":"Tasks","text":"<p>A <code>Task</code> is a unit of work that can be scheduled by the <code>Scheduler</code>.</p> <p>It is defined by its <code>function=</code> to call. Whenever a <code>Task</code> has its <code>submit()</code> method called, the function will be dispatched to run by a <code>Scheduler</code>.</p> <p>When a task has returned, either successfully, or with an exception, it will emit <code>@events</code> to indicate so. You can subscribe to these events with callbacks and act accordingly.</p> <code>@events</code> <p>Check out the <code>@events</code> reference for more on how to customize these callbacks. You can also take a look at the API of <code>on()</code> for more information.</p> <code>@on-result</code><code>@on-exception</code><code>@on-done</code><code>@on-submitted</code><code>@on-cancelled</code> <p>Called when a task has successfully returned a value. Comes with Future <pre><code>@task.on_result\ndef on_result(future: Future[R], result: R):\n    print(f\"Future {future} returned {result}\")\n</code></pre></p> <p>Called when a task failed to return anything but an exception. Comes with Future <pre><code>@task.on_exception\ndef on_exception(future: Future[R], error: BaseException):\n    print(f\"Future {future} exceptioned {error}\")\n</code></pre></p> <p>Called when a task is done running with a result or exception. <pre><code>@task.on_done\ndef on_done(future: Future[R]):\n    print(f\"Future {future} is done\")\n</code></pre></p> <p>An event that is emitted when a future is submitted to the scheduler. It will pass the future as the first argument with args and kwargs following.</p> <p>This is done before any callbacks are attached to the future. <pre><code>@task.on_submitted\ndef on_submitted(future: Future[R], *args, **kwargs):\n    print(f\"Future {future} was submitted with {args=} and {kwargs=}\")\n</code></pre></p> <p>Called when a task is cancelled. <pre><code>@task.on_cancelled\ndef on_cancelled(future: Future[R]):\n    print(f\"Future {future} was cancelled\")\n</code></pre></p> Usage <p>The usual way to create a task is with <code>Scheduler.task()</code>, where you provide the <code>function=</code> to call.</p> <pre><code>from amltk import Scheduler\nfrom asyncio import Future\n\ndef f(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(2)\ntask = scheduler.task(f)\n\n@scheduler.on_start\ndef on_start():\n    task.submit(1)\n\n@task.on_result\ndef on_result(future: Future[int], result: int):\n    print(f\"Task {future} returned {result}\")\n\nscheduler.run()\n</code></pre>  Task  returned 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def on_start() (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 1\n\u2503   @on_future_done 1\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 1\n\u2517\u2501\u2501 \u256d\u2500 Task f(x: int) -&gt; int \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Task-f-5eEmRRtG \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>If you'd like to simply just call the original function, without submitting it to the scheduler, you can always just call the task directly, i.e. <code>task(1)</code>.</p> <p>You can also provide <code>Plugins</code> to the task, to modify tasks, add functionality and add new events.</p>"}]}