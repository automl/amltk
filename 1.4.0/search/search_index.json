{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the AutoML-Toolkit framework docs.</p> <p>Tip</p> <p>See the navigation links in the header or side-bars. Click the  button (top left) on mobile.</p> <p>For a quick-start, check out examples for copy-pastable snippets to start from. For a more guided tour through what AutoML-Toolkit can offer, please check out our guides. If you've used AutoML-Toolkit before but need some refreshers, you can look through our reference pages or the API docs.</p>"},{"location":"#what-is-automl-toolkit","title":"What is AutoML-Toolkit?","text":"<p>AutoML-Toolkit is a highly-flexible set of modules and components, allowing you to define, search and build machine learning systems.</p> <ul> <li> <p> Python</p> <p>Use the programming language that defines modern machine learning research. We use mypy internally and for external API so you can identify and fix errors before a single line of code runs.</p> </li> </ul> <ul> <li> <p> Minimal Dependencies</p> <p>AutoML-Toolkit was designed to not introduce dependencies on your code. We support some tool integrations but only if they are optionally installed!.</p> </li> </ul> <ul> <li> <p> Plug-and-play</p> <p>We can't support all frameworks, and thankfully we don't have to. AutoML-Toolkit was designed to be plug-and-play. Integrate in your own optimizers, search spaces, execution backends, builders and more.</p> <p>We've worked hard to make sure that how we integrate tools can be done for your own tools we don't cover.</p> </li> </ul> <ul> <li> <p> Event Driven</p> <p>AutoML-Toolkit is event driven, meaning you write code that reacts to events as they happen. You can ignore, extend and create new events that have meaning to the systems you build.</p> <p>This enables tools built from AutoML-Toolkit to support greater forms of interaction, automation and deployment.</p> </li> </ul> <ul> <li> <p> Task Agnostic</p> <p>AutoML-Toolkit is task agnostic, meaning you can use it for any machine learning task. We provide a base Task which you can extend with events and functionality specific to the tasks you care about.</p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thanks for checking out the contribution page! <code>amltk</code> is open to all open-source contributions, whether it be fixes, features, integrations or even just simple doc fixes.</p> <p>This guide will walk through some simple guidelines on the workflow of <code>amltk</code> but also some design principles that are core to its development. Some of these principles may be new for first time contributors and examples will be given where necessary.</p>"},{"location":"contributing/#clone-the-repo","title":"Clone the repo","text":"<p>Clone the repo manually or with the below <code>hub</code> cli from GitHub.</p> <pre><code>hub repo fork automl/amltk\n</code></pre>"},{"location":"contributing/#quickstart","title":"Quickstart","text":"<p>Below is a quickstart guide for those familiar with open-source development. You do not need to use <code>just</code>, however we provide it as a convenient workflow tool. Please refer to the <code>justfile</code> for the commands ran if you wish to use your own workflow.</p> <p>NOTE: If you are using Windows, please go to the Windows installation section.</p> <pre><code># Install just, the Makefile-like tool for this repo\n# https://github.com/casey/just#installation\nsudo apt install just\n\n# Make virtual env (however you like)\npython -m venv .my-virtual-env\nsource ./.my-virtual-env/bin/activate\n\n# Install the library with dev dependancies\njust install\n\n# ... make a new branch\njust pr-feat my-new-feature\n\n# ... make changes\n# ... commit changes\n\n# Run tests\njust test\n\n# Run the documentation, fix any warnings\njust docs\n\n# Run pre-commit checks\njust check\n\n# ... fix anything that needs fixing\n\n# Push to your fork\ngit push\n\n# Create a PR (opening the browser too)\nhub pull-request --browse\n</code></pre> <p>Below we will go into more detail on each of these steps.</p>"},{"location":"contributing/#setting-up","title":"Setting up","text":"<p>The core workflows of <code>amltk</code> are accessed through the <code>justfile</code> It is recommended to have this installed with their simple one-liners on their repo. All of these were developed with bash in mind and your usage with other platforms may vary, please use the <code>justfile</code> as reference if this is the case.</p>"},{"location":"contributing/#forking","title":"Forking","text":"<p>If you are contributing from outside the <code>automl</code> org and under your own github profile, you'll have to create your own fork.</p> <p>If you use the <code>hub</code> tool from github, you can do this locally with:</p> <pre><code># Clones this repo\nhub clone automl/amltk\n\n# Forks the repo to your own user account and sets up tracking\n# to match your repo, not the automl/amltk version.\nhub fork\n</code></pre>"},{"location":"contributing/#installation","title":"Installation","text":"<p>To install <code>amltk</code> for development, we rely on specific dependencies that are not required for the actual library to run. These are listed in the <code>pyproject.toml</code> under the <code>[project.optional-dependencies]</code> header.</p> <p>You can install all of these by doing the following:</p> <pre><code># Create a virtual environment in your preferred way.\npython -m venv .my-virtual-env\nsource ./.my-virtual-env/bin/activate\n\n# Install all required dependencies\njust install\n</code></pre>"},{"location":"contributing/#setting-up-code-quality-tools","title":"Setting up code quality tools","text":"<p>When you ran <code>just install</code>, the tool <code>pre-commit</code> was installed.</p> <p>This is a framework that the repo has set up to run a set of code quality tools upon each commit, fixing up easy to fix issues, run some automatic formatting and run a static type checker on the code in the repository. The configuration for <code>pre-commit</code> can be found in <code>.pre-commit-config.yaml</code>.</p> <p>To run these checks at any time, use the command <code>just fix</code>, followed by <code>just check</code>. Any list of errors will be presented to you, and we recommend fixing these before committing.</p> <p>While these can certainly be skipped, these checks will be run using github actions, a Continuous Integration (CI) service. If there are problems you are not sure how to fix, please feel free to discuss them in the Pull Request and we will help you solve them!</p> <p>To see a list of tools used and their purposes, please see the section on Code Quality.</p>"},{"location":"contributing/#creating-a-new-branch","title":"Creating a new branch","text":"<p>We follow a Pull Request into <code>main</code> workflow, which is essentially that any contributions to <code>amltk</code> should be done in a branch with a pull request to the <code>main</code> branch. We prefer a branch name that describes the kind of pull request that it is. We have provided some default options but please feel free to use your own if you are familiar with these workflows:</p> <pre><code># These utilities will pull the most recent `main` branch,\n# create a new branch with your `branchname` and and push\n# the new branch back to github\njust pr-feat branchname  # Creates a branch feat-branchname\njust pr-doc branchname   # Creates a branch doc-branchname\njust pr-fix branchname   # Creates a branch fix-branchname\njust pr-other branchname # Creates a branch other-branchname\n</code></pre>"},{"location":"contributing/#submitting-a-pr","title":"Submitting a PR","text":"<p>If you are unfamiliar with creating a PR on github, please check out this guide.</p> <p>Please provide a short summary of your changes to help give context to any reviewers who may be looking at your code. If submitting a more complex PR that changes behaviours, please consider more context when describing not only what you changed but why and how.</p> <p>We may ask you to break up your changes into smaller atomic units that are easier to verify and review, but we will describe this process to you if required.</p>"},{"location":"contributing/#reviews","title":"Reviews","text":"<p>Once your PR is submitted, we will likely have comments and changes that are required. Please be patient as we may not be able to respond immediately. If there are only minor comments, we will simply annotate your code where these changes are required and upon fixing them, we will happily merge these into the <code>main</code> branch and thank you for your open-source contributions!</p> <p>Good practice is to actually review your own PR after submitting it. You'll often find small issues such as out-of-sync doc strings or even small logical issues. In general, if you can't understand your own PR, it's likely we won't either.</p>"},{"location":"contributing/#granting-access-to-your-fork","title":"Granting access to your fork","text":"<p>If the PR requires larger structural changes or more discussion, there will likely be a few back-and-forth discussion points which we will actively respond to help get your contribution in.</p> <p>If you do not wish to actively monitor the PR for whatever reason, granting us access to modify your PR will substantially help integration. To do so, please follow the instructions here.</p>"},{"location":"contributing/#commits","title":"Commits","text":"<p>This library uses conventional commits as a way to write commits. This makes commit messages simpler and easier to read as well as allows for an easier time managing the repo, such as changelogs and versioning. This is important enough that we even enforce this through <code>pre-commit</code> to fail the commit if the message does not follow the format. Please follow the link above to find out more but for reference, here are some short examples:</p> <pre><code>fix(scheduler): Use X instead of Y\nfeat(pipeline): Allow for Z\nrefactor(Optuna): Move integrations to seperate file\ndoc(Example): Integrating custom space parser\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Our testing for the library is done using <code>pytest</code>, with some additional utilities from <code>pytest-coverage</code> for code coverage and <code>pytest-cases</code> for test structure.</p> <p>In general, writing a test and running <code>just test</code> to test the whole library should be sufficient. If you need more fine-grained control, such as only testing a particular test, please refer to this cheatsheet.</p> <pre><code>pytest                              # Test whole library and examples\npytest \"tests/path/to/testfile.py\"  # Test a particular file\npytest -k \"test_name_of_my_test\"    # Test a particular test\n</code></pre> <p> In general, you should prefer to run <code>just test</code> over <code>pytest</code> if new to testing. This will run all test until it hits its first failure which allows for better incremental testing. It will also avoid running the examples which are often longer and saved for CI.</p>"},{"location":"contributing/#testing-examples","title":"Testing examples","text":"<p>If testing any added examples, please use the <code>just test-examples</code> command, which is a shortcut for <code>pytest \"tests/test_examples.py\" -x --lf</code>. There is unfortunately no way to sub-select one.</p> <p>If you are not sure how to test your contribution or need some pointers to get started, please reach out in the PR comments and we will be happy to assist!</p>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<p>To ensure a consistent code quality and to reduce noise in the PR, there are a selection of code quality tools that run.</p> <p>These will be run automatically before a commit can be done with <code>pre-commit</code>. The configuration for this can be found in <code>.pre-commit-config.yaml</code>. All of these can be manually triggered using <code>just check</code>.</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This will automatically run the below tools.</p>"},{"location":"contributing/#ruff-code-linting-and-formatting","title":"Ruff - Code Linting and formatting","text":"<p>The primary linter we use is <code>ruff</code>. The fixes and formatting can be done manually as so:</p> <pre><code>ruff --fix src\nruff format src\n\n# Or this which does both\njust fix\n</code></pre>"},{"location":"contributing/#mypy-static-type-checking","title":"Mypy - Static Type Checking","text":"<p>This codebase also relies heavily on pythons <code>typing</code> and <code>mypy</code> to ensure correctness across modules. Running this standalone on all files can take some time, so we don't require you to run this, our automated testers will. If you wish to do so manually, then use <code>just check-types</code>.</p> <p>If any of the typing concepts are confusing, now is a good chance to learn, and we would be happy to assist in helping properly type your PR if things do not work. If all else fails, please feel free to introduce a <code># type: ignore</code> to tell <code>mypy</code> to shut up along with a description to why it is there. This will help future contributors and maintainers understand the reasons behind these ignores. You can find a cheatsheet for basic mypy type hinting here.</p>"},{"location":"contributing/#git-workflow","title":"Git workflow","text":"<p>We follow a PR into trunk development flow (whatever that's meant to be called), in which all development is done in feature branches and the merged into <code>main</code>. The reason for feature branches is to allow multiple maintainers to actively work on <code>amltk</code> without interfering. The <code>main</code> branch is locked down, meaning commits can not be made directly to main, and features actions to trigger releases.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>The documentation is handled by <code>mkdocs-material</code> with some additional plugins. This is a markdown based documentation generator which allows custom documentation and renders API documentation written in the code itself.</p> <p>Document features where the code lives. For example, if you introduce a new function, prefer to add documentation with an example in the docstring of the function itself. There is also some module level documentation which is used as a reference for the API. Most likely updating this is only required for larger changes.</p> <p>You can find a collection of features for custom documentation here as well as code reference documentation here</p>"},{"location":"contributing/#viewing-documentation","title":"Viewing Documentation","text":"<p>You can live view documentation changes by running <code>just docs</code>, which will open your webbrowser and run <code>mkdocs --serve</code> to watch all files and live update any changes that occur.</p> <p>Please do not ignore warnings. The CI will fail if there are any warnings and we will not merge any PR's that have warnings.</p>"},{"location":"contributing/#example-syntax","title":"Example Syntax","text":"<p>If creating an example, there is a custom format used to render <code>.py</code> files and convert them to markdown that we can host.</p> <p>An example is just a python file, using the triple quote <code>\"\"\"</code> comments to switch between commentary and code blocks.</p> <p>The first <code>\"\"\"</code> block is special, in that the first line, in this case My Example Name is the name of the example, with anything following it being simple commentary.</p> <pre><code>\"\"\"My Example Name\n\nHere's a short description.\n\"\"\"\nfrom x import a\nfrom y import b\n\n\"\"\"\nThis is a commentary section. To see what can go in here,\ntake a look at https://squidfunk.github.io/mkdocs-material/reference/\n\nBelow we set some variables to some values.\n\n!!! note \"Special note\"\n\n    We use the variables p, q for fun.\n\"\"\"\np = 2  # (1)!\np = 3  # &lt;!&gt; (2)!\n\nprint(p)  # (3)!\n\n# 1. You can add annotations to lines, where the text to annotate goes at\n    the bottom of the code block, before the next commentary section.\n    https://squidfunk.github.io/mkdocs-material/reference/annotations/\n# 2. You can use &lt;!&gt; to highlight specific lines\n# 3. Anything printed out using `print` will be rendered\n\"\"\"\nThis concludes this example, check out ./examples for examples on how\nto create ... examples.\n\"\"\"\n</code></pre>"},{"location":"contributing/#maintainer-guide","title":"Maintainer Guide","text":"<p>This section serves as a guide for active maintainers of <code>amltk</code> to keep the ship running smoothly and help foster a growing user-base. All maintainers must be familiar with the rest of the <code>CONTRIBUTING.md</code>.</p>"},{"location":"contributing/#ethos","title":"Ethos","text":"<p>We appreciate all open-source contributions, whether that be a question, issue or PR. This also pertains to potentially first-time contributors and people new to Python and open-source in general. This includes objective non-personal criticisms. We will try to be as helpful and communicative as possible with respect to our availability, and encourage open discussion.</p> <p>To foster growth and contribution, we will guide users through the library as required and encourage any and all contributions. If more work is required on a PR, please encourage users to grant access to their fork such that we can actively contribute to their contribution and utilize a collaborative approach. This will also help prevent staling contributions.</p> <p>In the event of any individual who makes personal attacks or derogative comments, please maintain decorum, respond nicely, and if issues persist, then inform the user they will be blocked.</p> <p>Please check the <code>CODE_OF_CONDUCT.md</code> for more details.</p>"},{"location":"contributing/#merging","title":"Merging","text":"<p>We use <code>squash-merge</code> from feature branches to keep the commit log to the <code>convential-commits</code> standard. This helps automate systems.</p> <p>Please familiarize yourself with conventional commits and ensure that the PR is up-to-date with the <code>main</code> branch before merging.</p> <p>There should be no manual versioning, as this will take place during releases automatically.</p> <p>In the case of staling PR's, these will likely need a forceful rebase from the <code>main</code> branch. This often has a negative impact on the commit history of a pull request but this will be removed by <code>squash-merge</code>.</p>"},{"location":"contributing/#workflows","title":"Workflows","text":"<p>To keep things relatively uniform, we try support recommended workflows through the <code>justfile</code>. If there is a workflow that you prefer and is not covered, please add your own.</p>"},{"location":"contributing/#automation","title":"Automation","text":"<p>Maintaining repositories is time-consuming work, whether that be benchmarking, experimenting, testing, versioning, issues, pull requests, documentation and anything else tangential to code features. Any and all automation to the repository is greatly appreciated but should be documented in the <code>Maintainers</code> section.</p>"},{"location":"contributing/#dependencies","title":"Dependencies","text":"<p>One of the hardest parts of maintenance for a mature library, especially one that supports integrations from both mature and research code is managing dependencies. Where possible, prefer not adding an explicit dependency. This mainly holds for the required dependencies which all users must install. For developer dependencies, please feel free to add one with good justification. When integrating some machine learning ecosystem like <code>scikit-learn</code> or <code>pytorch</code>, please try to bundle these dependencies as optional and reflect so accordingly in the code.</p> <p>There is some utility to work with optional dependencies in <code>amltk.types</code>, such as <code>safe_isinstance</code> and <code>safe_issubclass</code>, to not rely on the library being installed for runtime type checking. For static compile time type checking, please use mypy's <code>if TYPE_CHECKING:</code> idiom. This will prevent runtime errors for users who do not have these dependencies installed. For example:</p> <pre><code>from typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from ConfigSpace import ConfigurationSpace\n\n\ndef draw_configspace(self, space: ConfigurationSpace) -&gt; None:\n    ...\n</code></pre> <p>The exception to this rule is any modules a user must explicitly import for the integration. In this case, it is fine to assume the user has the required dependencies and any error generated is considered user error and if possible guide them to the <code>pip install \"amltk[optional_dep]\"</code> that they require for the integration.</p>"},{"location":"contributing/#dependency-updates","title":"Dependency updates","text":"<p>We have <code>dependabot</code> enabled in the repository using the <code>.github/dependabot.yml</code>. This bot will periodically make pull requests to the repository that update dependencies. Do not accept these blindly but rather wait for any CI to finish and ensure all tests still pass.</p>"},{"location":"contributing/#windows-installation","title":"Windows Installation","text":"<p>If you are not using Windows, feel free to skip this section.</p>"},{"location":"contributing/#installing-wsl-windows-subsystem-for-linux","title":"Installing WSL (Windows Subsystem for Linux)","text":"<ol> <li>Install WSL and Ubuntu by following the steps outlined in    the official Ubuntu installation guide.</li> </ol>"},{"location":"contributing/#setting-up-pycharm-with-wsl","title":"Setting up PyCharm with WSL","text":"<ol> <li>Open the cloned repo in PyCharm and navigate to \"Add new Interpreter\" -&gt; \"On WSL...\"</li> <li>Choose WSL and specify the directory of the virtual env.</li> <li>Open a PyCharm terminal and click \"New predefined session\" and select Ubuntu.</li> </ol>"},{"location":"contributing/#installing-dependencies","title":"Installing Dependencies","text":"<p>Since WSL is a Linux environment, you need to install Python separately, even if you have it on your Windows machine.</p> <p>In the terminal, run the following commands to set up the project dependencies:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\nsudo apt install python3.10\nsudo apt install python3-pip\npip install --upgrade pip setuptools\nsudo apt install python3.10-venv\npython3 -m venv .venv\n</code></pre> <p>Then create a virtual environment and activate it:</p> <pre><code>source .venv/bin/activate\njust install\n</code></pre>"},{"location":"api/","title":"Index","text":""},{"location":"api/#api","title":"API","text":"<p>This houses all the documentation for the various modules in the project. Use the navigation bar to the left to view more.</p>"},{"location":"api/amltk/distances/","title":"Distances","text":"<p>Distance functions.</p> <p>This module contains functions for calculating the distance between two vectors.</p>"},{"location":"api/amltk/distances/#amltk.distances.DistanceMetric","title":"<code>DistanceMetric: TypeAlias</code>   <code>module-attribute</code>","text":"<p>A metric used for calculating distances.</p> <p>Takes two arrays-like objects and returns a float.</p>"},{"location":"api/amltk/distances/#amltk.distances.l1_distance","title":"<code>l1_distance</code>   <code>module-attribute</code>","text":"<p>Calculates the l1 distance between each column in x and y.</p> <p>The l1 distance is defined as:</p> <pre><code>`||x - y||_1 = sum_i(|x_i - y_i|)`\n</code></pre> <p>This is the sum of the absolute differences between each element in x and y.</p> See Also <ul> <li><code>pnorm()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.l2_distance","title":"<code>l2_distance</code>   <code>module-attribute</code>","text":"<p>Calculates the l2 distance between each column in x and y.</p> <p>The l2 distance is defined as:</p> <pre><code>`||x - y||_2 = sqrt(sum_i(|x_i - y_i|^2))`\n</code></pre> <p>This is the square root of the sum of the squared differences between each element in x and y.</p> See Also <ul> <li><code>pnorm()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.linf_distance","title":"<code>linf_distance</code>   <code>module-attribute</code>","text":"<p>Calculates the linf distance between each column in x and y.</p> <p>The linf distance is defined as:</p> <pre><code>`||x - y||_inf = max_i(|x_i - y_i|)`\n</code></pre> <p>This is the maximum absolute difference between each element in x and y.</p> See Also <ul> <li><code>pnorm()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.euclidean_distance","title":"<code>euclidean_distance</code>   <code>module-attribute</code>","text":"<p>Calculates the euclidean distance between each column in x and y.</p> <p>Same as <code>l2_distance()</code>.</p>"},{"location":"api/amltk/distances/#amltk.distances.NamedDistance","title":"<code>NamedDistance: TypeAlias</code>   <code>module-attribute</code>","text":"<p>Predefined distance metrics.</p> <p>Possible values are:</p> <ul> <li><code>\"l1\"</code>: <code>l1_distance()</code></li> <li><code>\"l2\"</code>: <code>l2_distance()</code></li> <li><code>\"euclidean\"</code>: <code>euclidean_distance()</code></li> <li><code>\"cosine\"</code>: <code>cosine_distance()</code></li> <li><code>\"max\"</code>: <code>linf_distance()</code></li> </ul>"},{"location":"api/amltk/distances/#amltk.distances.NearestNeighborsDistance","title":"<code>class NearestNeighborsDistance(**nn_kwargs)</code>","text":"<p>Uses sklearn.neighbors.NearestNeighbors to calculate the distance.</p> PARAMETER  DESCRIPTION <code>**nn_kwargs</code> <p>Keyword arguments to pass to sklearn.neighbors.NearestNeighbors.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/amltk/distances.py</code> <pre><code>def __init__(self, **nn_kwargs: Any):\n    \"\"\"Creates a new NearestNeighborsDistance.\n\n    Args:\n        **nn_kwargs: Keyword arguments to pass to\n            [sklearn.neighbors.NearestNeighbors][].\n    \"\"\"\n    super().__init__()\n    self.nn_kwargs = nn_kwargs\n</code></pre>"},{"location":"api/amltk/distances/#amltk.distances.NearestNeighborsDistance.__call__","title":"<code>def __call__(x, y)</code>","text":"<p>Calculates the distance between each column in x and y.</p> PARAMETER  DESCRIPTION <code>x</code> <p>An array-like with columns being the features and rows being the samples.</p> <p> TYPE: <code>ArrayLike</code> </p> <code>y</code> <p>A array with the same index as x.</p> <p> TYPE: <code>ArrayLike</code> </p> RETURNS DESCRIPTION <code>NDArray[floating]</code> <p>An array with the same index as x.</p> Source code in <code>src/amltk/distances.py</code> <pre><code>def __call__(\n    self,\n    x: npt.ArrayLike,\n    y: npt.ArrayLike,\n) -&gt; npt.NDArray[np.floating]:\n    \"\"\"Calculates the distance between each column in x and y.\n\n    Args:\n        x: An array-like with columns being the features and rows being the samples.\n        y: A array with the same index as x.\n\n    Returns:\n        An array with the same index as x.\n    \"\"\"\n    from sklearn.neighbors import NearestNeighbors\n\n    self.nn = NearestNeighbors(**self.nn_kwargs)\n\n    _x = np.asarray(x)\n    _y = np.asarray(y)\n\n    if _y.ndim != 1:\n        raise ValueError(f\"y must be a 1-dimensional array. Got shape {_y.shape}\")\n\n    _y = _y.reshape(1, -1)\n    _x = _x.T\n\n    if _x.ndim == 1:\n        _x = np.asarray([_x])\n\n    self.nn.fit(_x)\n    distances, _ = self.nn.kneighbors(\n        _y,\n        n_neighbors=len(_x),\n        return_distance=True,\n    )\n    return np.asarray(distances.reshape(-1), dtype=float)\n</code></pre>"},{"location":"api/amltk/distances/#amltk.distances.pnorm","title":"<code>def pnorm(x, y, p=2)</code>","text":"<p>Calculates the p-norm between each column in x and y.</p> <p>The p-norm is defined as:</p> <pre><code>`||x - y||_p = (sum_i(|x_i - y_i|^p))^(1/p)`\n</code></pre> <p>The common values for p are 1, 2 and infinity.</p> <ul> <li><code>l1_distance()</code></li> <li><code>l2_distance()</code></li> <li><code>linf_distance()</code></li> </ul> <p>Using a <code>partial</code></p> <p>To use this function with <code>dataset_distance()</code>, you can wrap this in <code>functools.partial()</code>.</p> <pre><code>from functools import partial\nfrom amltk.metalearning import dataset_distance\nfrom amltk.distances import pnorm\n\ndataset_distance(\n    target,\n    dataset_metafeatures,\n    method=partial(pnorm, p=3), # (1)!\n)\n</code></pre> <ol> <li><code>partial()</code> creates a new function with the <code>p</code> argument set to 3.</li> </ol> PARAMETER  DESCRIPTION <code>x</code> <p>The vector to compare.</p> <p> TYPE: <code>ArrayLike</code> </p> <code>y</code> <p>The vector to compute the distance to</p> <p> TYPE: <code>ArrayLike</code> </p> <code>p</code> <p>The p in p-norm.</p> <p> TYPE: <code>int | float</code> DEFAULT: <code>2</code> </p> RETURNS DESCRIPTION <code>float</code> <p>A series with the same index as x.</p> Source code in <code>src/amltk/distances.py</code> <pre><code>def pnorm(\n    x: npt.ArrayLike,\n    y: npt.ArrayLike,\n    p: int | float = 2,\n) -&gt; float:\n    \"\"\"Calculates the p-norm between each column in x and y.\n\n    The p-norm is defined as:\n\n        `||x - y||_p = (sum_i(|x_i - y_i|^p))^(1/p)`\n\n    The common values for p are 1, 2 and infinity.\n\n    * [`l1_distance()`][amltk.distances.l1_distance]\n    * [`l2_distance()`][amltk.distances.l2_distance]\n    * [`linf_distance()`][amltk.distances.linf_distance]\n\n    !!! tip \"Using a `partial`\"\n\n        To use this function with\n        [`dataset_distance()`][amltk.metalearning.dataset_distance],\n        you can wrap this in [`functools.partial()`][functools.partial].\n\n        ```python\n        from functools import partial\n        from amltk.metalearning import dataset_distance\n        from amltk.distances import pnorm\n\n        dataset_distance(\n            target,\n            dataset_metafeatures,\n            method=partial(pnorm, p=3), # (1)!\n        )\n        ```\n\n        1. [`partial()`][functools.partial] creates a new function with the\n        `p` argument set to 3.\n\n    Args:\n        x: The vector to compare.\n        y: The vector to compute the distance to\n        p: The p in p-norm.\n\n    Returns:\n        A series with the same index as x.\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    if p is np.inf:\n        return float(np.max(np.abs(x - y)))\n\n    return float(np.linalg.norm(x - y, ord=p))\n</code></pre>"},{"location":"api/amltk/distances/#amltk.distances.cosine_distance","title":"<code>def cosine_distance(x, y)</code>","text":"<p>Calculates the cosine distance between each column in x and y.</p> <p>The cosine distance is defined as 1 - cosine_similarity. This means the distance is 0 when the vectors are identical, 1 when orthogonal and 2 when they are opposite.</p> PARAMETER  DESCRIPTION <code>x</code> <p>A dataframe with columns being the features and rows being the samples.</p> <p> TYPE: <code>ArrayLike</code> </p> <code>y</code> <p>A series with the same index as x.</p> <p> TYPE: <code>ArrayLike</code> </p> RETURNS DESCRIPTION <code>float</code> <p>A series with the same index as x.</p> Source code in <code>src/amltk/distances.py</code> <pre><code>def cosine_distance(x: npt.ArrayLike, y: npt.ArrayLike) -&gt; float:\n    \"\"\"Calculates the cosine distance between each column in x and y.\n\n    The cosine distance is defined as 1 - cosine_similarity. This means\n    the distance is 0 when the vectors are identical, 1 when orthogonal\n    and 2 when they are opposite.\n\n    Args:\n        x: A dataframe with columns being the features and rows being the samples.\n        y: A series with the same index as x.\n\n    Returns:\n        A series with the same index as x.\n    \"\"\"\n    x = np.asarray(x)\n    y = np.asarray(y)\n\n    cosine_similarity = np.dot(x, y) / (_norm(x) * _norm(y))\n    return float(1 - cosine_similarity)\n</code></pre>"},{"location":"api/amltk/exceptions/","title":"Exceptions","text":"<p>A module holding a decorator to wrap a function to add a traceback to any exception raised.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.IntegrationNotFoundError","title":"<code>class IntegrationNotFoundError(name)</code>","text":"<p>         Bases: <code>Exception</code></p> <p>An exception raised when no integration is found.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the integration that was not found.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        name: The name of the integration that was not found.\n    \"\"\"\n    super().__init__(f\"No integration found for {name}.\")\n</code></pre>"},{"location":"api/amltk/exceptions/#amltk.exceptions.SchedulerNotRunningError","title":"<code>class SchedulerNotRunningError</code>","text":"<p>         Bases: <code>RuntimeError</code></p> <p>The scheduler is not running.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.EventNotKnownError","title":"<code>class EventNotKnownError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>The event is not a known one.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.NoChoiceMadeError","title":"<code>class NoChoiceMadeError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>No choice was made.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.NodeNotFoundError","title":"<code>class NodeNotFoundError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>The node was not found.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.RequestNotMetError","title":"<code>class RequestNotMetError</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>Raised when a request is not met.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.ComponentBuildError","title":"<code>class ComponentBuildError</code>","text":"<p>         Bases: <code>TypeError</code></p> <p>Raised when failing to build a component.</p>"},{"location":"api/amltk/exceptions/#amltk.exceptions.DuplicateNamesError","title":"<code>class DuplicateNamesError(node)</code>","text":"<p>         Bases: <code>ValueError</code></p> <p>Raised when duplicate names are found.</p> PARAMETER  DESCRIPTION <code>node</code> <p>The node that has children with duplicate names.</p> <p> TYPE: <code>Node</code> </p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def __init__(self, node: Node) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        node: The node that has children with duplicate names.\n    \"\"\"\n    super().__init__(node)\n    self.node = node\n</code></pre>"},{"location":"api/amltk/exceptions/#amltk.exceptions.safe_map","title":"<code>def safe_map(f, args)</code>","text":"<p>Map a function over an iterable, catching any exceptions.</p> PARAMETER  DESCRIPTION <code>f</code> <p>The function to map.</p> <p> TYPE: <code>Callable[..., R]</code> </p> <code>args</code> <p>The iterable to map over.</p> <p> TYPE: <code>Iterable[Any]</code> </p> YIELDS DESCRIPTION <code>R | tuple[Exception, str]</code> <p>The return value of the function, or the exception raised.</p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def safe_map(\n    f: Callable[..., R],\n    args: Iterable[Any],\n) -&gt; Iterator[R | tuple[Exception, str]]:\n    \"\"\"Map a function over an iterable, catching any exceptions.\n\n    Args:\n        f: The function to map.\n        args: The iterable to map over.\n\n    Yields:\n        The return value of the function, or the exception raised.\n    \"\"\"\n    for arg in args:\n        try:\n            yield f(arg)\n        except Exception as e:  # noqa: BLE001\n            yield e, traceback.format_exc()\n</code></pre>"},{"location":"api/amltk/exceptions/#amltk.exceptions.safe_starmap","title":"<code>def safe_starmap(f, args)</code>","text":"<p>Map a function over an iterable, catching any exceptions.</p> PARAMETER  DESCRIPTION <code>f</code> <p>The function to map.</p> <p> TYPE: <code>Callable[..., R]</code> </p> <code>args</code> <p>The iterable to map over.</p> <p> TYPE: <code>Iterable[Iterable[Any]]</code> </p> YIELDS DESCRIPTION <code>R | tuple[Exception, str]</code> <p>The return value of the function, or the exception raised.</p> Source code in <code>src/amltk/exceptions.py</code> <pre><code>def safe_starmap(\n    f: Callable[..., R],\n    args: Iterable[Iterable[Any]],\n) -&gt; Iterator[R | tuple[Exception, str]]:\n    \"\"\"Map a function over an iterable, catching any exceptions.\n\n    Args:\n        f: The function to map.\n        args: The iterable to map over.\n\n    Yields:\n        The return value of the function, or the exception raised.\n    \"\"\"\n    for arg in args:\n        try:\n            yield f(*arg)\n        except Exception as e:  # noqa: BLE001\n            yield e, traceback.format_exc()\n</code></pre>"},{"location":"api/amltk/options/","title":"Options","text":"<p>Options for the AMTLK package.</p> <p>In general, these options are not intended for functional differences but more for any output generated by the package, such as rich or logging.</p>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions","title":"<code>class AMLTKOptions</code>","text":"<p>         Bases: <code>TypedDict</code></p> <p>The options available for AMTLK.</p> <pre><code>from amltk import options\n\nprint(options)\n</code></pre> <pre><code>&lt;module 'amltk.options' from '/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/amltk/options.py'&gt;\n</code></pre>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions.rich_signatures","title":"<code>rich_signatures: bool</code>   <code>attr</code>","text":"<p>Whether to display full signatures in rich output.</p>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions.rich_link","title":"<code>rich_link: Literal['auto', False]</code>   <code>attr</code>","text":"<p>Whether to use links in rich output.</p>"},{"location":"api/amltk/options/#amltk.options.AMLTKOptions.links","title":"<code>links: dict[str, str | Callable[[str], str]]</code>   <code>attr</code>","text":"<p>The links to use in rich output.</p> <p>The keys are the names of the packages, and the values are either the direct link to use or a callable that takes the fully qualified name of the object and returns the link to use.</p>"},{"location":"api/amltk/options/#amltk.options.get_option","title":"<code>def get_option(name, default=None)</code>","text":"<p>Get an option.</p> <pre><code>from amltk import options\n\nprint(options.get_option(\"rich_signatures\"))\n</code></pre> <pre><code>True\n</code></pre> Source code in <code>src/amltk/options.py</code> <pre><code>def get_option(name: str, default: T | None = None) -&gt; Any | T | None:\n    \"\"\"Get an option.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\"\n    from amltk import options\n\n    print(options.get_option(\"rich_signatures\"))\n    ```\n    \"\"\"\n    return _amltk_options.get(name, default)\n</code></pre>"},{"location":"api/amltk/randomness/","title":"Randomness","text":"<p>Utilities for dealing with randomness.</p>"},{"location":"api/amltk/randomness/#amltk.randomness.as_rng","title":"<code>def as_rng(seed=None)</code>","text":"<p>Converts a valid seed arg into a numpy.random.Generator instance.</p> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Generator</code> <p>A valid np.random.Generator object to use</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def as_rng(seed: Seed | None = None) -&gt; np.random.Generator:\n    \"\"\"Converts a valid seed arg into a numpy.random.Generator instance.\n\n    Args:\n        seed: The seed to use\n\n    Returns:\n        A valid np.random.Generator object to use\n    \"\"\"\n    match seed:\n        case None | int() | np.integer():\n            return np.random.default_rng(seed)\n        case np.random.Generator():\n            return seed\n        case np.random.RandomState():\n            _seed = seed.randint(0, MAX_INT)\n            return np.random.default_rng(_seed)\n\n    raise ValueError(f\"Can't {seed=} ({type(seed)}) to create numpy.random.Generator\")\n</code></pre>"},{"location":"api/amltk/randomness/#amltk.randomness.as_randomstate","title":"<code>def as_randomstate(seed=None)</code>","text":"<p>Converts a valid seed arg into a numpy.random.RandomState instance.</p> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>RandomState</code> <p>A valid np.random.RandomSTate object to use</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def as_randomstate(seed: Seed | None = None) -&gt; np.random.RandomState:\n    \"\"\"Converts a valid seed arg into a numpy.random.RandomState instance.\n\n    Args:\n        seed: The seed to use\n\n    Returns:\n        A valid np.random.RandomSTate object to use\n    \"\"\"\n    match seed:\n        case None | int() | np.integer():\n            return np.random.RandomState(seed)\n        case np.random.RandomState():\n            return seed\n        case np.random.Generator():\n            _seed = seed.integers(0, MAX_INT)\n            return np.random.RandomState(_seed)\n\n    raise ValueError(f\"Can't {seed=} ({type(seed)}) to create numpy.random.RandomState\")\n</code></pre>"},{"location":"api/amltk/randomness/#amltk.randomness.as_int","title":"<code>def as_int(seed=None)</code>","text":"<p>Converts a valid seed arg into an integer.</p> PARAMETER  DESCRIPTION <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>int</code> <p>A valid integer to use as a seed</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def as_int(seed: Seed | None = None) -&gt; int:\n    \"\"\"Converts a valid seed arg into an integer.\n\n    Args:\n        seed: The seed to use\n\n    Returns:\n        A valid integer to use as a seed\n    \"\"\"\n    match seed:\n        case None:\n            return int(np.random.default_rng().integers(0, MAX_INT))\n        case np.integer() | int():\n            return int(seed)\n        case np.random.Generator():\n            return int(seed.integers(0, MAX_INT))\n        case np.random.RandomState():\n            return int(seed.randint(0, MAX_INT))\n\n    raise ValueError(f\"Can't {seed=} ({type(seed)}) to create int\")\n</code></pre>"},{"location":"api/amltk/randomness/#amltk.randomness.randuid","title":"<code>def randuid(k=8, *, charset=ALPHABET, seed=None)</code>","text":"<p>Generate a random alpha-numeric uuid of a specified length.</p> <p>See: stackoverflow.com/a/56398787/5332072</p> PARAMETER  DESCRIPTION <code>k</code> <p>The length of the uuid to generate</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>charset</code> <p>The charset to use</p> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>ALPHABET</code> </p> <code>seed</code> <p>The seed to use</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A random uid</p> Source code in <code>src/amltk/randomness.py</code> <pre><code>def randuid(\n    k: int = 8,\n    *,\n    charset: Sequence[str] = ALPHABET,\n    seed: Seed | None = None,\n) -&gt; str:\n    \"\"\"Generate a random alpha-numeric uuid of a specified length.\n\n    See: https://stackoverflow.com/a/56398787/5332072\n\n    Args:\n        k: The length of the uuid to generate\n        charset: The charset to use\n        seed: The seed to use\n\n    Returns:\n        A random uid\n    \"\"\"\n    rng = as_rng(seed)\n    return \"\".join(rng.choice(np.asarray(charset), size=k))\n</code></pre>"},{"location":"api/amltk/types/","title":"Types","text":"<p>Stores low-level types used through the library.</p>"},{"location":"api/amltk/types/#amltk.types.SortedSequence","title":"<code>SortedSequence: TypeAlias</code>   <code>module-attribute</code>","text":"<p>A sequence that is sorted. Only useful for typing</p>"},{"location":"api/amltk/types/#amltk.types.SortedIterable","title":"<code>SortedIterable: TypeAlias</code>   <code>module-attribute</code>","text":"<p>An iterable that is sorted. Only useful for typing</p>"},{"location":"api/amltk/types/#amltk.types.Item","title":"<code>Item</code>   <code>module-attribute</code>","text":"<p>The type associated with components, splits and choices</p>"},{"location":"api/amltk/types/#amltk.types.Config","title":"<code>Config: TypeAlias</code>   <code>module-attribute</code>","text":"<p>An object representing a configuration of a pipeline.</p>"},{"location":"api/amltk/types/#amltk.types.Space","title":"<code>Space</code>   <code>module-attribute</code>","text":"<p>Generic for objects that are aware of a space but not the specific kind</p>"},{"location":"api/amltk/types/#amltk.types.Seed","title":"<code>Seed: TypeAlias</code>   <code>module-attribute</code>","text":"<p>Type alias for kinds of Seeded objects.</p>"},{"location":"api/amltk/types/#amltk.types.FidT","title":"<code>FidT: TypeAlias</code>   <code>module-attribute</code>","text":"<p>Type alias for a fidelity bound.</p>"},{"location":"api/amltk/types/#amltk.types.Comparable","title":"<code>class Comparable</code>","text":"<p>         Bases: <code>Protocol</code></p> <p>Protocol for annotating comparable types.</p>"},{"location":"api/amltk/types/#amltk.types.Requeue","title":"<code>class Requeue(generator)</code>","text":"<p>         Bases: <code>Iterator[T]</code></p> <p>A queue that can have items requeued.</p> Requeue<pre><code>import random\nfrom amltk.types import Requeue\n\nname_generator = iter([\"Alice\", \"Bob\", \"Charlie\"])\nqueue: Requeue[str] = Requeue(name_generator)\n\nrng = random.Random(1)\n\ndef process_name(name: str) -&gt; bool:\n    return rng.choice([True, False])\n\nfor name in queue:\n    print(f\"Processing {name}\")\n    processed = process_name(name)\n    if not processed:\n        print(f\"Failed to process {name}, requeuing\")\n        queue.requeue(name)\n</code></pre> <pre><code>Processing Alice\nProcessing Bob\nProcessing Charlie\nFailed to process Charlie, requeuing\nProcessing Charlie\n</code></pre> See Also <ul> <li> <p><code>Requeue.from_func(f)</code></p> <p>If you have a function which will generate items, you can use this to create a requeue from it.</p> </li> <li> <p><code>.append(item)</code></p> <p>Append an item to the end of the queue</p> </li> <li> <p><code>.requeue(item)</code></p> <p>Requeue an item to the start of the queue</p> </li> </ul> Source code in <code>src/amltk/types.py</code> <pre><code>def __init__(self, generator: Iterable[T]) -&gt; None:\n    \"\"\"Create a requeue from an iterable.\"\"\"\n    super().__init__()\n    self.generator = iter(generator)\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.Requeue.append","title":"<code>def append(item)</code>","text":"<p>Append an item to the queue.</p> Source code in <code>src/amltk/types.py</code> <pre><code>def append(self, item: T) -&gt; None:\n    \"\"\"Append an item to the queue.\"\"\"\n    self.generator = chain(self.generator, [item])\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.Requeue.requeue","title":"<code>def requeue(item)</code>","text":"<p>Requeue an item.</p> Source code in <code>src/amltk/types.py</code> <pre><code>def requeue(self, item: T) -&gt; None:\n    \"\"\"Requeue an item.\"\"\"\n    self.generator = chain([item], self.generator)\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.Requeue.from_func","title":"<code>def from_func(func, n=None)</code>   <code>classmethod</code>","text":"<p>Create a Requeue from a function.</p> Source code in <code>src/amltk/types.py</code> <pre><code>@classmethod\ndef from_func(cls, func: Callable[[], T], n: int | None = None) -&gt; Requeue[T]:\n    \"\"\"Create a Requeue from a function.\"\"\"\n    repeater = repeat(None) if n is None else repeat(None, times=n)\n    return cls(func() for _ in repeater)\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.assert_never","title":"<code>def assert_never(value)</code>","text":"<p>Utility function for asserting that a value is never reached.</p> Source code in <code>src/amltk/types.py</code> <pre><code>def assert_never(value: NoReturn) -&gt; NoReturn:\n    \"\"\"Utility function for asserting that a value is never reached.\"\"\"\n    # This also works in runtime as well:\n    raise AssertionError(f\"This code should never be reached, got: {value}\")\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.safe_issubclass","title":"<code>def safe_issubclass(cls, classes)</code>","text":"<p>Check if a class is a subclass of a given type.</p> <p>This is a safe version of issubclass that relies on strings, which is useful for when the type is not importable.</p> PARAMETER  DESCRIPTION <code>cls</code> <p>The class to check</p> <p> TYPE: <code>type</code> </p> <code>classes</code> <p>The type to check for.</p> <p> TYPE: <code>str | tuple[str, ...]</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>bool</p> Source code in <code>src/amltk/types.py</code> <pre><code>def safe_issubclass(cls: type, classes: str | tuple[str, ...]) -&gt; bool:\n    \"\"\"Check if a class is a subclass of a given type.\n\n    This is a safe version of issubclass that relies on strings,\n    which is useful for when the type is not importable.\n\n    Args:\n        cls: The class to check\n        classes: The type to check for.\n\n    Returns:\n        bool\n    \"\"\"\n\n    def type_names(o: type) -&gt; Iterator[str]:\n        yield o.__qualname__\n        for parent in o.__bases__:\n            yield from type_names(parent)\n\n    allowable_names = {classes} if isinstance(classes, str) else set(classes)\n    return any(name in allowable_names for name in type_names(cls))\n</code></pre>"},{"location":"api/amltk/types/#amltk.types.safe_isinstance","title":"<code>def safe_isinstance(obj, t)</code>","text":"<p>Check if an object is of a given type.</p> <p>This is a safe version of isinstance that relies on strings, which is useful for when the type is not importable.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to check</p> <p> TYPE: <code>Any</code> </p> <code>t</code> <p>The type to check for.</p> <p> TYPE: <code>str | tuple[str, ...]</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>bool</p> Source code in <code>src/amltk/types.py</code> <pre><code>def safe_isinstance(obj: Any, t: str | tuple[str, ...]) -&gt; bool:\n    \"\"\"Check if an object is of a given type.\n\n    This is a safe version of isinstance that relies on strings,\n    which is useful for when the type is not importable.\n\n    Args:\n        obj: The object to check\n        t: The type to check for.\n\n    Returns:\n        bool\n    \"\"\"\n    return safe_issubclass(type(obj), t)\n</code></pre>"},{"location":"api/amltk/data/conversions/","title":"Conversions","text":"<p>Conversions between different data repesentations and formats.</p>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.probabilities_to_classes","title":"<code>def probabilities_to_classes(probabilities, classes)</code>","text":"<p>Convert probabilities to classes.</p> <p>Note</p> <p>Converts using the logic of <code>predict()</code> of <code>RandomForestClassifier</code>.</p> PARAMETER  DESCRIPTION <code>probabilities</code> <p>The probabilities to convert</p> <p> TYPE: <code>NDArray[floating]</code> </p> <code>classes</code> <p>The classes to use.</p> <p> TYPE: <code>ndarray | ArrayLike | list</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The classes corresponding to the probabilities</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def probabilities_to_classes(\n    probabilities: npt.NDArray[np.floating],\n    classes: np.ndarray | npt.ArrayLike | list,\n) -&gt; np.ndarray:\n    \"\"\"Convert probabilities to classes.\n\n    !!! note\n\n        Converts using the logic of `predict()` of `RandomForestClassifier`.\n\n    Args:\n        probabilities: The probabilities to convert\n        classes: The classes to use.\n\n    Returns:\n        The classes corresponding to the probabilities\n    \"\"\"\n    # Taken from `predict()` of RandomForestclassifier\n    classes = np.asarray(classes)\n    n_outputs = 1 if classes.ndim == 1 else classes.shape[1]\n    if n_outputs == 1:\n        return classes.take(np.argmax(probabilities, axis=1), axis=0)  # type: ignore\n\n    n_samples = probabilities[0].shape[0]\n    # all dtypes should be the same, so just take the first\n    class_type = classes[0].dtype\n    predictions = np.empty((n_samples, n_outputs), dtype=class_type)\n\n    for k in range(n_outputs):\n        predictions[:, k] = classes[k].take(np.argmax(probabilities[k], axis=1), axis=0)\n\n    return predictions\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.to_numpy","title":"<code>def to_numpy(x, *, flatten_if_1d=False)</code>","text":"<p>Convert to numpy array.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The data to convert</p> <p> TYPE: <code>ndarray | DataFrame | Series</code> </p> <code>flatten_if_1d</code> <p>Whether to flatten the array if it is 1d</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The converted data</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def to_numpy(\n    x: np.ndarray | pd.DataFrame | pd.Series,\n    *,\n    flatten_if_1d: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Convert to numpy array.\n\n    Args:\n        x: The data to convert\n        flatten_if_1d: Whether to flatten the array if it is 1d\n\n    Returns:\n        The converted data\n    \"\"\"\n    _x = x.to_numpy() if isinstance(x, pd.DataFrame | pd.Series) else np.asarray(x)\n\n    if (\n        flatten_if_1d\n        and x.ndim == 2  # noqa: PLR2004 # type: ignore\n        and x.shape[1] == 1  # type: ignore\n    ):\n        _x = np.ravel(_x)\n\n    assert isinstance(_x, np.ndarray)\n    return _x\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.flatten_if_1d","title":"<code>def flatten_if_1d(x)</code>","text":"<p>Flatten if 1d.</p> <p>Retains the type of the input, i.e. pandas stays pandas and numpy stays numpy.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The data to flatten</p> <p> TYPE: <code>ndarray | DataFrame | Series</code> </p> RETURNS DESCRIPTION <code>ndarray | DataFrame | Series</code> <p>The flattened data</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def flatten_if_1d(\n    x: np.ndarray | pd.DataFrame | pd.Series,\n) -&gt; np.ndarray | pd.DataFrame | pd.Series:\n    \"\"\"Flatten if 1d.\n\n    Retains the type of the input, i.e. pandas stays pandas and numpy stays numpy.\n\n    Args:\n        x: The data to flatten\n\n    Returns:\n        The flattened data\n    \"\"\"\n    if isinstance(x, np.ndarray) and x.ndim == 2 and x.shape[1] == 1:  # noqa: PLR2004\n        x = np.ravel(x)\n    elif (\n        isinstance(x, pd.DataFrame) and x.ndim == 2 and x.shape[1] == 1  # noqa: PLR2004\n    ):\n        x = x.iloc[:, 0]\n\n    return x\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.is_str_object_dtype","title":"<code>def is_str_object_dtype(x)</code>","text":"<p>Check if object dtype and string values.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The data to check</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether it is object dtype and string values</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def is_str_object_dtype(x: np.ndarray) -&gt; bool:\n    \"\"\"Check if object dtype and string values.\n\n    Args:\n        x: The data to check\n\n    Returns:\n        Whether it is object dtype and string values\n    \"\"\"\n    return x.dtype == object and isinstance(x[0], str)\n</code></pre>"},{"location":"api/amltk/data/conversions/#amltk.data.conversions.as_str_dtype_if_str_object","title":"<code>def as_str_dtype_if_str_object(x)</code>","text":"<p>Convert to string dtype if object dtype and string values.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The data to convert</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The converted data if it can be done</p> Source code in <code>src/amltk/data/conversions.py</code> <pre><code>def as_str_dtype_if_str_object(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert to string dtype if object dtype and string values.\n\n    Args:\n        x: The data to convert\n\n    Returns:\n        The converted data if it can be done\n    \"\"\"\n    if is_str_object_dtype(x):\n        return x.astype(str)\n    return x\n</code></pre>"},{"location":"api/amltk/data/dtype_reduction/","title":"Dtype reduction","text":"<p>Reduce the dtypes of data.</p>"},{"location":"api/amltk/data/dtype_reduction/#amltk.data.dtype_reduction.reduce_floating_precision","title":"<code>def reduce_floating_precision(x)</code>","text":"<p>Reduce the floating point precision of the data.</p> <p>For a float array, will reduce by one step, i.e. float32 -&gt; float16, float64 -&gt; float32.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The data to reduce.</p> <p> TYPE: <code>D</code> </p> RETURNS DESCRIPTION <code>D</code> <p>The reduced data.</p> Source code in <code>src/amltk/data/dtype_reduction.py</code> <pre><code>def reduce_floating_precision(x: D) -&gt; D:\n    \"\"\"Reduce the floating point precision of the data.\n\n    For a float array, will reduce by one step, i.e. float32 -&gt; float16, float64\n    -&gt; float32.\n\n    Args:\n        x: The data to reduce.\n\n    Returns:\n        The reduced data.\n    \"\"\"\n    # For a dataframe, we recurse over all columns\n    if isinstance(x, pd.DataFrame):\n        # Using `apply` doesn't work\n        for col in x.columns:\n            x[col] = reduce_floating_precision(x[col])\n        return x  # type: ignore\n\n    if x.dtype.kind != \"f\":\n        return x\n\n    _reduction_map = {\n        # Base numpy dtypes\n        \"float128\": \"float64\",\n        \"float96\": \"float64\",\n        \"float64\": \"float32\",\n        \"float32\": \"float16\",\n        # Nullable pandas dtypes (only supports 64 and 32 bit)\n        \"Float64\": \"Float32\",\n    }\n\n    if (dtype := _reduction_map.get(x.dtype.name)) is not None:\n        return x.astype(dtype)  # type: ignore\n\n    return x\n</code></pre>"},{"location":"api/amltk/data/dtype_reduction/#amltk.data.dtype_reduction.reduce_int_span","title":"<code>def reduce_int_span(x)</code>","text":"<p>Reduce the integer span of the data.</p> <p>For an int array, will reduce to the smallest dtype that can hold the minimum and maximum values of the array.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The data to reduce.</p> <p> TYPE: <code>D</code> </p> RETURNS DESCRIPTION <code>D</code> <p>The reduced data.</p> Source code in <code>src/amltk/data/dtype_reduction.py</code> <pre><code>def reduce_int_span(x: D) -&gt; D:\n    \"\"\"Reduce the integer span of the data.\n\n    For an int array, will reduce to the smallest dtype that can hold the\n    minimum and maximum values of the array.\n\n    Args:\n        x: The data to reduce.\n\n    Returns:\n        The reduced data.\n    \"\"\"\n    # For a dataframe, we recurse over all columns\n    if isinstance(x, pd.DataFrame):\n        # Using `apply` doesn't work\n        for col in x.columns:\n            x[col] = reduce_int_span(x[col])\n        return x  # type: ignore\n\n    if x.dtype.kind not in \"iu\":\n        return x\n\n    min_dtype = np.min_scalar_type(x.min())  # type: ignore\n    max_dtype = np.min_scalar_type(x.max())  # type: ignore\n    dtype = np.result_type(min_dtype, max_dtype)\n\n    # The above dtype is a numpy dtype and may not allow for nullable values,\n    # which are permissible in pandas. `to_numeric` will convert to appropriate\n    # pandas nullable dtypes.\n    if isinstance(x, pd.Series):\n        dc = \"unsigned\" if \"uint\" in dtype.name else \"integer\"\n        return pd.to_numeric(x, downcast=dc)\n\n    return x.astype(dtype)\n</code></pre>"},{"location":"api/amltk/data/dtype_reduction/#amltk.data.dtype_reduction.reduce_dtypes","title":"<code>def reduce_dtypes(x, *, reduce_int=True, reduce_float=True)</code>","text":"<p>Reduce the dtypes of data.</p> <p>When a dataframe, will reduce the dtypes of all columns. When applied to an iterable, will apply to all elements of the iterable.</p> <p>For an int array, will reduce to the smallest dtype that can hold the minimum and maximum values of the array. Otherwise for floats, will reduce by one step, i.e. float32 -&gt; float16, float64 -&gt; float32.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The data to reduce.</p> <p> TYPE: <code>D</code> </p> <code>reduce_int</code> <p>Whether to reduce integer dtypes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>reduce_float</code> <p>Whether to reduce floating point dtypes.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/data/dtype_reduction.py</code> <pre><code>def reduce_dtypes(x: D, *, reduce_int: bool = True, reduce_float: bool = True) -&gt; D:\n    \"\"\"Reduce the dtypes of data.\n\n    When a dataframe, will reduce the dtypes of all columns.\n    When applied to an iterable, will apply to all elements of the iterable.\n\n    For an int array, will reduce to the smallest dtype that can hold the\n    minimum and maximum values of the array. Otherwise for floats, will reduce\n    by one step, i.e. float32 -&gt; float16, float64 -&gt; float32.\n\n    Args:\n        x: The data to reduce.\n        reduce_int: Whether to reduce integer dtypes.\n        reduce_float: Whether to reduce floating point dtypes.\n    \"\"\"\n    if not isinstance(x, pd.DataFrame | pd.Series | np.ndarray):\n        raise TypeError(f\"Cannot reduce data of type {type(x)}.\")\n\n    if isinstance(x, pd.Series | pd.DataFrame):\n        x = x.convert_dtypes()\n\n    if reduce_int:\n        x = reduce_int_span(x)\n    if reduce_float:\n        x = reduce_floating_precision(x)\n\n    return x\n</code></pre>"},{"location":"api/amltk/data/measure/","title":"Measure","text":"<p>Measure things about data.</p>"},{"location":"api/amltk/data/measure/#amltk.data.measure.byte_size","title":"<code>def byte_size(data)</code>","text":"<p>Measure the size of data.</p> <p>Works for numpy-arrays, pandas DataFrames and Series, and iterables of any of these.</p> PARAMETER  DESCRIPTION <code>data</code> <p>The data to measure.</p> <p> TYPE: <code>Any | Iterable[Any]</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The size of the data.</p> Source code in <code>src/amltk/data/measure.py</code> <pre><code>def byte_size(data: Any | Iterable[Any]) -&gt; int:\n    \"\"\"Measure the size of data.\n\n    Works for numpy-arrays, pandas DataFrames and Series, and iterables of any of\n    these.\n\n    Args:\n        data: The data to measure.\n\n    Returns:\n        The size of the data.\n    \"\"\"\n    if isinstance(data, np.ndarray):\n        return data.nbytes\n    if isinstance(data, pd.DataFrame):\n        return int(data.memory_usage(deep=True).sum())\n    if isinstance(data, pd.Series):\n        return int(data.memory_usage(deep=True))\n    if isinstance(data, str):\n        return sys.getsizeof(data)\n    if isinstance(data, Iterable):\n        return sum(byte_size(d) for d in data)\n\n    return sys.getsizeof(data)\n</code></pre>"},{"location":"api/amltk/ensembling/weighted_ensemble_caruana/","title":"Weighted ensemble caruana","text":"<p>Implementation of the weighted ensemble procedure from Caruana et al. 2004.</p> Reference <p>Ensemble selection from libraries of models</p> <p>Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew and Alex Ksikes</p> <p>ICML 2004</p> <p>dl.acm.org/doi/10.1145/1015330.1015432</p> <p>www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf</p>"},{"location":"api/amltk/ensembling/weighted_ensemble_caruana/#amltk.ensembling.weighted_ensemble_caruana.weighted_ensemble_caruana","title":"<code>def weighted_ensemble_caruana(*, model_predictions, targets, size, metric, select, seed=None)</code>","text":"<p>Calculate a weighted ensemble of <code>n</code> models.</p> PARAMETER  DESCRIPTION <code>model_predictions</code> <p>Mapping from model id to predictions</p> <p> TYPE: <code>Mapping[K, ndarray]</code> </p> <code>targets</code> <p>The targets</p> <p> TYPE: <code>ndarray</code> </p> <code>size</code> <p>The size of the ensemble to create</p> <p> TYPE: <code>int</code> </p> <code>metric</code> <p>The metric to use in calculating which models to add to the ensemble.</p> <p> TYPE: <code>Callable[[ndarray, ndarray], T]</code> </p> <code>select</code> <p>Selects a models from the list based on the values of the metric on their predictions. Can return a single ID or a list of them, in which case a random selection will be made.</p> <p> TYPE: <code>Callable[[Iterable[T]], T]</code> </p> <code>seed</code> <p>The seed to use for breaking ties</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[dict[K, float], list[tuple[K, T]], ndarray]</code> <p>A mapping from id's to it's weight in the ensemble and the trajectory.</p> Source code in <code>src/amltk/ensembling/weighted_ensemble_caruana.py</code> <pre><code>def weighted_ensemble_caruana(\n    *,\n    model_predictions: Mapping[K, np.ndarray],\n    targets: np.ndarray,\n    size: int,\n    metric: Callable[[np.ndarray, np.ndarray], T],\n    select: Callable[[Iterable[T]], T],\n    seed: Seed | None = None,\n) -&gt; tuple[dict[K, float], list[tuple[K, T]], np.ndarray]:\n    \"\"\"Calculate a weighted ensemble of `n` models.\n\n    Args:\n        model_predictions: Mapping from model id to predictions\n        targets: The targets\n        size: The size of the ensemble to create\n        metric: The metric to use in calculating which models to add to the ensemble.\n        select: Selects a models from the list based on the values of the metric on\n            their predictions. Can return a single ID or a list of them, in which\n            case a random selection will be made.\n        seed: The seed to use for breaking ties\n\n    Returns:\n        A mapping from id's to it's weight in the ensemble and the trajectory.\n    \"\"\"\n    if not size &gt; 0:\n        raise ValueError(\"`size` must be positive\")\n\n    if len(model_predictions) == 0:\n        raise ValueError(\"`model_predictions` is empty\")\n\n    rng = as_rng(seed)\n    predictions = list(model_predictions.values())\n\n    dtype = predictions[0].dtype\n    if np.issubdtype(dtype, np.integer):\n        logger.warning(\n            f\"Predictions were {dtype=}, converting to np.float64 to\"\n            \" allow for weighted ensemble procedure.\",\n        )\n        dtype = np.float64\n\n    # Current sum of predictions in the ensemble\n    current = np.zeros_like(predictions[0], dtype=dtype)\n\n    # Buffer where new models predictions are added to current to try them\n    buffer = np.empty_like(predictions[0], dtype=dtype)\n\n    ensemble: list[K] = []\n    trajectory: list[tuple[K, T]] = []\n\n    def value_if_added(_pred: np.ndarray) -&gt; T:\n        # Get the value if the model was added to the current set of predicitons\n        np.add(current, _pred, out=buffer)\n        np.multiply(buffer, (1.0 / float(len(ensemble) + 1)), out=buffer)\n\n        return metric(targets, buffer)\n\n    for _ in range(size):\n        # Get the value if added for each model\n        scores = {_id: value_if_added(pred) for _id, pred in model_predictions.items()}\n\n        # Get the choices that produce the best value\n        chosen_val = select(scores.values())\n\n        choices = [_id for _id, score in scores.items() if score == chosen_val]\n        choice = rng.choice(np.asarray(choices))\n\n        # Add the predictions of the chosen model\n        np.add(current, model_predictions[choice], out=current)\n\n        # Record it's addition and the score of the ensemble with this\n        # choice added\n        ensemble.append(choice)\n        trajectory.append((choice, chosen_val))\n\n        # In the case of only one model, have calculated it's loss\n        # and it's the only available model to add to the ensemble\n        if len(model_predictions) == 1:\n            ensemble *= size\n            trajectory *= size\n            break\n\n    final = np.multiply(current, (1.0 / float(len(ensemble))))\n\n    return (\n        {_id: count / size for _id, count in Counter(ensemble).items()},\n        trajectory,\n        final,\n    )\n</code></pre>"},{"location":"api/amltk/metalearning/dataset_distances/","title":"Dataset distances","text":"<p>One common way to define how similar two datasets are is to compute some \"similarity\" between them. This notion of \"similarity\" requires computing some features of a dataset (metafeatures) first, such that we can numerically compute some distance function.</p> <p>Let's see how we can quickly compute the distance between some datasets with <code>dataset_distance()</code>!</p> Dataset Distances P.1<pre><code>import pandas as pd\nimport openml\n\nfrom amltk.metalearning import compute_metafeatures\n\ndef get_dataset(dataset_id: int) -&gt; tuple[pd.DataFrame, pd.Series]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n    X, y, _, _ = dataset.get_data(\n        dataset_format=\"dataframe\",\n        target=dataset.default_target_attribute,\n    )\n    return X, y\n\nd31 = get_dataset(31)\nd3 = get_dataset(3)\nd4 = get_dataset(4)\n\nmetafeatures_dict = {\n    \"dataset_31\": compute_metafeatures(*d31),\n    \"dataset_3\": compute_metafeatures(*d3),\n    \"dataset_4\": compute_metafeatures(*d4),\n}\n\nmetafeatures = pd.DataFrame(metafeatures_dict)\nprint(metafeatures)\n</code></pre> <pre><code>                                                     dataset_31  ...  dataset_4\ninstance_count                                      1000.000000  ...  57.000000\nlog_instance_count                                     6.907755  ...   4.043051\nnumber_of_classes                                      2.000000  ...   2.000000\nnumber_of_features                                    20.000000  ...  16.000000\nlog_number_of_features                                 2.995732  ...   2.772589\npercentage_missing_values                              0.000000  ...   0.357456\npercentage_of_instances_with_missing_values            0.000000  ...   0.982456\npercentage_of_features_with_missing_values             0.000000  ...   1.000000\npercentage_of_categorical_columns_with_missing_...     0.000000  ...   1.000000\npercentage_of_categorical_values_with_missing_v...     0.000000  ...   0.410088\npercentage_of_numeric_columns_with_missing_values      0.000000  ...   1.000000\npercentage_of_numeric_values_with_missing_values       0.000000  ...   0.304825\nnumber_of_numeric_features                             7.000000  ...   8.000000\nnumber_of_categorical_features                        13.000000  ...   8.000000\nratio_numerical_features                               0.350000  ...   0.500000\nratio_categorical_features                             0.650000  ...   0.500000\nratio_features_to_instances                            0.020000  ...   0.280702\nminority_class_imbalance                               0.200000  ...   0.149123\nmajority_class_imbalance                               0.200000  ...   0.149123\nclass_imbalance                                        0.400000  ...   0.298246\nmean_categorical_imbalance                             0.500500  ...   0.308063\nstd_categorical_imbalance                              0.234994  ...   0.228906\nskewness_mean                                          0.920379  ...   0.255076\nskewness_std                                           0.904952  ...   1.420729\nskewness_min                                          -0.531348  ...  -2.007217\nskewness_max                                           1.949628  ...   3.318064\nkurtosis_mean                                          0.924278  ...   2.046258\nkurtosis_std                                           1.785467  ...   4.890029\nkurtosis_min                                          -1.381449  ...  -2.035406\nkurtosis_max                                           4.292590  ...  13.193069\n\n[30 rows x 3 columns]\n</code></pre> <p>Now we want to know which one of <code>\"dataset_3\"</code> or <code>\"dataset_4\"</code> is more similar to <code>\"dataset_31\"</code>.</p> Dataset Distances P.2<pre><code>from amltk.metalearning import dataset_distance\n\ntarget = metafeatures_dict.pop(\"dataset_31\")\nothers = metafeatures_dict\n\ndistances = dataset_distance(target, others, distance_metric=\"l2\")\nprint(distances)\n</code></pre> <pre><code>dataset_4     943.079572\ndataset_3    2196.197231\nName: l2, dtype: float64\n</code></pre> <p>Seems like <code>\"dataset_3\"</code> is some notion of closer to <code>\"dataset_31\"</code> than <code>\"dataset_4\"</code>. However the scale of the metafeatures are not exactly all close. For example, many lie between <code>(0, 1)</code> but some like <code>instance_count</code> can completely dominate the show.</p> <p>Lets repeat the computation but specify that we should apply a <code>\"minmax\"</code> scaling across the rows.</p> Dataset Distances P.3<pre><code>distances = dataset_distance(\n    target,\n    others,\n    distance_metric=\"l2\",\n    scaler=\"minmax\"\n)\nprint(distances)\n</code></pre> <pre><code>dataset_3    3.293831\ndataset_4    3.480296\nName: l2, dtype: float64\n</code></pre> <p>Now <code>\"dataset_3\"</code> is considered more similar but the difference between the two is a lot less dramatic. In general, applying some scaling to values of different scales is required for metalearning.</p> <p>You can also use an sklearn.preprocessing.MinMaxScaler or anything other scaler from scikit-learn for that matter.</p> Dataset Distances P.3<pre><code>from sklearn.preprocessing import MinMaxScaler\n\ndistances = dataset_distance(\n    target,\n    others,\n    distance_metric=\"l2\",\n    scaler=MinMaxScaler()\n)\nprint(distances)\n</code></pre> <pre><code>dataset_3    3.293831\ndataset_4    3.480296\nName: l2, dtype: float64\n</code></pre>"},{"location":"api/amltk/metalearning/dataset_distances/#amltk.metalearning.dataset_distances.dataset_distance","title":"<code>def dataset_distance(target, dataset_metafeatures, *, distance_metric='l2', scaler=None, closest_n=None)</code>","text":"<p>Calculates the distance between a target dataset and a set of datasets.</p> <p>This uses the metafeatures of the datasets to calculate the distance.</p> PARAMETER  DESCRIPTION <code>target</code> <p>The target dataset's metafeatures.</p> <p> TYPE: <code>Series</code> </p> <code>dataset_metafeatures</code> <p>A dictionary of dataset names to their metafeatures.</p> <p> TYPE: <code>Mapping[str, Series]</code> </p> <code>distance_metric</code> <p>The method to use to calculate the distance. Takes in the target dataset's metafeatures and a dataset's metafeatures Should return the distance between the two.</p> <p> TYPE: <code>DistanceMetric | NearestNeighborsDistance | NamedDistance</code> DEFAULT: <code>'l2'</code> </p> <code>scaler</code> <p>A scaler to use to scale the metafeatures.</p> <p> TYPE: <code>TransformerMixin | Callable[[DataFrame], DataFrame] | Literal['minmax'] | None</code> DEFAULT: <code>None</code> </p> <code>closest_n</code> <p>The number of closest datasets to return. If None, all datasets are returned.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>Series with the index being the dataset name and the values being the distance.</p> Source code in <code>src/amltk/metalearning/dataset_distances.py</code> <pre><code>def dataset_distance(  # noqa: C901, PLR0912\n    target: pd.Series,\n    dataset_metafeatures: Mapping[str, pd.Series],\n    *,\n    distance_metric: (DistanceMetric | NearestNeighborsDistance | NamedDistance) = \"l2\",\n    scaler: TransformerMixin\n    | Callable[[pd.DataFrame], pd.DataFrame]\n    | Literal[\"minmax\"]\n    | None = None,\n    closest_n: int | None = None,\n) -&gt; pd.Series:\n    \"\"\"Calculates the distance between a target dataset and a set of datasets.\n\n    This uses the metafeatures of the datasets to calculate the distance.\n\n    Args:\n        target: The target dataset's metafeatures.\n        dataset_metafeatures: A dictionary of dataset names to their metafeatures.\n        distance_metric: The method to use to calculate the distance.\n            Takes in the target dataset's metafeatures and a dataset's metafeatures\n            Should return the distance between the two.\n        scaler: A scaler to use to scale the metafeatures.\n        closest_n: The number of closest datasets to return. If None, all datasets\n            are returned.\n\n    Returns:\n        Series with the index being the dataset name and the values being the distance.\n    \"\"\"\n    outname: str\n    if isinstance(distance_metric, str):\n        outname = distance_metric\n    else:\n        outname = funcname(distance_metric)\n\n    if target.name is None:\n        target = target.copy()\n        target.name = \"target-dataset\"\n\n    _method = (\n        distance_metrics[distance_metric]\n        if isinstance(distance_metric, str)\n        else distance_metric\n    )\n\n    if not isinstance(_method, NearestNeighborsDistance):\n        _method = _metric_for_frame(_method)\n\n    metafeatures = {\n        name: ds_metafeatures.rename(name)\n        for name, ds_metafeatures in dataset_metafeatures.items()\n    }\n\n    # Index is dataset name with columns being the values\n    #      | mf1 | mf2\n    # d1\n    # d2\n    # d3\n    combined = pd.concat([target, *metafeatures.values()], axis=1).T\n\n    if scaler is None:\n        pass\n    elif scaler == \"minmax\":\n        min_maxs = combined.agg([\"min\", \"max\"], axis=0).T\n\n        mins = min_maxs[\"min\"]\n        maxs = min_maxs[\"max\"]\n        normalizer = maxs - mins\n        normalizer[normalizer == 0] = 1\n        mins[normalizer == 0] = 0\n\n        norm = lambda col: (col - mins) / normalizer\n        combined = combined.apply(norm, axis=1)\n    elif safe_isinstance(scaler, \"TransformerMixin\"):\n        combined = scaler.set_output(transform=\"pandas\").fit_transform(  # type: ignore\n            combined,\n        )\n    elif callable(scaler):\n        combined = scaler(combined)\n    else:\n        raise ValueError(f\"Unsure how to handle {scaler=}\")\n\n    # We now transpose the dataframe so that the index is the metafeature name\n    # while the columns are the dataset names\n    #   x   | d1 | d2 | d3          y | dy\n    #  mf1                      mf1\n    #  mf2                      mf2\n    x = combined.T.drop(columns=target.name)\n    y = combined.loc[target.name]\n\n    # Should return a series with index being dataset names and values being the\n    #     | distance\n    # d1\n    # d2\n    dataset_distances = _method(x, y)\n\n    if not isinstance(dataset_distances, pd.Series):\n        dataset_distances = pd.Series(\n            dataset_distances,\n            dtype=float,\n            index=list(dataset_metafeatures.keys()),\n            name=outname,\n        )\n    else:\n        dataset_distances = dataset_distances.astype(float).rename(outname)\n\n    dataset_distances = dataset_distances.sort_values()\n\n    if closest_n is not None:\n        if closest_n &gt; len(dataset_distances):\n            warnings.warn(\n                f\"Cannot get {closest_n} closest datasets when there are\"\n                f\" only {len(dataset_distances)} datasets. Returning all.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n        dataset_distances = dataset_distances.iloc[:closest_n]\n\n    return dataset_distances\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/","title":"Metafeatures","text":"<p>A <code>MetaFeature</code> is some statistic about a dataset/task, that can be used to make datasets or tasks more comparable, thus enabling meta-learning methods.</p> <p>Calculating meta-features of a dataset is quite straight foward.</p> Metafeatures<pre><code>import openml\nfrom amltk.metalearning import compute_metafeatures\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nmfs = compute_metafeatures(X, y)\n\nprint(mfs)\n</code></pre> <pre><code>instance_count                                           1000.000000\nlog_instance_count                                          6.907755\nnumber_of_classes                                           2.000000\nnumber_of_features                                         20.000000\nlog_number_of_features                                      2.995732\npercentage_missing_values                                   0.000000\npercentage_of_instances_with_missing_values                 0.000000\npercentage_of_features_with_missing_values                  0.000000\npercentage_of_categorical_columns_with_missing_values       0.000000\npercentage_of_categorical_values_with_missing_values        0.000000\npercentage_of_numeric_columns_with_missing_values           0.000000\npercentage_of_numeric_values_with_missing_values            0.000000\nnumber_of_numeric_features                                  7.000000\nnumber_of_categorical_features                             13.000000\nratio_numerical_features                                    0.350000\nratio_categorical_features                                  0.650000\nratio_features_to_instances                                 0.020000\nminority_class_imbalance                                    0.200000\nmajority_class_imbalance                                    0.200000\nclass_imbalance                                             0.400000\nmean_categorical_imbalance                                  0.500500\nstd_categorical_imbalance                                   0.234994\nskewness_mean                                               0.920379\nskewness_std                                                0.904952\nskewness_min                                               -0.531348\nskewness_max                                                1.949628\nkurtosis_mean                                               0.924278\nkurtosis_std                                                1.785467\nkurtosis_min                                               -1.381449\nkurtosis_max                                                4.292590\ndtype: float64\n</code></pre> <p>By default <code>compute_metafeatures()</code> will calculate all the <code>MetaFeature</code> implemented, iterating through their subclasses to do so. You can pass an explicit list as well to <code>compute_metafeatures(X, y, features=[...])</code>.</p> <p>To implement your own is also quite straight forward:</p> Create Metafeature<pre><code>from amltk.metalearning import MetaFeature, compute_metafeatures\nimport openml\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nclass TotalValues(MetaFeature):\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; int:\n        return int(x.shape[0] * x.shape[1])\n\nmfs = compute_metafeatures(X, y, features=[TotalValues])\nprint(mfs)\n</code></pre> <pre><code>total_values    20000\ndtype: int64\n</code></pre> <p>As many metafeatures rely on pre-computed dataset statistics, and they do not need to be calculated more than once, you can specify the dependancies of a meta feature. When a metafeature would return something other than a single value, i.e. a <code>dict</code> or a <code>pd.DataFrame</code>, we instead call those a <code>DatasetStatistic</code>. These will not be included in the result of <code>compute_metafeatures()</code>. These <code>DatasetStatistic</code>s will only be calculated once on a call to <code>compute_metafeatures()</code> so they can be re-used across all <code>MetaFeature</code>s that require that dependancy.</p> Metafeature Dependancy<pre><code>from amltk.metalearning import MetaFeature, DatasetStatistic, compute_metafeatures\nimport openml\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nclass NAValues(DatasetStatistic):\n    \"\"\"A mask of all NA values in a dataset\"\"\"\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; pd.DataFrame:\n        return x.isna()\n\n\nclass PercentageNA(MetaFeature):\n    \"\"\"The percentage of values missing\"\"\"\n\n    dependencies = (NAValues,)\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; int:\n        na_values = dependancy_values[NAValues]\n        n_na = na_values.sum().sum()\n        n_values = int(x.shape[0] * x.shape[1])\n        return float(n_na / n_values)\n\nmfs = compute_metafeatures(X, y, features=[PercentageNA])\nprint(mfs)\n</code></pre> <pre><code>percentage_n_a    0.0\ndtype: float64\n</code></pre> <p>To view the description of a particular <code>MetaFeature</code>, you can call <code>.description()</code> on it. Otherwise you can access all of them in the following way:</p> SourceResult Metafeature Descriptions<pre><code>from pprint import pprint\nfrom amltk.metalearning import metafeature_descriptions\n\ndescriptions = metafeature_descriptions()\nfor name, description in descriptions.items():\n    print(\"---\")\n    print(name)\n    print(\"---\")\n    print(\" * \" + description)\n</code></pre> <pre><code>---\ninstance_count\n---\n * Number of instances in the dataset.\n---\nlog_instance_count\n---\n * Logarithm of the number of instances in the dataset.\n---\nnumber_of_classes\n---\n * Number of classes in the dataset.\n---\nnumber_of_features\n---\n * Number of features in the dataset.\n---\nlog_number_of_features\n---\n * Logarithm of the number of features in the dataset.\n---\npercentage_missing_values\n---\n * Percentage of missing values in the dataset.\n---\npercentage_of_instances_with_missing_values\n---\n * Percentage of instances with missing values.\n---\npercentage_of_features_with_missing_values\n---\n * Percentage of features with missing values.\n---\npercentage_of_categorical_columns_with_missing_values\n---\n * Percentage of categorical columns with missing values.\n---\npercentage_of_categorical_values_with_missing_values\n---\n * Percentage of categorical values with missing values.\n---\npercentage_of_numeric_columns_with_missing_values\n---\n * Percentage of numeric columns with missing values.\n---\npercentage_of_numeric_values_with_missing_values\n---\n * Percentage of numeric values with missing values.\n---\nnumber_of_numeric_features\n---\n * Number of numeric features in the dataset.\n---\nnumber_of_categorical_features\n---\n * Number of categorical features in the dataset.\n---\nratio_numerical_features\n---\n * Ratio of numerical features to total features in the dataset.\n---\nratio_categorical_features\n---\n * Ratio of categoricals features to total features in the dataset.\n---\nratio_features_to_instances\n---\n * Ratio of features to instances in the dataset.\n---\nminority_class_imbalance\n---\n * Imbalance of the minority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.\n---\nmajority_class_imbalance\n---\n * Imbalance of the majority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.\n---\nclass_imbalance\n---\n * Mean Target Imbalance of the classes in general.\n\n    0 =&gt; Balanced. 1 Imbalanced.\n\n---\nmean_categorical_imbalance\n---\n * The mean imbalance of categorical features.\n---\nstd_categorical_imbalance\n---\n * The std imbalance of categorical features.\n---\nskewness_mean\n---\n * The mean skewness of numerical features.\n---\nskewness_std\n---\n * The std skewness of numerical features.\n---\nskewness_min\n---\n * The min skewness of numerical features.\n---\nskewness_max\n---\n * The max skewness of numerical features.\n---\nkurtosis_mean\n---\n * The mean kurtosis of numerical features.\n---\nkurtosis_std\n---\n * The std kurtosis of numerical features.\n---\nkurtosis_min\n---\n * The min kurtosis of numerical features.\n---\nkurtosis_max\n---\n * The max kurtosis of numerical features.\n---\ntotal_values\n---\n * \n---\npercentage_n_a\n---\n * The percentage of values missing\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic","title":"<code>class DatasetStatistic</code>","text":"<p>         Bases: <code>ABC</code>, <code>Generic[S]</code></p> <p>Base class for a dataset statistic.</p> <p>A dataset statistic is a function that takes a dataset and returns some value(s) that describe the dataset.</p> <p>If looking to create meta-features, see the <code>MetaFeature</code> class which restricts the statistic to be a single number.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.description","title":"<code>def description()</code>   <code>classmethod</code>","text":"<p>Return the description of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef description(cls) -&gt; str:\n    \"\"\"Return the description of this statistic.\"\"\"\n    return cls.__doc__ or \"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.name","title":"<code>def name()</code>   <code>classmethod</code>","text":"<p>Return the name of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef name(cls) -&gt; str:\n    \"\"\"Return the name of this statistic.\"\"\"\n    return CAMEL_CASE_PATTERN.sub(\"_\", cls.__name__).lower()\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.compute","title":"<code>def compute(x, y, dependancy_values)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Compute the value of this statistic.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The features of the dataset.</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The labels of the dataset.</p> <p> TYPE: <code>Series | DataFrame</code> </p> <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>DSdict</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\n@abstractmethod\ndef compute(\n    cls,\n    x: pd.DataFrame,\n    y: pd.Series | pd.DataFrame,\n    dependancy_values: DSdict,\n) -&gt; S:\n    \"\"\"Compute the value of this statistic.\n\n    Args:\n        x: The features of the dataset.\n        y: The labels of the dataset.\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.DatasetStatistic.retrieve","title":"<code>def retrieve(dependancy_values)</code>   <code>classmethod</code>","text":"<p>Retrieve the value of this statistic from the dependency values.</p> PARAMETER  DESCRIPTION <code>dependancy_values</code> <p>A dictionary of dependency values.</p> <p> TYPE: <code>Mapping[type[DatasetStatistic[T]], T]</code> </p> RETURNS DESCRIPTION <code>S</code> <p>The value of this statistic.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef retrieve(\n    cls,\n    dependancy_values: Mapping[type[DatasetStatistic[T]], T],\n) -&gt; S:\n    \"\"\"Retrieve the value of this statistic from the dependency values.\n\n    Args:\n        dependancy_values: A dictionary of dependency values.\n\n    Returns:\n        The value of this statistic.\n    \"\"\"\n    return dependancy_values[cls]  # type: ignore\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature","title":"<code>class MetaFeature</code>","text":"<p>         Bases: <code>DatasetStatistic[M]</code></p> <p>Used to indicate a metafeature to include.</p> <p>This differs from DatasetStatistic in that it must return a single value.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.skip","title":"<code>skip: bool</code>   <code>classvar</code>","text":"<p>Whether to skip this metafeature when <code>iter()</code> is called.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MetaFeature.iter","title":"<code>def iter()</code>   <code>classmethod</code>","text":"<p>Return all the subclasses of MetaFeature.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>@classmethod\ndef iter(cls) -&gt; Iterator[type[MetaFeature]]:\n    \"\"\"Return all the subclasses of MetaFeature.\"\"\"\n    for c in cls.__subclasses__():\n        if not c.skip:\n            yield c\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NAValues","title":"<code>class NAValues</code>","text":"<p>         Bases: <code>DatasetStatistic[DataFrame]</code></p> <p>Mask of missing values in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalanceRatios","title":"<code>class ClassImbalanceRatios</code>","text":"<p>         Bases: <code>DatasetStatistic[tuple[Series, float]]</code></p> <p>Imbalance ratios of each class in the dataset.</p> <p>Will return the ratios of each class, the ratio expected if perfectly balanced,</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalImbalanceRatios","title":"<code>class CategoricalImbalanceRatios</code>","text":"<p>         Bases: <code>DatasetStatistic[dict[str, tuple[Series, float]]]</code></p> <p>Imbalance ratios of each class in the dataset.</p> <p>Will return the ratios of each class, the ratio expected if perfectly balanced,</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.CategoricalColumns","title":"<code>class CategoricalColumns</code>","text":"<p>         Bases: <code>DatasetStatistic[DataFrame]</code></p> <p>The categorical columns in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumericalColumns","title":"<code>class NumericalColumns</code>","text":"<p>         Bases: <code>DatasetStatistic[DataFrame]</code></p> <p>The numerical columns in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.InstanceCount","title":"<code>class InstanceCount</code>","text":"<p>         Bases: <code>MetaFeature[int]</code></p> <p>Number of instances in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogInstanceCount","title":"<code>class LogInstanceCount</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Logarithm of the number of instances in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfClasses","title":"<code>class NumberOfClasses</code>","text":"<p>         Bases: <code>MetaFeature[int]</code></p> <p>Number of classes in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfFeatures","title":"<code>class NumberOfFeatures</code>","text":"<p>         Bases: <code>MetaFeature[int]</code></p> <p>Number of features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.LogNumberOfFeatures","title":"<code>class LogNumberOfFeatures</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Logarithm of the number of features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageMissingValues","title":"<code>class PercentageMissingValues</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Percentage of missing values in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfInstancesWithMissingValues","title":"<code>class PercentageOfInstancesWithMissingValues</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Percentage of instances with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfFeaturesWithMissingValues","title":"<code>class PercentageOfFeaturesWithMissingValues</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Percentage of features with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalColumnsWithMissingValues","title":"<code>class PercentageOfCategoricalColumnsWithMissingValues</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Percentage of categorical columns with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfCategoricalValuesWithMissingValues","title":"<code>class PercentageOfCategoricalValuesWithMissingValues</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Percentage of categorical values with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericColumnsWithMissingValues","title":"<code>class PercentageOfNumericColumnsWithMissingValues</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Percentage of numeric columns with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.PercentageOfNumericValuesWithMissingValues","title":"<code>class PercentageOfNumericValuesWithMissingValues</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Percentage of numeric values with missing values.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfNumericFeatures","title":"<code>class NumberOfNumericFeatures</code>","text":"<p>         Bases: <code>MetaFeature[int]</code></p> <p>Number of numeric features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.NumberOfCategoricalFeatures","title":"<code>class NumberOfCategoricalFeatures</code>","text":"<p>         Bases: <code>MetaFeature[int]</code></p> <p>Number of categorical features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioNumericalFeatures","title":"<code>class RatioNumericalFeatures</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Ratio of numerical features to total features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioCategoricalFeatures","title":"<code>class RatioCategoricalFeatures</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Ratio of categoricals features to total features in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.RatioFeaturesToInstances","title":"<code>class RatioFeaturesToInstances</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Ratio of features to instances in the dataset.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassCounts","title":"<code>class ClassCounts</code>","text":"<p>         Bases: <code>DatasetStatistic[Series]</code></p> <p>Number of instances per class.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MinorityClassImbalance","title":"<code>class MinorityClassImbalance</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Imbalance of the minority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MajorityClassImbalance","title":"<code>class MajorityClassImbalance</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Imbalance of the majority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ClassImbalance","title":"<code>class ClassImbalance</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>Mean Target Imbalance of the classes in general.</p> <p>0 =&gt; Balanced. 1 Imbalanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.ImbalancePerCategory","title":"<code>class ImbalancePerCategory</code>","text":"<p>         Bases: <code>DatasetStatistic[dict[str, float]]</code></p> <p>Imbalance of each categorical feature. 0 =&gt; Balanced. 1 most imbalanced.</p> <p>No categories implies perfectly balanced.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.MeanCategoricalImbalance","title":"<code>class MeanCategoricalImbalance</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The mean imbalance of categorical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.StdCategoricalImbalance","title":"<code>class StdCategoricalImbalance</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The std imbalance of categorical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessPerNumericalColumn","title":"<code>class SkewnessPerNumericalColumn</code>","text":"<p>         Bases: <code>DatasetStatistic[dict[str, float]]</code></p> <p>Skewness of each numerical feature.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMean","title":"<code>class SkewnessMean</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The mean skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessStd","title":"<code>class SkewnessStd</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The std skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMin","title":"<code>class SkewnessMin</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The min skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.SkewnessMax","title":"<code>class SkewnessMax</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The max skewness of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisPerNumericalColumn","title":"<code>class KurtosisPerNumericalColumn</code>","text":"<p>         Bases: <code>DatasetStatistic[dict[str, float]]</code></p> <p>Kurtosis of each numerical feature.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMean","title":"<code>class KurtosisMean</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The mean kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisStd","title":"<code>class KurtosisStd</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The std kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMin","title":"<code>class KurtosisMin</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The min kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.KurtosisMax","title":"<code>class KurtosisMax</code>","text":"<p>         Bases: <code>MetaFeature[float]</code></p> <p>The max kurtosis of numerical features.</p>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.imbalance_ratios","title":"<code>def imbalance_ratios(col)</code>","text":"<p>Compute the imbalance ratio of a categorical column.</p> <p>This is done by computing the distance of each item's ratio to what a perfectly balanced ratio would be. We then sum up the distances, dividing by the worst case to normalize between 0 and 1.</p> PARAMETER  DESCRIPTION <code>col</code> <p>A column of values. If a DataFrame, the values from the subset of columns will be used.</p> <p> TYPE: <code>Series | DataFrame</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A tuple of the imbalance ratios, sorted from lowest (0) to highest (1)</p> <code>float</code> <p>and the expected ratio if perfectly balanced.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def imbalance_ratios(col: pd.Series | pd.DataFrame) -&gt; tuple[pd.Series, float]:\n    \"\"\"Compute the imbalance ratio of a categorical column.\n\n    This is done by computing the distance of each item's ratio to what\n    a perfectly balanced ratio would be. We then sum up the distances,\n    dividing by the worst case to normalize between 0 and 1.\n\n    Args:\n        col: A column of values. If a DataFrame, the values from the subset of columns\n            will be used.\n\n    Returns:\n        A tuple of the imbalance ratios, sorted from lowest (0) to highest (1)\n        and the expected ratio if perfectly balanced.\n    \"\"\"\n    ratios = col.value_counts(dropna=True, normalize=True, ascending=True)\n    if len(ratios) == 1:\n        return ratios, 1.0\n\n    n_uniq = len(ratios)\n\n    # A balanced ratio is one where all items are equally distributed\n    balanced_ratio = float(1 / n_uniq)\n    return ratios, balanced_ratio\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.column_imbalance","title":"<code>def column_imbalance(ratios, balanced_ratio)</code>","text":"<p>Compute the imbalance of a column.</p> <p>This is done by computing the distance of each item's ratio to what a perfectly balanced ratio would be. We then sum up the distances, dividing by the worst case to normalize between 0 and 1. 0 indicates a perfectly balanced column, 1 indicates a column where all items are of the same type.</p> PARAMETER  DESCRIPTION <code>ratios</code> <p>The ratios of each item in the column.</p> <p> TYPE: <code>Series</code> </p> <code>balanced_ratio</code> <p>The ratio of a column if perfectly balanced.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The imbalance of the column.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def column_imbalance(ratios: pd.Series, balanced_ratio: float) -&gt; float:\n    \"\"\"Compute the imbalance of a column.\n\n    This is done by computing the distance of each item's ratio to what\n    a perfectly balanced ratio would be. We then sum up the distances,\n    dividing by the worst case to normalize between 0 and 1. 0 indicates\n    a perfectly balanced column, 1 indicates a column where all items\n    are of the same type.\n\n    Args:\n        ratios: The ratios of each item in the column.\n        balanced_ratio: The ratio of a column if perfectly balanced.\n\n    Returns:\n        The imbalance of the column.\n    \"\"\"\n    item_ratios_distance_from_balanced_ratio = np.abs(ratios - balanced_ratio)\n\n    # The most imbalanced dataset would be one where we somehow have 0\n    # items of each type **except** 1 type, which has all the instances.\n\n    # In the case of a symbol group with 0 instance, their distance to the balanced\n    # ratio is just the balanced ratio itself.\n    zero_instance_ratio_distance = balanced_ratio\n    dominant_ratio_distance = np.abs(1 - balanced_ratio)\n    n_items = len(ratios)\n\n    worst = (n_items - 1) * zero_instance_ratio_distance + dominant_ratio_distance\n    normalizer = 1 / worst\n\n    return float(normalizer * np.sum(item_ratios_distance_from_balanced_ratio))\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.metafeature_descriptions","title":"<code>def metafeature_descriptions(features=None)</code>","text":"<p>Get the descriptions of meatfeatures available.</p> PARAMETER  DESCRIPTION <code>features</code> <p>The metafeatures. If None, all metafeatures subclasses of <code>MetaFeature</code> will be returned.</p> <p> TYPE: <code>Iterable[type[DatasetStatistic]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, str]</code> <p>The descriptions of the metafeatures.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def metafeature_descriptions(\n    features: Iterable[type[DatasetStatistic]] | None = None,\n) -&gt; dict[str, str]:\n    \"\"\"Get the descriptions of meatfeatures available.\n\n    Args:\n        features: The metafeatures. If None, all metafeatures subclasses\n            of [`MetaFeature`][amltk.metalearning.MetaFeature] will be returned.\n\n    Returns:\n        The descriptions of the metafeatures.\n    \"\"\"\n    if features is None:\n        features = MetaFeature.iter()\n\n    return {mf.name(): mf.description() for mf in features}\n</code></pre>"},{"location":"api/amltk/metalearning/metafeatures/#amltk.metalearning.metafeatures.compute_metafeatures","title":"<code>def compute_metafeatures(X, y, *, features=None)</code>","text":"<p>Compute metafeatures for a dataset.</p> PARAMETER  DESCRIPTION <code>X</code> <p>The features of the dataset.</p> <p> TYPE: <code>DataFrame</code> </p> <code>y</code> <p>The labels of the dataset.</p> <p> TYPE: <code>Series | DataFrame</code> </p> <code>features</code> <p>The metafeatures to compute. If None, all metafeatures subclasses of <code>MetaFeature</code> will be computed.</p> <p> TYPE: <code>Iterable[type[MetaFeature]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Series</code> <p>A series of metafeatures.</p> Source code in <code>src/amltk/metalearning/metafeatures.py</code> <pre><code>def compute_metafeatures(\n    X: pd.DataFrame,  # noqa: N803\n    y: pd.Series | pd.DataFrame,\n    *,\n    features: Iterable[type[MetaFeature]] | None = None,\n) -&gt; pd.Series:\n    \"\"\"Compute metafeatures for a dataset.\n\n    Args:\n        X: The features of the dataset.\n        y: The labels of the dataset.\n        features: The metafeatures to compute. If None, all metafeatures subclasses\n            of [`MetaFeature`][amltk.metalearning.MetaFeature] will be computed.\n\n    Returns:\n        A series of metafeatures.\n    \"\"\"\n    if features is None:\n        features = MetaFeature.iter()\n\n    def _calc(\n        _x: pd.DataFrame,\n        _y: pd.Series | pd.DataFrame,\n        _metafeature: type[DatasetStatistic],\n        _values: dict[type[DatasetStatistic], Any],\n    ) -&gt; dict[type[DatasetStatistic], Any]:\n        for dep in _metafeature.dependencies:\n            _values = _calc(_x, _y, dep, _values)\n\n        if _metafeature not in _values:\n            _values[_metafeature] = _metafeature.compute(_x, _y, _values)\n\n        return _values\n\n    values: dict[type[DatasetStatistic], Any] = {}\n    for mf in features:\n        values = _calc(X, y, mf, values)\n\n    return pd.Series(\n        {\n            key.name(): value\n            for key, value in values.items()\n            if issubclass(key, MetaFeature)\n        },\n    )\n</code></pre>"},{"location":"api/amltk/metalearning/portfolio/","title":"Portfolio","text":"<p>A portfolio in meta-learning is to a set (ordered or not) of configurations that maximize some notion of coverage across datasets or tasks. The intuition here is that this also means that any new dataset is also covered!</p> <p>Suppose we have the given performances of some configurations across some datasets. Initial Portfolio<pre><code>import pandas as pd\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\nprint(portfolio)\n</code></pre> <pre><code>           c1  c2  c3  c4\ndataset_1  90  20  10  90\ndataset_2  60  10  20  10\ndataset_3  20  90  40  10\ndataset_4  10  20  90  10\n</code></pre> </p> <p>If we could only choose <code>k=3</code> of these configurations on some new given dataset, which ones would you choose and in what priority? Here is where we can apply <code>portfolio_selection()</code>!</p> <p>The idea is that we pick a subset of these algorithms that maximise some value of utility for the portfolio. We do this by adding a single configuration from the entire set, 1-by-1 until we reach <code>k</code>, beginning with the empty portfolio.</p> <p>Let's see this in action!</p> Portfolio Selection<pre><code>import pandas as pd\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\"\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre> <p>The trajectory tells us which configuration was added at each time stamp along with the utility of the portfolio with that configuration added. However we havn't specified how exactly we defined the utility of a given portfolio. We could define our own function to do so:</p> Portfolio Selection Custom<pre><code>import pandas as pd\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\ndef my_function(p: pd.DataFrame) -&gt; float:\n    # Take the maximum score for each dataset and then take the mean across them.\n    return p.max(axis=1).mean()\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\",\n    portfolio_value=my_function,\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre> <p>This notion of reducing across all configurations for a dataset and then aggregating these is common enough that we can also directly just define these operations and we will perform the rest.</p> Portfolio Selection With Reduction<pre><code>import pandas as pd\nimport numpy as np\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\",\n    row_reducer=np.max,  # This is actually the default\n    aggregator=np.mean,  # This is actually the default\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre>"},{"location":"api/amltk/metalearning/portfolio/#amltk.metalearning.portfolio.portfolio_selection","title":"<code>def portfolio_selection(items, k, *, row_reducer=np.max, aggregator=np.mean, portfolio_value=None, maximize=True, scaler='minmax', with_replacement=False, stop_if_worse=False, seed=None)</code>","text":"<p>Selects a portfolio of <code>k</code> items from <code>items</code>.</p> <p>A portfolio is a subset of the items, and is selected by maximizing the <code>portfolio_value</code> function in a greedy selection approach.</p> <p>At each iteration <code>0 &lt;= i &lt; k</code>, the <code>portfolio_value</code> function is calculated for the portfolio obtained by adding the <code>i</code>th item to the portfolio. The item that maximizes the <code>portfolio_value</code> function is then added to the portfolio for the next iteration.</p> <p>The <code>portfolio_function</code> can often be define by a row wise reduction (<code>row_reducer=</code>) followed by some aggregation over these reductions (<code>aggregator=</code>). You can also supply your own value function if desired (<code>portfolio_value=</code>).</p> <p>A Single Iteration</p> <p>This uses the <code>row_reducer=np.max</code> and <code>aggregator=np.mean</code> to calculate the value of a portfolio.</p> <p>In this case, we have 4 datasets and our current portfolio consists of <code>config_1</code> and <code>config_2</code>. We are going to calculate the value of adding <code>config_try</code> to the current best portfolio.</p> <pre><code>            | config_1 | config_2 | config_try\ndataset_1   |    1     |    0     |    0\ndataset_2   |    0     |   0.5    |    1\ndataset_3   |    0     |   0.5    |   0.5\ndataset_4   |    1     |    1     |    0\n</code></pre> <p>Apply <code>row_reducer</code> to each row, in this case <code>np.max</code></p> <pre><code>            |   max\ndataset_1   |    1\ndataset_2   |    1\ndataset_3   |   0.5\ndataset_4   |    1\n</code></pre> <p>Apply <code>aggregator</code> to the reduced rows, in this case <code>np.mean</code></p> <pre><code>portfolio_value = np.mean([1, 1, 0.5, 1]) # 0.875\n</code></pre> PARAMETER  DESCRIPTION <code>items</code> <p>A dictionary of items to select from.</p> <p> TYPE: <code>dict[K, Series] | DataFrame</code> </p> <code>k</code> <p>The number of items to select.</p> <p> TYPE: <code>int</code> </p> <code>row_reducer</code> <p>A function to aggregate the rows of the portfolio. This is applied to a potential portfolio, for example to calculate the max score of all configs, for a given dataset (row).</p> <p> TYPE: <code>Callable[[Series], float]</code> DEFAULT: <code>max</code> </p> <code>aggregator</code> <p>A function to take all the single values reduced by <code>row_reducer</code>, and aggregate them into a final value for the portfolio.</p> <p> TYPE: <code>Callable[[Series], float]</code> DEFAULT: <code>mean</code> </p> <code>portfolio_value</code> <p>A custom function to calculate the value of a portfolio. This will take precedence over <code>row_reducer</code> and <code>aggregator</code>.</p> <p> TYPE: <code>Callable[[DataFrame], float] | None</code> DEFAULT: <code>None</code> </p> <code>maximize</code> <p>Whether to maximize or minimize the portfolio value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>scaler</code> <p>A scaler to use to scale the portfolio values. Is applied across the rows.</p> <p> TYPE: <code>TransformerMixin | Literal['minmax'] | None</code> DEFAULT: <code>'minmax'</code> </p> <code>with_replacement</code> <p>Whether to select items with replacement.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>stop_if_worse</code> <p>Whether to stop if the portfolio value is worse than the current best.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>seed</code> <p>The seed to use for breaking ties.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The final portfolio</p> <code>Series</code> <p>The trajectory, where the entry is the value once added to the portfolio.</p> Source code in <code>src/amltk/metalearning/portfolio.py</code> <pre><code>def portfolio_selection(\n    items: dict[K, pd.Series] | pd.DataFrame,\n    k: int,\n    *,\n    row_reducer: Callable[[pd.Series], float] = np.max,\n    aggregator: Callable[[pd.Series], float] = np.mean,\n    portfolio_value: Callable[[pd.DataFrame], float] | None = None,\n    maximize: bool = True,\n    scaler: TransformerMixin | Literal[\"minmax\"] | None = \"minmax\",\n    with_replacement: bool = False,\n    stop_if_worse: bool = False,\n    seed: Seed | None = None,\n) -&gt; tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Selects a portfolio of `k` items from `items`.\n\n    A portfolio is a subset of the items, and is selected by maximizing the\n    `portfolio_value` function in a greedy selection approach.\n\n    At each iteration `0 &lt;= i &lt; k`, the `portfolio_value` function is calculated\n    for the portfolio obtained by adding the `i`th item to the portfolio. The item\n    that maximizes the `portfolio_value` function is then added to the portfolio for\n    the next iteration.\n\n    The `portfolio_function` can often be define by a row wise reduction\n    (`row_reducer=`) followed by some aggregation over these reductions (`aggregator=`).\n    You can also supply your own value function if desired (`portfolio_value=`).\n\n    !!! example \"A Single Iteration\"\n\n        This uses the `row_reducer=np.max` and `aggregator=np.mean` to calculate the\n        value of a portfolio.\n\n        In this case, we have 4 datasets and our current portfolio\n        consists of `config_1` and `config_2`. We are going to calculate the value of\n        adding `config_try` to the current best portfolio.\n\n        ```python\n                    | config_1 | config_2 | config_try\n        dataset_1   |    1     |    0     |    0\n        dataset_2   |    0     |   0.5    |    1\n        dataset_3   |    0     |   0.5    |   0.5\n        dataset_4   |    1     |    1     |    0\n        ```\n\n        Apply `row_reducer` to each row, in this case `np.max`\n\n        ```python\n                    |   max\n        dataset_1   |    1\n        dataset_2   |    1\n        dataset_3   |   0.5\n        dataset_4   |    1\n        ```\n\n        Apply `aggregator` to the reduced rows, in this case `np.mean`\n\n        ```python\n        portfolio_value = np.mean([1, 1, 0.5, 1]) # 0.875\n        ```\n\n    Args:\n        items: A dictionary of items to select from.\n        k: The number of items to select.\n        row_reducer: A function to aggregate the rows of the portfolio.\n            This is applied to a potential portfolio, for example to calculate\n            the max score of all configs, for a given dataset (row).\n        aggregator: A function to take all the single values reduced by `row_reducer`,\n            and aggregate them into a final value for the portfolio.\n        portfolio_value: A custom function to calculate the value of a portfolio.\n            This will take precedence over `row_reducer` and `aggregator`.\n        maximize: Whether to maximize or minimize the portfolio value.\n        scaler: A scaler to use to scale the portfolio values. Is applied across\n            the rows.\n        with_replacement: Whether to select items with replacement.\n        stop_if_worse: Whether to stop if the portfolio value is worse than the\n            current best.\n        seed: The seed to use for breaking ties.\n\n    Returns:\n        The final portfolio\n        The trajectory, where the entry is the value once added to the portfolio.\n    \"\"\"\n    if not (1 &lt;= k &lt; len(items)):\n        raise ValueError(f\"k must be in [1, {len(items)=})\")\n\n    all_portfolio = pd.DataFrame(items)\n\n    # Normalize if needed\n    if scaler is None:\n        pass\n    elif scaler == \"minmax\":\n        min_maxs = all_portfolio.agg([\"min\", \"max\"], axis=1)\n\n        mins = min_maxs[\"min\"]\n        maxs = min_maxs[\"max\"]\n        normalizer = maxs - mins\n\n        # If everything is equal, we need to make sure the normalizing\n        # doesn't do anything\n        normalizer[normalizer == 0] = 1\n        mins[normalizer == 0] = 0\n\n        norm = lambda col: (col - mins) / normalizer\n        all_portfolio: pd.DataFrame = all_portfolio.apply(norm, axis=0)  # type: ignore\n    elif safe_isinstance(scaler, \"TransformerMixin\"):\n        assert not isinstance(scaler, str)\n        all_portfolio = scaler.fit_transform(all_portfolio.T).T\n    else:\n        raise ValueError(f\"Invalid scaler: {scaler}\")\n\n    # Set up the portfolio value function\n    if portfolio_value is None:\n        portfolio_value = lambda _portfolio: float(\n            aggregator(_portfolio.apply(row_reducer, axis=1)),\n        )\n\n    # Make a copy as we will del from it\n    items = dict(items)\n    rng = as_rng(seed)\n    best = max if maximize else min\n\n    # Running counters during the algorithm loop\n    added_items: list[K] = []\n    values: list[float] = []\n    current_best: float = -np.inf if maximize else np.inf\n\n    for _ in range(k):\n        possible_portfolios = [(k, all_portfolio[[*added_items, k]]) for k in items]\n        values_possible = {\n            k: portfolio_value(possible_portfolio)\n            for k, possible_portfolio in possible_portfolios\n        }\n\n        # This is the highest value we can get from a portfolio of the current size\n        best_possible = best(values_possible.values())\n\n        # If the best possible value of what we can do does not improve over the current\n        #    best portfolio, stop (if enabled)\n        if stop_if_worse and current_best == best(best_possible, current_best):\n            break\n\n        current_best = best_possible\n\n        # Possible get multiple best choices, we choose one at random if so\n        best_keys = [k for k, v in values_possible.items() if v == best_possible]\n        best_key = (\n            best_keys[0] if len(best_keys) == 1 else rng.choice(best_keys)  # type: ignore\n        )\n\n        # We found something better, add it in\n        added_items.append(best_key)\n        values.append(best_possible)\n\n        if not with_replacement:\n            del items[best_key]\n\n    # Rename the columns of the portfolio to be the keys\n    return all_portfolio[added_items], pd.Series(values, index=added_items)\n</code></pre>"},{"location":"api/amltk/optimization/history/","title":"History","text":"<p>The <code>History</code> is used to keep a structured record of what occured with <code>Trial</code>s and their associated <code>Report</code>s.</p> Usage <p><pre><code>from amltk.optimization import Trial, History, Metric\nfrom amltk.store import PathBucket\n\nloss = Metric(\"loss\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n    trial.store({\"config.json\": trial.config})\n\n    with trial.begin():\n        loss = x**2 - y\n\n    if trial.exception:\n        return trial.fail()\n\n    return trial.success(loss=loss)\n\n# ... usually obtained from an optimizer\nbucket = PathBucket(\"all-trial-results\")\nhistory = History()\n\nfor x, y in zip([1, 2, 3], [4, 5, 6]):\n    trial = Trial(name=\"some-unique-name\", config={\"x\": x, \"y\": y}, bucket=bucket, metrics=[loss])\n    report = target_function(trial)\n    history.add(report)\n\nprint(history.df())\nbucket.rmdir()  # markdon-exec: hide\n</code></pre> <p>                   status  trial_seed  ... time:kind time:unit name                                   ...                     some-unique-name  success          ...      wall   seconds some-unique-name  success          ...      wall   seconds some-unique-name  success          ...      wall   seconds  [3 rows x 20 columns]  <p>You'll often need to perform some operations on a <code>History</code> so we provide some utility functions here:</p> <ul> <li><code>filter(key=...)</code> - Filters the history by some     predicate, e.g. <code>history.filter(lambda report: report.status == \"success\")</code></li> <li><code>groupby(key=...)</code> - Groups the history by some     key, e.g. <code>history.groupby(lambda report: report.config[\"x\"] &lt; 5)</code></li> <li><code>sortby(key=...)</code> - Sorts the history by some     key, e.g. <code>history.sortby(lambda report: report.time.end)</code></li> </ul> <p>There is also some serialization capabilities built in, to allow you to store your reports and load them back in later:</p> <ul> <li><code>df(...)</code> - Output a <code>pd.DataFrame</code> of all  the information available.</li> <li><code>from_df(...)</code> - Create a <code>History</code> from     a <code>pd.DataFrame</code>.</li> </ul> <p>You can also retrieve individual reports from the history by using their name, e.g. <code>history[\"some-unique-name\"]</code> or iterate through the history with <code>for report in history: ...</code>.</p>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History","title":"<code>class History</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RichRenderable</code></p> <p>A history of trials.</p> <p>This is a collections of reports from trials, where you can access the reports by their trial name. It is unsorted in general, but by using <code>sortby()</code> you can sort the history.</p> History<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [\n    Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric])\n    for i in range(10)\n]\nhistory = History()\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n\nfor report in history:\n    print(f\"{report.name=}, {report}\")\n\nprint(history.metrics)\nprint(history.df())\n</code></pre> <pre><code>report.name='trial_0', Trial.Report(trial=Trial(name='trial_0', config={'x': 0}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 4.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=4.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_1', Trial.Report(trial=Trial(name='trial_1', config={'x': 1}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 3.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=3.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_2', Trial.Report(trial=Trial(name='trial_2', config={'x': 2}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 4.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=4.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_3', Trial.Report(trial=Trial(name='trial_3', config={'x': 3}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 7.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=7.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_4', Trial.Report(trial=Trial(name='trial_4', config={'x': 4}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 12.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=12.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_5', Trial.Report(trial=Trial(name='trial_5', config={'x': 5}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 19.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=19.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_6', Trial.Report(trial=Trial(name='trial_6', config={'x': 6}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 28.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=28.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_7', Trial.Report(trial=Trial(name='trial_7', config={'x': 7}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 39.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=39.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_8', Trial.Report(trial=Trial(name='trial_8', config={'x': 8}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 52.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=52.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\nreport.name='trial_9', Trial.Report(trial=Trial(name='trial_9', config={'x': 9}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 67.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=67.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': Metric(name='cost', minimize=True, bounds=None)}\n          status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                    ...                                   \ntrial_0  success        &lt;NA&gt;        NA  ...      0.000035      wall    seconds\ntrial_1  success        &lt;NA&gt;        NA  ...      0.000024      wall    seconds\ntrial_2  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_3  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_4  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_5  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_6  success        &lt;NA&gt;        NA  ...      0.000019      wall    seconds\ntrial_7  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_8  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_9  success        &lt;NA&gt;        NA  ...      0.000019      wall    seconds\n\n[10 rows x 19 columns]\n</code></pre> ATTRIBUTE DESCRIPTION <code>reports</code> <p>A mapping of trial names to reports.</p> <p> TYPE: <code>list[Report]</code> </p>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.from_reports","title":"<code>def from_reports(reports)</code>   <code>classmethod</code>","text":"<p>Creates a history from reports.</p> PARAMETER  DESCRIPTION <code>reports</code> <p>An iterable of reports.</p> <p> TYPE: <code>Iterable[Report]</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A history.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>@classmethod\ndef from_reports(cls, reports: Iterable[Trial.Report]) -&gt; History:\n    \"\"\"Creates a history from reports.\n\n    Args:\n        reports: An iterable of reports.\n\n    Returns:\n        A history.\n    \"\"\"\n    history = cls()\n    history.add(reports)\n    return history\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.add","title":"<code>def add(report)</code>","text":"<p>Adds a report or reports to the history.</p> PARAMETER  DESCRIPTION <code>report</code> <p>A report or reports to add.</p> <p> TYPE: <code>Report | Iterable[Report]</code> </p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def add(self, report: Trial.Report | Iterable[Trial.Report]) -&gt; None:\n    \"\"\"Adds a report or reports to the history.\n\n    Args:\n        report: A report or reports to add.\n    \"\"\"\n    match report:\n        case Trial.Report():\n            for m in report.metric_values:\n                if (_m := self.metrics.get(m.name)) is not None:\n                    if m.metric != _m:\n                        raise ValueError(\n                            f\"Metric {m.name} has conflicting definitions:\"\n                            f\"\\n{m.metric} != {_m}\",\n                        )\n                else:\n                    self.metrics[m.name] = m.metric\n\n            self.reports.append(report)\n            self._lookup[report.name] = len(self.reports) - 1\n        case reports:\n            for _report in reports:\n                self.add(_report)\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.find","title":"<code>def find(name)</code>","text":"<p>Finds a report by trial name.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the trial.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Report</code> <p>The report.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def find(self, name: str) -&gt; Trial.Report:\n    \"\"\"Finds a report by trial name.\n\n    Args:\n        name: The name of the trial.\n\n    Returns:\n        The report.\n    \"\"\"\n    return self.reports[self._lookup[name]]\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.df","title":"<code>def df(*, profiles=True, configs=True, summary=True, metrics=True, normalize_time=True)</code>","text":"<p>Returns a pandas DataFrame of the history.</p> <p>Each individual trial will be a row in the dataframe.</p> <p>Prefixes</p> <ul> <li><code>summary</code>: Entries will be prefixed with <code>\"summary:\"</code></li> <li><code>config</code>: Entries will be prefixed with <code>\"config:\"</code></li> <li><code>metrics</code>: Entries will be prefixed with <code>\"metrics:\"</code></li> </ul> df<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n\nprint(history.df())\n</code></pre> <pre><code>          status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                    ...                                   \ntrial_0  success        &lt;NA&gt;        NA  ...      0.000028      wall    seconds\ntrial_1  success        &lt;NA&gt;        NA  ...      0.000023      wall    seconds\ntrial_2  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_3  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_4  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_5  success        &lt;NA&gt;        NA  ...      0.000019      wall    seconds\ntrial_6  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_7  success        &lt;NA&gt;        NA  ...      0.000018      wall    seconds\ntrial_8  success        &lt;NA&gt;        NA  ...      0.000019      wall    seconds\ntrial_9  success        &lt;NA&gt;        NA  ...      0.000019      wall    seconds\n\n[10 rows x 19 columns]\n</code></pre> PARAMETER  DESCRIPTION <code>profiles</code> <p>Whether to include the profiles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>configs</code> <p>Whether to include the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>summary</code> <p>Whether to include the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>metrics</code> <p>Whether to include the metrics.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>normalize_time</code> <p>Whether to normalize the time to the first report. If given a <code>float</code>, it will normalize to that value.</p> <p>Will normalize all columns with <code>\"time:end\"</code>. and <code>\"time:start\"</code> in their name. It will use the time of the earliest report as the offset.</p> <p> TYPE: <code>bool | float</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas DataFrame of the history.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def df(\n    self,\n    *,\n    profiles: bool = True,\n    configs: bool = True,\n    summary: bool = True,\n    metrics: bool = True,\n    normalize_time: bool | float = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Returns a pandas DataFrame of the history.\n\n    Each individual trial will be a row in the dataframe.\n\n    !!! note \"Prefixes\"\n\n        * `summary`: Entries will be prefixed with `#!python \"summary:\"`\n        * `config`: Entries will be prefixed with `#!python \"config:\"`\n        * `metrics`: Entries will be prefixed with `#!python \"metrics:\"`\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"df\" hl_lines=\"12\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        with trial.begin():\n            x = trial.config[\"x\"]\n            report = trial.success(cost=x**2 - x*2 + 4)\n            history.add(report)\n\n    print(history.df())\n    ```\n\n    Args:\n        profiles: Whether to include the profiles.\n        configs: Whether to include the configs.\n        summary: Whether to include the summary.\n        metrics: Whether to include the metrics.\n        normalize_time: Whether to normalize the time to the first\n            report. If given a `#!python float`, it will normalize\n            to that value.\n\n            Will normalize all columns with `#!python \"time:end\"`.\n            and `#!python \"time:start\"` in their name. It will use\n            the time of the earliest report as the offset.\n\n    Returns:\n        A pandas DataFrame of the history.\n    \"\"\"  # noqa: E501\n    if len(self) == 0:\n        return pd.DataFrame()\n\n    _df = pd.concat(\n        [\n            report.df(\n                profiles=profiles,\n                configs=configs,\n                summary=summary,\n                metrics=metrics,\n            )\n            for report in self.reports\n        ],\n    )\n    _df = _df.convert_dtypes()\n\n    match normalize_time:\n        case True if \"time:start\" in _df.columns:\n            time_columns = (\"time:start\", \"time:end\")\n            cols = [c for c in _df.columns if c.endswith(time_columns)]\n            _df[cols] -= _df[\"time:start\"].min()\n        case float():\n            time_columns = (\"time:start\", \"time:end\")\n            cols = [c for c in _df.columns if c.endswith(time_columns)]\n            _df[cols] -= normalize_time\n        case _:\n            pass\n\n    return _df\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.filter","title":"<code>def filter(key)</code>","text":"<p>Filters the history by a predicate.</p> filter<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n\nfiltered_history = history.filter(lambda report: report.metrics[\"cost\"] &lt; 10)\nfor report in filtered_history:\n    cost = report.metrics[\"cost\"]\n    print(f\"{report.name}, {cost=}, {report}\")\n</code></pre> <pre><code>trial_0, cost=4.0, Trial.Report(trial=Trial(name='trial_0', config={'x': 0}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 4.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=4.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\ntrial_1, cost=3.0, Trial.Report(trial=Trial(name='trial_1', config={'x': 1}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 3.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=3.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\ntrial_2, cost=4.0, Trial.Report(trial=Trial(name='trial_2', config={'x': 2}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 4.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=4.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\ntrial_3, cost=7.0, Trial.Report(trial=Trial(name='trial_3', config={'x': 3}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 7.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=7.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>A predicate to filter by.</p> <p> TYPE: <code>Callable[[Report], bool]</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A new history with the filtered reports.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def filter(self, key: Callable[[Trial.Report], bool]) -&gt; History:\n    \"\"\"Filters the history by a predicate.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"filter\" hl_lines=\"12\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        with trial.begin():\n            x = trial.config[\"x\"]\n            report = trial.success(cost=x**2 - x*2 + 4)\n            history.add(report)\n\n    filtered_history = history.filter(lambda report: report.metrics[\"cost\"] &lt; 10)\n    for report in filtered_history:\n        cost = report.metrics[\"cost\"]\n        print(f\"{report.name}, {cost=}, {report}\")\n    ```\n\n    Args:\n        key: A predicate to filter by.\n\n    Returns:\n        A new history with the filtered reports.\n    \"\"\"  # noqa: E501\n    return History.from_reports([report for report in self.reports if key(report)])\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.groupby","title":"<code>def groupby(key)</code>","text":"<p>Groups the history by the values of a key.</p> groupby<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        if x % 2 == 0:\n            report = trial.fail(cost=1_000)\n        else:\n            report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n\nfor status, history in history.groupby(\"status\").items():\n    print(f\"{status=}, {len(history)=}\")\n</code></pre> <pre><code>status=&lt;Status.FAIL: 'fail'&gt;, len(history)=5\nstatus=&lt;Status.SUCCESS: 'success'&gt;, len(history)=5\n</code></pre> <p>You can pass a <code>Callable</code> to group by any key you like:</p> <pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        report = trial.fail(cost=x)\n        history.add(report)\n\nfor below_5, history in history.groupby(lambda r: r.metrics[\"cost\"] &lt; 5).items():\n    print(f\"{below_5=}, {len(history)=}\")\n</code></pre> <pre><code>below_5=True, len(history)=5\nbelow_5=False, len(history)=5\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>A key to group by. If <code>\"status\"</code> is passed, the history will be grouped by the status of the reports.</p> <p> TYPE: <code>Literal['status'] | Callable[[Report], Hashable]</code> </p> RETURNS DESCRIPTION <code>dict[Hashable, History]</code> <p>A mapping of keys to histories.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def groupby(\n    self,\n    key: Literal[\"status\"] | Callable[[Trial.Report], Hashable],\n) -&gt; dict[Hashable, History]:\n    \"\"\"Groups the history by the values of a key.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"groupby\" hl_lines=\"15\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        with trial.begin():\n            x = trial.config[\"x\"]\n            if x % 2 == 0:\n                report = trial.fail(cost=1_000)\n            else:\n                report = trial.success(cost=x**2 - x*2 + 4)\n            history.add(report)\n\n    for status, history in history.groupby(\"status\").items():\n        print(f\"{status=}, {len(history)=}\")\n    ```\n\n    You can pass a `#!python Callable` to group by any key you like:\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        with trial.begin():\n            x = trial.config[\"x\"]\n            report = trial.fail(cost=x)\n            history.add(report)\n\n    for below_5, history in history.groupby(lambda r: r.metrics[\"cost\"] &lt; 5).items():\n        print(f\"{below_5=}, {len(history)=}\")\n    ```\n\n    Args:\n        key: A key to group by. If `\"status\"` is passed, the history will be\n            grouped by the status of the reports.\n\n    Returns:\n        A mapping of keys to histories.\n    \"\"\"  # noqa: E501\n    d = defaultdict(list)\n\n    if key == \"status\":\n        key = operator.attrgetter(\"status\")\n\n    for report in self.reports:\n        d[key(report)].append(report)\n\n    return {k: History.from_reports(v) for k, v in d.items()}\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.incumbents","title":"<code>def incumbents(key, *, sortby=lambda : report.time.end, reverse=None, ffill=False)</code>","text":"<p>Returns a trace of the incumbents, where only the report that is better than the previous best report is kept.</p> incumbents<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n\nincumbents = (\n    history\n    .incumbents(\"cost\", sortby=lambda r: r.time.end)\n)\nfor report in incumbents:\n    print(f\"{report.metrics=}, {report.config=}\")\n</code></pre> <pre><code>report.metrics={'cost': 4.0}, report.config={'x': 0}\nreport.metrics={'cost': 3.0}, report.config={'x': 1}\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>The key to use. If given a str, it will use that as the key to use in the metrics, defining if one report is better than another. If given a <code>Callable</code>, it should return a <code>bool</code>, indicating if the first argument report is better than the second argument report.</p> <p> TYPE: <code>Callable[[Report, Report], bool] | str</code> </p> <code>sortby</code> <p>The key to sort by. If given a str, it will sort by the value of that key in the <code>.metrics</code> and also filter out anything that does not contain this key. By default, it will sort by the end time of the report.</p> <p> TYPE: <code>Callable[[Report], Comparable] | str</code> DEFAULT: <code>lambda : end</code> </p> <code>reverse</code> <p>Whether to sort in some given order. By default (<code>None</code>), if given a metric key, the reports with the best metric values will be sorted first. If given a <code>Callable</code>, the reports with the smallest values will be sorted first. Using <code>reverse=True</code> will always reverse this order, while <code>reverse=False</code> will always preserve it.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>ffill</code> <p>Whether to forward fill the incumbents. This means that if a report is not an incumbent, it will be replaced with the current best. This is useful if you want to visualize the incumbents over some x axis, where the you have a point at every place along the axis.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list[Report]</code> <p>The history of incumbents.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def incumbents(\n    self,\n    key: Callable[[Trial.Report, Trial.Report], bool] | str,\n    *,\n    sortby: Callable[[Trial.Report], Comparable]\n    | str = lambda report: report.time.end,\n    reverse: bool | None = None,\n    ffill: bool = False,\n) -&gt; list[Trial.Report]:\n    \"\"\"Returns a trace of the incumbents, where only the report that is better than the previous\n    best report is kept.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"incumbents\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        with trial.begin():\n            x = trial.config[\"x\"]\n            report = trial.success(cost=x**2 - x*2 + 4)\n            history.add(report)\n\n    incumbents = (\n        history\n        .incumbents(\"cost\", sortby=lambda r: r.time.end)\n    )\n    for report in incumbents:\n        print(f\"{report.metrics=}, {report.config=}\")\n    ```\n\n    Args:\n        key: The key to use. If given a str, it will use that as the\n            key to use in the metrics, defining if one report is better\n            than another. If given a `#!python Callable`, it should\n            return a `bool`, indicating if the first argument report\n            is better than the second argument report.\n        sortby: The key to sort by. If given a str, it will sort by\n            the value of that key in the `.metrics` and also filter\n            out anything that does not contain this key.\n            By default, it will sort by the end time of the report.\n        reverse: Whether to sort in some given order. By\n            default (`None`), if given a metric key, the reports with\n            the best metric values will be sorted first. If\n            given a `#!python Callable`, the reports with the\n            smallest values will be sorted first. Using\n            `reverse=True` will always reverse this order, while\n            `reverse=False` will always preserve it.\n        ffill: Whether to forward fill the incumbents. This means that\n            if a report is not an incumbent, it will be replaced with\n            the current best. This is useful if you want to\n            visualize the incumbents over some x axis, where the\n            you have a point at every place along the axis.\n\n    Returns:\n        The history of incumbents.\n    \"\"\"  # noqa: E501\n    match key:\n        case str():\n            metric = self.metrics[key]\n            __op = operator.lt if metric.minimize else operator.gt  # type: ignore\n            op = lambda r1, r2: __op(r1.metrics[key], r2.metrics[key])\n        case _:\n            op = key\n\n    sorted_reports = self.sortby(sortby, reverse=reverse)\n    return list(compare_accumulate(sorted_reports, op=op, ffill=ffill))\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.sortby","title":"<code>def sortby(key, *, reverse=None)</code>","text":"<p>Sorts the history by a key and returns a sorted History.</p> sortby<pre><code>from amltk.optimization import Trial, History, Metric\n\nmetric = Metric(\"cost\", minimize=True)\ntrials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\nhistory = History()\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        report = trial.success(cost=x**2 - x*2 + 4)\n        history.add(report)\n\ntrace = (\n    history\n    .filter(lambda report: report.status == \"success\")\n    .sortby(\"cost\")\n)\n\nfor report in trace:\n    print(f\"{report.metrics}, {report}\")\n</code></pre> <pre><code>{'cost': 3.0}, Trial.Report(trial=Trial(name='trial_1', config={'x': 1}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 3.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=3.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 4.0}, Trial.Report(trial=Trial(name='trial_0', config={'x': 0}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 4.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=4.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 4.0}, Trial.Report(trial=Trial(name='trial_2', config={'x': 2}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 4.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=4.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 7.0}, Trial.Report(trial=Trial(name='trial_3', config={'x': 3}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 7.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=7.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 12.0}, Trial.Report(trial=Trial(name='trial_4', config={'x': 4}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 12.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=12.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 19.0}, Trial.Report(trial=Trial(name='trial_5', config={'x': 5}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 19.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=19.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 28.0}, Trial.Report(trial=Trial(name='trial_6', config={'x': 6}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 28.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=28.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 39.0}, Trial.Report(trial=Trial(name='trial_7', config={'x': 7}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 39.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=39.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 52.0}, Trial.Report(trial=Trial(name='trial_8', config={'x': 8}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 52.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=52.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n{'cost': 67.0}, Trial.Report(trial=Trial(name='trial_9', config={'x': 9}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='cost', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'cost': 67.0}, metric_values=(Metric.Value(metric=Metric(name='cost', minimize=True, bounds=None), value=67.0),), metric_defs={'cost': Metric(name='cost', minimize=True, bounds=None)}, metric_names=('cost',))\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>The key to sort by. If given a str, it will sort by the value of that key in the <code>.metrics</code> and also filter out anything that does not contain this key.</p> <p> TYPE: <code>Callable[[Report], Comparable] | str</code> </p> <code>reverse</code> <p>Whether to sort in some given order. By default (<code>None</code>), if given a metric key, the reports with the best metric values will be sorted first. If given a <code>Callable</code>, the reports with the smallest values will be sorted first. Using <code>reverse=True</code> will always reverse this order, while <code>reverse=False</code> will always preserve it.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Report]</code> <p>A sorted list of reports</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>def sortby(\n    self,\n    key: Callable[[Trial.Report], Comparable] | str,\n    *,\n    reverse: bool | None = None,\n) -&gt; list[Trial.Report]:\n    \"\"\"Sorts the history by a key and returns a sorted History.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"sortby\" hl_lines=\"15\"\n    from amltk.optimization import Trial, History, Metric\n\n    metric = Metric(\"cost\", minimize=True)\n    trials = [Trial(name=f\"trial_{i}\", config={\"x\": i}, metrics=[metric]) for i in range(10)]\n    history = History()\n\n    for trial in trials:\n        with trial.begin():\n            x = trial.config[\"x\"]\n            report = trial.success(cost=x**2 - x*2 + 4)\n            history.add(report)\n\n    trace = (\n        history\n        .filter(lambda report: report.status == \"success\")\n        .sortby(\"cost\")\n    )\n\n    for report in trace:\n        print(f\"{report.metrics}, {report}\")\n    ```\n\n    Args:\n        key: The key to sort by. If given a str, it will sort by\n            the value of that key in the `.metrics` and also filter\n            out anything that does not contain this key.\n        reverse: Whether to sort in some given order. By\n            default (`None`), if given a metric key, the reports with\n            the best metric values will be sorted first. If\n            given a `#!python Callable`, the reports with the\n            smallest values will be sorted first. Using\n            `reverse=True` will always reverse this order, while\n            `reverse=False` will always preserve it.\n\n    Returns:\n        A sorted list of reports\n    \"\"\"  # noqa: E501\n    # If given a str, filter out anything that doesn't have that key\n    if isinstance(key, str):\n        history = self.filter(lambda report: key in report.metric_names)\n        sort_key: Callable[[Trial.Report], Comparable] = lambda r: r.metrics[key]\n        reverse = (\n            reverse if reverse is not None else (not self.metrics[key].minimize)\n        )\n    else:\n        history = self\n        sort_key = key\n        reverse = False if reverse is None else reverse\n\n    return sorted(history.reports, key=sort_key, reverse=reverse)\n</code></pre>"},{"location":"api/amltk/optimization/history/#amltk.optimization.history.History.from_df","title":"<code>def from_df(df)</code>   <code>classmethod</code>","text":"<p>Loads a history from a pandas DataFrame.</p> PARAMETER  DESCRIPTION <code>df</code> <p>The DataFrame to load the history from.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>History</code> <p>A History.</p> Source code in <code>src/amltk/optimization/history.py</code> <pre><code>@classmethod\ndef from_df(cls, df: pd.DataFrame) -&gt; History:\n    \"\"\"Loads a history from a pandas DataFrame.\n\n    Args:\n        df: The DataFrame to load the history from.\n\n    Returns:\n        A History.\n    \"\"\"\n    if len(df) == 0:\n        return cls()\n    return History.from_reports(Trial.Report.from_df(s) for _, s in df.iterrows())\n</code></pre>"},{"location":"api/amltk/optimization/metric/","title":"Metric","text":"<p>A <code>Metric</code> to let optimizers know how to handle numeric values properly.</p> <p>A <code>Metric</code> is defined by a <code>.name: str</code> and whether it is better to <code>.minimize: bool</code> the metric. Further, you can specify <code>.bounds: tuple[lower, upper]</code> which can help optimizers and other code know how to treat metrics.</p> <p>To easily convert between <code>loss</code>, <code>score</code> of a a value in a <code>Metric.Value</code> object.</p> <p>If the metric is bounded, you can also get the <code>distance_to_optimal</code> which is the distance to the optimal value.</p> <pre><code>from amltk.optimization import Metric\n\nacc = Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0))\n\nacc_value = acc.as_value(0.9)\nprint(f\"Cost: {acc_value.distance_to_optimal}\")  # Distance to optimal.\nprint(f\"Loss: {acc_value.loss}\")  # Something that can be minimized\nprint(f\"Score: {acc_value.score}\")  # Something that can be maximized\n</code></pre> <pre><code>Cost: 0.09999999999999998\nLoss: -0.9\nScore: 0.9\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric","title":"<code>class Metric</code>   <code>dataclass</code>","text":"<p>A metric with a given name, optimal direction, and possible bounds.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.name","title":"<code>name: str</code>   <code>attr</code>","text":"<p>The name of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.minimize","title":"<code>minimize: bool</code>   <code>classvar</code> <code>attr</code>","text":"<p>Whether to minimize or maximize the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.bounds","title":"<code>bounds: tuple[float, float] | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The bounds of the metric, if any.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.worst","title":"<code>worst: Metric.Value</code>   <code>prop</code>","text":"<p>The worst possible value of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.optimal","title":"<code>optimal: Metric.Value</code>   <code>prop</code>","text":"<p>The optimal value of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value","title":"<code>class Value</code>   <code>dataclass</code>","text":"<p>A recorded value of an metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.metric","title":"<code>metric: Metric</code>   <code>classvar</code> <code>attr</code>","text":"<p>The metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.value","title":"<code>value: float</code>   <code>classvar</code> <code>attr</code>","text":"<p>The value of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.minimize","title":"<code>minimize: bool</code>   <code>prop</code>","text":"<p>Whether to minimize or maximize the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.bounds","title":"<code>bounds: tuple[float, float] | None</code>   <code>prop</code>","text":"<p>Whether to minimize or maximize the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.name","title":"<code>name: str</code>   <code>prop</code>","text":"<p>The name of the metric.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.loss","title":"<code>loss: float</code>   <code>prop</code>","text":"<p>Convert a value to a loss.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.score","title":"<code>score: float</code>   <code>prop</code>","text":"<p>Convert a value to a score.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.distance_to_optimal","title":"<code>distance_to_optimal: float | None</code>   <code>prop</code>","text":"<p>The distance to the optimal value, using the bounds if possible.</p>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.__float__","title":"<code>def __float__()</code>","text":"<p>Convert a value to a float.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def __float__(self) -&gt; float:\n    \"\"\"Convert a value to a float.\"\"\"\n    return float(self.value)\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.Value.__eq__","title":"<code>def __eq__(__value)</code>","text":"<p>Check if two values are equal.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>@override\ndef __eq__(self, __value: object) -&gt; bool:\n    \"\"\"Check if two values are equal.\"\"\"\n    if isinstance(__value, Metric.Value):\n        return self.value == __value.value\n    if isinstance(__value, float | int):\n        return self.value == float(__value)\n    return NotImplemented\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.from_str","title":"<code>def from_str(s)</code>   <code>classmethod</code>","text":"<p>Create an metric from a str.</p> <pre><code>from amltk.optimization import Metric\n\ns = \"loss (minimize)\"\nmetric = Metric.from_str(s)\nprint(metric)\n\ns = \"accuracy [0.0, 1.0] (maximize)\"\nmetric = Metric.from_str(s)\nprint(metric)\n</code></pre> <pre><code>loss (minimize)\naccuracy [0.0, 1.0] (maximize)\n</code></pre> PARAMETER  DESCRIPTION <code>s</code> <p>The string to parse.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The parsed metric.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>@classmethod\ndef from_str(cls, s: str) -&gt; Self:\n    \"\"\"Create an metric from a str.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\"\n    from amltk.optimization import Metric\n\n    s = \"loss (minimize)\"\n    metric = Metric.from_str(s)\n    print(metric)\n\n    s = \"accuracy [0.0, 1.0] (maximize)\"\n    metric = Metric.from_str(s)\n    print(metric)\n    ```\n\n    Args:\n        s: The string to parse.\n\n    Returns:\n        The parsed metric.\n    \"\"\"\n    splits = s.split(\" \")\n    # No bounds\n    if len(splits) == 2:  # noqa: PLR2004\n        name, minimize_str = splits\n        bounds = None\n    else:\n        name, lower_str, upper_str, minimize_str = splits\n        bounds = (float(lower_str[1:-1]), float(upper_str[:-1]))\n\n    minimize = minimize_str == \"(minimize)\"\n    return cls(name=name, minimize=minimize, bounds=bounds)\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.as_value","title":"<code>def as_value(value)</code>","text":"<p>Convert a value to an metric value.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def as_value(self, value: float | int) -&gt; Metric.Value:\n    \"\"\"Convert a value to an metric value.\"\"\"\n    return Metric.Value(metric=self, value=float(value))\n</code></pre>"},{"location":"api/amltk/optimization/metric/#amltk.optimization.metric.Metric.__call__","title":"<code>def __call__(value)</code>","text":"<p>Convert a value to an metric value.</p> Source code in <code>src/amltk/optimization/metric.py</code> <pre><code>def __call__(self, value: float | int) -&gt; Metric.Value:\n    \"\"\"Convert a value to an metric value.\"\"\"\n    return Metric.Value(metric=self, value=float(value))\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/","title":"Optimizer","text":"<p>The base <code>Optimizer</code> class, defines the API we require optimizers to implement.</p> <ul> <li><code>ask()</code> - Ask the optimizer for a     new <code>Trial</code> to evaluate.</li> <li><code>tell()</code> - Tell the optimizer     the result of the sampled config. This comes in the form of a     <code>Trial.Report</code>.</li> </ul> <p>Additionally, to aid users from switching between optimizers, the <code>preferred_parser()</code> method should return either a <code>parser</code> function or a string that can be used with <code>node.search_space(parser=..._)</code> to extract the search space for the optimizer.</p>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer","title":"<code>class Optimizer(metrics, bucket=None)</code>","text":"<p>         Bases: <code>Generic[I]</code></p> <p>An optimizer protocol.</p> <p>An optimizer is an object that can be asked for a trail using <code>ask</code> and a <code>tell</code> to inform the optimizer of the report from that trial.</p> PARAMETER  DESCRIPTION <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket to store results of individual trials from this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>def __init__(\n    self,\n    metrics: Sequence[Metric],\n    bucket: PathBucket | None = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        metrics: The metrics to optimize.\n        bucket: The bucket to store results of individual trials from this\n            optimizer.\n    \"\"\"\n    super().__init__()\n    self.metrics = metrics\n    self.bucket = (\n        bucket\n        if bucket is not None\n        else PathBucket(f\"{self.__class__.__name__}-{datetime.now().isoformat()}\")\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.metrics","title":"<code>metrics: Sequence[Metric]</code>   <code>attr</code>","text":"<p>The metrics to optimize.</p>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.bucket","title":"<code>bucket: PathBucket</code>   <code>attr</code>","text":"<p>The bucket to give to trials generated by this optimizer.</p>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.tell","title":"<code>def tell(report)</code>   <code>abstractmethod</code>","text":"<p>Tell the optimizer the report for an asked trial.</p> PARAMETER  DESCRIPTION <code>report</code> <p>The report for a trial</p> <p> TYPE: <code>Report[I]</code> </p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>@abstractmethod\ndef tell(self, report: Trial.Report[I]) -&gt; None:\n    \"\"\"Tell the optimizer the report for an asked trial.\n\n    Args:\n        report: The report for a trial\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.ask","title":"<code>def ask()</code>   <code>abstractmethod</code>","text":"<p>Ask the optimizer for a trial to evaluate.</p> RETURNS DESCRIPTION <code>Trial[I]</code> <p>A config to sample.</p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>@abstractmethod\ndef ask(self) -&gt; Trial[I]:\n    \"\"\"Ask the optimizer for a trial to evaluate.\n\n    Returns:\n        A config to sample.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizer/#amltk.optimization.optimizer.Optimizer.preferred_parser","title":"<code>def preferred_parser()</code>   <code>classmethod</code>","text":"<p>The preferred parser for this optimizer.</p> <p>Note</p> <p>Subclasses should override this as required.</p> Source code in <code>src/amltk/optimization/optimizer.py</code> <pre><code>@classmethod\ndef preferred_parser(\n    cls,\n) -&gt; str | Callable[Concatenate[Node, ...], Any] | Callable[[Node], Any] | None:\n    \"\"\"The preferred parser for this optimizer.\n\n    !!! note\n\n        Subclasses should override this as required.\n\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/amltk/optimization/trial/","title":"Trial","text":"<p>A <code>Trial</code> is typically the output of <code>Optimizer.ask()</code>, indicating what the optimizer would like to evaluate next. We provide a host of convenience methods attached to the <code>Trial</code> to make it easy to save results, store artifacts, and more.</p> <p>Paired with the <code>Trial</code> is the <code>Trial.Report</code>, class, providing an easy way to report back to the optimizer's <code>tell()</code> with a simple <code>trial.success(cost=...)</code> or <code>trial.fail(cost=...)</code> call..</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial--trial","title":"Trial","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I]</code></p> <p>A <code>Trial</code> encapsulates some configuration that needs to be evaluated. Typically this is what is generated by an <code>Optimizer.ask()</code> call.</p> Usage <p>To begin a trial, you can use the <code>trial.begin()</code>, which will catch exceptions/traceback and profile the block of code.</p> <p>If all went smooth, your trial was successful and you can use <code>trial.success()</code> to generate a success <code>Report</code>, typically passing what your chosen optimizer expects, e.g. <code>\"loss\"</code> or <code>\"cost\"</code>.</p> <p>If your trial failed, you can instead use the <code>trial.fail()</code> to generate a failure <code>Report</code>, where any caught exception will be attached to it. Each <code>Optimizer</code> will take care of what to do from here.</p> <p><pre><code>from amltk.optimization import Trial, Metric\nfrom amltk.store import PathBucket\n\ncost = Metric(\"cost\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n\n    with trial.begin():\n        cost = x**2 - y\n\n    if trial.exception:\n        return trial.fail()\n\n    return trial.success(cost=cost)\n\n# ... usually obtained from an optimizer\ntrial = Trial(name=\"some-unique-name\", config={\"x\": 1, \"y\": 2}, metrics=[cost])\n\nreport = target_function(trial)\nprint(report.df())\n</code></pre> <p>                   status  trial_seed  ... time:kind time:unit name                                   ...                     some-unique-name  success          ...      wall   seconds  [1 rows x 20 columns]  <p>What you can return with <code>trial.success()</code> or <code>trial.fail()</code> depends on the <code>metrics</code> of the trial. Typically an optimizer will provide the trial with the list of metrics.</p> Metrics <p>A metric with a given name, optimal direction, and possible bounds.</p> <p>Some important properties is that they have a unique <code>.name</code> given the optimization run, a candidate <code>.config</code>' to evaluate, a possible <code>.seed</code> to use, and an <code>.info</code> object which is the optimizer specific information, if required by you.</p> <p>If using <code>Plugins</code>, they may insert some extra objects in the <code>.extra</code> dict.</p> <p>To profile your trial, you can wrap the logic you'd like to check with <code>trial.begin()</code>, which will automatically catch any errors, record the traceback, and profile the block of code, in terms of time and memory.</p> <p>You can access the profiled time and memory using the <code>.time</code> and <code>.memory</code> attributes. If you've <code>profile()</code>'ed any other intervals, you can access them by name through <code>trial.profiles</code>. Please see the <code>Profiler</code> for more.</p> Profiling with a trial. <p>profile<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"some-unique-name\", config={})\n\n# ... somewhere where you've begun your trial.\nwith trial.profile(\"some_interval\"):\n    for work in range(100):\n        pass\n\nprint(trial.profiler.df())\n</code></pre> <pre><code>               memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nsome_interval      1.561518e+09      1561518080  ...       wall    seconds\n\n[1 rows x 12 columns]\n</code></pre> </p> <p>You can also record anything you'd like into the <code>.summary</code>, a plain <code>dict</code> or use <code>trial.store()</code> to store artifacts related to the trial.</p> What to put in <code>.summary</code>? <p>For large items, e.g. predictions or models, these are highly advised to <code>.store()</code> to disk, especially if using a <code>Task</code> for multiprocessing.</p> <p>Further, if serializing the report using the <code>report.df()</code>, returning a single row, or a <code>History</code> with <code>history.df()</code> for a dataframe consisting of many of the reports, then you'd likely only want to store things that are scalar and can be serialised to disk by a pandas DataFrame.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial--report","title":"Report","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I2]</code></p> <p>The <code>Trial.Report</code> encapsulates a <code>Trial</code>, its status and any metrics/exceptions that may have occured.</p> <p>Typically you will not create these yourself, but instead use <code>trial.success()</code> or <code>trial.fail()</code> to generate them.</p> <pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True)\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\nwith trial.begin():\n    # Do some work\n    # ...\n    report: Trial.Report = trial.success(loss=1)\n\nprint(report.df())\n</code></pre> <pre><code>        status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                  ...                                   \ntrial  success        &lt;NA&gt;        NA  ...      0.000022      wall    seconds\n\n[1 rows x 19 columns]\n</code></pre> <p>These reports are used to report back metrics to an <code>Optimizer</code> with <code>Optimizer.tell()</code> but can also be stored for your own uses.</p> <p>You can access the original trial with the <code>.trial</code> attribute, and the <code>Status</code> of the trial with the <code>.status</code> attribute.</p> <p>You may also want to check out the <code>History</code> class for storing a collection of <code>Report</code>s, allowing for an easier time to convert them to a dataframe or perform some common Hyperparameter optimization parsing of metrics.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial","title":"<code>class Trial</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I]</code></p> <p>A <code>Trial</code> encapsulates some configuration that needs to be evaluated. Typically this is what is generated by an <code>Optimizer.ask()</code> call.</p> Usage <p>To begin a trial, you can use the <code>trial.begin()</code>, which will catch exceptions/traceback and profile the block of code.</p> <p>If all went smooth, your trial was successful and you can use <code>trial.success()</code> to generate a success <code>Report</code>, typically passing what your chosen optimizer expects, e.g. <code>\"loss\"</code> or <code>\"cost\"</code>.</p> <p>If your trial failed, you can instead use the <code>trial.fail()</code> to generate a failure <code>Report</code>, where any caught exception will be attached to it. Each <code>Optimizer</code> will take care of what to do from here.</p> <p><pre><code>from amltk.optimization import Trial, Metric\nfrom amltk.store import PathBucket\n\ncost = Metric(\"cost\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n\n    with trial.begin():\n        cost = x**2 - y\n\n    if trial.exception:\n        return trial.fail()\n\n    return trial.success(cost=cost)\n\n# ... usually obtained from an optimizer\ntrial = Trial(name=\"some-unique-name\", config={\"x\": 1, \"y\": 2}, metrics=[cost])\n\nreport = target_function(trial)\nprint(report.df())\n</code></pre> <p>                   status  trial_seed  ... time:kind time:unit name                                   ...                     some-unique-name  success          ...      wall   seconds  [1 rows x 20 columns]  <p>What you can return with <code>trial.success()</code> or <code>trial.fail()</code> depends on the <code>metrics</code> of the trial. Typically an optimizer will provide the trial with the list of metrics.</p> Metrics <p>A metric with a given name, optimal direction, and possible bounds.</p> <p>Some important properties is that they have a unique <code>.name</code> given the optimization run, a candidate <code>.config</code>' to evaluate, a possible <code>.seed</code> to use, and an <code>.info</code> object which is the optimizer specific information, if required by you.</p> <p>If using <code>Plugins</code>, they may insert some extra objects in the <code>.extra</code> dict.</p> <p>To profile your trial, you can wrap the logic you'd like to check with <code>trial.begin()</code>, which will automatically catch any errors, record the traceback, and profile the block of code, in terms of time and memory.</p> <p>You can access the profiled time and memory using the <code>.time</code> and <code>.memory</code> attributes. If you've <code>profile()</code>'ed any other intervals, you can access them by name through <code>trial.profiles</code>. Please see the <code>Profiler</code> for more.</p> Profiling with a trial. <p>profile<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"some-unique-name\", config={})\n\n# ... somewhere where you've begun your trial.\nwith trial.profile(\"some_interval\"):\n    for work in range(100):\n        pass\n\nprint(trial.profiler.df())\n</code></pre> <pre><code>               memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nsome_interval      1.561518e+09      1561518080  ...       wall    seconds\n\n[1 rows x 12 columns]\n</code></pre> </p> <p>You can also record anything you'd like into the <code>.summary</code>, a plain <code>dict</code> or use <code>trial.store()</code> to store artifacts related to the trial.</p> What to put in <code>.summary</code>? <p>For large items, e.g. predictions or models, these are highly advised to <code>.store()</code> to disk, especially if using a <code>Task</code> for multiprocessing.</p> <p>Further, if serializing the report using the <code>report.df()</code>, returning a single row, or a <code>History</code> with <code>history.df()</code> for a dataframe consisting of many of the reports, then you'd likely only want to store things that are scalar and can be serialised to disk by a pandas DataFrame.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.name","title":"<code>name: str</code>   <code>attr</code>","text":"<p>The unique name of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.config","title":"<code>config: Mapping[str, Any]</code>   <code>attr</code>","text":"<p>The config of the trial provided by the optimizer.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.bucket","title":"<code>bucket: PathBucket</code>   <code>classvar</code> <code>attr</code>","text":"<p>The bucket to store trial related output to.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.info","title":"<code>info: I | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The info of the trial provided by the optimizer.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.metrics","title":"<code>metrics: Sequence[Metric]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The metrics associated with the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.seed","title":"<code>seed: int | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The seed to use if suggested by the optimizer.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.fidelities","title":"<code>fidelities: dict[str, Any] | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The fidelities at which to evaluate the trial, if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.time","title":"<code>time: Timer.Interval</code>   <code>classvar</code> <code>attr</code>","text":"<p>The time taken by the trial, once ended.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.memory","title":"<code>memory: Memory.Interval</code>   <code>classvar</code> <code>attr</code>","text":"<p>The memory used by the trial, once ended.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.profiler","title":"<code>profiler: Profiler</code>   <code>classvar</code> <code>attr</code>","text":"<p>A profiler for this trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.summary","title":"<code>summary: dict[str, Any]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The summary of the trial. These are for summary statistics of a trial and are single values.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.exception","title":"<code>exception: BaseException | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The exception raised by the trial, if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.traceback","title":"<code>traceback: str | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The traceback of the exception, if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.storage","title":"<code>storage: set[Any]</code>   <code>classvar</code> <code>attr</code>","text":"<p>Anything stored in the trial, the elements of the list are keys that can be used to retrieve them later, such as a Path.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.extras","title":"<code>extras: dict[str, Any]</code>   <code>classvar</code> <code>attr</code>","text":"<p>Any extras attached to the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.profiles","title":"<code>profiles: Mapping[str, Profile.Interval]</code>   <code>prop</code>","text":"<p>The profiles of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status","title":"<code>class Status</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>The status of a trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.SUCCESS","title":"<code>SUCCESS</code>   <code>classvar</code> <code>attr</code>","text":"<p>The trial was successful.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.FAIL","title":"<code>FAIL</code>   <code>classvar</code> <code>attr</code>","text":"<p>The trial failed.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.CRASHED","title":"<code>CRASHED</code>   <code>classvar</code> <code>attr</code>","text":"<p>The trial crashed.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Status.UNKNOWN","title":"<code>UNKNOWN</code>   <code>classvar</code> <code>attr</code>","text":"<p>The status of the trial is unknown.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report","title":"<code>class Report</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I2]</code></p> <p>The <code>Trial.Report</code> encapsulates a <code>Trial</code>, its status and any metrics/exceptions that may have occured.</p> <p>Typically you will not create these yourself, but instead use <code>trial.success()</code> or <code>trial.fail()</code> to generate them.</p> <pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True)\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\nwith trial.begin():\n    # Do some work\n    # ...\n    report: Trial.Report = trial.success(loss=1)\n\nprint(report.df())\n</code></pre> <pre><code>        status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                  ...                                   \ntrial  success        &lt;NA&gt;        NA  ...      0.000024      wall    seconds\n\n[1 rows x 19 columns]\n</code></pre> <p>These reports are used to report back metrics to an <code>Optimizer</code> with <code>Optimizer.tell()</code> but can also be stored for your own uses.</p> <p>You can access the original trial with the <code>.trial</code> attribute, and the <code>Status</code> of the trial with the <code>.status</code> attribute.</p> <p>You may also want to check out the <code>History</code> class for storing a collection of <code>Report</code>s, allowing for an easier time to convert them to a dataframe or perform some common Hyperparameter optimization parsing of metrics.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.trial","title":"<code>trial: Trial[I2]</code>   <code>attr</code>","text":"<p>The trial that was run.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.status","title":"<code>status: Trial.Status</code>   <code>attr</code>","text":"<p>The status of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.metrics","title":"<code>metrics: dict[str, float]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The metric values of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.metric_values","title":"<code>metric_values: tuple[Metric.Value, ...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The metrics of the trial, linked to the metrics.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.metric_defs","title":"<code>metric_defs: dict[str, Metric]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A lookup to the metric definitions</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.metric_names","title":"<code>metric_names: tuple[str, ...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The names of the metrics.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.exception","title":"<code>exception: BaseException | None</code>   <code>prop</code>","text":"<p>The exception of the trial, if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.traceback","title":"<code>traceback: str | None</code>   <code>prop</code>","text":"<p>The traceback of the trial, if any.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.name","title":"<code>name: str</code>   <code>prop</code>","text":"<p>The name of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.config","title":"<code>config: Mapping[str, Any]</code>   <code>prop</code>","text":"<p>The config of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.profiles","title":"<code>profiles: Mapping[str, Profile.Interval]</code>   <code>prop</code>","text":"<p>The profiles of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.summary","title":"<code>summary: dict[str, Any]</code>   <code>prop</code>","text":"<p>The summary of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.storage","title":"<code>storage: set[str]</code>   <code>prop</code>","text":"<p>The storage of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.time","title":"<code>time: Timer.Interval</code>   <code>prop</code>","text":"<p>The time of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.memory","title":"<code>memory: Memory.Interval</code>   <code>prop</code>","text":"<p>The memory of the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.bucket","title":"<code>bucket: PathBucket</code>   <code>prop</code>","text":"<p>The bucket attached to the trial.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.info","title":"<code>info: I2 | None</code>   <code>prop</code>","text":"<p>The info of the trial, specific to the optimizer that issued it.</p>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.df","title":"<code>def df(*, profiles=True, configs=True, summary=True, metrics=True)</code>","text":"<p>Get a dataframe of the trial.</p> <p>Prefixes</p> <ul> <li><code>summary</code>: Entries will be prefixed with <code>\"summary:\"</code></li> <li><code>config</code>: Entries will be prefixed with <code>\"config:\"</code></li> <li><code>storage</code>: Entries will be prefixed with <code>\"storage:\"</code></li> <li><code>metrics</code>: Entries will be prefixed with <code>\"metrics:\"</code></li> <li><code>profile:&lt;name&gt;</code>: Entries will be prefixed with     <code>\"profile:&lt;name&gt;:\"</code></li> </ul> PARAMETER  DESCRIPTION <code>profiles</code> <p>Whether to include the profiles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>configs</code> <p>Whether to include the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>summary</code> <p>Whether to include the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>metrics</code> <p>Whether to include the metrics.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def df(\n    self,\n    *,\n    profiles: bool = True,\n    configs: bool = True,\n    summary: bool = True,\n    metrics: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Get a dataframe of the trial.\n\n    !!! note \"Prefixes\"\n\n        * `summary`: Entries will be prefixed with `#!python \"summary:\"`\n        * `config`: Entries will be prefixed with `#!python \"config:\"`\n        * `storage`: Entries will be prefixed with `#!python \"storage:\"`\n        * `metrics`: Entries will be prefixed with `#!python \"metrics:\"`\n        * `profile:&lt;name&gt;`: Entries will be prefixed with\n            `#!python \"profile:&lt;name&gt;:\"`\n\n    Args:\n        profiles: Whether to include the profiles.\n        configs: Whether to include the configs.\n        summary: Whether to include the summary.\n        metrics: Whether to include the metrics.\n    \"\"\"\n    items = {\n        \"name\": self.name,\n        \"status\": str(self.status),\n        \"trial_seed\": self.trial.seed if self.trial.seed else np.nan,\n        \"exception\": str(self.exception) if self.exception else \"NA\",\n        \"traceback\": str(self.traceback) if self.traceback else \"NA\",\n        \"bucket\": str(self.bucket.path),\n    }\n    if metrics:\n        for value in self.metric_values:\n            items[f\"metric:{value.metric}\"] = value.value\n    if summary:\n        items.update(**prefix_keys(self.trial.summary, \"summary:\"))\n    if configs:\n        items.update(**prefix_keys(self.trial.config, \"config:\"))\n    if profiles:\n        for name, profile in sorted(self.profiles.items(), key=lambda x: x[0]):\n            # We log this one seperatly\n            if name == \"trial\":\n                items.update(profile.to_dict())\n            else:\n                items.update(profile.to_dict(prefix=f\"profile:{name}\"))\n\n    return pd.DataFrame(items, index=[0]).convert_dtypes().set_index(\"name\")\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.retrieve","title":"<code>def retrieve(key, *, where=None, check=None)</code>","text":"<p>Retrieve items related to the trial.</p> <p>Same argument for <code>where=</code></p> <p>Use the same argument for <code>where=</code> as you did for <code>store()</code>.</p> retrieve<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\nwith trial.begin():\n    report = trial.success()\n\nconfig = report.retrieve(\"config.json\")\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> <p>You could also create a Bucket and use that instead.</p> retrieve-bucket<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\n\nwith trial.begin():\n    report = trial.success()\n\nconfig = report.retrieve(\"config.json\")\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>The key of the item to retrieve as said in <code>.storage</code>.</p> <p> TYPE: <code>str</code> </p> <code>check</code> <p>If provided, will check that the retrieved item is of the provided type. If not, will raise a <code>TypeError</code>. This is only used if <code>where=</code> is a <code>str</code>, <code>Path</code> or <code>Bucket</code>.</p> <p> TYPE: <code>type[R] | None</code> DEFAULT: <code>None</code> </p> <code>where</code> <p>Where to retrieve the items from.</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will store a bucket will be created at the path, and the items will be retrieved from a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will retrieve the items from a sub-bucket with the name of the trial.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R | Any</code> <p>The retrieved item.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>check=</code> is provided and  the retrieved item is not of the provided type.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def retrieve(\n    self,\n    key: str,\n    *,\n    where: str | Path | Bucket[str, Any] | None = None,\n    check: type[R] | None = None,\n) -&gt; R | Any:\n    \"\"\"Retrieve items related to the trial.\n\n    !!! note \"Same argument for `where=`\"\n\n         Use the same argument for `where=` as you did for `store()`.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve\" hl_lines=\"7\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    with trial.begin():\n        report = trial.success()\n\n    config = report.retrieve(\"config.json\")\n    print(config)\n    ```\n\n    You could also create a Bucket and use that instead.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve-bucket\" hl_lines=\"11\"\n\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n\n    with trial.begin():\n        report = trial.success()\n\n    config = report.retrieve(\"config.json\")\n    print(config)\n    ```\n\n    Args:\n        key: The key of the item to retrieve as said in `.storage`.\n        check: If provided, will check that the retrieved item is of the\n            provided type. If not, will raise a `TypeError`. This\n            is only used if `where=` is a `str`, `Path` or `Bucket`.\n        where: Where to retrieve the items from.\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will store\n            a bucket will be created at the path, and the items will be\n            retrieved from a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will retrieve the items from a sub-bucket with the\n            name of the trial.\n\n    Returns:\n        The retrieved item.\n\n    Raises:\n        TypeError: If `check=` is provided and  the retrieved item is not of the provided\n            type.\n    \"\"\"  # noqa: E501\n    return self.trial.retrieve(key, where=where, check=check)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.store","title":"<code>def store(items, *, where=None)</code>","text":"<p>Store items related to the trial.</p> <p>See: <code>Trial.store()</code></p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def store(\n    self,\n    items: Mapping[str, T],\n    *,\n    where: (\n        str | Path | Bucket | Callable[[str, Mapping[str, T]], None] | None\n    ) = None,\n) -&gt; None:\n    \"\"\"Store items related to the trial.\n\n    See: [`Trial.store()`][amltk.optimization.trial.Trial.store]\n    \"\"\"\n    self.trial.store(items, where=where)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.from_df","title":"<code>def from_df(df)</code>   <code>classmethod</code>","text":"<p>Create a report from a dataframe.</p> See Also <ul> <li><code>.from_dict()</code></li> </ul> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@classmethod\ndef from_df(cls, df: pd.DataFrame | pd.Series) -&gt; Trial.Report:\n    \"\"\"Create a report from a dataframe.\n\n    See Also:\n        * [`.from_dict()`][amltk.optimization.Trial.Report.from_dict]\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        if len(df) != 1:\n            raise ValueError(\n                f\"Expected a dataframe with one row, got {len(df)} rows.\",\n            )\n        series = df.iloc[0]\n    else:\n        series = df\n\n    data_dict = {\"name\": series.name, **series.to_dict()}\n    return cls.from_dict(data_dict)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create a report from a dictionary.</p> <p>Prefixes</p> <p>Please see <code>.df()</code> for information on what the prefixes should be for certain fields.</p> PARAMETER  DESCRIPTION <code>d</code> <p>The dictionary to create the report from.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Report</code> <p>The created report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Trial.Report:\n    \"\"\"Create a report from a dictionary.\n\n    !!! note \"Prefixes\"\n\n        Please see [`.df()`][amltk.optimization.Trial.Report.df]\n        for information on what the prefixes should be for certain fields.\n\n    Args:\n        d: The dictionary to create the report from.\n\n    Returns:\n        The created report.\n    \"\"\"\n    prof_dict = mapping_select(d, \"profile:\")\n    if any(prof_dict):\n        profile_names = sorted(\n            {name.rsplit(\":\", maxsplit=2)[0] for name in prof_dict},\n        )\n        profiles = {\n            name: Profile.from_dict(mapping_select(prof_dict, f\"{name}:\"))\n            for name in profile_names\n        }\n    else:\n        profiles = {}\n\n    # NOTE: We assume the order of the objectives are in the right\n    # order in the dict. If we attempt to force a sort-order, we may\n    # deserialize incorrectly. By not having a sort order, we rely\n    # on serialization to keep the order, which is not ideal either.\n    # May revisit this if we need to\n    raw_metrics: dict[str, float] = mapping_select(d, \"metric:\")\n    _intermediate = {\n        Metric.from_str(name): value for name, value in raw_metrics.items()\n    }\n    metrics: dict[Metric, Metric.Value] = {\n        metric: metric.as_value(value)\n        for metric, value in _intermediate.items()\n    }\n\n    _trial_profile_items = {\n        k: v for k, v in d.items() if k.startswith((\"memory:\", \"time:\"))\n    }\n    if any(_trial_profile_items):\n        trial_profile = Profile.from_dict(_trial_profile_items)\n        profiles[\"trial\"] = trial_profile\n    else:\n        trial_profile = Profile.na()\n\n    exception = d.get(\"exception\")\n    traceback = d.get(\"traceback\")\n    trial_seed = d.get(\"trial_seed\")\n    if pd.isna(exception) or exception == \"NA\":  # type: ignore\n        exception = None\n    if pd.isna(traceback) or traceback == \"NA\":  # type: ignore\n        traceback = None\n    if pd.isna(trial_seed):  # type: ignore\n        trial_seed = None\n\n    if (_bucket := d.get(\"bucket\")) is not None:\n        bucket = PathBucket(_bucket)\n    else:\n        bucket = PathBucket(f\"uknown_trial_bucket-{datetime.now().isoformat()}\")\n\n    trial: Trial[None] = Trial(\n        name=d[\"name\"],\n        config=mapping_select(d, \"config:\"),\n        info=None,  # We don't save this to disk so we load it back as None\n        bucket=bucket,\n        seed=trial_seed,\n        fidelities=mapping_select(d, \"fidelities:\"),\n        time=trial_profile.time,\n        memory=trial_profile.memory,\n        profiler=Profiler(profiles=profiles),\n        metrics=list(metrics.keys()),\n        summary=mapping_select(d, \"summary:\"),\n        exception=exception,\n        traceback=traceback,\n    )\n    status = Trial.Status(dict_get_not_none(d, \"status\", \"unknown\"))\n    _values: dict[str, float] = {m.name: r.value for m, r in metrics.items()}\n    if status == Trial.Status.SUCCESS:\n        return trial.success(**_values)\n\n    if status == Trial.Status.FAIL:\n        return trial.fail(**_values)\n\n    if status == Trial.Status.CRASHED:\n        return trial.crashed(\n            exception=Exception(\"Unknown status.\")\n            if trial.exception is None\n            else None,\n        )\n\n    return trial.crashed(exception=Exception(\"Unknown status.\"))\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.Report.rich_renderables","title":"<code>def rich_renderables()</code>","text":"<p>The renderables for rich for this report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def rich_renderables(self) -&gt; Iterable[RenderableType]:\n    \"\"\"The renderables for rich for this report.\"\"\"\n    from rich.pretty import Pretty\n    from rich.text import Text\n\n    yield Text.assemble(\n        (\"Status\", \"bold\"),\n        (\"(\", \"default\"),\n        self.status.__rich__(),\n        (\")\", \"default\"),\n    )\n    yield Pretty(self.metrics)\n    yield from self.trial.rich_renderables()\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.begin","title":"<code>def begin(time=None, memory_unit=None)</code>","text":"<p>Begin the trial with a <code>contextmanager</code>.</p> <p>Will begin timing the trial in the <code>with</code> block, attaching the profiled time and memory to the trial once completed, under <code>.profile.time</code> and <code>.profile.memory</code> attributes.</p> <p>If an exception is raised, it will be attached to the trial under <code>.exception</code> with the traceback attached to the actual error message, such that it can be pickled and sent back to the main process loop.</p> begin<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\n\nwith trial.begin():\n    # Do some work\n    pass\n\nprint(trial.memory)\nprint(trial.time)\n</code></pre> <pre><code>Memory.Interval(start_vms=1561518080.0, start_rss=281694208.0, end_vms=1561518080, end_rss=281694208, unit=bytes)\nTimer.Interval(start=1702352440.9671986, end=1702352440.9672084, kind=wall, unit=seconds)\n</code></pre> begin-fail<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"trial\", config={\"x\": -1})\n\nwith trial.begin():\n    raise ValueError(\"x must be positive\")\n\nprint(trial.exception)\nprint(trial.traceback)\nprint(trial.memory)\nprint(trial.time)\n</code></pre> <pre><code>x must be positive\nTraceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/amltk/optimization/trial.py\", line 301, in begin\n    yield\n  File \"&lt;code block: n31; title begin-fail&gt;\", line 6, in &lt;module&gt;\nValueError: x must be positive\n\nMemory.Interval(start_vms=1561518080.0, start_rss=281694208.0, end_vms=1561518080, end_rss=281694208, unit=bytes)\nTimer.Interval(start=1702352440.970899, end=1702352440.9711232, kind=wall, unit=seconds)\n</code></pre> PARAMETER  DESCRIPTION <code>time</code> <p>The timer kind to use for the trial. Defaults to the default timer kind of the profiler.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> <code>memory_unit</code> <p>The memory unit to use for the trial. Defaults to the default memory unit of the profiler.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@contextmanager\ndef begin(\n    self,\n    time: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"Begin the trial with a `contextmanager`.\n\n    Will begin timing the trial in the `with` block, attaching the profiled time and memory\n    to the trial once completed, under `.profile.time` and `.profile.memory` attributes.\n\n    If an exception is raised, it will be attached to the trial under `.exception`\n    with the traceback attached to the actual error message, such that it can\n    be pickled and sent back to the main process loop.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"begin\" hl_lines=\"5\"\n    from amltk.optimization import Trial\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n\n    with trial.begin():\n        # Do some work\n        pass\n\n    print(trial.memory)\n    print(trial.time)\n    ```\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"begin-fail\" hl_lines=\"5\"\n    from amltk.optimization import Trial\n\n    trial = Trial(name=\"trial\", config={\"x\": -1})\n\n    with trial.begin():\n        raise ValueError(\"x must be positive\")\n\n    print(trial.exception)\n    print(trial.traceback)\n    print(trial.memory)\n    print(trial.time)\n    ```\n\n    Args:\n        time: The timer kind to use for the trial. Defaults to the default\n            timer kind of the profiler.\n        memory_unit: The memory unit to use for the trial. Defaults to the\n            default memory unit of the profiler.\n    \"\"\"  # noqa: E501\n    with self.profiler(name=\"trial\", memory_unit=memory_unit, time_kind=time):\n        try:\n            yield\n        except Exception as error:  # noqa: BLE001\n            self.exception = error\n            self.traceback = traceback.format_exc()\n        finally:\n            self.time = self.profiler[\"trial\"].time\n            self.memory = self.profiler[\"trial\"].memory\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.profile","title":"<code>def profile(name, *, time=None, memory_unit=None, summary=False)</code>","text":"<p>Measure some interval in the trial.</p> <p>The results of the profiling will be available in the <code>.summary</code> attribute with the name of the interval as the key.</p> profile<pre><code>from amltk.optimization import Trial\nimport time\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\n\nwith trial.profile(\"some_interval\"):\n    # Do some work\n    time.sleep(1)\n\nprint(trial.profiler[\"some_interval\"].time)\n</code></pre> <pre><code>Timer.Interval(start=1702352440.9828844, end=1702352441.983916, kind=wall, unit=seconds)\n</code></pre> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the interval.</p> <p> TYPE: <code>str</code> </p> <code>time</code> <p>The timer kind to use for the trial. Defaults to the default timer kind of the profiler.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> <code>memory_unit</code> <p>The memory unit to use for the trial. Defaults to the default memory unit of the profiler.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> <code>summary</code> <p>Whether to add the interval to the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Iterator[None]</code> <p>The interval measured. Values will be nan until the with block is finished.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@contextmanager\ndef profile(\n    self,\n    name: str,\n    *,\n    time: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    summary: bool = False,\n) -&gt; Iterator[None]:\n    \"\"\"Measure some interval in the trial.\n\n    The results of the profiling will be available in the `.summary` attribute\n    with the name of the interval as the key.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"profile\"\n    from amltk.optimization import Trial\n    import time\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n\n    with trial.profile(\"some_interval\"):\n        # Do some work\n        time.sleep(1)\n\n    print(trial.profiler[\"some_interval\"].time)\n    ```\n\n    Args:\n        name: The name of the interval.\n        time: The timer kind to use for the trial. Defaults to the default\n            timer kind of the profiler.\n        memory_unit: The memory unit to use for the trial. Defaults to the\n            default memory unit of the profiler.\n        summary: Whether to add the interval to the summary.\n\n    Yields:\n        The interval measured. Values will be nan until the with block is finished.\n    \"\"\"\n    with self.profiler(name=name, memory_unit=memory_unit, time_kind=time):\n        yield\n\n    if summary:\n        profile = self.profiler[name]\n        self.summary.update(profile.to_dict(prefix=name))\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.success","title":"<code>def success(**metrics)</code>","text":"<p>Generate a success report.</p> success<pre><code>from amltk.optimization import Trial, Metric\n\nloss_metric = Metric(\"loss\", minimize=True)\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss_metric])\n\nwith trial.begin():\n    # Do some work\n    report = trial.success(loss=1)\n\nprint(report)\n</code></pre> <pre><code>Trial.Report(trial=Trial(name='trial', config={'x': 1}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='loss', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'loss': 1.0}, metric_values=(Metric.Value(metric=Metric(name='loss', minimize=True, bounds=None), value=1.0),), metric_defs={'loss': Metric(name='loss', minimize=True, bounds=None)}, metric_names=('loss',))\n</code></pre> PARAMETER  DESCRIPTION <code>**metrics</code> <p>The metrics of the trial, where the key is the name of the metrics and the value is the metric.</p> <p> TYPE: <code>float | int</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Report[I]</code> <p>The report of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def success(self, **metrics: float | int) -&gt; Trial.Report[I]:\n    \"\"\"Generate a success report.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"success\" hl_lines=\"7\"\n    from amltk.optimization import Trial, Metric\n\n    loss_metric = Metric(\"loss\", minimize=True)\n\n    trial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss_metric])\n\n    with trial.begin():\n        # Do some work\n        report = trial.success(loss=1)\n\n    print(report)\n    ```\n\n    Args:\n        **metrics: The metrics of the trial, where the key is the name of the\n            metrics and the value is the metric.\n\n    Returns:\n        The report of the trial.\n    \"\"\"  # noqa: E501\n    _recorded_values: list[Metric.Value] = []\n    for _metric in self.metrics:\n        if (raw_value := metrics.get(_metric.name)) is not None:\n            _recorded_values.append(_metric.as_value(raw_value))\n        else:\n            raise ValueError(\n                f\"Cannot report success without {self.metrics=}.\"\n                f\" Please provide a value for the metric.\",\n            )\n\n    # Need to check if anything extra was reported!\n    extra = set(metrics.keys()) - {metric.name for metric in self.metrics}\n    if extra:\n        raise ValueError(\n            f\"Cannot report success with extra metrics: {extra=}.\"\n            f\"\\nOnly {self.metrics=} are allowed.\",\n        )\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.SUCCESS,\n        metric_values=tuple(_recorded_values),\n    )\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.fail","title":"<code>def fail(**metrics)</code>","text":"<p>Generate a failure report.</p> <p>Non specifed metrics</p> <p>If you do not specify metrics, this will use the <code>.metrics</code> to determine the <code>.worst</code> value of the metric, using that as the reported result</p> fail<pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True, bounds=(0, 1_000))\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\nwith trial.begin():\n    raise ValueError(\"This is an error\")  # Something went wrong\n\nif trial.exception: # You can check for an exception of the trial here\n    report = trial.fail()\n\nprint(report.metrics)\nprint(report)\n</code></pre> <pre><code>{'loss': 1000.0}\nTrial.Report(trial=Trial(name='trial', config={'x': 1}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='loss', minimize=True, bounds=(0.0, 1000.0))], seed=None, fidelities=None, summary={}, exception=ValueError('This is an error'), storage=set(), extras={}), status=&lt;Status.FAIL: 'fail'&gt;, metrics={'loss': 1000.0}, metric_values=(Metric.Value(metric=Metric(name='loss', minimize=True, bounds=(0.0, 1000.0)), value=1000.0),), metric_defs={'loss': Metric(name='loss', minimize=True, bounds=(0.0, 1000.0))}, metric_names=('loss',))\n</code></pre> RETURNS DESCRIPTION <code>Report[I]</code> <p>The result of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def fail(self, **metrics: float | int) -&gt; Trial.Report[I]:\n    \"\"\"Generate a failure report.\n\n    !!! note \"Non specifed metrics\"\n\n        If you do not specify metrics, this will use\n        the [`.metrics`][amltk.optimization.Trial.metrics] to determine\n        the [`.worst`][amltk.optimization.Metric.worst] value of the metric,\n        using that as the reported result\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"fail\"\n    from amltk.optimization import Trial, Metric\n\n    loss = Metric(\"loss\", minimize=True, bounds=(0, 1_000))\n    trial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\n    with trial.begin():\n        raise ValueError(\"This is an error\")  # Something went wrong\n\n    if trial.exception: # You can check for an exception of the trial here\n        report = trial.fail()\n\n    print(report.metrics)\n    print(report)\n    ```\n\n    Returns:\n        The result of the trial.\n    \"\"\"\n    _recorded_values: list[Metric.Value] = []\n    for _metric in self.metrics:\n        if (raw_value := metrics.get(_metric.name)) is not None:\n            _recorded_values.append(_metric.as_value(raw_value))\n        else:\n            _recorded_values.append(_metric.worst)\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.FAIL,\n        metric_values=tuple(_recorded_values),\n    )\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.crashed","title":"<code>def crashed(exception=None, traceback=None)</code>","text":"<p>Generate a crash report.</p> <p>Note</p> <p>You will typically not create these manually, but instead if we don't recieve a report from a target function evaluation, but only an error, we assume something crashed and generate a crash report for you.</p> <p>Non specifed metrics</p> <p>We will use the <code>.metrics</code> to determine the <code>.worst</code> value of the metric, using that as the reported metrics</p> PARAMETER  DESCRIPTION <code>exception</code> <p>The exception that caused the crash. If not provided, the exception will be taken from the trial. If this is still <code>None</code>, a <code>RuntimeError</code> will be raised.</p> <p> TYPE: <code>BaseException | None</code> DEFAULT: <code>None</code> </p> <code>traceback</code> <p>The traceback of the exception. If not provided, the traceback will be taken from the trial if there is one there.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Report[I]</code> <p>The report of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def crashed(\n    self,\n    exception: BaseException | None = None,\n    traceback: str | None = None,\n) -&gt; Trial.Report[I]:\n    \"\"\"Generate a crash report.\n\n    !!! note\n\n        You will typically not create these manually, but instead if we don't\n        recieve a report from a target function evaluation, but only an error,\n        we assume something crashed and generate a crash report for you.\n\n    !!! note \"Non specifed metrics\"\n\n        We will use the [`.metrics`][amltk.optimization.Trial.metrics] to determine\n        the [`.worst`][amltk.optimization.Metric.worst] value of the metric,\n        using that as the reported metrics\n\n    Args:\n        exception: The exception that caused the crash. If not provided, the\n            exception will be taken from the trial. If this is still `None`,\n            a `RuntimeError` will be raised.\n        traceback: The traceback of the exception. If not provided, the\n            traceback will be taken from the trial if there is one there.\n\n    Returns:\n        The report of the trial.\n    \"\"\"\n    if exception is None and self.exception is None:\n        raise RuntimeError(\n            \"Cannot generate a crash report without an exception.\"\n            \" Please provide an exception or use `with trial.begin():` to start\"\n            \" the trial.\",\n        )\n\n    self.exception = exception if exception else self.exception\n    self.traceback = traceback if traceback else self.traceback\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.CRASHED,\n        metric_values=tuple(metric.worst for metric in self.metrics),\n    )\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.store","title":"<code>def store(items, *, where=None)</code>","text":"<p>Store items related to the trial.</p> store<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=PathBucket(\"results\"))\ntrial.store({\"config.json\": trial.config})\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> <p>You could also specify <code>where=</code> exactly to store the thing</p> store-bucket<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\ntrial.store({\"config.json\": trial.config}, where=\"./results\")\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> PARAMETER  DESCRIPTION <code>items</code> <p>The items to store, a dict from the key to store it under to the item itself.If using a <code>str</code>, <code>Path</code> or <code>PathBucket</code>, the keys of the items should be a valid filename, including the correct extension. e.g. <code>{\"config.json\": trial.config}</code></p> <p> TYPE: <code>Mapping[str, T]</code> </p> <code>where</code> <p>Where to store the items.</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will store a bucket will be created at the path, and the items will be stored in a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will store the items in a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Callable</code>, will call the callable with the name of the trial and the key-valued pair of items to store.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket | Callable[[str, Mapping[str, T]], None] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def store(\n    self,\n    items: Mapping[str, T],\n    *,\n    where: (\n        str | Path | Bucket | Callable[[str, Mapping[str, T]], None] | None\n    ) = None,\n) -&gt; None:\n    \"\"\"Store items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"store\" hl_lines=\"5\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=PathBucket(\"results\"))\n    trial.store({\"config.json\": trial.config})\n\n    print(trial.storage)\n    ```\n\n    You could also specify `where=` exactly to store the thing\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"store-bucket\" hl_lines=\"7\"\n    from amltk.optimization import Trial\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n    trial.store({\"config.json\": trial.config}, where=\"./results\")\n\n    print(trial.storage)\n    ```\n\n    Args:\n        items: The items to store, a dict from the key to store it under\n            to the item itself.If using a `str`, `Path` or `PathBucket`,\n            the keys of the items should be a valid filename, including\n            the correct extension. e.g. `#!python {\"config.json\": trial.config}`\n\n        where: Where to store the items.\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will store\n            a bucket will be created at the path, and the items will be\n            stored in a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will store the items **in a sub-bucket** with the\n            name of the trial.\n\n            * If a `Callable`, will call the callable with the name of the\n            trial and the key-valued pair of items to store.\n    \"\"\"  # noqa: E501\n    method: Bucket\n    match where:\n        case None:\n            method = self.bucket\n            method.sub(self.name).store(items)\n        case str() | Path():\n            method = PathBucket(where, create=True)\n            method.sub(self.name).store(items)\n        case Bucket():\n            method = where\n            method.sub(self.name).store(items)\n        case _:\n            # Leave it up to supplied method\n            where(self.name, items)\n\n    # Add the keys to storage\n    self.storage.update(items.keys())\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.delete_from_storage","title":"<code>def delete_from_storage(items, *, where=None)</code>","text":"<p>Delete items related to the trial.</p> delete-storage<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\ntrial = Trial(name=\"trial\", config={\"x\": 1}, info={}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\ntrial.delete_from_storage(items=[\"config.json\"])\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> <p>You could also create a Bucket and use that instead.</p> delete-storage-bucket<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\ntrial.delete_from_storage(items=[\"config.json\"])\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> PARAMETER  DESCRIPTION <code>items</code> <p>The items to delete, an iterable of keys</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>where</code> <p>Where the items are stored</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will lookup a bucket at the path, and the items will be deleted from a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will delete the items in a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Callable</code>, will call the callable with the name of the trial and the keys of the items to delete. Should a mapping from the key to whether it was deleted or not.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket | Callable[[str, Iterable[str]], dict[str, bool]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, bool]</code> <p>A dict from the key to whether it was deleted or not.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def delete_from_storage(\n    self,\n    items: Iterable[str],\n    *,\n    where: (\n        str | Path | Bucket | Callable[[str, Iterable[str]], dict[str, bool]] | None\n    ) = None,\n) -&gt; dict[str, bool]:\n    \"\"\"Delete items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"delete-storage\" hl_lines=\"6\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n    trial = Trial(name=\"trial\", config={\"x\": 1}, info={}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    trial.delete_from_storage(items=[\"config.json\"])\n\n    print(trial.storage)\n    ```\n\n    You could also create a Bucket and use that instead.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"delete-storage-bucket\" hl_lines=\"9\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    trial.delete_from_storage(items=[\"config.json\"])\n\n    print(trial.storage)\n    ```\n\n    Args:\n        items: The items to delete, an iterable of keys\n        where: Where the items are stored\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will lookup a bucket at the path,\n            and the items will be deleted from a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will delete the items in a sub-bucket with the\n            name of the trial.\n\n            * If a `Callable`, will call the callable with the name of the\n            trial and the keys of the items to delete. Should a mapping from\n            the key to whether it was deleted or not.\n\n    Returns:\n        A dict from the key to whether it was deleted or not.\n    \"\"\"  # noqa: E501\n    # If not a Callable, we convert to a path bucket\n    method: Bucket\n    match where:\n        case None:\n            method = self.bucket\n        case str() | Path():\n            method = PathBucket(where, create=False)\n        case Bucket():\n            method = where\n        case _:\n            # Leave it up to supplied method\n            return where(self.name, items)\n\n    sub_bucket = method.sub(self.name)\n    return sub_bucket.remove(items)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.copy","title":"<code>def copy()</code>","text":"<p>Create a copy of the trial.</p> RETURNS DESCRIPTION <code>Self</code> <p>The copy of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Create a copy of the trial.\n\n    Returns:\n        The copy of the trial.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.retrieve","title":"<code>def retrieve(key, *, where=None, check=None)</code>","text":"<p>Retrieve items related to the trial.</p> <p>Same argument for <code>where=</code></p> <p>Use the same argument for <code>where=</code> as you did for <code>store()</code>.</p> retrieve<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\n\n# Create a trial, normally done by an optimizer\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\nconfig = trial.retrieve(\"config.json\")\n\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> <p>You could also manually specify where something get's stored and retrieved</p> retrieve-bucket<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\npath = \"./config_path\"\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\n\ntrial.store({\"config.json\": trial.config}, where=path)\n\nconfig = trial.retrieve(\"config.json\", where=path)\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>The key of the item to retrieve as said in <code>.storage</code>.</p> <p> TYPE: <code>str</code> </p> <code>check</code> <p>If provided, will check that the retrieved item is of the provided type. If not, will raise a <code>TypeError</code>. This is only used if <code>where=</code> is a <code>str</code>, <code>Path</code> or <code>Bucket</code>.</p> <p> TYPE: <code>type[R] | None</code> DEFAULT: <code>None</code> </p> <code>where</code> <p>Where to retrieve the items from.</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will store a bucket will be created at the path, and the items will be retrieved from a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will retrieve the items from a sub-bucket with the name of the trial.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R | Any</code> <p>The retrieved item.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>check=</code> is provided and  the retrieved item is not of the provided type.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def retrieve(\n    self,\n    key: str,\n    *,\n    where: str | Path | Bucket[str, Any] | None = None,\n    check: type[R] | None = None,\n) -&gt; R | Any:\n    \"\"\"Retrieve items related to the trial.\n\n    !!! note \"Same argument for `where=`\"\n\n         Use the same argument for `where=` as you did for `store()`.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve\" hl_lines=\"7\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n\n    # Create a trial, normally done by an optimizer\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    config = trial.retrieve(\"config.json\")\n\n    print(config)\n    ```\n\n    You could also manually specify where something get's stored and retrieved\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve-bucket\" hl_lines=\"11\"\n\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    path = \"./config_path\"\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n\n    trial.store({\"config.json\": trial.config}, where=path)\n\n    config = trial.retrieve(\"config.json\", where=path)\n    print(config)\n    import shutil; shutil.rmtree(path)  # markdown-exec: hide\n    ```\n\n    Args:\n        key: The key of the item to retrieve as said in `.storage`.\n        check: If provided, will check that the retrieved item is of the\n            provided type. If not, will raise a `TypeError`. This\n            is only used if `where=` is a `str`, `Path` or `Bucket`.\n\n        where: Where to retrieve the items from.\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will store\n            a bucket will be created at the path, and the items will be\n            retrieved from a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will retrieve the items from a sub-bucket with the\n            name of the trial.\n\n    Returns:\n        The retrieved item.\n\n    Raises:\n        TypeError: If `check=` is provided and  the retrieved item is not of the provided\n            type.\n    \"\"\"  # noqa: E501\n    # If not a Callable, we convert to a path bucket\n    method: Bucket[str, Any]\n    match where:\n        case None:\n            method = self.bucket\n        case str():\n            method = PathBucket(where, create=True)\n        case Path():\n            method = PathBucket(where, create=True)\n        case Bucket():\n            method = where\n\n    # Store in a sub-bucket\n    return method.sub(self.name)[key].load(check=check)\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.attach_extra","title":"<code>def attach_extra(name, plugin_item)</code>","text":"<p>Attach a plugin item to the trial.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the plugin item.</p> <p> TYPE: <code>str</code> </p> <code>plugin_item</code> <p>The plugin item.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def attach_extra(self, name: str, plugin_item: Any) -&gt; None:\n    \"\"\"Attach a plugin item to the trial.\n\n    Args:\n        name: The name of the plugin item.\n        plugin_item: The plugin item.\n    \"\"\"\n    self.extras[name] = plugin_item\n</code></pre>"},{"location":"api/amltk/optimization/trial/#amltk.optimization.trial.Trial.rich_renderables","title":"<code>def rich_renderables()</code>","text":"<p>The renderables for rich for this report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def rich_renderables(self) -&gt; Iterable[RenderableType]:  # noqa: C901\n    \"\"\"The renderables for rich for this report.\"\"\"\n    from rich.panel import Panel\n    from rich.pretty import Pretty\n    from rich.table import Table\n    from rich.text import Text\n\n    items: list[RenderableType] = []\n    table = Table.grid(padding=(0, 1), expand=False)\n\n    # Predfined things\n    table.add_row(\"config\", Pretty(self.config))\n\n    if self.fidelities:\n        table.add_row(\"fidelities\", Pretty(self.fidelities))\n\n    if any(self.extras):\n        table.add_row(\"extras\", Pretty(self.extras))\n\n    if self.seed:\n        table.add_row(\"seed\", Pretty(self.seed))\n\n    if self.bucket:\n        table.add_row(\"bucket\", Pretty(self.bucket))\n\n    if self.metrics:\n        items.append(\n            Panel(Pretty(self.metrics), title=\"Metrics\", title_align=\"left\"),\n        )\n\n    # Dynamic things\n    if self.summary:\n        table.add_row(\"summary\", Pretty(self.summary))\n\n    if any(self.storage):\n        table.add_row(\"storage\", Pretty(self.storage))\n\n    if self.exception:\n        table.add_row(\"exception\", Text(str(self.exception), style=\"bold red\"))\n\n    if self.traceback:\n        table.add_row(\"traceback\", Text(self.traceback, style=\"bold red\"))\n\n    for name, profile in self.profiles.items():\n        table.add_row(\"profile:\" + name, Pretty(profile))\n\n    items.append(table)\n\n    yield from items\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/","title":"Neps","text":"<p>The <code>NEPSOptimizer</code>, is a wrapper around the <code>NePs</code> optimizer.</p> <p>Requirements</p> <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[neps]\n\n# Or directly\npip install neural-pipeline-search\n</code></pre> <p>NePs is still in development</p> <p>NePs is still in development and is not yet stable. There are likely going to be issues. Please report any issues to NePs or in AMLTK.</p> <p>This uses <code>ConfigSpace</code> as its <code>search_space()</code> to optimize.</p> <p>Users should report results using <code>trial.success(loss=...)</code> where <code>loss=</code> is a scaler value to minimize. Optionally, you can also return a <code>cost=</code> which is used for more budget aware algorithms. Again, please see NeP's documentation for more.</p> <p>Conditionals in ConfigSpace</p> <p>NePs does not support conditionals in its search space. This is account for when using the <code>preferred_parser()</code>. during search space creation. In this case, it will simply remove all conditionals from the search space, which may not be ideal for the given problem at hand.</p> <p>Visit their documentation for what you can pass to <code>NEPSOptimizer.create()</code>.</p> <p>The below example shows how you can use neps to optimize an sklearn pipeline.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.neps import NEPSOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Pipeline) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        loss = 1 - accuracy\n        return trial.success(loss=loss, accuracy=accuracy)\n\n    return trial.fail()\nfrom amltk._doc import make_picklable; make_picklable(target_function)  # markdown-exec: hide\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\nmetric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = NEPSOptimizer.create(space=pipeline, metrics=metric, bucket=\"neps-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\noptimizer.bucket.rmdir()  # markdown-exec: hide\n</code></pre> <p>Deep Learning</p> <p>Write an example demonstrating NEPS with continuations</p> <p>Graph Search Spaces</p> <p>Write an example demonstrating NEPS with its graph search spaces</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSPreferredParser","title":"<code>class NEPSPreferredParser</code>","text":"<p>         Bases: <code>Protocol</code></p> <p>The preferred parser call signature for NEPSOptimizer.</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSPreferredParser.__call__","title":"<code>def __call__(node, *, seed=None, flat=False, delim=':')</code>","text":"<p>See <code>configspace_parser</code>.</p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>def __call__(\n    self,\n    node: Node,\n    *,\n    seed: int | None = None,\n    flat: bool = False,\n    delim: str = \":\",\n) -&gt; ConfigurationSpace:\n    \"\"\"See [`configspace_parser`][amltk.pipeline.parsers.configspace.parser].\"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSTrialInfo","title":"<code>class NEPSTrialInfo</code>   <code>dataclass</code>","text":"<p>The info for a trial.</p>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer","title":"<code>class NEPSOptimizer(*, space, loss_metric, cost_metric=None, optimizer, working_dir, seed=None, bucket=None)</code>","text":"<p>         Bases: <code>Optimizer[NEPSTrialInfo]</code></p> <p>An optimizer that uses SMAC to optimize a config space.</p> PARAMETER  DESCRIPTION <code>space</code> <p>The space to use.</p> <p> TYPE: <code>SearchSpace</code> </p> <code>loss_metric</code> <p>The metric to optimize.</p> <p> TYPE: <code>Metric</code> </p> <code>cost_metric</code> <p>The cost metric to use. Only certain NePs optimizers support</p> <p> TYPE: <code>Metric | None</code> DEFAULT: <code>None</code> </p> <code>optimizer</code> <p>The optimizer to use.</p> <p> TYPE: <code>BaseOptimizer</code> </p> <code>seed</code> <p>The seed to use for the trials (and not optimizers).</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>working_dir</code> <p>The directory to use for the trials.</p> <p> TYPE: <code>Path</code> </p> <code>bucket</code> <p>The bucket to give to trials generated from this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>def __init__(\n    self,\n    *,\n    space: SearchSpace,\n    loss_metric: Metric,\n    cost_metric: Metric | None = None,\n    optimizer: BaseOptimizer,\n    working_dir: Path,\n    seed: Seed | None = None,\n    bucket: PathBucket | None = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        space: The space to use.\n        loss_metric: The metric to optimize.\n        cost_metric: The cost metric to use. Only certain NePs optimizers support\n        optimizer: The optimizer to use.\n        seed: The seed to use for the trials (and not optimizers).\n        working_dir: The directory to use for the trials.\n        bucket: The bucket to give to trials generated from this optimizer.\n    \"\"\"\n    if isinstance(loss_metric, Sequence):\n        raise ValueError(\"NePs does not support multiple metrics\")\n\n    if cost_metric is not None and cost_metric.minimize is False:\n        raise ValueError(\"NePs only supports minimizing cost metrics\")\n\n    if cost_metric is None and optimizer.budget is not None:\n        raise ValueError(\n            \"NePs optimizers with a budget require a cost metric to be provided\",\n        )\n\n    metrics = [loss_metric]\n    if cost_metric is not None:\n        metrics.append(cost_metric)\n\n    super().__init__(bucket=bucket, metrics=metrics)\n    self.space = space\n    self.seed = amltk.randomness.as_int(seed)\n    self.optimizer = optimizer\n    self.working_dir = working_dir\n    self.loss_metric = loss_metric\n    self.cost_metric = cost_metric\n\n    self.optimizer_state_file = self.working_dir / \"optimizer_state.yaml\"\n    self.base_result_directory = self.working_dir / \"results\"\n    self.serializer = metahyper.utils.YamlSerializer(self.optimizer.load_config)\n\n    self.working_dir.mkdir(parents=True, exist_ok=True)\n    self.base_result_directory.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.create","title":"<code>def create(*, space, metrics, cost_metric=None, bucket=None, searcher='default', working_dir='neps', overwrite=True, seed=None, max_cost_total=None, searcher_kwargs=None)</code>   <code>classmethod</code>","text":"<p>Create a new NEPS optimizer.</p> PARAMETER  DESCRIPTION <code>space</code> <p>The space to use.</p> <p> TYPE: <code>SearchSpace | ConfigurationSpace | Mapping[str, ConfigurationSpace | Parameter] | Node</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p>Warning</p> <p>NePs does not support multiple metrics. Please only pass a single metric.</p> <p> TYPE: <code>Metric</code> </p> <code>cost_metric</code> <p>The cost metric to use. Only certain NePs optimizers support this.</p> <p> TYPE: <code>Metric | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the trials.</p> <p>Warning</p> <p>NePS optimizers do not support an explicit seeding. If you'd like to seed their optimizers, they use the global <code>torch.manual_seed</code>, <code>np.random.seed</code>, and <code>random.seed</code>. This is not considered a good practice and there is not much we can do from AMLTK to help with this.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>bucket</code> <p>The bucket to give to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | str | Path | None</code> DEFAULT: <code>None</code> </p> <code>searcher</code> <p>The searcher to use.</p> <p> TYPE: <code>str | BaseOptimizer</code> DEFAULT: <code>'default'</code> </p> <code>working_dir</code> <p>The directory to use for the optimization.</p> <p> TYPE: <code>str | Path</code> DEFAULT: <code>'neps'</code> </p> <code>overwrite</code> <p>Whether to overwrite the working directory if it exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>max_cost_total</code> <p>The maximum cost to use for the optimization.</p> <p>Warning</p> <p>This only effects the optimization if the searcher utilizes the budget for it's actual suggestion of the next config. If the searcher does not use the budget. This parameter has no effect.</p> <p>The user is still expected to stop <code>ask()</code>'ing for configs when they have reached some budget.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>searcher_kwargs</code> <p>Additional kwargs to pass to the searcher.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@classmethod\ndef create(  # noqa: PLR0913\n    cls,\n    *,\n    space: (\n        SearchSpace\n        | ConfigurationSpace\n        | Mapping[str, ConfigurationSpace | Parameter]\n        | Node\n    ),\n    metrics: Metric,\n    cost_metric: Metric | None = None,\n    bucket: PathBucket | str | Path | None = None,\n    searcher: str | BaseOptimizer = \"default\",\n    working_dir: str | Path = \"neps\",\n    overwrite: bool = True,\n    seed: Seed | None = None,\n    max_cost_total: float | None = None,\n    searcher_kwargs: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Create a new NEPS optimizer.\n\n    Args:\n        space: The space to use.\n        metrics: The metrics to optimize.\n\n            !!! warning\n\n                NePs does not support multiple metrics. Please only pass a single\n                metric.\n\n        cost_metric: The cost metric to use. Only certain NePs optimizers support\n            this.\n        seed: The seed to use for the trials.\n\n            !!! warning\n\n                NePS optimizers do not support an explicit seeding. If you'd\n                like to seed their optimizers, they use the global\n                `torch.manual_seed`, `np.random.seed`, and `random.seed`.\n                This is not considered a good practice and there is not\n                much we can do from AMLTK to help with this.\n\n        bucket: The bucket to give to trials generated by this optimizer.\n        searcher: The searcher to use.\n        working_dir: The directory to use for the optimization.\n        overwrite: Whether to overwrite the working directory if it exists.\n        max_cost_total: The maximum cost to use for the optimization.\n\n            !!! warning\n\n                This only effects the optimization if the searcher utilizes the\n                budget for it's actual suggestion of the next config. If the\n                searcher does not use the budget. This parameter has no effect.\n\n                The user is still expected to stop `ask()`'ing for configs when\n                they have reached some budget.\n        searcher_kwargs: Additional kwargs to pass to the searcher.\n    \"\"\"\n    if isinstance(space, Node):\n        space = space.search_space(parser=NEPSOptimizer.preferred_parser())\n\n    match bucket:\n        case None:\n            bucket = PathBucket(\n                f\"{cls.__name__}-{datetime.now().isoformat()}\",\n            )\n        case str() | Path():\n            bucket = PathBucket(bucket)\n        case bucket:\n            bucket = bucket  # noqa: PLW0127\n\n    space = _to_neps_space(space)\n    searcher = _to_neps_searcher(\n        space=space,\n        searcher=searcher,\n        max_cost_total=max_cost_total,\n        searcher_kwargs=searcher_kwargs,\n    )\n    working_dir = Path(working_dir)\n    if working_dir.exists() and overwrite:\n        logger.info(f\"Removing existing working directory {working_dir}\")\n        shutil.rmtree(working_dir)\n\n    return cls(\n        space=space,\n        bucket=bucket,\n        seed=seed,\n        loss_metric=metrics,\n        cost_metric=cost_metric,\n        optimizer=searcher,\n        working_dir=working_dir,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.ask","title":"<code>def ask()</code>","text":"<p>Ask the optimizer for a new config.</p> RETURNS DESCRIPTION <code>Trial[NEPSTrialInfo]</code> <p>The trial info for the new config.</p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@override\ndef ask(self) -&gt; Trial[NEPSTrialInfo]:\n    \"\"\"Ask the optimizer for a new config.\n\n    Returns:\n        The trial info for the new config.\n    \"\"\"\n    with self.optimizer.using_state(self.optimizer_state_file, self.serializer):\n        (\n            config_id,\n            config,\n            pipeline_directory,\n            previous_pipeline_directory,\n        ) = metahyper.api._sample_config(  # type: ignore\n            optimization_dir=self.working_dir,\n            sampler=self.optimizer,\n            serializer=self.serializer,\n            logger=logger,\n        )\n\n    match config:\n        case SearchSpace():\n            _config = config.hp_values()\n        case _:  # type: ignore\n            _config = {\n                k: v.value if isinstance(v, Parameter) else v\n                for k, v in config.items()  # type: ignore\n            }\n\n    info = NEPSTrialInfo(\n        name=str(config_id),\n        config=deepcopy(_config),\n        pipeline_directory=pipeline_directory,\n        previous_pipeline_directory=previous_pipeline_directory,\n    )\n\n    match self.cost_metric:\n        case None:\n            metrics = [self.loss_metric]\n        case cost_metric:\n            metrics = [self.loss_metric, cost_metric]\n\n    trial = Trial(\n        name=info.name,\n        config=info.config,\n        info=info,\n        seed=self.seed,\n        bucket=self.bucket,\n        metrics=metrics,\n    )\n    logger.debug(f\"Asked for trial {trial.name}\")\n    return trial\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.tell","title":"<code>def tell(report)</code>","text":"<p>Tell the optimizer the result of the sampled config.</p> PARAMETER  DESCRIPTION <code>report</code> <p>The report of the trial.</p> <p> TYPE: <code>Report[NEPSTrialInfo]</code> </p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@override\ndef tell(self, report: Trial.Report[NEPSTrialInfo]) -&gt; None:\n    \"\"\"Tell the optimizer the result of the sampled config.\n\n    Args:\n        report: The report of the trial.\n    \"\"\"\n    logger.debug(f\"Telling report for trial {report.trial.name}\")\n    info = report.info\n    assert info is not None\n\n    # Get a metric result\n    metric_result = first_true(\n        report.metric_values,\n        pred=lambda value: value.metric.name == self.loss_metric.name,\n        default=self.loss_metric.worst,\n    )\n\n    # Convert metric result to a minimization loss\n    neps_loss: float\n    if (_loss := metric_result.distance_to_optimal) is not None:\n        neps_loss = _loss\n    else:\n        neps_loss = metric_result.loss\n\n    result: dict[str, Any] = {\"loss\": neps_loss}\n    metadata: dict[str, Any] = {\"time_end\": report.time.end}\n\n    if self.cost_metric is not None:\n        cost_metric: Metric = self.cost_metric\n        _cost = first_true(\n            report.metric_values,\n            pred=lambda value: value.metric.name == cost_metric.name,\n            default=self.cost_metric.worst,\n        )\n        cost = _cost.value\n        result[\"cost\"] = cost\n\n        # If it's a budget aware optimizer\n        if self.optimizer.budget is not None:\n            with self.optimizer.using_state(\n                self.optimizer_state_file,\n                self.serializer,\n            ):\n                self.optimizer.used_budget += cost\n\n            metadata[\"budget\"] = {\n                \"max\": self.optimizer.budget,\n                \"used\": self.optimizer.used_budget,\n                \"eval_cost\": cost,\n                \"account_for_cost\": True,\n            }\n\n    # Dump results\n    self.serializer.dump(result, info.pipeline_directory / \"result\")\n\n    # Load and dump metadata\n    config_metadata = self.serializer.load(info.pipeline_directory / \"metadata\")\n    config_metadata.update(metadata)\n    self.serializer.dump(config_metadata, info.pipeline_directory / \"metadata\")\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/neps/#amltk.optimization.optimizers.neps.NEPSOptimizer.preferred_parser","title":"<code>def preferred_parser()</code>   <code>classmethod</code>","text":"<p>The preferred parser for this optimizer.</p> Source code in <code>src/amltk/optimization/optimizers/neps.py</code> <pre><code>@override\n@classmethod\ndef preferred_parser(cls) -&gt; NEPSPreferredParser:\n    \"\"\"The preferred parser for this optimizer.\"\"\"\n    # TODO: We might want a custom one for neps.SearchSpace, for now we will\n    # use config space but without conditions as NePs doesn't support conditionals\n    return partial(configspace_parser, conditionals=False)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/","title":"Optuna","text":"<p>Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.</p> <p>Requirements</p> <p>This requires <code>Optuna</code> which can be installed with:</p> <pre><code>pip install amltk[optuna]\n\n# Or directly\npip install optuna\n</code></pre> <p>We provide a thin wrapper called <code>OptunaOptimizer</code> from which you can integrate <code>Optuna</code> into your workflow.</p> <p>This uses an Optuna-like <code>search_space()</code> for its optimization.</p> <p>Users should report results using <code>trial.success()</code> with either <code>cost=</code> or <code>values=</code> depending on any optimization directions given to the underyling optimizer created. Please see their documentation for more.</p> <p>Visit their documentation for what you can pass to <code>OptunaOptimizer.create()</code>, which is forward to <code>optun.create_study()</code>.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.optuna import OptunaOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Pipeline) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        return trial.success(accuracy=accuracy_score(y_test, y_pred))\n\n    return trial.fail()\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\naccuracy_metric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = OptunaOptimizer.create(space=pipeline, metrics=accuracy_metric, bucket=\"optuna-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\n</code></pre> <pre><code>                  status  trial_seed  ... time:kind time:unit\nname                                  ...                    \ntrial_number=0   success  1836121168  ...      wall   seconds\ntrial_number=1   success  1836121168  ...      wall   seconds\ntrial_number=2   success  1836121168  ...      wall   seconds\ntrial_number=3   success  1836121168  ...      wall   seconds\ntrial_number=4   success  1836121168  ...      wall   seconds\n...                  ...         ...  ...       ...       ...\ntrial_number=75  success  1836121168  ...      wall   seconds\ntrial_number=76  success  1836121168  ...      wall   seconds\ntrial_number=77  success  1836121168  ...      wall   seconds\ntrial_number=79  success  1836121168  ...      wall   seconds\ntrial_number=78  success  1836121168  ...      wall   seconds\n\n[80 rows x 19 columns]\n</code></pre> <p>Some more documentation</p> <p>Sorry!</p>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaParser","title":"<code>class OptunaParser</code>","text":"<p>         Bases: <code>Protocol</code></p> <p>A protocol for Optuna search space parser.</p>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaParser.__call__","title":"<code>def __call__(node, *, flat=False, delim=':')</code>","text":"<p>See <code>optuna_parser</code>.</p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>def __call__(\n    self,\n    node: Node,\n    *,\n    flat: bool = False,\n    delim: str = \":\",\n) -&gt; OptunaSearchSpace:\n    \"\"\"See [`optuna_parser`][amltk.pipeline.parsers.optuna.parser].\"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer","title":"<code>class OptunaOptimizer(*, study, metrics, bucket=None, seed=None, space)</code>","text":"<p>         Bases: <code>Optimizer[Trial]</code></p> <p>An optimizer that uses Optuna to optimize a search space.</p> PARAMETER  DESCRIPTION <code>study</code> <p>The Optuna Study to use.</p> <p> TYPE: <code>Study</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>Defines the current search space.</p> <p> TYPE: <code>OptunaSearchSpace</code> </p> <code>seed</code> <p>The seed to use for the sampler and trials.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@override\ndef __init__(\n    self,\n    *,\n    study: Study,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | None = None,\n    seed: Seed | None = None,\n    space: OptunaSearchSpace,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        study: The Optuna Study to use.\n        metrics: The metrics to optimize.\n        bucket: The bucket given to trials generated by this optimizer.\n        space: Defines the current search space.\n        seed: The seed to use for the sampler and trials.\n    \"\"\"\n    # Verify the study has the same directions as the metrics\n    match metrics:\n        case Metric(minimize=minimize):\n            _dir = StudyDirection.MINIMIZE if minimize else StudyDirection.MAXIMIZE\n            if study.direction != _dir:\n                raise ValueError(\n                    f\"The study direction is {_dir}, but the metric minimize is \"\n                    f\"{minimize}.\",\n                )\n        case metrics:\n            _dirs = [\n                StudyDirection.MINIMIZE if m.minimize else StudyDirection.MAXIMIZE\n                for m in metrics\n            ]\n            if study.directions != _dirs:\n                raise ValueError(\n                    f\"The study directions are {_dirs}, but the metrics minimize \"\n                    f\"are {[m.minimize for m in metrics]}.\",\n                )\n\n    metrics = [metrics] if isinstance(metrics, Metric) else metrics\n    super().__init__(bucket=bucket, metrics=metrics)\n    self.seed = amltk.randomness.as_int(seed)\n    self.study = study\n    self.metrics = metrics\n    self.space = space\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.create","title":"<code>def create(*, space, metrics, bucket=None, sampler=None, seed=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a new Optuna optimizer. For more information, check Optuna     documentation     here.</p> PARAMETER  DESCRIPTION <code>space</code> <p>Defines the current search space.</p> <p> TYPE: <code>OptunaSearchSpace | Node</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | str | Path | None</code> DEFAULT: <code>None</code> </p> <code>sampler</code> <p>The sampler to use. Default is to use:</p> <ul> <li>Single metric: TPESampler</li> <li>Multiple metrics: NSGAIISampler</li> </ul> <p> TYPE: <code>BaseSampler | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed to use for the sampler and trials.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments to pass to <code>optuna.create_study</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The newly created optimizer.</p> <p> TYPE: <code>Self</code> </p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    *,\n    space: OptunaSearchSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    sampler: BaseSampler | None = None,\n    seed: Seed | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a new Optuna optimizer. For more information, check Optuna\n        documentation\n        [here](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html#).\n\n    Args:\n        space: Defines the current search space.\n        metrics: The metrics to optimize.\n        bucket: The bucket given to trials generated by this optimizer.\n        sampler: The sampler to use. Default is to use:\n\n            * Single metric: [TPESampler][optuna.samplers.TPESampler]\n            * Multiple metrics: [NSGAIISampler][optuna.samplers.NSGAIISampler]\n\n        seed: The seed to use for the sampler and trials.\n\n        **kwargs: Additional arguments to pass to\n            [`optuna.create_study`][optuna.create_study].\n\n    Returns:\n        Self: The newly created optimizer.\n    \"\"\"\n    if \"direction\" in kwargs:\n        raise ValueError(\n            \"The direction should be provided through the 'metrics' argument.\",\n        )\n\n    if isinstance(space, Node):\n        space = space.search_space(parser=cls.preferred_parser())\n\n    match bucket:\n        case None:\n            bucket = PathBucket(\n                f\"{cls.__name__}-{datetime.now().isoformat()}\",\n            )\n        case str() | Path():\n            bucket = PathBucket(bucket)\n        case bucket:\n            bucket = bucket  # noqa: PLW0127\n\n    match metrics:\n        case Metric(minimize=minimize):\n            direction = (\n                StudyDirection.MINIMIZE if minimize else StudyDirection.MAXIMIZE\n            )\n            study = optuna.create_study(direction=direction, **kwargs)\n        case metrics:\n            directions = [\n                StudyDirection.MINIMIZE if m.minimize else StudyDirection.MAXIMIZE\n                for m in metrics\n            ]\n            study = optuna.create_study(directions=directions, **kwargs)\n\n    if sampler is None:\n        sampler_seed = amltk.randomness.as_int(seed)\n        match metrics:\n            case Metric():\n                sampler = TPESampler(seed=sampler_seed)  # from `create_study()`\n            case metrics:\n                sampler = NSGAIISampler(seed=sampler_seed)  # from `create_study()`\n\n    return cls(study=study, metrics=metrics, space=space, bucket=bucket, seed=seed)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.ask","title":"<code>def ask()</code>","text":"<p>Ask the optimizer for a new config.</p> RETURNS DESCRIPTION <code>Trial[Trial]</code> <p>The trial info for the new config.</p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@override\ndef ask(self) -&gt; Trial[OptunaTrial]:\n    \"\"\"Ask the optimizer for a new config.\n\n    Returns:\n        The trial info for the new config.\n    \"\"\"\n    optuna_trial: optuna.Trial = self.study.ask(self.space)\n    config = optuna_trial.params\n    trial_number = optuna_trial.number\n    unique_name = f\"{trial_number=}\"\n    metrics = [self.metrics] if isinstance(self.metrics, Metric) else self.metrics\n    return Trial(\n        name=unique_name,\n        seed=self.seed,\n        config=config,\n        info=optuna_trial,\n        bucket=self.bucket,\n        metrics=metrics,\n    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/optuna/#amltk.optimization.optimizers.optuna.OptunaOptimizer.tell","title":"<code>def tell(report)</code>","text":"<p>Tell the optimizer the result of the sampled config.</p> PARAMETER  DESCRIPTION <code>report</code> <p>The report of the trial.</p> <p> TYPE: <code>Report[Trial]</code> </p> Source code in <code>src/amltk/optimization/optimizers/optuna.py</code> <pre><code>@override\ndef tell(self, report: Trial.Report[OptunaTrial]) -&gt; None:\n    \"\"\"Tell the optimizer the result of the sampled config.\n\n    Args:\n        report: The report of the trial.\n    \"\"\"\n    trial = report.trial.info\n    assert trial is not None\n\n    match report.status:\n        case Trial.Status.CRASHED | Trial.Status.UNKNOWN | Trial.Status.FAIL:\n            # NOTE: Can't tell any values if the trial crashed or failed\n            self.study.tell(trial=trial, state=TrialState.FAIL)\n        case Trial.Status.SUCCESS:\n            match self.metrics:\n                case [metric]:\n                    metric_value: Metric.Value = first_true(\n                        report.metric_values,\n                        pred=lambda m: m.metric == metric,\n                        default=metric.worst,\n                    )\n                    self.study.tell(\n                        trial=trial,\n                        state=TrialState.COMPLETE,\n                        values=metric_value.value,\n                    )\n                case metrics:\n                    # NOTE: We need to make sure that there sorted in the order\n                    # that Optuna expects, with any missing metrics filled in\n                    _lookup = {v.metric.name: v for v in report.metric_values}\n                    values = [\n                        _lookup.get(metric.name, metric.worst).value\n                        for metric in metrics\n                    ]\n                    self.study.tell(\n                        trial=trial,\n                        state=TrialState.COMPLETE,\n                        values=values,\n                    )\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/","title":"Smac","text":"<p>The <code>SMACOptimizer</code>, is a wrapper around the <code>smac</code> optimizer.</p> <p>Requirements</p> <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[smac]\n\n# Or directly\npip install smac\n</code></pre> <p>This uses <code>ConfigSpace</code> as its <code>search_space()</code> to optimize.</p> <p>Users should report results using <code>trial.success()</code>.</p> <p>Visit their documentation for what you can pass to <code>SMACOptimizer.create()</code>.</p> <p>The below example shows how you can use SMAC to optimize an sklearn pipeline.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component, Node\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Node) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        return trial.success(accuracy=accuracy)\n\n    return trial.fail()\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100), \"max_samples\": (0.1, 0.9)})\n\nmetric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = SMACOptimizer.create(space=pipeline, metrics=metric, bucket=\"smac-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\n</code></pre> <pre><code>                                                     status  ...  time:unit\nname                                                         ...           \nconfig_id=1_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=3_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=2_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=5_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=4_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=7_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=6_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=9_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=8_seed=366778667_budget=None_instance...  success  ...    seconds\nconfig_id=11_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=10_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=13_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=12_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=15_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=14_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=17_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=16_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=19_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=18_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=20_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=21_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=22_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=23_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=24_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=25_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=26_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=28_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=27_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=29_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=30_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=31_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=32_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=33_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=34_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=35_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=36_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=37_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=38_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=39_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=40_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=41_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=42_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=43_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=44_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=45_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=46_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=47_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=48_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=49_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=50_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=52_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=51_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=53_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=55_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=54_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=56_seed=366778667_budget=None_instanc...  success  ...    seconds\nconfig_id=57_seed=366778667_budget=None_instanc...  success  ...    seconds\n\n[57 rows x 20 columns]\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer","title":"<code>class SMACOptimizer(*, facade, bucket=None, metrics, fidelities=None)</code>","text":"<p>         Bases: <code>Optimizer[TrialInfo]</code></p> <p>An optimizer that uses SMAC to optimize a config space.</p> PARAMETER  DESCRIPTION <code>facade</code> <p>The SMAC facade to use.</p> <p> TYPE: <code>AbstractFacade</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | None</code> DEFAULT: <code>None</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>fidelities</code> <p>The fidelities to use, if any.</p> <p> TYPE: <code>Mapping[str, FidT] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>def __init__(\n    self,\n    *,\n    facade: AbstractFacade,\n    bucket: PathBucket | None = None,\n    metrics: Metric | Sequence[Metric],\n    fidelities: Mapping[str, FidT] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the optimizer.\n\n    Args:\n        facade: The SMAC facade to use.\n        bucket: The bucket given to trials generated by this optimizer.\n        metrics: The metrics to optimize.\n        fidelities: The fidelities to use, if any.\n    \"\"\"\n    # We need to very that the scenario is correct incase user pass in\n    # their own facade construction\n    assert self.crash_cost(metrics) == facade.scenario.crash_cost\n\n    metrics = metrics if isinstance(metrics, Sequence) else [metrics]\n    super().__init__(metrics=metrics, bucket=bucket)\n    self.facade = facade\n    self.metrics = metrics\n    self.fidelities = fidelities\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.create","title":"<code>def create(*, space, metrics, bucket=None, deterministic=True, seed=None, fidelities=None, continue_from_last_run=False, logging_level=False)</code>   <code>classmethod</code>","text":"<p>Create a new SMAC optimizer using either the HPO facade or a mutli-fidelity facade.</p> PARAMETER  DESCRIPTION <code>space</code> <p>The config space to optimize.</p> <p> TYPE: <code>ConfigurationSpace | Node</code> </p> <code>metrics</code> <p>The metrics to optimize.</p> <p> TYPE: <code>Metric | Sequence[Metric]</code> </p> <code>bucket</code> <p>The bucket given to trials generated by this optimizer.</p> <p> TYPE: <code>PathBucket | str | Path | None</code> DEFAULT: <code>None</code> </p> <code>deterministic</code> <p>Whether the function your optimizing is deterministic, given a seed and config.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>seed</code> <p>The seed to use for the optimizer.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>fidelities</code> <p>The fidelities to use, if any.</p> <p> TYPE: <code>Mapping[str, FidT] | None</code> DEFAULT: <code>None</code> </p> <code>continue_from_last_run</code> <p>Whether to continue from a previous run.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>logging_level</code> <p>The logging level to use. This argument is passed forward to SMAC, use False to disable SMAC's handling of logging.</p> <p> TYPE: <code>int | Path | Literal[False] | None</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@classmethod\ndef create(\n    cls,\n    *,\n    space: ConfigurationSpace | Node,\n    metrics: Metric | Sequence[Metric],\n    bucket: PathBucket | str | Path | None = None,\n    deterministic: bool = True,\n    seed: Seed | None = None,\n    fidelities: Mapping[str, FidT] | None = None,\n    continue_from_last_run: bool = False,\n    logging_level: int | Path | Literal[False] | None = False,\n) -&gt; Self:\n    \"\"\"Create a new SMAC optimizer using either the HPO facade or\n    a mutli-fidelity facade.\n\n    Args:\n        space: The config space to optimize.\n        metrics: The metrics to optimize.\n        bucket: The bucket given to trials generated by this optimizer.\n        deterministic: Whether the function your optimizing is deterministic, given\n            a seed and config.\n        seed: The seed to use for the optimizer.\n        fidelities: The fidelities to use, if any.\n        continue_from_last_run: Whether to continue from a previous run.\n        logging_level: The logging level to use.\n            This argument is passed forward to SMAC, use False to disable\n            SMAC's handling of logging.\n    \"\"\"\n    seed = as_int(seed)\n    match bucket:\n        case None:\n            bucket = PathBucket(\n                f\"{cls.__name__}-{datetime.now().isoformat()}\",\n            )\n        case str() | Path():\n            bucket = PathBucket(bucket)\n        case bucket:\n            bucket = bucket  # noqa: PLW0127\n\n    # NOTE SMAC always minimizes! Hence we make it a minimization problem\n    metric_names: str | list[str]\n    if isinstance(metrics, Sequence):\n        metric_names = [metric.name for metric in metrics]\n    else:\n        metric_names = metrics.name\n\n    if isinstance(space, Node):\n        space = space.search_space(parser=cls.preferred_parser())\n\n    facade_cls: type[AbstractFacade]\n    if fidelities:\n        if len(fidelities) == 1:\n            v = next(iter(fidelities.values()))\n            min_budget, max_budget = v\n        else:\n            min_budget, max_budget = 1.0, 100.0\n\n        scenario = Scenario(\n            objectives=metric_names,\n            configspace=space,\n            output_directory=bucket.path / \"smac3_output\",\n            seed=seed,\n            min_budget=min_budget,\n            max_budget=max_budget,\n            crash_cost=cls.crash_cost(metrics),\n        )\n        facade_cls = MultiFidelityFacade\n    else:\n        scenario = Scenario(\n            configspace=space,\n            seed=seed,\n            output_directory=bucket.path / \"smac3_output\",\n            deterministic=deterministic,\n            objectives=metric_names,\n            crash_cost=cls.crash_cost(metrics),\n        )\n        facade_cls = HyperparameterOptimizationFacade\n\n    facade = facade_cls(\n        scenario=scenario,\n        target_function=\"dummy\",  # NOTE: https://github.com/automl/SMAC3/issues/946\n        overwrite=not continue_from_last_run,\n        logging_level=logging_level,\n        multi_objective_algorithm=facade_cls.get_multi_objective_algorithm(\n            scenario=scenario,\n        ),\n    )\n    return cls(facade=facade, fidelities=fidelities, bucket=bucket, metrics=metrics)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.ask","title":"<code>def ask()</code>","text":"<p>Ask the optimizer for a new config.</p> RETURNS DESCRIPTION <code>Trial[TrialInfo]</code> <p>The trial info for the new config.</p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@override\ndef ask(self) -&gt; Trial[SMACTrialInfo]:\n    \"\"\"Ask the optimizer for a new config.\n\n    Returns:\n        The trial info for the new config.\n    \"\"\"\n    smac_trial_info = self.facade.ask()\n    config = smac_trial_info.config\n    budget = smac_trial_info.budget\n    instance = smac_trial_info.instance\n    seed = smac_trial_info.seed\n\n    if self.fidelities and budget:\n        if len(self.fidelities) == 1:\n            k, _ = next(iter(self.fidelities.items()))\n            trial_fids = {k: budget}\n        else:\n            trial_fids = {\"budget\": budget}\n    else:\n        trial_fids = None\n\n    config_id = self.facade.runhistory.config_ids[config]\n    unique_name = f\"{config_id=}_{seed=}_{budget=}_{instance=}\"\n    trial: Trial[SMACTrialInfo] = Trial(\n        name=unique_name,\n        config=dict(config),\n        info=smac_trial_info,\n        seed=seed,\n        fidelities=trial_fids,\n        bucket=self.bucket,\n        metrics=self.metrics,\n    )\n    logger.debug(f\"Asked for trial {trial.name}\")\n    return trial\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.tell","title":"<code>def tell(report)</code>","text":"<p>Tell the optimizer the result of the sampled config.</p> PARAMETER  DESCRIPTION <code>report</code> <p>The report of the trial.</p> <p> TYPE: <code>Report[TrialInfo]</code> </p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@override\ndef tell(self, report: Trial.Report[SMACTrialInfo]) -&gt; None:\n    \"\"\"Tell the optimizer the result of the sampled config.\n\n    Args:\n        report: The report of the trial.\n    \"\"\"\n    assert report.trial.info is not None\n\n    cost: float | list[float]\n    match self.metrics:\n        case [metric]:  # Single obj\n            val: Metric.Value = first_true(\n                report.metric_values,\n                pred=lambda m: m.metric == metric,\n                default=metric.worst,\n            )\n            cost = self.cost(val)\n        case metrics:\n            # NOTE: We need to make sure that there sorted in the order\n            # that SMAC expects, with any missing metrics filled in\n            _lookup = {v.metric.name: v for v in report.metric_values}\n            cost = [\n                self.cost(_lookup.get(metric.name, metric.worst))\n                for metric in metrics\n            ]\n\n    logger.debug(f\"Telling report for trial {report.trial.name}\")\n\n    # If we're successful, get the cost and times and report them\n    params: dict[str, Any]\n    match report.status:\n        case Trial.Status.SUCCESS:\n            params = {\n                \"time\": report.time.duration,\n                \"starttime\": report.time.start,\n                \"endtime\": report.time.end,\n                \"cost\": cost,\n                \"status\": StatusType.SUCCESS,\n            }\n        case Trial.Status.FAIL:\n            params = {\n                \"time\": report.time.duration,\n                \"starttime\": report.time.start,\n                \"endtime\": report.time.end,\n                \"cost\": cost,\n                \"status\": StatusType.CRASHED,\n            }\n        case Trial.Status.CRASHED | Trial.Status.UNKNOWN:\n            params = {\n                \"cost\": cost,\n                \"status\": StatusType.CRASHED,\n            }\n\n    match report.exception:\n        case None:\n            pass\n        case MemoryLimitException():\n            params[\"status\"] = StatusType.MEMORYOUT\n            params[\"additional_info\"] = {\n                \"exception\": str(report.exception),\n                \"traceback\": report.traceback,\n            }\n        case TimeoutException():\n            params[\"status\"] = StatusType.TIMEOUT\n            params[\"additional_info\"] = {\n                \"exception\": str(report.exception),\n                \"traceback\": report.traceback,\n            }\n        case _:\n            params[\"additional_info\"] = {\n                \"exception\": str(report.exception),\n                \"traceback\": report.traceback,\n            }\n\n    self.facade.tell(report.trial.info, value=SMACTrialValue(**params), save=True)\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.preferred_parser","title":"<code>def preferred_parser()</code>   <code>classmethod</code>","text":"<p>The preferred parser for this optimizer.</p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@override\n@classmethod\ndef preferred_parser(cls) -&gt; Literal[\"configspace\"]:\n    \"\"\"The preferred parser for this optimizer.\"\"\"\n    return \"configspace\"\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.crash_cost","title":"<code>def crash_cost(metric)</code>   <code>classmethod</code>","text":"<p>Get the crash cost for a metric for SMAC.</p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@classmethod\ndef crash_cost(cls, metric: Metric | Sequence[Metric]) -&gt; float | list[float]:\n    \"\"\"Get the crash cost for a metric for SMAC.\"\"\"\n    match metric:\n        case Metric(bounds=(lower, upper)):  # Bounded metrics\n            return abs(upper - lower)\n        case Metric():  # Unbounded metric\n            return np.inf\n        case metrics:\n            return [cls.crash_cost(m) for m in metrics]\n</code></pre>"},{"location":"api/amltk/optimization/optimizers/smac/#amltk.optimization.optimizers.smac.SMACOptimizer.cost","title":"<code>def cost(value)</code>   <code>classmethod</code>","text":"<p>Get the cost for a metric value for SMAC.</p> Source code in <code>src/amltk/optimization/optimizers/smac.py</code> <pre><code>@classmethod\ndef cost(cls, value: Metric.Value) -&gt; float:\n    \"\"\"Get the cost for a metric value for SMAC.\"\"\"\n    match value.distance_to_optimal:\n        case None:  # If we can't compute the distance, use the loss\n            return value.loss\n        case distance:  # If we can compute the distance, use that\n            return distance\n</code></pre>"},{"location":"api/amltk/pipeline/components/","title":"Components","text":"<p>You can use the various different node types to build a pipeline.</p> <p>You can connect these nodes together using either the constructors explicitly, as shown in the examples. We also provide some index operators:</p> <ul> <li><code>&gt;&gt;</code> - Connect nodes together to form a <code>Sequential</code></li> <li><code>&amp;</code> - Connect nodes together to form a <code>Join</code></li> <li><code>|</code> - Connect nodes together to form a <code>Choice</code></li> </ul> <p>There is also another short-hand that you may find useful to know:</p> <ul> <li><code>{comp1, comp2, comp3}</code> - This will automatically be converted into a     <code>Choice</code> between the given components.</li> <li><code>(comp1, comp2, comp3)</code> - This will automatically be converted into a     <code>Join</code> between the given components.</li> <li><code>[comp1, comp2, comp3]</code> - This will automatically be converted into a     <code>Sequential</code> between the given components.</li> </ul> <p>For each of these components we will show examples using the <code>\"sklearn\"</code> builder</p> <p>The components are:</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components--component","title":"Component","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Component</code> of the pipeline with a possible item and no children.</p> <p>This is the basic building block of most pipelines, it accepts as it's <code>item=</code> some function that will be called with <code>build_item()</code> to build that one part of the pipeline.</p> <p>When <code>build_item()</code> is called, The <code>.config</code> on this node will be passed to the function to build the item.</p> <p>A common pattern is to use a <code>Component</code> to wrap a constructor, specifying the <code>space=</code> and <code>config=</code> to be used when building the item.</p> <pre><code>from amltk.pipeline import Component\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = Component(\n    RandomForestClassifier,\n    config={\"max_depth\": 3},\n    space={\"n_estimators\": (10, 100)}\n)\n\nconfig = {\"n_estimators\": 50}  # Sample from some space or something\nconfigured_rf = rf.configure(config)\n\nestimator = configured_rf.build_item()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class RandomForestClassifier(...) \u2502\n\u2502 config {'max_depth': 3}                  \u2502\n\u2502 space  {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier<pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre> </p> <p>Whenever some other node sees a function/constructor, i.e. <code>RandomForestClassifier</code>, this will automatically be converted into a <code>Component</code>.</p> <pre><code>from amltk.pipeline import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(RandomForestClassifier, name=\"my_pipeline\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The default <code>.name</code> of a component is the name of the class/function that it will use. You can explicitly set the <code>name=</code> if you want to when constructing the component.</p> <p>Like all <code>Node</code>s, a <code>Component</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    item: Callable[..., Item],\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components--sequential","title":"Sequential","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Sequential</code> set of operations in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act one after another, feeding the output of one into the next.</p> <pre><code>from amltk.pipeline import Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)}),\n    name=\"my_pipeline\"\n)\n\nspace = pipeline.search_space(\"configspace\")\n\nconfiguration = space.sample_configuration()\n\nconfigured_pipeline = pipeline.configure(configuration)\n\nsklearn_pipeline = pipeline.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_pipeline:RandomForestClassifier:n_estimators, Type: UniformInteger, \nRange: [10, 100], Default: 55\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'my_pipeline:RandomForestClassifier:n_estimators': 20,\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                 \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                 \u2502\n\u2502                      \u2193                       \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item   class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 config {'n_estimators': 20}              \u2502 \u2502\n\u2502 \u2502 space  {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier', RandomForestClassifier())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier', RandomForestClassifier())])</pre>PCA<pre>PCA(n_components=3)</pre>RandomForestClassifier<pre>RandomForestClassifier()</pre> </p> <p>You may also just chain together nodes using an infix operator <code>&gt;&gt;</code> if you prefer:</p> <pre><code>from amltk.pipeline import Join, Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = (\n    Sequential(name=\"my_pipeline\")\n    &gt;&gt; PCA(n_components=3)\n    &gt;&gt; Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees a list, i.e. <code>[comp1, comp2, comp3]</code>, this will automatically be converted into a <code>Sequential</code>.</p> <pre><code>from amltk.pipeline import Choice\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\npipeline_choice = Choice(\n    [SimpleImputer(), RandomForestClassifier()],\n    [StandardScaler(), MLPClassifier()],\n    name=\"pipeline_choice\"\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(pipeline_choice) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(Seq-Gw3abSAq) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(Seq-yxVSK4JH) \u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u256e          \u2502 \u2502 \u256d\u2500 Fixed(StandardScaler) \u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer()   \u2502          \u2502 \u2502 \u2502 item StandardScaler()   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f          \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502                  \u2193                  \u2502 \u2502              \u2193              \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(MLPClassifier) \u2500\u256e  \u2502 \u2502\n\u2502 \u2502 \u2502 item RandomForestClassifier()   \u2502 \u2502 \u2502 \u2502 item MLPClassifier()   \u2502  \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Sequential</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n\n    # Perhaps we need to do a deeper check on this...\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise DuplicateNamesError(self)\n\n    if name is None:\n        name = f\"Seq-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components--choice","title":"Choice","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Choice</code> between different subcomponents.</p> <p>This indicates that a choice should be made between the different children in <code>.nodes</code>, usually done when you <code>configure()</code> with some <code>config</code> from a <code>search_space()</code>.</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = Choice(rf, mlp, name=\"estimator\")\n\nspace = estimator_choice.search_space(\"configspace\")\n\nconfig = space.sample_configuration()\n\nconfigured_choice = estimator_choice.configure(config)\n\nchosen_estimator = configured_choice.chosen()\n\nestimator = chosen_estimator.build_item()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    estimator:MLPClassifier:activation, Type: Categorical, Choices: {logistic, \nrelu, tanh}, Default: logistic\n    estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: \n[10, 100], Default: 55\n    estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, \nRandomForestClassifier}, Default: MLPClassifier\n  Conditions:\n    estimator:MLPClassifier:activation | estimator:__choice__ == 'MLPClassifier'\n    estimator:RandomForestClassifier:n_estimators | estimator:__choice__ == \n'RandomForestClassifier'\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'estimator:MLPClassifier:activation': 'logistic',\n  'estimator:__choice__': 'MLPClassifier',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {'__choice__': 'MLPClassifier'}                                       \u2502\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e \u2502\n\u2502 \u2502 item   class MLPClassifier(...)   \u2502 \u2502 item  class                        \u2502 \u2502\n\u2502 \u2502 config {'activation': 'logistic'} \u2502 \u2502       RandomForestClassifier(...)  \u2502 \u2502\n\u2502 \u2502 space  {                          \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502 \u2502\n\u2502 \u2502            'activation': [        \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u2502                'logistic',        \u2502                                        \u2502\n\u2502 \u2502                'relu',            \u2502                                        \u2502\n\u2502 \u2502                'tanh'             \u2502                                        \u2502\n\u2502 \u2502            ]                      \u2502                                        \u2502\n\u2502 \u2502        }                          \u2502                                        \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MLPClassifier(...)                     \u2502\n\u2502 config {'activation': 'logistic'}                   \u2502\n\u2502 space  {'activation': ['logistic', 'relu', 'tanh']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>MLPClassifier(activation='logistic')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifier<pre>MLPClassifier(activation='logistic')</pre> </p> <p>You may also just add nodes to a <code>Choice</code> using an infix operator <code>|</code> if you prefer:</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = (\n    Choice(name=\"estimator\") | mlp | rf\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees a set, i.e. <code>{comp1, comp2, comp3}</code>, this will automatically be converted into a <code>Choice</code>.</p> <pre><code>from amltk.pipeline import Choice, Component, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.impute import SimpleImputer\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\npipeline = Sequential(\n    SimpleImputer(fill_value=0),\n    {mlp, rf},\n    name=\"my_pipeline\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                                         \u2502\n\u2502 \u2502 item SimpleImputer(fill_value=0) \u2502                                         \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                         \u2502\n\u2502                                      \u2193                                       \u2502\n\u2502 \u256d\u2500 Choice(Choice-Qyqnn9rl) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifie\u2500\u256e  \u2502 \u2502\n\u2502 \u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(..\u2026 \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502           'activation': [      \u2502 \u2502 space {                          \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'logistic',      \u2502 \u2502           'n_estimators': (      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'relu',          \u2502 \u2502               10,                \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'tanh'           \u2502 \u2502               100                \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502           ]                    \u2502 \u2502           )                      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502       }                        \u2502 \u2502       }                          \u2502  \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Choice</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> <p>Order of nodes</p> <p>The given nodes of a choice are always ordered according to their name, so indexing <code>choice.nodes</code> may not be reliable if modifying the choice dynamically.</p> <p>Please use <code>choice[\"name\"]</code> to access the nodes instead.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes: tuple[Node, ...] = tuple(\n        sorted((as_node(n) for n in nodes), key=lambda n: n.name),\n    )\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes as we can not generate a __choice__ for {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Choice-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components--split","title":"Split","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Split</code> of data in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in parallel but on different subsets of data.</p> <pre><code>from amltk.pipeline import Component, Split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(drop=\"first\"),\n]\nnumerical_pipeline = Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})\n\npreprocessor = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numerical\": numerical_pipeline,\n    },\n    config={\n        # This is how you would configure the split for the sklearn builder in particular\n        \"categories\": make_column_selector(dtype_include=\"category\"),\n        \"numerical\": make_column_selector(dtype_exclude=\"category\"),\n    },\n    name=\"my_split\"\n)\n\nspace = preprocessor.search_space(\"configspace\")\n\nconfiguration = space.sample_configuration()\n\nconfigured_preprocessor = preprocessor.configure(configuration)\n\nbuilt_preprocessor = configured_preprocessor.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f249933f0a0&gt;,                                                      \u2502\n\u2502            'numerical':                                                      \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f249933cf10&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502 space {                        \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           'strategy': [        \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502               'mean',          \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'median'         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502           ]                    \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502       }                        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_split:numerical:SimpleImputer:strategy, Type: Categorical, Choices: \n{mean, median}, Default: mean\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'my_split:numerical:SimpleImputer:strategy': 'mean',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f249933f0a0&gt;,                                                      \u2502\n\u2502            'numerical':                                                      \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f249933cf10&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item   class                   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502        SimpleImputer(...)      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502 config {'strategy': 'mean'}    \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502 space  {                       \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502            'strategy': [       \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502                'mean',         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502                'median'        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502            ]                   \u2502 \u2502 \u2502\n\u2502                                       \u2502 \u2502        }                       \u2502 \u2502 \u2502\n\u2502                                       \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('my_split',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933f0a0&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933cf10&gt;)]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_split',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933f0a0&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933cf10&gt;)]))])</pre>my_split: ColumnTransformer<pre>ColumnTransformer(transformers=[('categories',\n                                 Pipeline(steps=[('SimpleImputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('OneHotEncoder',\n                                                  OneHotEncoder(drop='first'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933f0a0&gt;),\n                                ('SimpleImputer', SimpleImputer(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933cf10&gt;)])</pre>categories<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933f0a0&gt;</pre>SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre>OneHotEncoder<pre>OneHotEncoder(drop='first')</pre>SimpleImputer<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f249933cf10&gt;</pre>SimpleImputer<pre>SimpleImputer()</pre> </p> <p>The split is a slight oddity when compared to the other kinds of components in that it allows a <code>dict</code> as it's first argument, where the keys are the names of the different paths through which data will go and the values are the actual nodes that will receive the data.</p> <p>If nodes are passed in as they are for all other components, usually the name of the first node will be important for any builder trying to make sense of how to use the <code>Split</code></p> <p>Like all <code>Node</code>s, a <code>Split</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike | dict[str, Node | NodeLike],\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    if any(isinstance(n, dict) for n in nodes):\n        if len(nodes) &gt; 1:\n            raise ValueError(\n                \"Can't handle multiple nodes with a dictionary as a node.\\n\"\n                f\"{nodes=}\",\n            )\n        _node = nodes[0]\n        assert isinstance(_node, dict)\n\n        def _construct(key: str, value: Node | NodeLike) -&gt; Node:\n            match value:\n                case list():\n                    return Sequential(*value, name=key)\n                case set() | tuple():\n                    return as_node(value, name=key)\n                case _:\n                    return Sequential(value, name=key)\n\n        _nodes = tuple(_construct(key, value) for key, value in _node.items())\n    else:\n        _nodes = tuple(as_node(n) for n in nodes)\n\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes they do not all contain unique names, {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Split-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components--join","title":"Join","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p><code>Join</code> together different parts of the pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in tandem with one another, for example, concatenating the outputs of the various members of the <code>Join</code>.</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\njoin = Join(pca, kbest, name=\"my_feature_union\")\n\nspace = join.search_space(\"configspace\")\n\npipeline = join.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Join(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_feature_union:PCA:n_components, Type: UniformInteger, Range: [1, 3], \nDefault: 2\n    my_feature_union:SelectKBest:k, Type: UniformInteger, Range: [1, 3], \nDefault: 2\n\n</code>\n</pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA()), ('SelectKBest', SelectKBest())])</pre>PCAPCA<pre>PCA()</pre>SelectKBestSelectKBest<pre>SelectKBest()</pre> </p> <p>You may also just join together nodes using an infix operator <code>&amp;</code> if you prefer:</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\n# Can not parametrize or name the join\njoin = pca &amp; kbest\n\n# With a parametrized join\njoin = (\n    Join(name=\"my_feature_union\") &amp; pca &amp; kbest\n)\nitem = join.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Join(Join-uUFXU9w2) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA()), ('SelectKBest', SelectKBest())])</pre>PCAPCA<pre>PCA()</pre>SelectKBestSelectKBest<pre>SelectKBest()</pre> </p> <p>Whenever some other node sees a tuple, i.e. <code>(comp1, comp2, comp3)</code>, this will automatically be converted into a <code>Join</code>.</p> <pre><code>from amltk.pipeline import Sequential, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\n# Can not parametrize or name the join\njoin = Sequential(\n    (pca, kbest),\n    RandomForestClassifier(n_estimators=5),\n    name=\"my_feature_union\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Join(Join-ma0kewvE) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                    \u2193                                    \u2502\n\u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                         \u2502\n\u2502 \u2502 item RandomForestClassifier(n_estimators=5) \u2502                         \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Join</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes they do not all contain unique names, {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Join-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components--fixed","title":"Fixed","text":"<p>         Bases: <code>Node[Item, None]</code></p> <p>A <code>Fixed</code> part of the pipeline that represents something that can not be configured and used directly as is.</p> <p>It consists of an <code>.item</code> that is fixed, non-configurable and non-searchable. It also has no children.</p> <p>This is useful for representing parts of the pipeline that are fixed, for example if you have a pipeline that is a <code>Sequential</code> of nodes, but you want to fix the first component to be a <code>PCA</code> with <code>n_components=3</code>, you can use a <code>Fixed</code> to represent that.</p> <pre><code>from amltk.pipeline import Component, Fixed, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\npca = Fixed(PCA(n_components=3))\n\npipeline = Sequential(pca, rf, name=\"my_pipeline\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees an instance of something, i.e. something that can't be called, this will automatically be converted into a <code>Fixed</code>.</p> <pre><code>from amltk.pipeline import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\npipeline = Sequential(\n    PCA(n_components=3),\n    RandomForestClassifier(n_estimators=50),\n    name=\"my_pipeline\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                     \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                     \u2502\n\u2502                        \u2193                         \u2502\n\u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item RandomForestClassifier(n_estimators=50) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The default <code>.name</code> of a component is the class name of the item that it will use. You can explicitly set the <code>name=</code> if you want to when constructing the component.</p> <p>A <code>Fixed</code> accepts only an explicit <code>name=</code>, <code>item=</code>, <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    item: Item,\n    *,\n    name: str | None = None,\n    config: None = None,\n    space: None = None,\n    fidelities: None = None,\n    config_transform: None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components--searchable","title":"Searchable","text":"<p>         Bases: <code>Node[None, Space]</code></p> <p>A <code>Searchable</code> node of the pipeline which just represents a search space, no item attached.</p> <p>While not usually applicable to pipelines you want to build, this component is useful for creating a search space, especially if the real pipeline you want to optimize can not be built directly. For example, if you are optimize a script, you may wish to use a <code>Searchable</code> to represent the search space of that script.</p> <pre><code>from amltk.pipeline import Searchable\n\nscript_space = Searchable({\"mode\": [\"orange\", \"blue\", \"red\"], \"n\": (10, 100)})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Searchable(Searchable-q1lKFCiL) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 space {'mode': ['orange', 'blue', 'red'], 'n': (10, 100)} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A <code>Searchable</code> explicitly does not allow for <code>item=</code> to be set, nor can it have any children. A <code>Searchable</code> accepts an explicit <code>name=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    space: Space | None = None,\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    if name is None:\n        name = f\"Searchable-{randuid(8)}\"\n\n    super().__init__(\n        name=name,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join","title":"<code>class Join(*nodes, name=None, item=None, config=None, space=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p><code>Join</code> together different parts of the pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in tandem with one another, for example, concatenating the outputs of the various members of the <code>Join</code>.</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\njoin = Join(pca, kbest, name=\"my_feature_union\")\n\nspace = join.search_space(\"configspace\")\n\npipeline = join.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Join(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_feature_union:PCA:n_components, Type: UniformInteger, Range: [1, 3], \nDefault: 2\n    my_feature_union:SelectKBest:k, Type: UniformInteger, Range: [1, 3], \nDefault: 2\n\n</code>\n</pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA()), ('SelectKBest', SelectKBest())])</pre>PCAPCA<pre>PCA()</pre>SelectKBestSelectKBest<pre>SelectKBest()</pre> </p> <p>You may also just join together nodes using an infix operator <code>&amp;</code> if you prefer:</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\n# Can not parametrize or name the join\njoin = pca &amp; kbest\n\n# With a parametrized join\njoin = (\n    Join(name=\"my_feature_union\") &amp; pca &amp; kbest\n)\nitem = join.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Join(Join-XMWzqEcB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA()), ('SelectKBest', SelectKBest())])</pre>PCAPCA<pre>PCA()</pre>SelectKBestSelectKBest<pre>SelectKBest()</pre> </p> <p>Whenever some other node sees a tuple, i.e. <code>(comp1, comp2, comp3)</code>, this will automatically be converted into a <code>Join</code>.</p> <pre><code>from amltk.pipeline import Sequential, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\n# Can not parametrize or name the join\njoin = Sequential(\n    (pca, kbest),\n    RandomForestClassifier(n_estimators=5),\n    name=\"my_feature_union\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Join(Join-GETzIKZw) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                    \u2193                                    \u2502\n\u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                         \u2502\n\u2502 \u2502 item RandomForestClassifier(n_estimators=5) \u2502                         \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Join</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes they do not all contain unique names, {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Join-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Join.nodes","title":"<code>nodes: tuple[Node, ...]</code>   <code>attr</code>","text":"<p>The nodes that this node leads to.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice","title":"<code>class Choice(*nodes, name=None, item=None, config=None, space=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Choice</code> between different subcomponents.</p> <p>This indicates that a choice should be made between the different children in <code>.nodes</code>, usually done when you <code>configure()</code> with some <code>config</code> from a <code>search_space()</code>.</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = Choice(rf, mlp, name=\"estimator\")\n\nspace = estimator_choice.search_space(\"configspace\")\n\nconfig = space.sample_configuration()\n\nconfigured_choice = estimator_choice.configure(config)\n\nchosen_estimator = configured_choice.chosen()\n\nestimator = chosen_estimator.build_item()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    estimator:MLPClassifier:activation, Type: Categorical, Choices: {logistic, \nrelu, tanh}, Default: logistic\n    estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: \n[10, 100], Default: 55\n    estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, \nRandomForestClassifier}, Default: MLPClassifier\n  Conditions:\n    estimator:MLPClassifier:activation | estimator:__choice__ == 'MLPClassifier'\n    estimator:RandomForestClassifier:n_estimators | estimator:__choice__ == \n'RandomForestClassifier'\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'estimator:RandomForestClassifier:n_estimators': 33,\n  'estimator:__choice__': 'RandomForestClassifier',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {'__choice__': 'RandomForestClassifier'}                              \u2502\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item   class                       \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502        RandomForestClassifier(...) \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 config {'n_estimators': 33}        \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2502 space  {'n_estimators': (10, 100)} \u2502    \u2502\n\u2502 \u2502               'relu',          \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class RandomForestClassifier(...) \u2502\n\u2502 config {'n_estimators': 33}              \u2502\n\u2502 space  {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>RandomForestClassifier(n_estimators=33)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier<pre>RandomForestClassifier(n_estimators=33)</pre> </p> <p>You may also just add nodes to a <code>Choice</code> using an infix operator <code>|</code> if you prefer:</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = (\n    Choice(name=\"estimator\") | mlp | rf\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees a set, i.e. <code>{comp1, comp2, comp3}</code>, this will automatically be converted into a <code>Choice</code>.</p> <pre><code>from amltk.pipeline import Choice, Component, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.impute import SimpleImputer\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\npipeline = Sequential(\n    SimpleImputer(fill_value=0),\n    {mlp, rf},\n    name=\"my_pipeline\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                                         \u2502\n\u2502 \u2502 item SimpleImputer(fill_value=0) \u2502                                         \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                         \u2502\n\u2502                                      \u2193                                       \u2502\n\u2502 \u256d\u2500 Choice(Choice-GRlVoAzw) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifie\u2500\u256e  \u2502 \u2502\n\u2502 \u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(..\u2026 \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502           'activation': [      \u2502 \u2502 space {                          \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'logistic',      \u2502 \u2502           'n_estimators': (      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'relu',          \u2502 \u2502               10,                \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'tanh'           \u2502 \u2502               100                \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502           ]                    \u2502 \u2502           )                      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502       }                        \u2502 \u2502       }                          \u2502  \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Choice</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> <p>Order of nodes</p> <p>The given nodes of a choice are always ordered according to their name, so indexing <code>choice.nodes</code> may not be reliable if modifying the choice dynamically.</p> <p>Please use <code>choice[\"name\"]</code> to access the nodes instead.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes: tuple[Node, ...] = tuple(\n        sorted((as_node(n) for n in nodes), key=lambda n: n.name),\n    )\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes as we can not generate a __choice__ for {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Choice-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.nodes","title":"<code>nodes: tuple[Node, ...]</code>   <code>attr</code>","text":"<p>The nodes that this node leads to.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.chosen","title":"<code>def chosen()</code>","text":"<p>The chosen branch.</p> RETURNS DESCRIPTION <code>Node</code> <p>The chosen branch</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def chosen(self) -&gt; Node:\n    \"\"\"The chosen branch.\n\n    Returns:\n        The chosen branch\n    \"\"\"\n    match self.config:\n        case {\"__choice__\": choice}:\n            chosen = first_true(\n                self.nodes,\n                pred=lambda node: node.name == choice,\n                default=None,\n            )\n            if chosen is None:\n                raise NodeNotFoundError(choice, self.name)\n\n            return chosen\n        case _:\n            raise NoChoiceMadeError(self.name)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Choice.configure","title":"<code>def configure(config, *, prefixed_name=None, transform_context=None, params=None)</code>","text":"<p>Configure this node and anything following it with the given config.</p> <p>Configuring a choice</p> <p>For a Choice, if the config has a <code>__choice__</code> key, then only the node chosen will be configured. The others will not be configured at all and their config will be discarded.</p> PARAMETER  DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>@override\ndef configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    !!! note \"Configuring a choice\"\n\n        For a Choice, if the config has a `__choice__` key, then only the node\n        chosen will be configured. The others will not be configured at all and\n        their config will be discarded.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    # This part is what differs for a Choice\n    if len(self.nodes) &gt; 0:\n        choice_made = config.get(\"__choice__\", None)\n        if choice_made is not None:\n            matching_child = first_true(\n                self.nodes,\n                pred=lambda node: node.name == choice_made,\n                default=None,\n            )\n            if matching_child is None:\n                raise ValueError(\n                    f\"Can not find matching child for choice {self.name} with child\"\n                    f\" {choice_made}.\"\n                    \"\\nPlease check the config and ensure that the choice is one of\"\n                    f\" {[n.name for n in self.nodes]}.\"\n                    f\"\\nThe config recieved at this choice node was {config=}.\",\n                )\n\n            # We still iterate over all of them just to ensure correct ordering\n            nodes = tuple(\n                node.copy()\n                if node.name != choice_made\n                else matching_child.configure(\n                    config,\n                    prefixed_name=True,\n                    transform_context=transform_context,\n                    params=params,\n                )\n                for node in self.nodes\n            )\n            _kwargs[\"nodes\"] = nodes\n        else:\n            nodes = tuple(\n                node.configure(\n                    config,\n                    prefixed_name=True,\n                    transform_context=transform_context,\n                    params=params,\n                )\n                for node in self.nodes\n            )\n            _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential","title":"<code>class Sequential(*nodes, name=None, item=None, config=None, space=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Sequential</code> set of operations in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act one after another, feeding the output of one into the next.</p> <pre><code>from amltk.pipeline import Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)}),\n    name=\"my_pipeline\"\n)\n\nspace = pipeline.search_space(\"configspace\")\n\nconfiguration = space.sample_configuration()\n\nconfigured_pipeline = pipeline.configure(configuration)\n\nsklearn_pipeline = pipeline.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_pipeline:RandomForestClassifier:n_estimators, Type: UniformInteger, \nRange: [10, 100], Default: 55\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'my_pipeline:RandomForestClassifier:n_estimators': 31,\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                 \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                 \u2502\n\u2502                      \u2193                       \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item   class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 config {'n_estimators': 31}              \u2502 \u2502\n\u2502 \u2502 space  {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier', RandomForestClassifier())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier', RandomForestClassifier())])</pre>PCA<pre>PCA(n_components=3)</pre>RandomForestClassifier<pre>RandomForestClassifier()</pre> </p> <p>You may also just chain together nodes using an infix operator <code>&gt;&gt;</code> if you prefer:</p> <pre><code>from amltk.pipeline import Join, Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = (\n    Sequential(name=\"my_pipeline\")\n    &gt;&gt; PCA(n_components=3)\n    &gt;&gt; Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees a list, i.e. <code>[comp1, comp2, comp3]</code>, this will automatically be converted into a <code>Sequential</code>.</p> <pre><code>from amltk.pipeline import Choice\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\npipeline_choice = Choice(\n    [SimpleImputer(), RandomForestClassifier()],\n    [StandardScaler(), MLPClassifier()],\n    name=\"pipeline_choice\"\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(pipeline_choice) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(Seq-7jUTPqjc) \u2500\u2500\u256e \u256d\u2500 Sequential(Seq-SSL5txZE) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(StandardScaler) \u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u256e          \u2502 \u2502\n\u2502 \u2502 \u2502 item StandardScaler()   \u2502 \u2502 \u2502 \u2502 item SimpleImputer()   \u2502          \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f          \u2502 \u2502\n\u2502 \u2502              \u2193              \u2502 \u2502                  \u2193                  \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(MLPClassifier) \u2500\u256e  \u2502 \u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item MLPClassifier()   \u2502  \u2502 \u2502 \u2502 item RandomForestClassifier()   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Sequential</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n\n    # Perhaps we need to do a deeper check on this...\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise DuplicateNamesError(self)\n\n    if name is None:\n        name = f\"Seq-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.nodes","title":"<code>nodes: tuple[Node, ...]</code>   <code>attr</code>","text":"<p>The nodes in series.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.tail","title":"<code>tail: Node</code>   <code>prop</code>","text":"<p>The last step in the pipeline.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.__len__","title":"<code>def __len__()</code>","text":"<p>Get the number of nodes in the pipeline.</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of nodes in the pipeline.\"\"\"\n    return len(self.nodes)\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Sequential.walk","title":"<code>def walk(path=None)</code>","text":"<p>Walk the nodes in this chain.</p> PARAMETER  DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>@override\ndef walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    path = [*path, self]\n    for node in self.nodes:\n        yield from node.walk(path=path)\n\n        # Append the previous node so that the next node in the sequence is\n        # lead to from the previous node\n        path = [*path, node]\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split","title":"<code>class Split(*nodes, name=None, item=None, config=None, space=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Split</code> of data in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in parallel but on different subsets of data.</p> <pre><code>from amltk.pipeline import Component, Split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(drop=\"first\"),\n]\nnumerical_pipeline = Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})\n\npreprocessor = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numerical\": numerical_pipeline,\n    },\n    config={\n        # This is how you would configure the split for the sklearn builder in particular\n        \"categories\": make_column_selector(dtype_include=\"category\"),\n        \"numerical\": make_column_selector(dtype_exclude=\"category\"),\n    },\n    name=\"my_split\"\n)\n\nspace = preprocessor.search_space(\"configspace\")\n\nconfiguration = space.sample_configuration()\n\nconfigured_preprocessor = preprocessor.configure(configuration)\n\nbuilt_preprocessor = configured_preprocessor.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f247ab1a4a0&gt;,                                                      \u2502\n\u2502            'numerical':                                                      \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f247ab1a170&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502 space {                        \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           'strategy': [        \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502               'mean',          \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'median'         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502           ]                    \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502       }                        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_split:numerical:SimpleImputer:strategy, Type: Categorical, Choices: \n{mean, median}, Default: mean\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'my_split:numerical:SimpleImputer:strategy': 'mean',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f247ab1a4a0&gt;,                                                      \u2502\n\u2502            'numerical':                                                      \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f247ab1a170&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item   class                   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502        SimpleImputer(...)      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502 config {'strategy': 'mean'}    \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502 space  {                       \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502            'strategy': [       \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502                'mean',         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502                'median'        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502            ]                   \u2502 \u2502 \u2502\n\u2502                                       \u2502 \u2502        }                       \u2502 \u2502 \u2502\n\u2502                                       \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('my_split',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a4a0&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a170&gt;)]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_split',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a4a0&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a170&gt;)]))])</pre>my_split: ColumnTransformer<pre>ColumnTransformer(transformers=[('categories',\n                                 Pipeline(steps=[('SimpleImputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('OneHotEncoder',\n                                                  OneHotEncoder(drop='first'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a4a0&gt;),\n                                ('SimpleImputer', SimpleImputer(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a170&gt;)])</pre>categories<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a4a0&gt;</pre>SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre>OneHotEncoder<pre>OneHotEncoder(drop='first')</pre>SimpleImputer<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f247ab1a170&gt;</pre>SimpleImputer<pre>SimpleImputer()</pre> </p> <p>The split is a slight oddity when compared to the other kinds of components in that it allows a <code>dict</code> as it's first argument, where the keys are the names of the different paths through which data will go and the values are the actual nodes that will receive the data.</p> <p>If nodes are passed in as they are for all other components, usually the name of the first node will be important for any builder trying to make sense of how to use the <code>Split</code></p> <p>Like all <code>Node</code>s, a <code>Split</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike | dict[str, Node | NodeLike],\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    if any(isinstance(n, dict) for n in nodes):\n        if len(nodes) &gt; 1:\n            raise ValueError(\n                \"Can't handle multiple nodes with a dictionary as a node.\\n\"\n                f\"{nodes=}\",\n            )\n        _node = nodes[0]\n        assert isinstance(_node, dict)\n\n        def _construct(key: str, value: Node | NodeLike) -&gt; Node:\n            match value:\n                case list():\n                    return Sequential(*value, name=key)\n                case set() | tuple():\n                    return as_node(value, name=key)\n                case _:\n                    return Sequential(value, name=key)\n\n        _nodes = tuple(_construct(key, value) for key, value in _node.items())\n    else:\n        _nodes = tuple(as_node(n) for n in nodes)\n\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes they do not all contain unique names, {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Split-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Split.nodes","title":"<code>nodes: tuple[Node, ...]</code>   <code>attr</code>","text":"<p>The nodes that this node leads to.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component","title":"<code>class Component(item, *, name=None, config=None, space=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Component</code> of the pipeline with a possible item and no children.</p> <p>This is the basic building block of most pipelines, it accepts as it's <code>item=</code> some function that will be called with <code>build_item()</code> to build that one part of the pipeline.</p> <p>When <code>build_item()</code> is called, The <code>.config</code> on this node will be passed to the function to build the item.</p> <p>A common pattern is to use a <code>Component</code> to wrap a constructor, specifying the <code>space=</code> and <code>config=</code> to be used when building the item.</p> <pre><code>from amltk.pipeline import Component\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = Component(\n    RandomForestClassifier,\n    config={\"max_depth\": 3},\n    space={\"n_estimators\": (10, 100)}\n)\n\nconfig = {\"n_estimators\": 50}  # Sample from some space or something\nconfigured_rf = rf.configure(config)\n\nestimator = configured_rf.build_item()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class RandomForestClassifier(...) \u2502\n\u2502 config {'max_depth': 3}                  \u2502\n\u2502 space  {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier<pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre> </p> <p>Whenever some other node sees a function/constructor, i.e. <code>RandomForestClassifier</code>, this will automatically be converted into a <code>Component</code>.</p> <pre><code>from amltk.pipeline import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(RandomForestClassifier, name=\"my_pipeline\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The default <code>.name</code> of a component is the name of the class/function that it will use. You can explicitly set the <code>name=</code> if you want to when constructing the component.</p> <p>Like all <code>Node</code>s, a <code>Component</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    item: Callable[..., Item],\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.item","title":"<code>item: Callable[..., Item]</code>   <code>attr</code>","text":"<p>A node which constructs an item in the pipeline.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.nodes","title":"<code>nodes: tuple[]</code>   <code>attr</code>","text":"<p>A component has no children.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Component.build_item","title":"<code>def build_item(**kwargs)</code>","text":"<p>Build the item attached to this component.</p> PARAMETER  DESCRIPTION <code>**kwargs</code> <p>Any additional arguments to pass to the item</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Item</code> <p>Item The built item</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def build_item(self, **kwargs: Any) -&gt; Item:\n    \"\"\"Build the item attached to this component.\n\n    Args:\n        **kwargs: Any additional arguments to pass to the item\n\n    Returns:\n        Item\n            The built item\n    \"\"\"\n    config = self.config or {}\n    try:\n        return self.item(**{**config, **kwargs})\n    except TypeError as e:\n        new_msg = f\"Failed to build `{self.item=}` with `{self.config=}`.\\n\"\n        if any(kwargs):\n            new_msg += f\"Extra {kwargs=} were also provided.\\n\"\n        new_msg += (\n            \"If the item failed to initialize, a common reason can be forgetting\"\n            \" to call `configure()` on the `Component` or the pipeline it is in or\"\n            \" not calling `build()`/`build_item()` on the **returned** value of\"\n            \" `configure()`.\\n\"\n            \"Reasons may also include not having fully specified the `config`\"\n            \" initially, it having not being configured fully from `configure()`\"\n            \" or from misspecfying parameters in the `space`.\"\n        )\n        raise ComponentBuildError(new_msg) from e\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable","title":"<code>class Searchable(space=None, *, name=None, config=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Node[None, Space]</code></p> <p>A <code>Searchable</code> node of the pipeline which just represents a search space, no item attached.</p> <p>While not usually applicable to pipelines you want to build, this component is useful for creating a search space, especially if the real pipeline you want to optimize can not be built directly. For example, if you are optimize a script, you may wish to use a <code>Searchable</code> to represent the search space of that script.</p> <pre><code>from amltk.pipeline import Searchable\n\nscript_space = Searchable({\"mode\": [\"orange\", \"blue\", \"red\"], \"n\": (10, 100)})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Searchable(Searchable-lW1ucEjf) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 space {'mode': ['orange', 'blue', 'red'], 'n': (10, 100)} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A <code>Searchable</code> explicitly does not allow for <code>item=</code> to be set, nor can it have any children. A <code>Searchable</code> accepts an explicit <code>name=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    space: Space | None = None,\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    if name is None:\n        name = f\"Searchable-{randuid(8)}\"\n\n    super().__init__(\n        name=name,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.item","title":"<code>item: None</code>   <code>classvar</code> <code>attr</code>","text":"<p>A searchable has no item.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Searchable.nodes","title":"<code>nodes: tuple[]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A component has no children.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed","title":"<code>class Fixed(item, *, name=None, config=None, space=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Node[Item, None]</code></p> <p>A <code>Fixed</code> part of the pipeline that represents something that can not be configured and used directly as is.</p> <p>It consists of an <code>.item</code> that is fixed, non-configurable and non-searchable. It also has no children.</p> <p>This is useful for representing parts of the pipeline that are fixed, for example if you have a pipeline that is a <code>Sequential</code> of nodes, but you want to fix the first component to be a <code>PCA</code> with <code>n_components=3</code>, you can use a <code>Fixed</code> to represent that.</p> <pre><code>from amltk.pipeline import Component, Fixed, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\npca = Fixed(PCA(n_components=3))\n\npipeline = Sequential(pca, rf, name=\"my_pipeline\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees an instance of something, i.e. something that can't be called, this will automatically be converted into a <code>Fixed</code>.</p> <pre><code>from amltk.pipeline import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\npipeline = Sequential(\n    PCA(n_components=3),\n    RandomForestClassifier(n_estimators=50),\n    name=\"my_pipeline\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                     \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                     \u2502\n\u2502                        \u2193                         \u2502\n\u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item RandomForestClassifier(n_estimators=50) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The default <code>.name</code> of a component is the class name of the item that it will use. You can explicitly set the <code>name=</code> if you want to when constructing the component.</p> <p>A <code>Fixed</code> accepts only an explicit <code>name=</code>, <code>item=</code>, <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    item: Item,\n    *,\n    name: str | None = None,\n    config: None = None,\n    space: None = None,\n    fidelities: None = None,\n    config_transform: None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.item","title":"<code>item: Item</code>   <code>classvar</code> <code>attr</code>","text":"<p>The fixed item that this node represents.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.space","title":"<code>space: None</code>   <code>classvar</code> <code>attr</code>","text":"<p>A frozen node has no search space.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.fidelities","title":"<code>fidelities: None</code>   <code>classvar</code> <code>attr</code>","text":"<p>A frozen node has no search space.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.config","title":"<code>config: None</code>   <code>classvar</code> <code>attr</code>","text":"<p>A frozen node has no config.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.config_transform","title":"<code>config_transform: None</code>   <code>classvar</code> <code>attr</code>","text":"<p>A frozen node has no config so no transform.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.Fixed.nodes","title":"<code>nodes: tuple[]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A component has no children.</p>"},{"location":"api/amltk/pipeline/components/#amltk.pipeline.components.as_node","title":"<code>def as_node(thing, name=None)</code>","text":"<p>Convert a node, pipeline, set or tuple into a component, copying anything in the process and removing all linking to other nodes.</p> PARAMETER  DESCRIPTION <code>thing</code> <p>The thing to convert</p> <p> TYPE: <code>Node | NodeLike[Item]</code> </p> <code>name</code> <p>The name of the node. If it already a node, it will be renamed to that one.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | Choice | Join | Sequential | Fixed[Item]</code> <p>The component</p> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def as_node(  # noqa: PLR0911\n    thing: Node | NodeLike[Item],\n    name: str | None = None,\n) -&gt; Node | Choice | Join | Sequential | Fixed[Item]:\n    \"\"\"Convert a node, pipeline, set or tuple into a component, copying anything\n    in the process and removing all linking to other nodes.\n\n    Args:\n        thing: The thing to convert\n        name: The name of the node. If it already a node, it will be renamed to that\n            one.\n\n    Returns:\n        The component\n    \"\"\"\n    match thing:\n        case set():\n            return Choice(*thing, name=name)\n        case tuple():\n            return Join(*thing, name=name)\n        case list():\n            return Sequential(*thing, name=name)\n        case Node():\n            name = thing.name if name is None else name\n            return thing.mutate(name=name)\n        case type():\n            return Component(thing, name=name)\n        case thing if (inspect.isfunction(thing) or inspect.ismethod(thing)):\n            return Component(thing, name=name)\n        case _:\n            return Fixed(thing, name=name)\n</code></pre>"},{"location":"api/amltk/pipeline/node/","title":"Node","text":"<p>A pipeline consists of <code>Node</code>s, which hold the various attributes required to build a pipeline, such as the <code>.item</code>, its <code>.space</code>, its <code>.config</code> and so on.</p> <p>The <code>Node</code>s are connected to each in a parent-child relation ship where the children are simply the <code>.nodes</code> that the parent leads to.</p> <p>To give these attributes and relations meaning, there are various subclasses of <code>Node</code> which give different syntactic meanings when you want to construct something like a <code>search_space()</code> or <code>build()</code> some concrete object out of the pipeline.</p> <p>For example, a <code>Sequential</code> node gives the meaning that each of its children in <code>.nodes</code> should follow one another while something like a <code>Choice</code> gives the meaning that only one of its children should be chosen.</p> <p>You will likely never have to create a <code>Node</code> directly, but instead use the various components to create the pipeline.</p> Hashing <p>When hashing a node, i.e. to put it in a <code>set</code> or as a key in a <code>dict</code>, only the name of the node and the hash of its children is used. This means that two nodes with the same connectivity will be equalling hashed,</p> Equality <p>When considering equality, this will be done by comparing all the fields of the node. This include even the <code>parent</code> and <code>branches</code> fields. This means two nodes are considered equal if they look the same and they are connected in to nodes that also look the same.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.RichOptions","title":"<code>class RichOptions</code>","text":"<p>         Bases: <code>NamedTuple</code></p> <p>Options for rich printing.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest","title":"<code>class ParamRequest</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Generic[T]</code></p> <p>A parameter request for a node. This is most useful for things like seeds.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest.key","title":"<code>key: str</code>   <code>attr</code>","text":"<p>The key to request under.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest.default","title":"<code>default: T | object</code>   <code>classvar</code> <code>attr</code>","text":"<p>The default value to use if the key is not found.</p> <p>If left as <code>_NotSet</code> (default) then an error will be raised if the parameter is not found during configuration with <code>configure()</code>.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.ParamRequest.has_default","title":"<code>has_default: bool</code>   <code>prop</code>","text":"<p>Whether this request has a default value.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node","title":"<code>class Node(*nodes, name, item=None, config=None, space=None, fidelities=None, config_transform=None, meta=None)</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[Item, Space]</code></p> <p>The core node class for the pipeline.</p> <p>These are simple objects that are named and linked together to form a chain. They are then wrapped in a <code>Pipeline</code> object to provide a convenient interface for interacting with the chain.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node,\n    name: str,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"Initialize a choice.\"\"\"\n    super().__init__()\n    object.__setattr__(self, \"name\", name)\n    object.__setattr__(self, \"item\", item)\n    object.__setattr__(self, \"config\", config)\n    object.__setattr__(self, \"space\", space)\n    object.__setattr__(self, \"fidelities\", fidelities)\n    object.__setattr__(self, \"config_transform\", config_transform)\n    object.__setattr__(self, \"meta\", meta)\n    object.__setattr__(self, \"nodes\", nodes)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.name","title":"<code>name: str</code>   <code>classvar</code> <code>attr</code>","text":"<p>Name of the node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.item","title":"<code>item: Callable[..., Item] | Item | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The item attached to this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.nodes","title":"<code>nodes: tuple[Node, ...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The nodes that this node leads to.</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.config","title":"<code>config: Config | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The configuration for this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.space","title":"<code>space: Space | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The search space for this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.fidelities","title":"<code>fidelities: Mapping[str, Any] | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The fidelities for this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.config_transform","title":"<code>config_transform: Callable[[Config, Any], Config] | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>A function that transforms the configuration of this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.meta","title":"<code>meta: Mapping[str, Any] | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>Any meta information about this node</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.RICH_OPTIONS","title":"<code>RICH_OPTIONS: RichOptions</code>   <code>classvar</code>","text":"<p>Options for rich printing</p>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.__getitem__","title":"<code>def __getitem__(key)</code>","text":"<p>Get the node with the given name.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Node:\n    \"\"\"Get the node with the given name.\"\"\"\n    found = first_true(\n        self.nodes,\n        None,\n        lambda node: node.name == key,\n    )\n    if found is None:\n        raise KeyError(\n            f\"Could not find node with name {key} in '{self.name}'.\"\n            f\" Available nodes are: {', '.join(node.name for node in self.nodes)}\",\n        )\n\n    return found\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.configure","title":"<code>def configure(config, *, prefixed_name=None, transform_context=None, params=None)</code>","text":"<p>Configure this node and anything following it with the given config.</p> PARAMETER  DESCRIPTION <code>config</code> <p>The configuration to apply</p> <p> TYPE: <code>Config</code> </p> <code>prefixed_name</code> <p>Whether items in the config are prefixed by the names of the nodes. * If <code>None</code>, the default, then <code>prefixed_name</code> will be assumed to     be <code>True</code> if this node has a next node or if the config has     keys that begin with this nodes name. * If <code>True</code>, then the config will be searched for items prefixed     by the name of the node (and subsequent chained nodes). * If <code>False</code>, then the config will be searched for items without     the prefix, i.e. the config keys are exactly those matching     this nodes search space.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <code>transform_context</code> <p>Any context to give to <code>config_transform=</code> of individual nodes.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>The params to match any requests when configuring this node. These will match against any ParamRequests in the config and will be used to fill in any missing values.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>The configured node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def configure(\n    self,\n    config: Config,\n    *,\n    prefixed_name: bool | None = None,\n    transform_context: Any | None = None,\n    params: Mapping[str, Any] | None = None,\n) -&gt; Self:\n    \"\"\"Configure this node and anything following it with the given config.\n\n    Args:\n        config: The configuration to apply\n        prefixed_name: Whether items in the config are prefixed by the names\n            of the nodes.\n            * If `None`, the default, then `prefixed_name` will be assumed to\n                be `True` if this node has a next node or if the config has\n                keys that begin with this nodes name.\n            * If `True`, then the config will be searched for items prefixed\n                by the name of the node (and subsequent chained nodes).\n            * If `False`, then the config will be searched for items without\n                the prefix, i.e. the config keys are exactly those matching\n                this nodes search space.\n        transform_context: Any context to give to `config_transform=` of individual\n            nodes.\n        params: The params to match any requests when configuring this node.\n            These will match against any ParamRequests in the config and will\n            be used to fill in any missing values.\n\n    Returns:\n        The configured node\n    \"\"\"\n    # Get the config for this node\n    match prefixed_name:\n        case True:\n            config = mapping_select(config, f\"{self.name}:\")\n        case False:\n            pass\n        case None if any(k.startswith(f\"{self.name}:\") for k in config):\n            config = mapping_select(config, f\"{self.name}:\")\n        case None:\n            pass\n\n    _kwargs: dict[str, Any] = {}\n\n    # Configure all the branches if exists\n    if len(self.nodes) &gt; 0:\n        nodes = tuple(\n            node.configure(\n                config,\n                prefixed_name=True,\n                transform_context=transform_context,\n                params=params,\n            )\n            for node in self.nodes\n        )\n        _kwargs[\"nodes\"] = nodes\n\n    this_config = {\n        hp: v\n        for hp, v in config.items()\n        if (\n            \":\" not in hp\n            and not any(hp.startswith(f\"{node.name}\") for node in self.nodes)\n        )\n    }\n    if self.config is not None:\n        this_config = {**self.config, **this_config}\n\n    this_config = dict(self._fufill_param_requests(this_config, params=params))\n\n    if self.config_transform is not None:\n        this_config = dict(self.config_transform(this_config, transform_context))\n\n    if len(this_config) &gt; 0:\n        _kwargs[\"config\"] = dict(this_config)\n\n    return self.mutate(**_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.fidelity_space","title":"<code>def fidelity_space()</code>","text":"<p>Get the fidelities for this node and any connected nodes.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def fidelity_space(self) -&gt; dict[str, Any]:\n    \"\"\"Get the fidelities for this node and any connected nodes.\"\"\"\n    fids = {}\n    for node in self.nodes:\n        fids.update(prefix_keys(node.fidelity_space(), f\"{self.name}:\"))\n\n    return fids\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.linearized_fidelity","title":"<code>def linearized_fidelity(value)</code>","text":"<p>Get the liniearized fidelities for this node and any connected nodes.</p> PARAMETER  DESCRIPTION <code>value</code> <p>The value to linearize. Must be between [0, 1]</p> <p> TYPE: <code>float</code> </p> Return <p>dictionary from key to it's linearized fidelity.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def linearized_fidelity(self, value: float) -&gt; dict[str, int | float | Any]:\n    \"\"\"Get the liniearized fidelities for this node and any connected nodes.\n\n    Args:\n        value: The value to linearize. Must be between [0, 1]\n\n    Return:\n        dictionary from key to it's linearized fidelity.\n    \"\"\"\n    assert 1.0 &lt;= value &lt;= 100.0, f\"{value=} not in [1.0, 100.0]\"  # noqa: PLR2004\n    d = {}\n    for node in self.nodes:\n        node_fids = prefix_keys(\n            node.linearized_fidelity(value),\n            f\"{self.name}:\",\n        )\n        d.update(node_fids)\n\n    if self.fidelities is None:\n        return d\n\n    for f_name, f_range in self.fidelities.items():\n        match f_range:\n            case (int() | float(), int() | float()):\n                low, high = f_range\n                fid = low + (high - low) * value\n                fid = low + (high - low) * (value - 1) / 100\n                fid = fid if isinstance(low, float) else round(fid)\n                d[f_name] = fid\n            case _:\n                raise ValueError(\n                    f\"Invalid fidelities to linearize {f_range} for {f_name}\"\n                    f\" in {self}. Only supports ranges of the form (low, high)\",\n                )\n\n    return prefix_keys(d, f\"{self.name}:\")\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.iter","title":"<code>def iter()</code>","text":"<p>Iterate the the nodes, including this node.</p> YIELDS DESCRIPTION <code>Node</code> <p>The nodes connected to this node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def iter(self) -&gt; Iterator[Node]:\n    \"\"\"Iterate the the nodes, including this node.\n\n    Yields:\n        The nodes connected to this node\n    \"\"\"\n    yield self\n    for node in self.nodes:\n        yield from node.iter()\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.mutate","title":"<code>def mutate(**kwargs)</code>","text":"<p>Mutate the node with the given keyword arguments.</p> PARAMETER  DESCRIPTION <code>**kwargs</code> <p>The keyword arguments to mutate</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>Self The mutated node</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def mutate(self, **kwargs: Any) -&gt; Self:\n    \"\"\"Mutate the node with the given keyword arguments.\n\n    Args:\n        **kwargs: The keyword arguments to mutate\n\n    Returns:\n        Self\n            The mutated node\n    \"\"\"\n    _args = ()\n    _kwargs = {**self.__dict__, **kwargs}\n\n    # If there's nodes in kwargs, we have to check if it's\n    # a positional or keyword argument and handle accordingly.\n    if (nodes := _kwargs.pop(\"nodes\", None)) is not None:\n        match self._NODES_INIT:\n            case \"args\":\n                _args = nodes\n            case \"kwargs\":\n                _kwargs[\"nodes\"] = nodes\n            case None if len(nodes) == 0:\n                pass  # Just ignore it, it's popped out\n            case None:\n                raise ValueError(\n                    \"Cannot mutate nodes when __init__ does not accept nodes\",\n                )\n\n    # If there's a config in kwargs, we have to check if it's actually got values\n    config = _kwargs.pop(\"config\", None)\n    if config is not None and len(config) &gt; 0:\n        _kwargs[\"config\"] = config\n\n    # Lastly, we remove anything that can't be passed to kwargs of the\n    # subclasses __init__\n    _available_kwargs = inspect.signature(self.__init__).parameters.keys()  # type: ignore\n    for k in list(_kwargs.keys()):\n        if k not in _available_kwargs:\n            _kwargs.pop(k)\n\n    return self.__class__(*_args, **_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.copy","title":"<code>def copy()</code>","text":"<p>Copy this node, removing all links in the process.</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Copy this node, removing all links in the process.\"\"\"\n    return self.mutate()\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.path_to","title":"<code>def path_to(key)</code>","text":"<p>Find a path to the given node.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> RETURNS DESCRIPTION <code>list[Node] | None</code> <p>The path to the node if found, else None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def path_to(self, key: str | Node | Callable[[Node], bool]) -&gt; list[Node] | None:\n    \"\"\"Find a path to the given node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n\n    Returns:\n        The path to the node if found, else None\n    \"\"\"\n    # We found our target, just return now\n\n    match key:\n        case Node():\n            pred = lambda node: node == key\n        case str():\n            pred = lambda node: node.name == key\n        case _:\n            pred = key\n\n    for path, node in self.walk():\n        if pred(node):\n            return path\n\n    return None\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.walk","title":"<code>def walk(path=None)</code>","text":"<p>Walk the nodes in this chain.</p> PARAMETER  DESCRIPTION <code>path</code> <p>The current path to this node</p> <p> TYPE: <code>Sequence[Node] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>list[Node]</code> <p>The parents of the node and the node itself</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def walk(\n    self,\n    path: Sequence[Node] | None = None,\n) -&gt; Iterator[tuple[list[Node], Node]]:\n    \"\"\"Walk the nodes in this chain.\n\n    Args:\n        path: The current path to this node\n\n    Yields:\n        The parents of the node and the node itself\n    \"\"\"\n    path = list(path) if path is not None else []\n    yield path, self\n\n    for node in self.nodes:\n        yield from node.walk(path=[*path, self])\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.find","title":"<code>def find(key, default=None)</code>","text":"<p>Find a node in that's nested deeper from this node.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to search for or a function that returns True if the node is the desired node</p> <p> TYPE: <code>str | Node | Callable[[Node], bool]</code> </p> <code>default</code> <p>The value to return if the node is not found. Defaults to None</p> <p> TYPE: <code>T | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Node | T | None</code> <p>The node if found, otherwise the default value. Defaults to None</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def find(\n    self,\n    key: str | Node | Callable[[Node], bool],\n    default: T | None = None,\n) -&gt; Node | T | None:\n    \"\"\"Find a node in that's nested deeper from this node.\n\n    Args:\n        key: The key to search for or a function that returns True if the node\n            is the desired node\n        default: The value to return if the node is not found. Defaults to None\n\n    Returns:\n        The node if found, otherwise the default value. Defaults to None\n    \"\"\"\n    itr = self.iter()\n    match key:\n        case Node():\n            return first_true(itr, default, lambda node: node == key)\n        case str():\n            return first_true(itr, default, lambda node: node.name == key)\n        case _:\n            return first_true(itr, default, key)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.search_space","title":"<code>def search_space(parser, *parser_args, **parser_kwargs)</code>","text":"<p>Get the search space for this node.</p> PARAMETER  DESCRIPTION <code>parser</code> <p>The parser to use. This can be a function that takes in the node and returns the search space or a string that is one of:</p> <ul> <li><code>\"configspace\"</code>: Build a     <code>ConfigSpace.ConfigurationSpace</code>     out of this node.</li> <li><code>\"optuna\"</code>: Build a dict of hyperparameters that Optuna can     use in its ask and tell methods</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], ParserOutput] | Literal['configspace', 'optuna']</code> </p> <code>parser_args</code> <p>The positional arguments to pass to the parser</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>parser_kwargs</code> <p>The keyword arguments to pass to the parser</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>ParserOutput | ConfigurationSpace | OptunaSearchSpace</code> <p>The search space</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def search_space(\n    self,\n    parser: (\n        Callable[Concatenate[Node, P], ParserOutput]\n        | Literal[\"configspace\", \"optuna\"]\n    ),\n    *parser_args: P.args,\n    **parser_kwargs: P.kwargs,\n) -&gt; ParserOutput | ConfigurationSpace | OptunaSearchSpace:\n    \"\"\"Get the search space for this node.\n\n    Args:\n        parser: The parser to use. This can be a function that takes in\n            the node and returns the search space or a string that is one of:\n\n            * `#!python \"configspace\"`: Build a\n                [`ConfigSpace.ConfigurationSpace`](https://automl.github.io/ConfigSpace/master/)\n                out of this node.\n            * `#!python \"optuna\"`: Build a dict of hyperparameters that Optuna can\n                use in its [ask and tell methods](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html#define-and-run)\n\n        parser_args: The positional arguments to pass to the parser\n        parser_kwargs: The keyword arguments to pass to the parser\n\n    Returns:\n        The search space\n    \"\"\"\n    match parser:\n        case \"configspace\":\n            from amltk.pipeline.parsers.configspace import parser as cs_parser\n\n            return cs_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case \"optuna\":\n            from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\n            return optuna_parser(self, *parser_args, **parser_kwargs)  # type: ignore\n        case str():  # type: ignore\n            raise ValueError(\n                f\"Invalid str for parser {parser}. \"\n                \"Please use 'configspace' or 'optuna' or pass in your own\"\n                \" parser function\",\n            )\n        case _:\n            return parser(self, *parser_args, **parser_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.build","title":"<code>def build(builder, *builder_args, **builder_kwargs)</code>","text":"<p>Build a concrete object out of this node.</p> PARAMETER  DESCRIPTION <code>builder</code> <p>The builder to use. This can be a function that takes in the node and returns the object or a string that is one of:</p> <ul> <li><code>\"sklearn\"</code>: Build a     <code>sklearn.pipeline.Pipeline</code>     out of this node.</li> </ul> <p> TYPE: <code>Callable[Concatenate[Node, P], BuilderOutput] | Literal['sklearn']</code> </p> <code>builder_args</code> <p>The positional arguments to pass to the builder</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>builder_kwargs</code> <p>The keyword arguments to pass to the builder</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BuilderOutput | Pipeline</code> <p>The built object</p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def build(\n    self,\n    builder: Callable[Concatenate[Node, P], BuilderOutput] | Literal[\"sklearn\"],\n    *builder_args: P.args,\n    **builder_kwargs: P.kwargs,\n) -&gt; BuilderOutput | SklearnPipeline:\n    \"\"\"Build a concrete object out of this node.\n\n    Args:\n        builder: The builder to use. This can be a function that takes in\n            the node and returns the object or a string that is one of:\n\n            * `#!python \"sklearn\"`: Build a\n                [`sklearn.pipeline.Pipeline`][sklearn.pipeline.Pipeline]\n                out of this node.\n\n        builder_args: The positional arguments to pass to the builder\n        builder_kwargs: The keyword arguments to pass to the builder\n\n    Returns:\n        The built object\n    \"\"\"\n    match builder:\n        case \"sklearn\":\n            from amltk.pipeline.builders.sklearn import build as _build\n\n            return _build(self, *builder_args, **builder_kwargs)  # type: ignore\n        case _:\n            return builder(self, *builder_args, **builder_kwargs)\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.Node.display","title":"<code>def display(*, full=False)</code>","text":"<p>Display this node.</p> PARAMETER  DESCRIPTION <code>full</code> <p>Whether to display the full node or just a summary</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def display(self, *, full: bool = False) -&gt; RenderableType:\n    \"\"\"Display this node.\n\n    Args:\n        full: Whether to display the full node or just a summary\n    \"\"\"\n    if not full:\n        return self.__rich__()\n\n    from rich.console import Group as RichGroup\n\n    return RichGroup(*self._rich_iter())\n</code></pre>"},{"location":"api/amltk/pipeline/node/#amltk.pipeline.node.request","title":"<code>def request(key, default=_NotSet)</code>","text":"<p>Create a new parameter request.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to request under.</p> <p> TYPE: <code>str</code> </p> <code>default</code> <p>The default value to use if the key is not found. If left as <code>_NotSet</code> (default) then the key will be removed from the config once <code>configure</code> is called and nothing has been provided.</p> <p> TYPE: <code>T | object</code> DEFAULT: <code>_NotSet</code> </p> Source code in <code>src/amltk/pipeline/node.py</code> <pre><code>def request(key: str, default: T | object = _NotSet) -&gt; ParamRequest[T]:\n    \"\"\"Create a new parameter request.\n\n    Args:\n        key: The key to request under.\n        default: The default value to use if the key is not found.\n            If left as `_NotSet` (default) then the key will be removed from the\n            config once [`configure`][amltk.pipeline.Node.configure] is called and\n            nothing has been provided.\n    \"\"\"\n    return ParamRequest(key=key, default=default)\n</code></pre>"},{"location":"api/amltk/pipeline/xgboost/","title":"Xgboost","text":"<p>Get an XGBoost component for your pipeline.</p> <p>A <code>Component</code> wrapped around XGBoost with two possible default spaces <code>\"large\"</code> and <code>\"performant\"</code> or you own custom <code>space=</code>.</p> <p>See <code>amltk.pipeline.xgboost.xgboost_component</code></p>"},{"location":"api/amltk/pipeline/xgboost/#amltk.pipeline.xgboost.xgboost_component","title":"<code>def xgboost_component(kind, name=None, space='large', config=None)</code>","text":"<p>Create a component with an XGBoost estimator.</p> PARAMETER  DESCRIPTION <code>kind</code> <p>The kind of estimator to create, either \"classifier\" or \"regressor\".</p> <p> TYPE: <code>Literal['classifier', 'regressor']</code> </p> <code>name</code> <p>The name of the step in the pipeline.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>space</code> <p>The space to use for hyperparameter optimization. Choose from \"large\", \"performant\" or provide a custom space.</p> <p>Todo</p> <p>Currently <code>\"performant\"</code> links to <code>\"large\"</code> and are the same.</p> <p>Warning</p> <p>The default space is by no means optimal, please adjust it to your needs. You can find the link to the XGBoost parameters here:</p> <p>xgboost.readthedocs.io/en/stable/parameter.html</p> <p> TYPE: <code>Any | Literal['large', 'performant']</code> DEFAULT: <code>'large'</code> </p> <code>config</code> <p>The keyword arguments to pass to the XGBoost estimator when it will be created. These will be hard set on the estimator and removed from the default space if no space is provided.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Component</code> <p>A pipeline with an XGBoost estimator.</p> Source code in <code>src/amltk/pipeline/xgboost.py</code> <pre><code>def xgboost_component(\n    kind: Literal[\"classifier\", \"regressor\"],\n    name: str | None = None,\n    space: Any | Literal[\"large\", \"performant\"] = \"large\",\n    config: Mapping[str, Any] | None = None,\n) -&gt; Component:\n    \"\"\"Create a component with an XGBoost estimator.\n\n    Args:\n        kind: The kind of estimator to create, either \"classifier\" or \"regressor\".\n        name: The name of the step in the pipeline.\n        space: The space to use for hyperparameter optimization. Choose from\n            \"large\", \"performant\" or provide a custom space.\n\n            !!! todo\n\n                Currently `#!python \"performant\"` links to `#!python \"large\"` and are\n                the same.\n\n            !!! warning \"Warning\"\n\n                The default space is by no means optimal, please adjust it to your\n                needs. You can find the link to the XGBoost parameters here:\n\n                https://xgboost.readthedocs.io/en/stable/parameter.html\n\n\n        config: The keyword arguments to pass to the XGBoost estimator when it will be\n            created. These will be hard set on the estimator and removed from the\n            default space if no space is provided.\n\n    Returns:\n        A pipeline with an XGBoost estimator.\n    \"\"\"\n    estimator_types = {\n        \"classifier\": XGBClassifier,\n        \"regressor\": XGBRegressor,\n    }\n    config = config or {}\n    estimator_type = estimator_types.get(kind)\n    if estimator_type is None:\n        raise ValueError(\n            f\"Unknown kind: {kind}, please choose from {list(estimator_types.keys())}\",\n        )\n\n    if name is None:\n        name = str(estimator_type.__name__)\n\n    if isinstance(space, str):\n        device = config.get(\"device\", \"cpu\")\n        tree_method = config.get(\"tree_method\", \"auto\")\n        is_classifier = estimator_type is XGBClassifier\n        _spaces = {\n            \"large\": xgboost_large_space,\n            \"performant\": xgboost_performant_space,\n        }\n        space_f = _spaces[space]\n        space = space_f(\n            is_classifier=is_classifier,\n            tree_method=tree_method,\n            device=device,\n        )\n        for key in config:\n            space.pop(key, None)\n    elif isinstance(space, Mapping):\n        overlap = set(space.keys()).intersection(config)\n        if any(overlap):\n            raise ValueError(\n                f\"Space and kwargs overlap: {overlap}, please remove one of them\",\n            )\n\n    return Component(name=name, item=estimator_type, config=config, space=space)\n</code></pre>"},{"location":"api/amltk/pipeline/xgboost/#amltk.pipeline.xgboost.xgboost_large_space","title":"<code>def xgboost_large_space(*, is_classifier, tree_method, device)</code>","text":"<p>Create a large space for XGBoost hyperparameter optimization.</p> Source code in <code>src/amltk/pipeline/xgboost.py</code> <pre><code>def xgboost_large_space(\n    *,\n    is_classifier: bool,\n    tree_method: str,\n    device: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Create a large space for XGBoost hyperparameter optimization.\"\"\"\n    # TODO: Do we want a general space kind\n    # For now we use ConfigSpace where needed\n    from ConfigSpace import Float, Integer\n\n    space = {\n        \"eta\": Float(\"eta\", (1e-3, 1), default=0.3, log=True),\n        \"min_split_loss\": Float(\"min_split_loss\", (0, 20), default=0),\n        \"max_depth\": Integer(\"max_depth\", (1, 20), default=6),\n        \"min_child_weight\": Float(\"min_child_weight\", (0, 20), default=1),\n        \"max_delta_step\": Float(\"max_delta_step\", (0, 10), default=0),\n        \"subsample\": Float(\"subsample\", (1e-5, 1), default=1),\n        \"colsample_bytree\": Float(\"colsample_bytree\", (1e-5, 1), default=1),\n        \"colsample_bylevel\": Float(\"colsample_bylevel\", (1e-5, 1), default=1),\n        \"colsample_bynode\": Float(\"colsample_bynode\", (1e-5, 1), default=1),\n        \"reg_lambda\": Float(\"reg_lambda\", (1e-5, 1e3), default=1),\n        \"reg_alpha\": Float(\"reg_alpha\", (0, 1e3), default=0),\n    }\n\n    if tree_method == \"hist\" and (\"cuda\" in device or \"gpu\" in device):\n        space[\"sampling_method\"] = [\"uniform\", \"gradient_based\"]\n\n    if tree_method in (\"hist\", \"approx\"):\n        space[\"max_bin\"] = Integer(\"max_bin\", (2, 512), default=256)\n\n    if is_classifier:\n        space[\"scale_pos_weight\"] = Float(\"scale_pos_weight\", (1e-5, 1e5), default=1)\n\n    return space\n</code></pre>"},{"location":"api/amltk/pipeline/xgboost/#amltk.pipeline.xgboost.xgboost_performant_space","title":"<code>def xgboost_performant_space(*, is_classifier, tree_method, device)</code>","text":"<p>Create a performant space for XGBoost hyperparameter optimization.</p> Source code in <code>src/amltk/pipeline/xgboost.py</code> <pre><code>def xgboost_performant_space(\n    *,\n    is_classifier: bool,\n    tree_method: str,\n    device: str,\n) -&gt; dict[str, Any]:\n    \"\"\"Create a performant space for XGBoost hyperparameter optimization.\"\"\"\n    warnings.warn(\n        \"This space is not yet optimized for performance\"\n        \" and is subject to change in the future.\",\n        FutureWarning,\n        stacklevel=2,\n    )\n    return xgboost_large_space(\n        is_classifier=is_classifier,\n        tree_method=tree_method,\n        device=device,\n    )\n</code></pre>"},{"location":"api/amltk/pipeline/builders/sklearn/","title":"Sklearn","text":"<p>The sklearn <code>builder()</code>, converts a pipeline made of <code>Node</code>s into a sklearn <code>Pipeline</code>.</p> <p>Requirements</p> <p>This requires <code>sklearn</code> which can be installed with:</p> <pre><code>pip install \"amltk[scikit-learn]\"\n\n# Or directly\npip install scikit-learn\n</code></pre> <p>Each kind of node corresponds to a different part of the end pipeline:</p> <code>Fixed</code><code>Component</code><code>Sequential</code><code>Split</code><code>Join</code><code>Choice</code> <p><code>Fixed</code> - The estimator will simply be cloned, allowing you to directly configure some object in a pipeline.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Fixed\n\nest = Fixed(RandomForestClassifier(n_estimators=25))\nbuilt_pipeline = est.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> </p> </p> <p><code>Component</code> - The estimator will be built from the component's config. This is mostly useful to allow a space to be defined for the component.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Component\n\nest = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\n# ... Likely get the configuration through an optimizer or sampling\nconfigured_est = est.configure({\"n_estimators\": 25})\n\nbuilt_pipeline = configured_est.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> </p> </p> <p><code>Sequential</code> - The sequential will be converted into a <code>Pipeline</code>, building whatever nodes are contained within in.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom amltk.pipeline import Component, Sequential\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, config={\"n_estimators\": 25})\n)\nbuilt_pipeline = pipeline.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>PCA<pre>PCA(n_components=3)</pre>RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> </p> </p> <p><code>Split</code> - The split will be converted into a <code>ColumnTransformer</code>, where each path and the data that should go through it is specified by the split's config. You can provide a <code>ColumnTransformer</code> directly as the item to the <code>Split</code>, or otherwise if left blank, it will default to the standard sklearn one.</p> <p>You can use a <code>Fixed</code> with the special keyword <code>\"passthrough\"</code> as you might normally do with a <code>ColumnTransformer</code>.</p> <p>By default, we provide two special keywords you can provide to a <code>Split</code>, namely <code>\"categorical\"</code> and <code>\"numerical\"</code>, which will automatically configure a <code>ColumnTransorfmer</code> to pass the appropraite columns of a data-frame to the given paths.</p> <p><pre><code>from amltk.pipeline import Split, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    Component(\n        OneHotEncoder,\n        space={\n            \"min_frequency\": (0.01, 0.1),\n            \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n        },\n        config={\"drop\": \"first\"},\n    ),\n]\nnumerical_pipeline = [SimpleImputer(strategy=\"median\"), StandardScaler()]\n\nsplit = Split(\n    {\n        \"categorical\": categorical_pipeline,\n        \"numerical\": numerical_pipeline\n    }\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Split(Split-X6U8GXun) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(categorical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item SimpleImputer(strategy='\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502                 \u2193                  \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u256d\u2500 Fixed(StandardScaler) \u2500\u256e        \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Component(OneHotEncoder) \u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 item StandardScaler()   \u2502        \u2502 \u2502\n\u2502 \u2502 \u2502 item   class                  \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f        \u2502 \u2502\n\u2502 \u2502 \u2502        OneHotEncoder(...)     \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u2502 \u2502 config {'drop': 'first'}      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502 space  {                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'min_frequency': ( \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.01,          \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.1            \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ),                 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'handle_unknown':  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        [                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'ignore',      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'infrequent_i\u2026 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ]                  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        }                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502                                        \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <p>You can manually specify the column selectors if you prefer.</p> <pre><code>split = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numbers\": numerical_pipeline,\n    },\n    config={\n        \"categories\": make_column_selector(dtype_include=object),\n        \"numbers\": make_column_selector(dtype_include=np.number),\n    },\n)\n</code></pre> <p><code>Join</code> - The join will be converted into a <code>FeatureUnion</code>.</p> <p><pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\njoin = Join(PCA(n_components=2), SelectKBest(k=3), name=\"my_feature_union\")\n\npipeline = join.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                               ('SelectKBest', SelectKBest(k=3))])</pre>PCAPCA<pre>PCA(n_components=2)</pre>SelectKBestSelectKBest<pre>SelectKBest(k=3)</pre> </p> </p> <p><code>Choice</code> - The estimator will be built from the chosen component's config. This is very similar to <code>Component</code>.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom amltk.pipeline import Choice\n\n# The choice here is usually provided during the `.configure()` step.\nestimator_choice = Choice(\n    RandomForestClassifier(),\n    MLPClassifier(),\n    config={\"__choice__\": \"RandomForestClassifier\"}\n)\n\nbuilt_pipeline = estimator_choice.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre>RandomForestClassifier<pre>RandomForestClassifier()</pre> </p> </p>"},{"location":"api/amltk/pipeline/builders/sklearn/#amltk.pipeline.builders.sklearn.build","title":"<code>def build(node, *, pipeline_type=SklearnPipeline, **pipeline_kwargs)</code>","text":"<p>Build a pipeline into a usable object.</p> PARAMETER  DESCRIPTION <code>node</code> <p>The node from which to build a pipeline.</p> <p> TYPE: <code>Node[Any, Any]</code> </p> <code>pipeline_type</code> <p>The type of pipeline to build. Defaults to the standard sklearn pipeline but can be any derivative of that, i.e. ImbLearn's pipeline.</p> <p> TYPE: <code>type[SklearnPipelineT]</code> DEFAULT: <code>Pipeline</code> </p> <code>**pipeline_kwargs</code> <p>The kwargs to pass to the pipeline_type.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>SklearnPipelineT</code> <p>The built pipeline</p> Source code in <code>src/amltk/pipeline/builders/sklearn.py</code> <pre><code>def build(\n    node: Node[Any, Any],\n    *,\n    pipeline_type: type[SklearnPipelineT] = SklearnPipeline,\n    **pipeline_kwargs: Any,\n) -&gt; SklearnPipelineT:\n    \"\"\"Build a pipeline into a usable object.\n\n    Args:\n        node: The node from which to build a pipeline.\n        pipeline_type: The type of pipeline to build. Defaults to the standard\n            sklearn pipeline but can be any derivative of that, i.e. ImbLearn's\n            pipeline.\n        **pipeline_kwargs: The kwargs to pass to the pipeline_type.\n\n    Returns:\n        The built pipeline\n    \"\"\"\n    return pipeline_type(list(_iter_steps(node)), **pipeline_kwargs)  # type: ignore\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/","title":"Configspace","text":"<p>ConfigSpace is a library for representing and sampling configurations for hyperparameter optimization. It features a straightforward API for defining hyperparameters, their ranges and even conditional dependencies.</p> <p>It is generally flexible enough for more complex use cases, even handling the complex pipelines of AutoSklearn and AutoPyTorch, large scale hyperparameter spaces over which to optimize entire pipelines at a time.</p> <p>Requirements</p> <p>This requires <code>ConfigSpace</code> which can be installed with:</p> <pre><code>pip install \"amltk[configspace]\"\n\n# Or directly\npip install ConfigSpace\n</code></pre> <p>In general, you should have the ConfigSpace documentation ready to consult for a full understanding of how to construct hyperparameter spaces with AMLTK.</p>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace--basic-usage","title":"Basic Usage","text":"<p>You can directly us the <code>parser()</code> function and pass that into the <code>search_space()</code> method of a <code>Node</code>, however you can also simply provide <code>search_space(parser=\"configspace\", ...)</code> for simplicity.</p> <pre><code>from amltk.pipeline import Component, Choice, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\nmy_pipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Component(PCA, space={\"n_components\": (1, 3)})\n    &gt;&gt; Choice(\n        Component(\n            SVC,\n            space={\"C\": (0.1, 10.0)}\n        ),\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n        ),\n        Component(\n            MLPClassifier,\n            space={\n                \"activation\": [\"identity\", \"logistic\", \"relu\"],\n                \"alpha\": (0.0001, 0.1),\n                \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n            },\n        ),\n        name=\"estimator\"\n    )\n)\n\nspace = my_pipeline.search_space(\"configspace\")\nprint(space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Pipeline:PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    Pipeline:estimator:MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    Pipeline:estimator:MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    Pipeline:estimator:MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    Pipeline:estimator:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    Pipeline:estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:estimator:SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    Pipeline:estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n  Conditions:\n    Pipeline:estimator:MLPClassifier:activation | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:MLPClassifier:alpha | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:MLPClassifier:learning_rate | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:RandomForestClassifier:criterion | Pipeline:estimator:__choice__ == 'RandomForestClassifier'\n    Pipeline:estimator:RandomForestClassifier:n_estimators | Pipeline:estimator:__choice__ == 'RandomForestClassifier'\n    Pipeline:estimator:SVC:C | Pipeline:estimator:__choice__ == 'SVC'\n</code></pre> <p>Here we have an example of a few different kinds of hyperparmeters,</p> <ul> <li><code>PCA:n_components</code> is a integer with a range of 1 to 3, uniform distribution, as specified     by it's integer bounds in a tuple.</li> <li><code>SVC:C</code> is a float with a range of 0.1 to 10.0, uniform distribution, as specified     by it's float bounds in a tuple.</li> <li><code>RandomForestClassifier:criterion</code> is a categorical hyperparameter, with two choices,     <code>\"gini\"</code> and <code>\"log_loss\"</code>.</li> </ul> <p>There is also a <code>Choice</code> node, which is a special node that indicates that we could choose from one of these estimators. This leads to the conditionals that you can see in the printed out space.</p> <p>You may wish to remove all conditionals if an <code>Optimizer</code> does not support them, or you may wish to remove them for other reasons. You can do this by passing <code>conditionals=False</code> to the <code>parser()</code> function.</p> <pre><code>print(my_pipeline.search_space(\"configspace\", conditionals=False))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Pipeline:PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    Pipeline:estimator:MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    Pipeline:estimator:MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    Pipeline:estimator:MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    Pipeline:estimator:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    Pipeline:estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:estimator:SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    Pipeline:estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n</code></pre> <p>Likewise, you can also remove all heirarchy from the space which may make downstream tasks easier, by passing <code>flat=True</code> to the <code>parser()</code> function.</p> <pre><code>print(my_pipeline.search_space(\"configspace\", flat=True))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n  Conditions:\n    MLPClassifier:activation | estimator:__choice__ == 'MLPClassifier'\n    MLPClassifier:alpha | estimator:__choice__ == 'MLPClassifier'\n    MLPClassifier:learning_rate | estimator:__choice__ == 'MLPClassifier'\n    RandomForestClassifier:criterion | estimator:__choice__ == 'RandomForestClassifier'\n    RandomForestClassifier:n_estimators | estimator:__choice__ == 'RandomForestClassifier'\n    SVC:C | estimator:__choice__ == 'SVC'\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace--more-specific-hyperparameters","title":"More Specific Hyperparameters","text":"<p>You'll often want to be a bit more specific with your hyperparameters, here we just show a few examples of how you'd couple your pipelines a bit more towards <code>ConfigSpace</code>.</p> <pre><code>from ConfigSpace import Float, Categorical, Normal\nfrom amltk.pipeline import Searchable\n\ns = Searchable(\n    space={\n        \"lr\": Float(\"lr\", bounds=(1e-5, 1.), log=True, default=0.3),\n        \"balance\": Float(\"balance\", bounds=(-1.0, 1.0), distribution=Normal(0.0, 0.5)),\n        \"color\": Categorical(\"color\", [\"red\", \"green\", \"blue\"], weights=[2, 1, 1], default=\"blue\"),\n    },\n    name=\"Something-To-Search\",\n)\nprint(s.search_space(\"configspace\"))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Something-To-Search:balance, Type: NormalFloat, Mu: 0.0 Sigma: 0.5, Range: [-1.0, 1.0], Default: 0.0\n    Something-To-Search:color, Type: Categorical, Choices: {red, green, blue}, Default: blue, Probabilities: (0.5, 0.25, 0.25)\n    Something-To-Search:lr, Type: UniformFloat, Range: [1e-05, 1.0], Default: 0.3, on log-scale\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace--conditional-ands-advanced-usage","title":"Conditional ands Advanced Usage","text":"<p>We will refer you to the ConfigSpace documentation for the construction of these. However once you've constructed a <code>ConfigurationSpace</code> and added any forbiddens and conditionals, you may simply set that as the <code>.space</code> attribute.</p> <pre><code>from amltk.pipeline import Component, Choice, Sequential\nfrom ConfigSpace import ConfigurationSpace, EqualsCondition, InCondition\n\nmyspace = ConfigurationSpace({\"A\": [\"red\", \"green\", \"blue\"], \"B\": (1, 10), \"C\": (-100.0, 0.0)})\nmyspace.add_conditions([\n    EqualsCondition(myspace[\"B\"], myspace[\"A\"], \"red\"),  # B is active when A is red\n    InCondition(myspace[\"C\"], myspace[\"A\"], [\"green\", \"blue\"]), # C is active when A is green or blue\n])\n\ncomponent = Component(object, space=myspace, name=\"MyThing\")\n\nparsed_space = component.search_space(\"configspace\")\nprint(parsed_space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MyThing:A, Type: Categorical, Choices: {red, green, blue}, Default: red\n    MyThing:B, Type: UniformInteger, Range: [1, 10], Default: 6\n    MyThing:C, Type: UniformFloat, Range: [-100.0, 0.0], Default: -50.0\n  Conditions:\n    MyThing:B | MyThing:A == 'red'\n    MyThing:C | MyThing:A in {'green', 'blue'}\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/configspace/#amltk.pipeline.parsers.configspace.parser","title":"<code>def parser(node, *, seed=None, flat=False, conditionals=True, delim=':')</code>","text":"<p>Parse a Node and its children into a ConfigurationSpace.</p> PARAMETER  DESCRIPTION <code>node</code> <p>The Node to parse</p> <p> TYPE: <code>Node</code> </p> <code>seed</code> <p>The seed to use for the ConfigurationSpace</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>flat</code> <p>Whether to have a heirarchical naming scheme for nodes and their children.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conditionals</code> <p>Whether to include conditionals in the space from a <code>Choice</code>. If this is <code>False</code>, this will also remove all forbidden clauses and other conditional clauses. The primary use of this functionality is that some optimizers do not support these features.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>delim</code> <p>The delimiter to use for the names of the hyperparameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>':'</code> </p> Source code in <code>src/amltk/pipeline/parsers/configspace.py</code> <pre><code>def parser(\n    node: Node,\n    *,\n    seed: int | None = None,\n    flat: bool = False,\n    conditionals: bool = True,\n    delim: str = \":\",\n) -&gt; ConfigurationSpace:\n    \"\"\"Parse a Node and its children into a ConfigurationSpace.\n\n    Args:\n        node: The Node to parse\n        seed: The seed to use for the ConfigurationSpace\n        flat: Whether to have a heirarchical naming scheme for nodes and their children.\n        conditionals: Whether to include conditionals in the space from a\n            [`Choice`][amltk.pipeline.Choice]. If this is `False`, this will\n            also remove all forbidden clauses and other conditional clauses.\n            The primary use of this functionality is that some optimizers do not\n            support these features.\n        delim: The delimiter to use for the names of the hyperparameters\n    \"\"\"\n    space = ConfigurationSpace(seed=seed)\n    space.add_configuration_space(\n        prefix=node.name,\n        delimiter=delim,\n        configuration_space=_parse_space(node, seed=seed, conditionals=conditionals),\n    )\n\n    children = node.nodes\n\n    choice = None\n    if isinstance(node, Choice) and any(children):\n        choice = Categorical(\n            name=f\"{node.name}{delim}__choice__\",\n            items=[child.name for child in children],\n        )\n        space.add_hyperparameter(choice)\n\n    for child in children:\n        space.add_configuration_space(\n            prefix=node.name if not flat else \"\",\n            delimiter=delim if not flat else \"\",\n            configuration_space=parser(\n                child,\n                seed=seed,\n                flat=flat,\n                conditionals=conditionals,\n                delim=delim,\n            ),\n            parent_hyperparameter=(\n                {\"parent\": choice, \"value\": child.name}\n                if choice and conditionals\n                else None\n            ),\n        )\n\n    return space\n</code></pre>"},{"location":"api/amltk/pipeline/parsers/optuna/","title":"Optuna","text":"<p>Optuna parser for parsing out a <code>search_space()</code>. from a pipeline.</p> <p>Requirements</p> <p>This requires <code>Optuna</code> which can be installed with:</p> <pre><code>pip install amltk[optuna]\n\n# Or directly\npip install optuna\n</code></pre> Limitations <p>Optuna feature a very dynamic search space (define-by-run), where people typically sample from some trial object and use traditional python control flow to define conditionality.</p> <p>This means we can not trivially represent this conditionality in a static search space. While band-aids are possible, it naturally does not sit well with the static output of a parser.</p> <p>As such, our parser does not support conditionals or choices!. Users may still use the define-by-run within their optimization function itself.</p> <p>If you have experience with Optuna and have any suggestions, please feel free to open an issue or PR on GitHub!</p>"},{"location":"api/amltk/pipeline/parsers/optuna/#amltk.pipeline.parsers.optuna--usage","title":"Usage","text":"<p>The typical way to represent a search space for Optuna is just to use a dictionary, where the keys are the names of the hyperparameters and the values are either integer/float tuples indicating boundaries or some discrete set of values. It is possible to have the value directly be a <code>BaseDistribution</code>, an optuna type, when you need to customize the distribution more.</p> <pre><code>from amltk.pipeline import Component\nfrom optuna.distributions import FloatDistribution\n\nc = Component(\n    object,\n    space={\n        \"myint\": (1, 10),\n        \"myfloat\": (1.0, 10.0),\n        \"mycategorical\": [\"a\", \"b\", \"c\"],\n        \"log-scale-custom\": FloatDistribution(1e-10, 1e-2, log=True),\n    },\n    name=\"name\",\n)\n\nspace = c.search_space(parser=\"optuna\")\n</code></pre> <p> <pre>\n<code>{\n    'name:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'name:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, step=None),\n    'name:mycategorical': CategoricalDistribution(choices=('a', 'b', 'c')),\n    'name:log-scale-custom': FloatDistribution(high=0.01, log=True, low=1e-10, \nstep=None)\n}\n</code>\n</pre> </p> <p>You may also just pass the <code>parser=</code> function directly if preferred</p> <pre><code>from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\nspace = c.search_space(parser=optuna_parser)\n</code></pre> <p> <pre>\n<code>{\n    'name:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'name:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, step=None),\n    'name:mycategorical': CategoricalDistribution(choices=('a', 'b', 'c')),\n    'name:log-scale-custom': FloatDistribution(high=0.01, log=True, low=1e-10, \nstep=None)\n}\n</code>\n</pre> </p> <p>When using <code>search_space()</code> on a some nested structures, you may want to flatten the names of the hyperparameters. For this you can use <code>flat=</code></p> <pre><code>from amltk.pipeline import Searchable, Sequential\n\nseq = Sequential(\n    Searchable({\"myint\": (1, 10)}, name=\"nested_1\"),\n    Searchable({\"myfloat\": (1.0, 10.0)}, name=\"nested_2\"),\n    name=\"seq\"\n)\n\nhierarchical_space = seq.search_space(parser=\"optuna\", flat=False)  # Default\n\nflat_space = seq.search_space(parser=\"optuna\", flat=False)  # Default\n</code></pre> <p> <pre>\n<code>{\n    'seq:nested_1:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'seq:nested_2:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, \nstep=None)\n}\n</code>\n</pre> <pre>\n<code>{\n    'seq:nested_1:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'seq:nested_2:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, \nstep=None)\n}\n</code>\n</pre> </p>"},{"location":"api/amltk/pipeline/parsers/optuna/#amltk.pipeline.parsers.optuna.parser","title":"<code>def parser(node, *, flat=False, conditionals=False, delim=':')</code>","text":"<p>Parse a Node and its children into a ConfigurationSpace.</p> PARAMETER  DESCRIPTION <code>node</code> <p>The Node to parse</p> <p> TYPE: <code>Node</code> </p> <code>flat</code> <p>Whether to have a hierarchical naming scheme for nodes and their children.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conditionals</code> <p>Whether to include conditionals in the space from a <code>Choice</code>. If this is <code>False</code>, this will also remove all forbidden clauses and other conditional clauses. The primary use of this functionality is that some optimizers do not support these features.</p> <p>Not yet supported</p> <p>This functionality is not yet supported as we can't encode this into a static Optuna search space.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>delim</code> <p>The delimiter to use for the names of the hyperparameters.</p> <p> TYPE: <code>str</code> DEFAULT: <code>':'</code> </p> Source code in <code>src/amltk/pipeline/parsers/optuna.py</code> <pre><code>def parser(\n    node: Node,\n    *,\n    flat: bool = False,\n    conditionals: bool = False,\n    delim: str = \":\",\n) -&gt; OptunaSearchSpace:\n    \"\"\"Parse a Node and its children into a ConfigurationSpace.\n\n    Args:\n        node: The Node to parse\n        flat: Whether to have a hierarchical naming scheme for nodes and their children.\n        conditionals: Whether to include conditionals in the space from a\n            [`Choice`][amltk.pipeline.Choice]. If this is `False`, this will\n            also remove all forbidden clauses and other conditional clauses.\n            The primary use of this functionality is that some optimizers do not\n            support these features.\n\n            !!! TODO \"Not yet supported\"\n\n                This functionality is not yet supported as we can't encode this into\n                a static Optuna search space.\n\n        delim: The delimiter to use for the names of the hyperparameters.\n    \"\"\"\n    if conditionals:\n        raise NotImplementedError(\"Conditionals are not yet supported with Optuna.\")\n\n    space = prefix_keys(_parse_space(node), prefix=f\"{node.name}{delim}\")\n\n    for child in node.nodes:\n        subspace = parser(child, flat=flat, conditionals=conditionals, delim=delim)\n        if not flat:\n            subspace = prefix_keys(subspace, prefix=f\"{node.name}{delim}\")\n\n        for name, hp in subspace.items():\n            if name in space:\n                raise ValueError(\n                    f\"Duplicate name {name} already in space from space of {node.name}\",\n                    f\"\\nCurrently parsed space: {space}\",\n                )\n            space[name] = hp\n\n    return space\n</code></pre>"},{"location":"api/amltk/profiling/memory/","title":"Memory","text":"<p>Module to measure memory.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory","title":"<code>class Memory</code>   <code>dataclass</code>","text":"<p>A timer for measuring the time between two events.</p> ATTRIBUTE DESCRIPTION <code>start_vms</code> <p>The virtual memory size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>start_rss</code> <p>The resident set size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>unit</code> <p>The unit of the memory.</p> <p> TYPE: <code>Unit | NAType</code> </p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval","title":"<code>class Interval</code>   <code>dataclass</code>","text":"<p>A class for representing a time interval.</p> ATTRIBUTE DESCRIPTION <code>start_vms</code> <p>The virtual memory size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>start_rss</code> <p>The resident set size at the start of the interval.</p> <p> TYPE: <code>float</code> </p> <code>end_vms</code> <p>The virtual memory size at the end of the interval.</p> <p> TYPE: <code>float</code> </p> <code>end_rss</code> <p>The resident set size at the end of the interval.</p> <p> TYPE: <code>float</code> </p> <code>unit</code> <p>The unit of memory.</p> <p> TYPE: <code>Unit | NAType</code> </p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.vms_used","title":"<code>vms_used: float</code>   <code>prop</code>","text":"<p>The amount of vms memory used in the interval.</p> <p>Warning</p> <p>This does not track peak memory usage. This will only give the difference between the start and end of the interval.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.rss_used","title":"<code>rss_used: float</code>   <code>prop</code>","text":"<p>The amount of rss memory used in the interval.</p> <p>Warning</p> <p>This does not track peak memory usage. This will only give the difference between the start and end of the interval.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.to_unit","title":"<code>def to_unit(unit)</code>","text":"<p>Return the memory used in a different unit.</p> PARAMETER  DESCRIPTION <code>unit</code> <p>The unit to convert to.</p> <p> TYPE: <code>Unit</code> </p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory used in the new unit.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>def to_unit(self, unit: Memory.Unit) -&gt; Memory.Interval:\n    \"\"\"Return the memory used in a different unit.\n\n    Args:\n        unit: The unit to convert to.\n\n    Returns:\n        The memory used in the new unit.\n    \"\"\"\n    if self.unit == unit:\n        return self\n\n    return Memory.Interval(\n        start_vms=Memory.convert(self.start_vms, self.unit, unit),\n        end_vms=Memory.convert(self.end_vms, self.unit, unit),\n        start_rss=Memory.convert(self.start_rss, self.unit, unit),\n        end_rss=Memory.convert(self.end_rss, self.unit, unit),\n        unit=unit,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Interval.to_dict","title":"<code>def to_dict(*, prefix='')</code>","text":"<p>Convert the time interval to a dictionary.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>def to_dict(self, *, prefix: str = \"\") -&gt; dict[str, Any]:\n    \"\"\"Convert the time interval to a dictionary.\"\"\"\n    return {\n        f\"{prefix}start_vms\": self.start_vms,\n        f\"{prefix}end_vms\": self.end_vms,\n        f\"{prefix}diff_vms\": self.vms_used,\n        f\"{prefix}start_rss\": self.start_rss,\n        f\"{prefix}end_rss\": self.end_rss,\n        f\"{prefix}diff_rss\": self.rss_used,\n        f\"{prefix}unit\": self.unit,\n    }\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Unit","title":"<code>class Unit</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>An enum for the units of time.</p>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.Unit.from_str","title":"<code>def from_str(s)</code>   <code>classmethod</code>","text":"<p>Convert a string to a unit.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef from_str(cls, s: Any) -&gt; Memory.Unit | NAType:\n    \"\"\"Convert a string to a unit.\"\"\"\n    if isinstance(s, Memory.Unit):\n        return s\n\n    if isinstance(s, str):\n        _mapping = {\n            \"bytes\": Memory.Unit.BYTES,\n            \"b\": Memory.Unit.BYTES,\n            \"kilobytes\": Memory.Unit.KILOBYTES,\n            \"kb\": Memory.Unit.KILOBYTES,\n            \"megabytes\": Memory.Unit.MEGABYTES,\n            \"mb\": Memory.Unit.MEGABYTES,\n            \"gigabytes\": Memory.Unit.GIGABYTES,\n            \"gb\": Memory.Unit.GIGABYTES,\n        }\n        return _mapping.get(s.lower(), pd.NA)\n\n    return pd.NA\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create a memory interval from a dictionary.</p> PARAMETER  DESCRIPTION <code>d</code> <p>The dictionary to create from.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory interval.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Memory.Interval:\n    \"\"\"Create a memory interval from a dictionary.\n\n    Args:\n        d: The dictionary to create from.\n\n    Returns:\n        The memory interval.\n    \"\"\"\n    return Memory.Interval(\n        start_vms=dict_get_not_none(d, \"start_vms\", np.nan),\n        start_rss=dict_get_not_none(d, \"start_rss\", np.nan),\n        end_vms=dict_get_not_none(d, \"end_vms\", np.nan),\n        end_rss=dict_get_not_none(d, \"end_rss\", np.nan),\n        unit=Memory.Unit.from_str(dict_get_not_none(d, \"unit\", pd.NA)),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.na","title":"<code>def na()</code>   <code>classmethod</code>","text":"<p>Create a memory interval that represents NA.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef na(cls) -&gt; Memory.Interval:\n    \"\"\"Create a memory interval that represents NA.\"\"\"\n    return Memory.Interval(\n        start_vms=np.nan,\n        end_vms=np.nan,\n        start_rss=np.nan,\n        end_rss=np.nan,\n        unit=pd.NA,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.convert","title":"<code>def convert(x, frm='B', to='B')</code>   <code>classmethod</code>","text":"<p>Convert a value from one unit to another.</p> PARAMETER  DESCRIPTION <code>x</code> <p>The value to convert.</p> <p> TYPE: <code>float</code> </p> <code>frm</code> <p>The unit of the value.</p> <p> TYPE: <code>Unit | NAType | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>to</code> <p>The unit to convert to.</p> <p> TYPE: <code>Unit | NAType | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The converted value.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef convert(\n    cls,\n    x: float,\n    frm: Memory.Unit | NAType | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n    to: Memory.Unit | NAType | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; float:\n    \"\"\"Convert a value from one unit to another.\n\n    Args:\n        x: The value to convert.\n        frm: The unit of the value.\n        to: The unit to convert to.\n\n    Returns:\n        The converted value.\n    \"\"\"\n    if frm == to:\n        return x\n\n    _frm = cls.Unit.from_str(frm) if isinstance(frm, str) else frm\n    _to = cls.Unit.from_str(to) if isinstance(to, str) else to\n    return x * _CONVERSION[_frm] / _CONVERSION[_to]\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.measure","title":"<code>def measure(unit='B')</code>   <code>classmethod</code>","text":"<p>Measure the memory used by a block of code.</p> PARAMETER  DESCRIPTION <code>unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> YIELDS DESCRIPTION <code>Interval</code> <p>The Memory Interval. The start and end memory will not be</p> <code>Interval</code> <p>valid until the context manager is exited.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\n@contextmanager\ndef measure(\n    cls,\n    unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; Iterator[Memory.Interval]:\n    \"\"\"Measure the memory used by a block of code.\n\n    Args:\n        unit: The unit of memory to use.\n\n    Yields:\n        The Memory Interval. The start and end memory will not be\n        valid until the context manager is exited.\n\n    \"\"\"\n    mem = cls.start(unit=unit)\n\n    interval = Memory.na()\n    interval.unit = mem.unit\n\n    try:\n        yield interval\n    finally:\n        _interval = mem.stop()\n        interval.start_vms = _interval.start_vms\n        interval.end_vms = _interval.end_vms\n        interval.start_rss = _interval.start_rss\n        interval.end_rss = _interval.end_rss\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.start","title":"<code>def start(unit='B')</code>   <code>classmethod</code>","text":"<p>Start a memory tracker.</p> PARAMETER  DESCRIPTION <code>unit</code> <p>The unit of memory to use (bytes).</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> RETURNS DESCRIPTION <code>Memory</code> <p>The Memory tracker.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef start(\n    cls,\n    unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; Memory:\n    \"\"\"Start a memory tracker.\n\n    Args:\n        unit: The unit of memory to use (bytes).\n\n    Returns:\n        The Memory tracker.\n    \"\"\"\n    proc = psutil.Process()\n    info = proc.memory_info()\n\n    return Memory(\n        start_vms=Memory.convert(info.vms, frm=Memory.Unit.BYTES, to=unit),\n        start_rss=Memory.convert(info.rss, frm=Memory.Unit.BYTES, to=unit),\n        unit=unit if isinstance(unit, Memory.Unit) else Memory.Unit.from_str(unit),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.stop","title":"<code>def stop()</code>","text":"<p>Stop the memory tracker.</p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory interval.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>def stop(self) -&gt; Memory.Interval:\n    \"\"\"Stop the memory tracker.\n\n    Returns:\n        The memory interval.\n    \"\"\"\n    proc = psutil.Process()\n    info = proc.memory_info()\n\n    return Memory.Interval(\n        start_vms=self.start_vms,\n        start_rss=self.start_rss,\n        end_vms=Memory.convert(info.vms, frm=Memory.Unit.BYTES, to=self.unit),\n        end_rss=Memory.convert(info.rss, frm=Memory.Unit.BYTES, to=self.unit),\n        unit=self.unit,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/memory/#amltk.profiling.memory.Memory.usage","title":"<code>def usage(kind='vms', unit='B')</code>   <code>classmethod</code>","text":"<p>Get the memory used.</p> PARAMETER  DESCRIPTION <code>kind</code> <p>The type of memory to measure.</p> <p> TYPE: <code>Literal['rss', 'vms']</code> DEFAULT: <code>'vms'</code> </p> <code>unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The memory used.</p> Source code in <code>src/amltk/profiling/memory.py</code> <pre><code>@classmethod\ndef usage(\n    cls,\n    kind: Literal[\"rss\", \"vms\"] = \"vms\",\n    unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n) -&gt; float:\n    \"\"\"Get the memory used.\n\n    Args:\n        kind: The type of memory to measure.\n        unit: The unit of memory to use.\n\n    Returns:\n        The memory used.\n    \"\"\"\n    proc = psutil.Process()\n    _kind = kind.lower()\n    if _kind == \"rss\":\n        return Memory.convert(\n            proc.memory_info().rss,\n            frm=Memory.Unit.BYTES,\n            to=unit,\n        )\n\n    if _kind == \"vms\":\n        return Memory.convert(\n            proc.memory_info().vms,\n            frm=Memory.Unit.BYTES,\n            to=unit,\n        )\n\n    raise ValueError(f\"Unknown memory type: {kind}\")\n</code></pre>"},{"location":"api/amltk/profiling/profiler/","title":"Profiler","text":"<p>Whether for debugging, building an AutoML system or for optimization purposes, we provide a powerful <code>Profiler</code>, which can generate a <code>Profile</code> of different sections of code. This is particularly useful with <code>Trial</code>s, so much so that we attach one to every <code>Trial</code> made as <code>trial.profiler</code>.</p> <p>When done profiling, you can export all generated profiles as a dataframe using <code>profiler.df()</code>.</p> <pre><code>from amltk.profiling import Profiler\nimport numpy as np\n\nprofiler = Profiler()\n\nwith profiler(\"loading-data\"):\n    X = np.random.rand(1000, 1000)\n\nwith profiler(\"training-model\"):\n    model = np.linalg.inv(X)\n\nwith profiler(\"predicting\"):\n    y = model @ X\n\nprint(profiler.df())\n</code></pre> <pre><code>                memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nloading-data        1.976168e+09      1984172032  ...       wall    seconds\ntraining-model      1.984172e+09      2045620224  ...       wall    seconds\npredicting          2.045620e+09      2054144000  ...       wall    seconds\n\n[3 rows x 12 columns]\n</code></pre> <p>You'll find these profiles as keys in the <code>Profiler</code>, e.g. <code>python profiler[\"loading-data\"]</code>.</p> <p>This will measure both the time it took within the block but also the memory consumed before and after the block finishes, allowing you to get an estimate of the memory consumed.</p> Memory, vms vs rms <p>While not entirely accurate, this should be enough for info for most use cases.</p> <p>Given the main process uses 2GB of memory and the process then spawns a new process in which you are profiling, as you might do from a <code>Task</code>. In this new process you use another 2GB on top of that, then:</p> <ul> <li> <p>The virtual memory size (vms) will show 4GB as the new process will share the 2GB with the main process and have it's own 2GB.</p> </li> <li> <p>The resident set size (rss) will show 2GB as the new process will only have 2GB of it's own memory.</p> </li> </ul> <p>If you need to profile some iterator, like a for loop, you can use <code>Profiler.each()</code> which will measure the entire loop but also each individual iteration. This can be useful for iterating batches of a deep-learning model, splits of a cross-validator or really any loop with work you want to profile.</p> <pre><code>from amltk.profiling import Profiler\nimport numpy as np\n\nprofiler = Profiler()\n\nfor i in profiler.each(range(3), name=\"for-loop\"):\n    X = np.random.rand(1000, 1000)\n\nprint(profiler.df())\n</code></pre> <pre><code>            memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nfor-loop        2.038137e+09      2045612032  ...       wall    seconds\nfor-loop:0      2.038137e+09      2038136832  ...       wall    seconds\nfor-loop:1      2.038137e+09      2045612032  ...       wall    seconds\nfor-loop:2      2.045612e+09      2045612032  ...       wall    seconds\n\n[4 rows x 12 columns]\n</code></pre> <p>Lastly, to disable profiling without editing much code, you can always use <code>Profiler.disable()</code> and <code>Profiler.enable()</code> to toggle profiling on and off.</p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile","title":"<code>class Profile</code>   <code>dataclass</code>","text":"<p>A profiler for measuring statistics between two events.</p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.Interval","title":"<code>class Interval</code>   <code>dataclass</code>","text":"<p>A class for representing a profiled interval.</p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.Interval.to_dict","title":"<code>def to_dict(*, prefix='')</code>","text":"<p>Convert the profile interval to a dictionary.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def to_dict(self, *, prefix: str = \"\") -&gt; dict[str, Any]:\n    \"\"\"Convert the profile interval to a dictionary.\"\"\"\n    _prefix = \"\" if prefix == \"\" else f\"{prefix}:\"\n    return {\n        **self.memory.to_dict(prefix=f\"{_prefix}memory:\"),\n        **self.time.to_dict(prefix=f\"{_prefix}time:\"),\n    }\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create a profile interval from a dictionary.</p> PARAMETER  DESCRIPTION <code>d</code> <p>The dictionary to create from.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Interval</code> <p>The profile interval.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Profile.Interval:\n    \"\"\"Create a profile interval from a dictionary.\n\n    Args:\n        d: The dictionary to create from.\n\n    Returns:\n        The profile interval.\n    \"\"\"\n    return Profile.Interval(\n        memory=Memory.from_dict(mapping_select(d, \"memory:\")),\n        time=Timer.from_dict(mapping_select(d, \"time:\")),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.measure","title":"<code>def measure(*, memory_unit='B', time_kind='wall')</code>   <code>classmethod</code>","text":"<p>Profile a block of code.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER  DESCRIPTION <code>memory_unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>time_kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process']</code> DEFAULT: <code>'wall'</code> </p> YIELDS DESCRIPTION <code>Interval</code> <p>The Profiler Interval. Memory and Timings will not be valid until</p> <code>Interval</code> <p>the context manager is exited.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\n@contextmanager\ndef measure(\n    cls,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] = \"wall\",\n) -&gt; Iterator[Profile.Interval]:\n    \"\"\"Profile a block of code.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        memory_unit: The unit of memory to use.\n        time_kind: The type of timer to use.\n\n    Yields:\n        The Profiler Interval. Memory and Timings will not be valid until\n        the context manager is exited.\n    \"\"\"\n    with Memory.measure(unit=memory_unit) as memory, Timer.time(\n        kind=time_kind,\n    ) as timer:\n        yield Profile.Interval(memory=memory, time=timer)\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.start","title":"<code>def start(memory_unit='B', time_kind='wall')</code>   <code>classmethod</code>","text":"<p>Start a memory tracker.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER  DESCRIPTION <code>memory_unit</code> <p>The unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>time_kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process']</code> DEFAULT: <code>'wall'</code> </p> RETURNS DESCRIPTION <code>Profile</code> <p>The Memory tracker.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\ndef start(\n    cls,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] = \"B\",\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] = \"wall\",\n) -&gt; Profile:\n    \"\"\"Start a memory tracker.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        memory_unit: The unit of memory to use.\n        time_kind: The type of timer to use.\n\n    Returns:\n        The Memory tracker.\n    \"\"\"\n    return Profile(\n        timer=Timer.start(kind=time_kind),\n        memory=Memory.start(unit=memory_unit),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.stop","title":"<code>def stop()</code>","text":"<p>Stop the memory tracker.</p> RETURNS DESCRIPTION <code>Interval</code> <p>The memory interval.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def stop(self) -&gt; Profile.Interval:\n    \"\"\"Stop the memory tracker.\n\n    Returns:\n        The memory interval.\n    \"\"\"\n    return Profile.Interval(\n        memory=self.memory.stop(),\n        time=self.timer.stop(),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profile.na","title":"<code>def na()</code>   <code>classmethod</code>","text":"<p>Create a profile interval that represents NA.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@classmethod\ndef na(cls) -&gt; Profile.Interval:\n    \"\"\"Create a profile interval that represents NA.\"\"\"\n    return Profile.Interval(memory=Memory.na(), time=Timer.na())\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler","title":"<code>class Profiler</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Mapping[str, Interval]</code></p> <p>Profile and record various events.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER  DESCRIPTION <code>memory_unit</code> <p>The default unit of memory to use.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB']</code> DEFAULT: <code>'B'</code> </p> <code>time_kind</code> <p>The default type of timer to use.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process']</code> DEFAULT: <code>'wall'</code> </p>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__getitem__","title":"<code>def __getitem__(key)</code>","text":"<p>Get a profile interval.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@override\ndef __getitem__(self, key: str) -&gt; Profile.Interval:\n    \"\"\"Get a profile interval.\"\"\"\n    return self.profiles[key]\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__iter__","title":"<code>def __iter__()</code>","text":"<p>Iterate over the profile names.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@override\ndef __iter__(self) -&gt; Iterator[str]:\n    \"\"\"Iterate over the profile names.\"\"\"\n    return iter(self.profiles)\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__len__","title":"<code>def __len__()</code>","text":"<p>Get the number of profiles.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@override\ndef __len__(self) -&gt; int:\n    \"\"\"Get the number of profiles.\"\"\"\n    return len(self.profiles)\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.disable","title":"<code>def disable()</code>","text":"<p>Disable the profiler.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def disable(self) -&gt; None:\n    \"\"\"Disable the profiler.\"\"\"\n    self.disabled = True\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.enable","title":"<code>def enable()</code>","text":"<p>Enable the profiler.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def enable(self) -&gt; None:\n    \"\"\"Enable the profiler.\"\"\"\n    self.disabled = False\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.each","title":"<code>def each(itr, *, name, itr_name=None)</code>","text":"<p>Profile each item in an iterable.</p> PARAMETER  DESCRIPTION <code>itr</code> <p>The iterable to profile.</p> <p> TYPE: <code>Iterable[T]</code> </p> <code>name</code> <p>The name of the profile that lasts until iteration is complete</p> <p> TYPE: <code>str</code> </p> <code>itr_name</code> <p>The name of the profile for each iteration. If a function is provided, it will be called with each item's index and the item. It should return a string. If <code>None</code> is provided, just the index will be used.</p> <p> TYPE: <code>Callable[[int, T], str] | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>T</code> <p>The the items</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def each(\n    self,\n    itr: Iterable[T],\n    *,\n    name: str,\n    itr_name: Callable[[int, T], str] | None = None,\n) -&gt; Iterator[T]:\n    \"\"\"Profile each item in an iterable.\n\n    Args:\n        itr: The iterable to profile.\n        name: The name of the profile that lasts until iteration is complete\n        itr_name: The name of the profile for each iteration.\n            If a function is provided, it will be called with each item's index\n            and the item. It should return a string. If `None` is provided,\n            just the index will be used.\n\n    Yields:\n        The the items\n    \"\"\"\n    if itr_name is None:\n        itr_name = lambda i, _: str(i)\n    with self.measure(name=name):\n        for i, item in enumerate(itr):\n            with self.measure(name=itr_name(i, item)):\n                yield item\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__call__","title":"<code>def __call__(name, *, memory_unit=None, time_kind=None)</code>","text":"<p>Profile a block of code. Store the result on this object.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the profile.</p> <p> TYPE: <code>str</code> </p> <code>memory_unit</code> <p>The unit of memory to use. Overwrites the default.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> <code>time_kind</code> <p>The type of timer to use. Overwrites the default.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@contextmanager\ndef measure(\n    self,\n    name: str,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"Profile a block of code. Store the result on this object.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        name: The name of the profile.\n        memory_unit: The unit of memory to use. Overwrites the default.\n        time_kind: The type of timer to use. Overwrites the default.\n    \"\"\"\n    if self.disabled:\n        yield\n        return\n\n    memory_unit = memory_unit or self.memory_unit\n    time_kind = time_kind or self.time_kind\n\n    self._running.append(name)\n    entry_name = \":\".join(self._running)\n\n    with Profile.measure(memory_unit=memory_unit, time_kind=time_kind) as profile:\n        self.profiles[entry_name] = profile\n        yield\n\n    self._running.pop()\n</code></pre> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@contextmanager\ndef __call__(\n    self,\n    name: str,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"::: amltk.profiling.Profiler.measure\"\"\"  # noqa: D415\n    with self.measure(name, memory_unit=memory_unit, time_kind=time_kind):\n        yield\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.measure","title":"<code>def measure(name, *, memory_unit=None, time_kind=None)</code>","text":"<p>Profile a block of code. Store the result on this object.</p> <p>Note</p> <ul> <li>See <code>Memory</code> for more information on memory.</li> <li>See <code>Timer</code> for more information on timing.</li> </ul> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the profile.</p> <p> TYPE: <code>str</code> </p> <code>memory_unit</code> <p>The unit of memory to use. Overwrites the default.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> <code>time_kind</code> <p>The type of timer to use. Overwrites the default.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>@contextmanager\ndef measure(\n    self,\n    name: str,\n    *,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    time_kind: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"Profile a block of code. Store the result on this object.\n\n    !!! note\n\n        * See [`Memory`][amltk.profiling.Memory] for more information on memory.\n        * See [`Timer`][amltk.profiling.Timer] for more information on timing.\n\n    Args:\n        name: The name of the profile.\n        memory_unit: The unit of memory to use. Overwrites the default.\n        time_kind: The type of timer to use. Overwrites the default.\n    \"\"\"\n    if self.disabled:\n        yield\n        return\n\n    memory_unit = memory_unit or self.memory_unit\n    time_kind = time_kind or self.time_kind\n\n    self._running.append(name)\n    entry_name = \":\".join(self._running)\n\n    with Profile.measure(memory_unit=memory_unit, time_kind=time_kind) as profile:\n        self.profiles[entry_name] = profile\n        yield\n\n    self._running.pop()\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.df","title":"<code>def df()</code>","text":"<p>Convert the profiler to a dataframe.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def df(self) -&gt; pd.DataFrame:\n    \"\"\"Convert the profiler to a dataframe.\"\"\"\n    return pd.DataFrame.from_dict(\n        {k: v.to_dict() for k, v in self.profiles.items()},\n        orient=\"index\",\n    )\n</code></pre>"},{"location":"api/amltk/profiling/profiler/#amltk.profiling.profiler.Profiler.__rich__","title":"<code>def __rich__()</code>","text":"<p>Render the profiler.</p> Source code in <code>src/amltk/profiling/profiler.py</code> <pre><code>def __rich__(self) -&gt; RenderableType:\n    \"\"\"Render the profiler.\"\"\"\n    from amltk._richutil import df_to_table\n\n    _df = self.df()\n    return df_to_table(_df, title=\"Profiler\", index_style=\"bold\")\n</code></pre>"},{"location":"api/amltk/profiling/timing/","title":"Timing","text":"<p>Module for timing things.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer","title":"<code>class Timer</code>   <code>dataclass</code>","text":"<p>A timer for measuring the time between two events.</p> ATTRIBUTE DESCRIPTION <code>start_time</code> <p>The time at which the timer was started.</p> <p> TYPE: <code>float</code> </p> <code>kind</code> <p>The method of timing.</p> <p> TYPE: <code>Kind | NAType</code> </p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Interval","title":"<code>class Interval</code>   <code>dataclass</code>","text":"<p>A time interval.</p> ATTRIBUTE DESCRIPTION <code>start</code> <p>The start time.</p> <p> TYPE: <code>float</code> </p> <code>end</code> <p>The end time.</p> <p> TYPE: <code>float</code> </p> <code>kind</code> <p>The type of timer used.</p> <p> TYPE: <code>Kind | NAType</code> </p> <code>unit</code> <p>The unit of time.</p> <p> TYPE: <code>Unit | NAType</code> </p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Interval.duration","title":"<code>duration: float</code>   <code>prop</code>","text":"<p>The duration of the time interval.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Interval.to_dict","title":"<code>def to_dict(*, prefix='')</code>","text":"<p>Convert the time interval to a dictionary.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>def to_dict(self, *, prefix: str = \"\") -&gt; dict[str, Any]:\n    \"\"\"Convert the time interval to a dictionary.\"\"\"\n    return {\n        f\"{prefix}start\": self.start,\n        f\"{prefix}end\": self.end,\n        f\"{prefix}duration\": self.duration,\n        f\"{prefix}kind\": self.kind,\n        f\"{prefix}unit\": self.unit,\n    }\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Kind","title":"<code>class Kind</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>An enum for the type of timer.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Kind.from_str","title":"<code>def from_str(key)</code>   <code>classmethod</code>","text":"<p>Get the enum value from a string.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The string to convert.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Kind | NAType</code> <p>The enum value.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef from_str(cls, key: Any) -&gt; Timer.Kind | NAType:\n    \"\"\"Get the enum value from a string.\n\n    Args:\n        key: The string to convert.\n\n    Returns:\n        The enum value.\n    \"\"\"\n    if isinstance(key, Timer.Kind):\n        return key\n\n    if isinstance(key, str):\n        try:\n            return Timer.Kind[key.upper()]\n        except KeyError:\n            return pd.NA\n\n    return pd.NA\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Unit","title":"<code>class Unit</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>An enum for the units of time.</p>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.Unit.from_str","title":"<code>def from_str(key)</code>   <code>classmethod</code>","text":"<p>Get the enum value from a string.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The string to convert.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>Unit | NAType</code> <p>The enum value.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef from_str(cls, key: Any) -&gt; Timer.Unit | NAType:\n    \"\"\"Get the enum value from a string.\n\n    Args:\n        key: The string to convert.\n\n    Returns:\n        The enum value.\n    \"\"\"\n    if isinstance(key, Timer.Unit):\n        return key\n\n    if isinstance(key, str):\n        try:\n            return Timer.Unit[key.upper()]\n        except KeyError:\n            return pd.NA\n\n    return pd.NA\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create a time interval from a dictionary.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Timer.Interval:\n    \"\"\"Create a time interval from a dictionary.\"\"\"\n    return Timer.Interval(\n        start=dict_get_not_none(d, \"start\", np.nan),\n        end=dict_get_not_none(d, \"end\", np.nan),\n        kind=Timer.Kind.from_str(dict_get_not_none(d, \"kind\", pd.NA)),\n        unit=Timer.Unit.from_str(dict_get_not_none(d, \"unit\", pd.NA)),\n    )\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.na","title":"<code>def na()</code>   <code>classmethod</code>","text":"<p>Create a time interval with all values set to <code>None</code>.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef na(cls) -&gt; Timer.Interval:\n    \"\"\"Create a time interval with all values set to `None`.\"\"\"\n    return Timer.Interval(\n        start=np.nan,\n        end=np.nan,\n        kind=pd.NA,\n        unit=pd.NA,\n    )\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.time","title":"<code>def time(kind='wall')</code>   <code>classmethod</code>","text":"<p>Time a block of code.</p> PARAMETER  DESCRIPTION <code>kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['cpu', 'wall', 'process']</code> DEFAULT: <code>'wall'</code> </p> YIELDS DESCRIPTION <code>Interval</code> <p>The Time Interval. The start and end times will not be</p> <code>Interval</code> <p>valid until the context manager is exited.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\n@contextmanager\ndef time(\n    cls,\n    kind: Timer.Kind | Literal[\"cpu\", \"wall\", \"process\"] = \"wall\",\n) -&gt; Iterator[Interval]:\n    \"\"\"Time a block of code.\n\n    Args:\n        kind: The type of timer to use.\n\n    Yields:\n        The Time Interval. The start and end times will not be\n        valid until the context manager is exited.\n    \"\"\"\n    timer = cls.start(kind=kind)\n\n    interval = Timer.na()\n    interval.kind = timer.kind\n    interval.unit = Timer.Unit.SECONDS\n\n    try:\n        yield interval\n    finally:\n        _interval = timer.stop()\n        interval.start = _interval.start\n        interval.end = _interval.end\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.start","title":"<code>def start(kind='wall')</code>   <code>classmethod</code>","text":"<p>Start a timer.</p> PARAMETER  DESCRIPTION <code>kind</code> <p>The type of timer to use.</p> <p> TYPE: <code>Kind | Literal['cpu', 'wall', 'process']</code> DEFAULT: <code>'wall'</code> </p> RETURNS DESCRIPTION <code>Timer</code> <p>The timer.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>@classmethod\ndef start(\n    cls,\n    kind: Timer.Kind | Literal[\"cpu\", \"wall\", \"process\"] = \"wall\",\n) -&gt; Timer:\n    \"\"\"Start a timer.\n\n    Args:\n        kind: The type of timer to use.\n\n    Returns:\n        The timer.\n    \"\"\"\n    if kind in (Timer.Kind.WALL, \"wall\"):\n        return Timer(time.time(), Timer.Kind.WALL)\n\n    if kind in (Timer.Kind.CPU, \"cpu\"):\n        return Timer(time.perf_counter(), Timer.Kind.CPU)\n\n    if kind in (Timer.Kind.PROCESS, \"process\"):\n        return Timer(time.process_time(), Timer.Kind.PROCESS)\n\n    raise ValueError(f\"Unknown timer type: {kind}\")\n</code></pre>"},{"location":"api/amltk/profiling/timing/#amltk.profiling.timing.Timer.stop","title":"<code>def stop()</code>","text":"<p>Stop the timer.</p> RETURNS DESCRIPTION <code>Interval</code> <p>A tuple of the start time, end time, and duration.</p> Source code in <code>src/amltk/profiling/timing.py</code> <pre><code>def stop(self) -&gt; Interval:\n    \"\"\"Stop the timer.\n\n    Returns:\n        A tuple of the start time, end time, and duration.\n    \"\"\"\n    if self.kind == Timer.Kind.WALL:\n        end = time.time()\n        return Timer.Interval(\n            self.start_time,\n            end,\n            Timer.Kind.WALL,\n            Timer.Unit.SECONDS,\n        )\n\n    if self.kind == Timer.Kind.CPU:\n        end = time.perf_counter()\n        return Timer.Interval(\n            self.start_time,\n            end,\n            Timer.Kind.CPU,\n            Timer.Unit.SECONDS,\n        )\n\n    if self.kind == Timer.Kind.PROCESS:\n        end = time.process_time()\n        return Timer.Interval(\n            self.start_time,\n            end,\n            Timer.Kind.PROCESS,\n            Timer.Unit.SECONDS,\n        )\n\n    raise ValueError(f\"Unknown timer type: {self.kind}\")\n</code></pre>"},{"location":"api/amltk/scheduling/events/","title":"Events","text":"<p>One of the primary ways to respond to <code>@events</code> emitted with by a <code>Task</code> the <code>Scheduler</code> is through use of a callback.</p> <p>The reason for this is to enable an easier time for API's to utilize multiprocessing and remote compute from the <code>Scheduler</code>, without having to burden users with knowing the details of how to use multiprocessing.</p> <p>A callback subscribes to some event using a decorator but can also be done in a functional style if preferred. The below example is based on the event <code>@scheduler.on_start</code> but the same applies to all events.</p> DecoratorsFunctional <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.run()\n</code></pre> <p>hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.on_start(print_hello)\nscheduler.run()\n</code></pre> <p>hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p>There are a number of ways to customize the behaviour of these callbacks, notably to control how often they get called and when they get called.</p> Callback customization <code>on('event', repeat=...)</code><code>on('event', max_calls=...)</code><code>on('event', when=...)</code><code>on('event', every=...)</code> <p>This will cause the callback to be called <code>repeat</code> times successively. This is most useful in combination with <code>@scheduler.on_start</code> to launch a number of tasks at the start of the scheduler.</p> <p><pre><code>from amltk import Scheduler\n\nN_WORKERS = 2\n\ndef f(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(f)\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    task.submit(1)\n\nscheduler.run()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def on_start() (2)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 2\n\u2503   @on_future_done 2\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 2\n\u2517\u2501\u2501 \u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 2                                                          \u2502\n    \u2502 @on_done 2                                                               \u2502\n    \u2502 @on_result 2                                                             \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: XJgFtaR8 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <p>Limit the number of times a callback can be called, after which, the callback will be ignored.</p> <p><pre><code>from asyncio import Future\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\ndef expensive_function(x: int) -&gt; int:\n    return x ** 2\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\n@scheduler.on_future_result(max_calls=3)\ndef print_result(future, result) -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\nscheduler.run()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 4\n    @on_future_done 4\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 4\n    \u2514\u2500\u2500 def print_result(future, result) -&gt; 'None' (3)\n</code>\n</pre> </p> </p> <p>A callable which takes no arguments and returns a <code>bool</code>. The callback will only be called when the <code>when</code> callable returns <code>True</code>.</p> <p>Below is a rather contrived example, but it shows how we can use the <code>when</code> parameter to control when the callback is called.</p> <p><pre><code>import random\nfrom amltk.scheduling import Scheduler\n\nLOCALE = random.choice([\"English\", \"German\"])\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start(when=lambda: LOCALE == \"English\")\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n@scheduler.on_start(when=lambda: LOCALE == \"German\")\ndef print_guten_tag() -&gt; None:\n    print(\"guten tag\")\n\nscheduler.run()\n</code></pre> <p>guten tag  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u251c\u2500\u2500 def print_hello() -&gt; 'None'\n    \u2514\u2500\u2500 def print_guten_tag() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p>Only call the callback every <code>every</code> times the event is emitted. This includes the first time it's called.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n# Print \"hello\" only every 2 times the scheduler starts.\n@scheduler.on_start(every=2)\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n# Run the scheduler 5 times\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\n</code></pre> <p>hello hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 5\n    @on_start 5\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (2)\n    @on_finishing 5\n    @on_finished 5\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events--emitter-subscribers-and-events","title":"Emitter, Subscribers and Events","text":"<p>This part of the documentation is not necessary to understand or use for AMLTK. People wishing to build tools upon AMLTK may still find this a useful component to add to their arsenal.</p> <p>The core of making this functionality work is the <code>Emitter</code>. Its purpose is to have <code>@events</code> that can be emitted and subscribed to. Classes like the <code>Scheduler</code> and <code>Task</code> carry around with them an <code>Emitter</code> to enable all of this functionality.</p> <p>Creating an <code>Emitter</code> is rather straight-forward, but we must also create <code>Events</code> that people can subscribe to.</p> <pre><code>from amltk.scheduling import Emitter, Event\nemitter = Emitter(\"my-emitter\")\n\nevent: Event[int] = Event(\"my-event\") # (1)!\n\n@emitter.on(event)\ndef my_callback(x: int) -&gt; None:\n    print(f\"Got {x}!\")\n\nemitter.emit(event, 42) # (2)!\n</code></pre> <ol> <li>The typing <code>Event[int]</code> is used to indicate that the event will be emitting     an integer. This is not necessary, but it is useful for type-checking and     documentation.</li> <li>The <code>emitter.emit(event, 42)</code> is used to emit the event. This will call     all the callbacks registered for the event, i.e. <code>my_callback()</code>.</li> </ol> <p>Independent Events</p> <p>Given a single <code>Emitter</code> and a single instance of an <code>Event</code>, there is no way to have different <code>@events</code> for callbacks. There are two options, both used extensively in AMLTK.</p> <p>The first is to have different <code>Events</code> quite naturally, i.e. you distinguish between different things that can happen. However, you often want to have different objects emit the same <code>Event</code> but have different callbacks for each object.</p> <p>This makes most sense in the context of a <code>Task</code> the <code>Event</code> instances are shared as class variables in the <code>Task</code> class, however a user likely want's to subscribe to the <code>Event</code> for a specific instance of the <code>Task</code>.</p> <p>This is where the second option comes in, in which each object carries around its own <code>Emitter</code> instance. This is how a user can subscribe to the same kind of <code>Event</code> but individually for each <code>Task</code>.</p> <p>However, to shield users from this and to create named access points for users to subscribe to, we can use the <code>Subscriber</code> class, conveniently created by the <code>Emitter.subscriber()</code> method.</p> <pre><code>from amltk.scheduling import Emitter, Event\nemitter = Emitter(\"my-emitter\")\n\nclass GPT:\n\n    event: Event[str] = Event(\"my-event\")\n\n    def __init__(self) -&gt; None:\n        self.on_answer: Subscriber[str] = emitter.subscriber(self.event)\n\n    def ask(self, question: str) -&gt; None:\n        emitter.emit(self.event, \"hello world!\")\n\ngpt = GPT()\n\n@gpt.on_answer\ndef print_answer(answer: str) -&gt; None:\n    print(answer)\n\ngpt.ask(\"What is the conical way for an AI to greet someone?\")\n</code></pre> <p>Typically these event based systems make little sense in a synchronous context, however with the <code>Scheduler</code> and <code>Task</code> classes, they are used to enable a simple way to use multiprocessing and remote compute.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.RegisteredTimeCallOrderStrategy","title":"<code>class RegisteredTimeCallOrderStrategy</code>   <code>dataclass</code>","text":"<p>A calling strategy that calls callbacks in the order they were registered.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.RegisteredTimeCallOrderStrategy.execute","title":"<code>def execute(events)</code>   <code>classmethod</code>","text":"<p>Call all events in the scheduler.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>@classmethod\ndef execute(\n    cls,\n    events: Iterable[\n        tuple[\n            Iterable[Handler[P]],\n            tuple[Any, ...] | None,\n            dict[str, Any] | None,\n        ]\n    ],\n) -&gt; None:\n    \"\"\"Call all events in the scheduler.\"\"\"\n    all_handlers = []\n    for handlers, args, kwargs in events:\n        all_handlers += [\n            (handler, args or (), kwargs or {}) for handler in handlers\n        ]\n\n    sorted_handlers = sorted(all_handlers, key=lambda item: item[0].registered_at)\n    for handler, args, kwargs in sorted_handlers:\n        handler(*args, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Event","title":"<code>class Event</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Generic[P]</code></p> <p>An event that can be emitted.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name of the event.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Event.__eq__","title":"<code>def __eq__(other)</code>","text":"<p>Check if two events are equal.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>@override\ndef __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check if two events are equal.\"\"\"\n    if isinstance(other, str):\n        return self.name == other\n    if isinstance(other, Event):\n        return self.name == other.name\n\n    return False\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber","title":"<code>class Subscriber</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Generic[P]</code></p> <p>An object that can be used to easily subscribe to a certain event.</p> <pre><code>from amltk.scheduling.events import Event, Subscriber\n\ntest_event: Event[[int, str]] = Event(\"test\")\n\nemitter = Emitter(\"hello world\")\nsubscribe = emitter.subscriber(test_event)\n\n# Use it as a decorator\n\n@subscribe\ndef callback(a: int, b: str) -&gt; None:\n    print(f\"Got {a} and {b}!\")\n\n# ... or just pass a function\n\nsubscribe(callback)\n\n# Will emit `test_event` with the arguments 1 and \"hello\"\n# and call the callback with those same arguments.\nemitter.emit(test_event, 1, \"hello\")\n</code></pre> ATTRIBUTE DESCRIPTION <code>manager</code> <p>The event manager to use.</p> <p> </p> <code>event</code> <p>The event to subscribe to.</p> <p> TYPE: <code>Event[P]</code> </p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber.event_counts","title":"<code>event_counts: int</code>   <code>prop</code>","text":"<p>The number of times this event has been emitted.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber.__call__","title":"<code>def __call__(callback=None, *, when=None, max_calls=None, repeat=1, every=1, hidden=False)</code>","text":"<p>Subscribe to the event associated with this object.</p> PARAMETER  DESCRIPTION <code>callback</code> <p>The callback to register.</p> <p> TYPE: <code>Callable[P, Any] | None</code> DEFAULT: <code>None</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Callable[P, Any] | partial[Callable[P, Any]]</code> <p>The callback if it was provided, otherwise it acts as a decorator.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def __call__(\n    self,\n    callback: Callable[P, Any] | None = None,\n    *,\n    when: Callable[[], bool] | None = None,\n    max_calls: int | None = None,\n    repeat: int = 1,\n    every: int = 1,\n    hidden: bool = False,\n) -&gt; Callable[P, Any] | partial[Callable[P, Any]]:\n    \"\"\"Subscribe to the event associated with this object.\n\n    Args:\n        callback: The callback to register.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n\n    Returns:\n        The callback if it was provided, otherwise it acts\n            as a decorator.\n    \"\"\"\n    if callback is None:\n        return partial(\n            self.__call__,\n            when=when,\n            max_calls=max_calls,\n            repeat=repeat,\n            every=every,\n        )  # type: ignore\n\n    self.emitter.on(\n        self.event,\n        callback,\n        when=when,\n        max_calls=max_calls,\n        repeat=repeat,\n        every=every,\n        hidden=hidden,\n    )\n    return callback\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Subscriber.emit","title":"<code>def emit(*args, **kwargs)</code>","text":"<p>Emit this subscribers event.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def emit(self, *args: P.args, **kwargs: P.kwargs) -&gt; None:\n    \"\"\"Emit this subscribers event.\"\"\"\n    self.emitter.emit(self.event, *args, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Handler","title":"<code>class Handler</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Generic[P]</code></p> <p>A handler for an event.</p> <p>This is a simple class that holds a callback and any predicate that must be satisfied for it to be triggered.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Handler.__call__","title":"<code>def __call__(*args, **kwargs)</code>","text":"<p>Call the callback if the predicate is satisfied.</p> <p>If the predicate is not satisfied, then <code>None</code> is returned.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def __call__(self, *args: P.args, **kwargs: P.kwargs) -&gt; None:\n    \"\"\"Call the callback if the predicate is satisfied.\n\n    If the predicate is not satisfied, then `None` is returned.\n    \"\"\"\n    self.n_calls_to_handler += 1\n    if self.every &gt; 1 and self.n_calls_to_handler % self.every != 0:\n        return\n\n    if self.when is not None and not self.when():\n        return\n\n    max_calls = self.max_calls if self.max_calls is not None else math.inf\n    for _ in range(self.repeat):\n        if self.n_calls_to_callback &gt;= max_calls:\n            return\n\n        logger.debug(f\"Calling: {callstring(self.callback)}\")\n        self.callback(*args, **kwargs)\n        self.n_calls_to_callback += 1\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter","title":"<code>class Emitter(name=None)</code>","text":"<p>         Bases: <code>Mapping[Event, list[Handler]]</code></p> <p>An event emitter.</p> <p>This class is used to emit events and register callbacks for those events. It also provides a convenience function <code>subscriber()</code> such that objects using an <code>Emitter</code> can easily create access points for users to directly subscribe to their <code>Events</code>.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the emitter. If not provided, then a UUID will be used.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def __init__(self, name: str | None = None) -&gt; None:\n    \"\"\"Initialise the emitter.\n\n    Args:\n        name: The name of the emitter. If not provided, then a UUID\n            will be used.\n    \"\"\"\n    super().__init__()\n    self.unique_ref = f\"{name}-{uuid4()}\"\n    self.emitted_events: set[Event] = set()\n\n    self.name = name\n    self.handlers = defaultdict(list)\n    self.event_counts = Counter()\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.name","title":"<code>name: str | None</code>   <code>attr</code>","text":"<p>The name of the emitter.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.handlers","title":"<code>handlers: dict[Event, list[Handler]]</code>   <code>attr</code>","text":"<p>A mapping of events to their handlers.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.event_counts","title":"<code>event_counts: Counter[Event]</code>   <code>attr</code>","text":"<p>A count of all events emitted by this emitter.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.events","title":"<code>events: list[Event]</code>   <code>prop</code>","text":"<p>Return a list of the events.</p>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.emit","title":"<code>def emit(event, *args, **kwargs)</code>","text":"<p>Emit an event.</p> <p>This will call all the handlers for the event.</p> PARAMETER  DESCRIPTION <code>event</code> <p>The event to emit. If passing a list, then the handlers for all events will be called, regardless of the order</p> <p> TYPE: <code>Event[P]</code> </p> <code>*args</code> <p>The positional arguments to pass to the handlers.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to pass to the handlers.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>None</code> <p>A list of the results from the handlers.</p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def emit(self, event: Event[P], *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Emit an event.\n\n    This will call all the handlers for the event.\n\n    Args:\n        event: The event to emit.\n            If passing a list, then the handlers for all events will be called,\n            regardless of the order\n        *args: The positional arguments to pass to the handlers.\n        **kwargs: The keyword arguments to pass to the handlers.\n\n    Returns:\n        A list of the results from the handlers.\n    \"\"\"\n    logger.debug(f\"{self.name}: Emitting {event}\")\n\n    self.event_counts[event] += 1\n    for handler in self.handlers[event]:\n        handler(*args, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.emit_many","title":"<code>def emit_many(events)</code>","text":"<p>Emit multiple events.</p> <p>This is useful for cases where you don't want to favour one callback over another, and so uses the time a callback was registered to call the callback instead.</p> PARAMETER  DESCRIPTION <code>events</code> <p>A mapping of event keys to a tuple of positional arguments and keyword arguments to pass to the handlers.</p> <p> TYPE: <code>dict[Event, tuple[tuple[Any, ...] | None, dict[str, Any] | None]]</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def emit_many(\n    self,\n    events: dict[Event, tuple[tuple[Any, ...] | None, dict[str, Any] | None]],\n) -&gt; None:\n    \"\"\"Emit multiple events.\n\n    This is useful for cases where you don't want to favour one callback\n    over another, and so uses the time a callback was registered to call\n    the callback instead.\n\n    Args:\n        events: A mapping of event keys to a tuple of positional\n            arguments and keyword arguments to pass to the handlers.\n    \"\"\"\n    for event in events:\n        self.event_counts[event] += 1\n\n    items = [\n        (handlers, args, kwargs)\n        for event, (args, kwargs) in events.items()\n        if (handlers := self.get(event)) is not None\n    ]\n\n    header = f\"{self.name}: Emitting many events\"\n    logger.debug(header)\n    logger.debug(\",\".join(str(event) for event in events))\n    RegisteredTimeCallOrderStrategy.execute(items)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.subscriber","title":"<code>def subscriber(event, *, when=None, every=1, repeat=1, max_calls=None)</code>","text":"<p>Create a subscriber for an event.</p> PARAMETER  DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P]</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def subscriber(\n    self,\n    event: Event[P],\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n) -&gt; Subscriber[P]:\n    \"\"\"Create a subscriber for an event.\n\n    Args:\n        event: The event to register the callback for.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n    \"\"\"\n    if event not in self.handlers:\n        self.handlers[event] = []\n\n    return Subscriber(\n        self,\n        event,  # type: ignore\n        when=when,\n        every=every,\n        repeat=repeat,\n        max_calls=max_calls,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.on","title":"<code>def on(event, callback, *, when=None, every=1, repeat=1, max_calls=None, hidden=False)</code>","text":"<p>Register a callback for an event.</p> PARAMETER  DESCRIPTION <code>event</code> <p>The event to register the callback for.</p> <p> TYPE: <code>Event[P]</code> </p> <code>callback</code> <p>The callback to register.</p> <p> TYPE: <code>Callable</code> </p> <code>when</code> <p>A predicate that must be satisfied for the callback to be called.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>every</code> <p>The callback will be called every <code>every</code> times the event is emitted.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>repeat</code> <p>The callback will be called <code>repeat</code> times successively.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>max_calls</code> <p>The maximum number of times the callback can be called.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def on(\n    self,\n    event: Event[P],\n    callback: Callable,\n    *,\n    when: Callable[[], bool] | None = None,\n    every: int = 1,\n    repeat: int = 1,\n    max_calls: int | None = None,\n    hidden: bool = False,\n) -&gt; None:\n    \"\"\"Register a callback for an event.\n\n    Args:\n        event: The event to register the callback for.\n        callback: The callback to register.\n        when: A predicate that must be satisfied for the callback to be called.\n        every: The callback will be called every `every` times the event is emitted.\n        repeat: The callback will be called `repeat` times successively.\n        max_calls: The maximum number of times the callback can be called.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n    \"\"\"\n    if repeat &lt;= 0:\n        raise ValueError(f\"{repeat=} must be a positive integer.\")\n\n    if every &lt;= 0:\n        raise ValueError(f\"{every=} must be a positive integer.\")\n\n    # Make sure it shows up in the event counts, setting it to 0 if it\n    # doesn't exist\n    self.event_counts.setdefault(event, 0)\n\n    # This hackery is just to get down to a flat list of events that need\n    # to be set up\n    self.handlers[event].append(\n        Handler(\n            callback,\n            when=when,\n            every=every,\n            repeat=repeat,\n            max_calls=max_calls,\n            hidden=hidden,\n        ),\n    )\n\n    _name = funcname(callback)\n    msg = f\"{self.name}: Registered callback '{_name}' for event {event}\"\n    if every &gt; 1:\n        msg += f\" every {every} times\"\n    if when:\n        msg += f\" with predicate ({funcname(when)})\"\n    if repeat &gt; 1:\n        msg += f\" called {repeat} times successively\"\n    if hidden:\n        msg += \" (hidden from visual output)\"\n    logger.debug(msg)\n</code></pre>"},{"location":"api/amltk/scheduling/events/#amltk.scheduling.events.Emitter.add_event","title":"<code>def add_event(*event)</code>","text":"<p>Add an event to the event manager so that it shows up in visuals.</p> PARAMETER  DESCRIPTION <code>event</code> <p>The event to add.</p> <p> TYPE: <code>Event</code> DEFAULT: <code>()</code> </p> Source code in <code>src/amltk/scheduling/events.py</code> <pre><code>def add_event(self, *event: Event) -&gt; None:\n    \"\"\"Add an event to the event manager so that it shows up in visuals.\n\n    Args:\n        event: The event to add.\n    \"\"\"\n    for e in event:\n        if e not in self.handlers:\n            self.handlers[e] = []\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/","title":"Queue monitor","text":"<p>A <code>QueueMonitor</code> is a monitor for the scheduler queue.</p> <p>This module contains a monitor for the scheduler queue. The monitor tracks the queue state at every event emitted by the scheduler. The data can be converted to a pandas DataFrame or plotted as a stacked barchart.</p> <p>Monitoring Frequency</p> <p>To prevent repeated polling, we sample the scheduler queue at every scheduler event. This is because the queue is only modified upon one of these events. This means we don't need to poll the queue at a fixed interval. However, if you need more fine grained updates, you can add extra events/timings at which the monitor should <code>update()</code>.</p> <p>Performance impact</p> <p>If your tasks and callbacks are very fast (~sub 10ms), then the monitor has a non-nelgible impact however for most use cases, this should not be a problem. As anything, you should profile how much work the scheduler can get done, with and without the monitor, to see if it is a problem for your use case.</p> <p>In the below example, we have a very fast running function that runs on repeat, sometimes too fast for the scheduler to keep up, letting some futures buildup needing to be processed.</p> <pre><code>import time\nimport matplotlib.pyplot as plt\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.queue_monitor import QueueMonitor\n\ndef fast_function(x: int) -&gt; int:\n    return x + 1\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\nmonitor = QueueMonitor(scheduler)\ntask = scheduler.task(fast_function)\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef start():\n    task.submit(1)\n\n@task.on_result\ndef result(_, x: int):\n    if scheduler.running():\n        task.submit(x)\n\nscheduler.run(timeout=1)\ndf = monitor.df()\nprint(df)\n</code></pre> <pre><code>                               queue_size  queued  finished  cancelled  idle\ntime                                                                        \n2023-12-12 03:40:52.213678600           0       0         0          0     2\n2023-12-12 03:40:52.230113927           1       1         0          0     1\n2023-12-12 03:40:52.230459700           2       2         0          0     0\n2023-12-12 03:40:52.244183692           1       1         0          0     1\n2023-12-12 03:40:52.244339962           2       2         0          0     0\n...                                   ...     ...       ...        ...   ...\n2023-12-12 03:40:53.232385340           2       2         0          0     0\n2023-12-12 03:40:53.232605250           2       2         0          0     0\n2023-12-12 03:40:53.245511451           2       2         0          0     0\n2023-12-12 03:40:53.245717153           1       0         1          0     1\n2023-12-12 03:40:53.245756677           0       0         0          0     2\n\n[1879 rows x 5 columns]\n</code></pre> <p>We can also <code>plot()</code> the data as a stacked barchart with a set interval.</p> <pre><code>fig, ax = plt.subplots()\nmonitor.plot(interval=(50, \"ms\"))\n</code></pre> <p> 2023-12-12T03:40:53.407658 image/svg+xml Matplotlib v3.8.2, https://matplotlib.org/ </p>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitorRecord","title":"<code>class QueueMonitorRecord</code>","text":"<p>         Bases: <code>NamedTuple</code></p> <p>A record of the queue state at a given time.</p>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor","title":"<code>class QueueMonitor(scheduler)</code>","text":"<p>A monitor for the scheduler queue.</p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def __init__(self, scheduler: Scheduler) -&gt; None:\n    \"\"\"Initializes the monitor.\"\"\"\n    super().__init__()\n    self.scheduler = scheduler\n    self.data: list[QueueMonitorRecord] = []\n\n    scheduler.on_start(self.update)\n    scheduler.on_finishing(self.update)\n    scheduler.on_finished(self.update)\n    scheduler.on_future_submitted(self.update)\n    scheduler.on_future_cancelled(self.update)\n    scheduler.on_future_done(self.update)\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor.df","title":"<code>def df(*, n_workers=None)</code>","text":"<p>Converts the data to a pandas DataFrame.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers that were in use. This helps idenify how many workers were idle at a given time. If None, the maximum length of the queue at any recorded time is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def df(\n    self,\n    *,\n    n_workers: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Converts the data to a pandas DataFrame.\n\n    Args:\n        n_workers: The number of workers that were in use. This helps idenify how\n            many workers were idle at a given time. If None, the maximum length of\n            the queue at any recorded time is used.\n    \"\"\"\n    _df = pd.DataFrame(self.data, columns=list(QueueMonitorRecord._fields)).astype(\n        {\n            # Windows might have a weird default here but it should be 64 at least\n            \"time\": \"int64\",\n            \"queue_size\": int,\n            \"queued\": int,\n            \"finished\": int,\n            \"cancelled\": int,\n        },\n    )\n    if n_workers is None:\n        n_workers = int(_df[\"queue_size\"].max())\n    _df[\"idle\"] = n_workers - _df[\"queue_size\"]\n    _df[\"time\"] = pd.to_datetime(_df[\"time\"], unit=\"ns\", origin=\"unix\")\n    return _df.set_index(\"time\")\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor.plot","title":"<code>def plot(*, ax=None, interval=(1, 's'), n_workers=None, **kwargs)</code>","text":"<p>Plots the data as a stacked barchart.</p> PARAMETER  DESCRIPTION <code>ax</code> <p>The axes to plot on. If None, a new figure is created.</p> <p> TYPE: <code>Axes | None</code> DEFAULT: <code>None</code> </p> <code>interval</code> <p>The interval to use for the x-axis. The first value is the interval and the second value is the unit. Must be a valid pandas timedelta unit. See to_timedelta() for more information.</p> <p> TYPE: <code>tuple[int, UnitChoices]</code> DEFAULT: <code>(1, 's')</code> </p> <code>n_workers</code> <p>The number of workers that were in use. This helps idenify how many workers were idle at a given time. If None, the maximum length of the queue at any recorded time is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional keyword arguments to pass to the pandas plot function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Axes</code> <p>The axes.</p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def plot(\n    self,\n    *,\n    ax: plt.Axes | None = None,\n    interval: tuple[int, UnitChoices] = (1, \"s\"),\n    n_workers: int | None = None,\n    **kwargs: Any,\n) -&gt; plt.Axes:\n    \"\"\"Plots the data as a stacked barchart.\n\n    Args:\n        ax: The axes to plot on. If None, a new figure is created.\n        interval: The interval to use for the x-axis. The first value is the\n            interval and the second value is the unit. Must be a valid pandas\n            timedelta unit. See [to_timedelta()][pandas.to_timedelta] for more\n            information.\n        n_workers: The number of workers that were in use. This helps idenify how\n            many workers were idle at a given time. If None, the maximum length of\n            the queue at any recorded time is used.\n        **kwargs: Additional keyword arguments to pass to the pandas plot function.\n\n    Returns:\n        The axes.\n    \"\"\"\n    if ax is None:\n        _, _ax = plt.subplots(1, 1)\n    else:\n        _ax = ax\n\n    _df = self.df(n_workers=n_workers)\n    _df = _df.resample(f\"{interval[0]}{interval[1]}\").mean()\n    _df.index = _df.index - _df.index[0]\n    _reversed_df = _df[::-1]\n\n    _reversed_df.plot.barh(\n        stacked=True,\n        y=[\"finished\", \"queued\", \"cancelled\", \"idle\"],\n        ax=_ax,\n        width=1,\n        edgecolor=\"k\",\n        **kwargs,\n    )\n\n    _ax.set_ylabel(\"Time\")\n    _ax.yaxis.set_major_locator(MaxNLocator(nbins=\"auto\"))\n\n    _ax.set_xlabel(\"Count\")\n    _ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    return _ax\n</code></pre>"},{"location":"api/amltk/scheduling/queue_monitor/#amltk.scheduling.queue_monitor.QueueMonitor.update","title":"<code>def update(*_)</code>","text":"<p>Updates the data when the scheduler has an event.</p> Source code in <code>src/amltk/scheduling/queue_monitor.py</code> <pre><code>def update(self, *_: Any) -&gt; None:\n    \"\"\"Updates the data when the scheduler has an event.\"\"\"\n    queue = self.scheduler.queue\n    # OPTIM: Not sure if this is fastenough\n    counter = Counter([f._state for f in queue])\n    self.data.append(\n        QueueMonitorRecord(\n            time.time_ns(),\n            len(queue),\n            counter[\"PENDING\"],\n            counter[\"FINISHED\"],\n            counter[\"CANCELLED\"],\n        ),\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/","title":"Scheduler","text":"<p>The <code>Scheduler</code> uses an <code>Executor</code>, a builtin python native with a <code>submit(f, *args, **kwargs)</code> function to submit compute to be compute else where, whether it be locally or remotely.</p> <p>The <code>Scheduler</code> is primarily used to dispatch compute to an <code>Executor</code> and emit <code>@events</code>, which can trigger user callbacks.</p> <p>Typically you should not use the <code>Scheduler</code> directly for dispatching and responding to computed functions, but rather use a <code>Task</code></p> Running in a Jupyter Notebook/Colab <p>If you are using a Jupyter Notebook, you likley need to use the following at the top of your notebook:</p> <pre><code>import nest_asyncio  # Only necessary in Notebooks\nnest_asyncio.apply()\n\nscheduler.run(...)\n</code></pre> <p>This is due to the fact a notebook runs in an async context. If you do not wish to use the above snippet, you can instead use:</p> <pre><code>await scheduler.async_run(...)\n</code></pre> Basic Usage <p>In this example, we create a scheduler that uses local processes as workers. We then create a task that will run a function <code>fn</code> and submit it to the scheduler. Lastly, a callback is registered to <code>@on_future_result</code> to print the result when the compute is done.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef launch_the_compute():\n    scheduler.submit(fn, 1)\n\n@scheduler.on_future_result\ndef callback(future, result):\n    print(f\"Result: {result}\")\n\nscheduler.run()\n</code></pre> <p>Result: 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def launch_the_compute() (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 1\n    @on_future_done 1\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 1\n    \u2514\u2500\u2500 def callback(future, result) (1)\n</code>\n</pre> </p> </p> <p>The last line in the previous example called <code>scheduler.run()</code> is what starts the scheduler running, in which it will first emit the <code>@on_start</code> event. This triggered the callback <code>launch_the_compute()</code> which submitted the function <code>fn</code> with the arguments <code>1</code>.</p> <p>The scheduler then ran the compute and waited for it to complete, emitting the <code>@on_future_result</code> event when it was done successfully. This triggered the callback <code>callback()</code> which printed the result.</p> <p>At this point, there is no more compute happening and no more events to respond to so the scheduler will halt.</p> <code>@events</code> Scheduler Status EventsSubmitted Compute Events <p>When the scheduler enters some important state, it will emit an event to let you know.</p> <code>@on_start</code><code>@on_finishing</code><code>@on_finished</code><code>@on_stop</code><code>@on_timeout</code><code>@on_empty</code> <p>A <code>Subscriber</code> which is called when the scheduler starts. This is the first event emitted by the scheduler and one of the only ways to submit the initial compute to the scheduler.</p> <pre><code>@scheduler.on_start\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finishing up. This occurs right before the scheduler shuts down the executor.</p> <pre><code>@scheduler.on_finishing\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finished, has shutdown the executor and possibly terminated any remaining compute.</p> <pre><code>@scheduler.on_finished\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is has been stopped due to the <code>stop()</code> method being called.</p> <pre><code>@scheduler.on_stop\ndef my_callback(stop_msg: str, exception: BaseException | None):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler reaches the timeout.</p> <pre><code>@scheduler.on_timeout\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the queue is empty. This can be useful to re-fill the queue and prevent the scheduler from exiting.</p> <pre><code>@scheduler.on_empty\ndef my_callback():\n    ...\n</code></pre> <p>When any compute goes through the <code>Scheduler</code>, it will emit an event to let you know. You should however prefer to use a <code>Task</code> as it will emit specific events for the task at hand, and not all compute.</p> <code>@on_future_submitted</code><code>@on_future_result</code><code>@on_future_exception</code><code>@on_future_done</code><code>@on_future_cancelled</code> <p>A <code>Subscriber</code> which is called when some compute is submitted.</p> <pre><code>@scheduler.on_future_submitted\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future returned with a result, no exception raise.</p> <pre><code>@scheduler.on_future_result\ndef my_callback(future: Future, result: Any):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute raised an uncaught exception.</p> <pre><code>@scheduler.on_future_exception\ndef my_callback(future: Future, exception: BaseException):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is done, regardless of whether it was successful or not.</p> <pre><code>@scheduler.on_future_done\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future is cancelled. This usually occurs due to the underlying Scheduler, and is not something we do directly, other than when shutting down the scheduler.</p> <pre><code>@scheduler.on_future_cancelled\ndef my_callback(future: Future):\n    ...\n</code></pre> Common usages of <code>run()</code> <p>There are various ways to <code>run()</code> the scheduler, notably how long it should run with <code>timeout=</code> and also how it should react to any exception that may have occurred within the <code>Scheduler</code> itself or your callbacks.</p> <p>Please see the <code>run()</code> API doc for more details and features, however we show two common use cases of using the <code>timeout=</code> parameter.</p> <p>You can render a live display using <code>run(display=...)</code>. This require <code>rich</code> to be installed. You can install this with <code>pip install rich</code> or <code>pip install amltk[rich]</code>.</p> <code>run(timeout=...)</code><code>run(timeout=..., wait=False)</code> <p>You can tell the <code>Scheduler</code> to stop after a certain amount of time with the <code>timeout=</code> argument to <code>run()</code>.</p> <p>This will also trigger the <code>@on_timeout</code> event as seen in the <code>Scheduler</code> output.</p> <p><pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; int:\n    time.sleep(0.1)\n    return 42\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\n# The will endlessly loop the scheduler\n@scheduler.on_future_done\ndef submit_again(future: Future) -&gt; None:\n    if scheduler.running():\n        scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1)  # End after 1 second\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 10\n    @on_future_done 10\n    \u2514\u2500\u2500 def submit_again(future: 'Future') -&gt; 'None' (10)\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 10\n</code>\n</pre> </p> </p> <p>By specifying that the <code>Scheduler</code> should not wait for ongoing tasks to finish, the <code>Scheduler</code> will attempt to cancel and possibly terminate any running tasks.</p> <p><pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; None:\n    time.sleep(10)\n\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1, wait=False)  # End after 1 second\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 1\n    @on_future_done\n    @on_future_cancelled 1\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> Forcibly Terminating Workers <p>As an <code>Executor</code> does not provide an interface to forcibly terminate workers, we provide <code>Scheduler(terminate=...)</code> as a custom strategy for cleaning up a provided executor. It is not possible to terminate running thread based workers, for example using <code>ThreadPoolExecutor</code> and any Executor using threads to spawn tasks will have to wait until all running tasks are finish before python can close.</p> <p>It's likely <code>terminate</code> will trigger the <code>EXCEPTION</code> event for any tasks that are running during the shutdown, not* a cancelled event. This is because we use a <code>Future</code> under the hood and these can not be cancelled once running. However there is no guarantee of this and is up to how the <code>Executor</code> handles this.</p> Scheduling something to be run later <p>You can schedule some function to be run later using the <code>scheduler.call_later()</code> method.</p> <p>Note</p> <p>This does not run the function in the background, it just schedules some function to be called later, where you could perhaps then use submit to scheduler a <code>Task</code> to run the function in the background.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef fn() -&gt; int:\n    print(\"Ending now!\")\n    scheduler.stop()\n\n@scheduler.on_start\ndef schedule_fn() -&gt; None:\n    scheduler.call_later(1, fn)\n\nscheduler.run(end_on_empty=False)\n</code></pre> <pre><code>Ending now!\n</code></pre> </p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler","title":"<code>class Scheduler(executor, *, terminate=True)</code>","text":"<p>         Bases: <code>RichRenderable</code></p> <p>A scheduler for submitting tasks to an Executor.</p> PARAMETER  DESCRIPTION <code>executor</code> <p>The dispatcher to use for submitting tasks.</p> <p> TYPE: <code>Executor</code> </p> <code>terminate</code> <p>Whether to call shutdown on the executor when <code>run(..., wait=False)</code>. If True, the executor will be <code>shutdown(wait=False)</code> and we will attempt to terminate any workers of the executor. For some <code>Executors</code> this is enough, i.e. Dask, however for something like <code>ProcessPoolExecutor</code>, we will use <code>psutil</code> to kill its worker processes. If a callable, we will use this function for custom worker termination. If False, shutdown will not be called and the executor will remain active.</p> <p> TYPE: <code>Callable[[Executor], None] | bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def __init__(\n    self,\n    executor: Executor,\n    *,\n    terminate: Callable[[Executor], None] | bool = True,\n) -&gt; None:\n    \"\"\"Initialize a scheduler.\n\n    Args:\n        executor: The dispatcher to use for submitting tasks.\n        terminate: Whether to call shutdown on the executor when\n            `run(..., wait=False)`. If True, the executor will be\n            `shutdown(wait=False)` and we will attempt to terminate\n            any workers of the executor. For some `Executors` this\n            is enough, i.e. Dask, however for something like\n            `ProcessPoolExecutor`, we will use `psutil` to kill\n            its worker processes. If a callable, we will use this\n            function for custom worker termination.\n            If False, shutdown will not be called and the executor will\n            remain active.\n    \"\"\"\n    super().__init__()\n    self.executor = executor\n    self.unique_ref = f\"Scheduler-{uuid4()}\"\n    self.emitter = Emitter()\n    self.event_counts = self.emitter.event_counts\n\n    # The current state of things and references to them\n    self.queue = {}\n\n    # Set up subscribers for events\n    self.on_start = self.emitter.subscriber(self.STARTED)\n    self.on_finishing = self.emitter.subscriber(self.FINISHING)\n    self.on_finished = self.emitter.subscriber(self.FINISHED)\n    self.on_stop = self.emitter.subscriber(self.STOP)\n    self.on_timeout = self.emitter.subscriber(self.TIMEOUT)\n    self.on_empty = self.emitter.subscriber(self.EMPTY)\n\n    self.on_future_submitted = self.emitter.subscriber(self.FUTURE_SUBMITTED)\n    self.on_future_done = self.emitter.subscriber(self.FUTURE_DONE)\n    self.on_future_cancelled = self.emitter.subscriber(self.FUTURE_CANCELLED)\n    self.on_future_exception = self.emitter.subscriber(self.FUTURE_EXCEPTION)\n    self.on_future_result = self.emitter.subscriber(self.FUTURE_RESULT)\n\n    self._terminate: Callable[[Executor], None] | None\n    if terminate is True:\n        self._terminate = termination_strategy(executor)\n    else:\n        self._terminate = terminate if callable(terminate) else None\n\n    # This can be triggered either by `scheduler.stop` in a callback.\n    # Has to be created inside the event loop so there's no issues\n    self._stop_event: ContextEvent | None = None\n\n    # This is a condition to make sure monitoring the queue will wait properly\n    self._queue_has_items_event = asyncio.Event()\n\n    # This is triggered when run is called\n    self._running_event = asyncio.Event()\n\n    # This is set once `run` is called\n    self._end_on_exception_flag = Flag(initial=False)\n\n    # This is used to manage suequential queues, where we need a Thread\n    # timer to ensure that we don't get caught in an endless loop waiting\n    # for the `timeout` in `_run_scheduler` to trigger. This won't trigger\n    # because the sync code of submit could possibly keep calling itself\n    # endlessly, preventing any of the async code from running.\n    self._timeout_timer: Timer | None = None\n\n    # A collection of things that want to register as being part of something\n    # to render when the Scheduler is rendered.\n    self._renderables: list[RenderableType] = [self.emitter]\n\n    # These are extra user provided renderables during a call to `run()`. We\n    # seperate these out so that we can remove them when the scheduler is\n    # stopped.\n    self._extra_renderables: list[RenderableType] | None = None\n\n    # An indicator an object to render live output (if requested) with\n    # `display=` on a call to `run()`\n    self._live_output: Live | None = None\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.executor","title":"<code>executor: Executor</code>   <code>attr</code>","text":"<p>The executor to use to run tasks.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.emitter","title":"<code>emitter: Emitter</code>   <code>attr</code>","text":"<p>The emitter to use for events.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.queue","title":"<code>queue: dict[Future, tuple[Callable, tuple, dict]]</code>   <code>attr</code>","text":"<p>The queue of tasks running.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_start","title":"<code>on_start: Subscriber[[]]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when the scheduler starts. This is the first event emitted by the scheduler and one of the only ways to submit the initial compute to the scheduler.</p> <pre><code>@scheduler.on_start\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_finishing","title":"<code>on_finishing: Subscriber[[]]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when the scheduler is finishing up. This occurs right before the scheduler shuts down the executor.</p> <pre><code>@scheduler.on_finishing\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_finished","title":"<code>on_finished: Subscriber[[]]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when the scheduler is finished, has shutdown the executor and possibly terminated any remaining compute.</p> <pre><code>@scheduler.on_finished\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_stop","title":"<code>on_stop: Subscriber[str, BaseException | None]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when the scheduler is has been stopped due to the <code>stop()</code> method being called.</p> <pre><code>@scheduler.on_stop\ndef my_callback(stop_msg: str, exception: BaseException | None):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_timeout","title":"<code>on_timeout: Subscriber[[]]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when the scheduler reaches the timeout.</p> <pre><code>@scheduler.on_timeout\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_empty","title":"<code>on_empty: Subscriber[[]]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when the queue is empty. This can be useful to re-fill the queue and prevent the scheduler from exiting.</p> <pre><code>@scheduler.on_empty\ndef my_callback():\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_submitted","title":"<code>on_future_submitted: Subscriber[Future]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when some compute is submitted.</p> <pre><code>@scheduler.on_future_submitted\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_done","title":"<code>on_future_done: Subscriber[Future]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when some compute is done, regardless of whether it was successful or not.</p> <pre><code>@scheduler.on_future_done\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_cancelled","title":"<code>on_future_cancelled: Subscriber[Future]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when a future is cancelled. This usually occurs due to the underlying Scheduler, and is not something we do directly, other than when shutting down the scheduler.</p> <pre><code>@scheduler.on_future_cancelled\ndef my_callback(future: Future):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_exception","title":"<code>on_future_exception: Subscriber[Future, BaseException]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when some compute raised an uncaught exception.</p> <pre><code>@scheduler.on_future_exception\ndef my_callback(future: Future, exception: BaseException):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.on_future_result","title":"<code>on_future_result: Subscriber[Future, Any]</code>   <code>attr</code>","text":"<p>A <code>Subscriber</code> which is called when a future returned with a result, no exception raise.</p> <pre><code>@scheduler.on_future_result\ndef my_callback(future: Future, result: Any):\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_processes","title":"<code>def with_processes(max_workers=None, mp_context=None, initializer=None, initargs=())</code>   <code>classmethod</code>","text":"<p>Create a scheduler with a <code>ProcessPoolExecutor</code>.</p> <p>See <code>ProcessPoolExecutor</code> for more details.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_processes(\n    cls,\n    max_workers: int | None = None,\n    mp_context: BaseContext | Literal[\"fork\", \"spawn\", \"forkserver\"] | None = None,\n    initializer: Callable[..., Any] | None = None,\n    initargs: tuple[Any, ...] = (),\n) -&gt; Self:\n    \"\"\"Create a scheduler with a `ProcessPoolExecutor`.\n\n    See [`ProcessPoolExecutor`][concurrent.futures.ProcessPoolExecutor]\n    for more details.\n    \"\"\"\n    if isinstance(mp_context, str):\n        from multiprocessing import get_context\n\n        mp_context = get_context(mp_context)\n\n    executor = ProcessPoolExecutor(\n        max_workers=max_workers,\n        mp_context=mp_context,\n        initializer=initializer,\n        initargs=initargs,\n    )\n    return cls(executor=executor)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_loky","title":"<code>def with_loky(max_workers=None, context=None, timeout=10, kill_workers=False, reuse='auto', job_reducers=None, result_reducers=None, initializer=None, initargs=(), env=None)</code>   <code>classmethod</code>","text":"<p>Create a scheduler with a <code>loky.get_reusable_executor</code>.</p> <p>See [loky documentation][loky.readthedocs.io/en/stable/API.html] for more details.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_loky(  # noqa: PLR0913\n    cls,\n    max_workers: int | None = None,\n    context: BaseContext | Literal[\"fork\", \"spawn\", \"forkserver\"] | None = None,\n    timeout: int = 10,\n    kill_workers: bool = False,  # noqa: FBT002, FBT001\n    reuse: bool | Literal[\"auto\"] = \"auto\",\n    job_reducers: Any | None = None,\n    result_reducers: Any | None = None,\n    initializer: Callable[..., Any] | None = None,\n    initargs: tuple[Any, ...] = (),\n    env: dict[str, str] | None = None,\n) -&gt; Self:\n    \"\"\"Create a scheduler with a `loky.get_reusable_executor`.\n\n    See [loky documentation][https://loky.readthedocs.io/en/stable/API.html]\n    for more details.\n    \"\"\"\n    from loky import get_reusable_executor\n\n    executor = get_reusable_executor(\n        max_workers=max_workers,\n        context=context,\n        timeout=timeout,\n        kill_workers=kill_workers,\n        reuse=reuse,  # type: ignore\n        job_reducers=job_reducers,\n        result_reducers=result_reducers,\n        initializer=initializer,\n        initargs=initargs,\n        env=env,\n    )\n    return cls(executor=executor)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_sequential","title":"<code>def with_sequential()</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs sequentially.</p> <p>This is useful for debugging and testing. Uses a <code>SequentialExecutor</code>.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_sequential(cls) -&gt; Self:\n    \"\"\"Create a Scheduler that runs sequentially.\n\n    This is useful for debugging and testing. Uses\n    a [`SequentialExecutor`][amltk.scheduling.SequentialExecutor].\n    \"\"\"\n    return cls(executor=SequentialExecutor())\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_slurm","title":"<code>def with_slurm(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs on a SLURM cluster.</p> <p>This is useful for running on a SLURM cluster. Uses dask_jobqueue.SLURMCluster.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a SLURM cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_slurm(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a SLURM cluster.\n\n    This is useful for running on a SLURM cluster. Uses\n    [dask_jobqueue.SLURMCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a SLURM cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"slurm\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_pbs","title":"<code>def with_pbs(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs on a PBS cluster.</p> <p>This is useful for running on a PBS cluster. Uses dask_jobqueue.PBSCluster.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a PBS cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_pbs(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a PBS cluster.\n\n    This is useful for running on a PBS cluster. Uses\n    [dask_jobqueue.PBSCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a PBS cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"pbs\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_sge","title":"<code>def with_sge(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs on a SGE cluster.</p> <p>This is useful for running on a SGE cluster. Uses dask_jobqueue.SGECluster.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a SGE cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_sge(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a SGE cluster.\n\n    This is useful for running on a SGE cluster. Uses\n    [dask_jobqueue.SGECluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a SGE cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"sge\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_oar","title":"<code>def with_oar(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs on a OAR cluster.</p> <p>This is useful for running on a OAR cluster. Uses dask_jobqueue.OARCluster.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a OAR cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_oar(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a OAR cluster.\n\n    This is useful for running on a OAR cluster. Uses\n    [dask_jobqueue.OARCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a OAR cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"oar\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_moab","title":"<code>def with_moab(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs on a Moab cluster.</p> <p>This is useful for running on a Moab cluster. Uses dask_jobqueue.MoabCluster.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a Moab cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_moab(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a Moab cluster.\n\n    This is useful for running on a Moab cluster. Uses\n    [dask_jobqueue.MoabCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a Moab cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"moab\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_lsf","title":"<code>def with_lsf(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs on a LSF cluster.</p> <p>This is useful for running on a LSF cluster. Uses dask_jobqueue.LSFCluster.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a LSF cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_lsf(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a LSF cluster.\n\n    This is useful for running on a LSF cluster. Uses\n    [dask_jobqueue.LSFCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a LSF cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"lsf\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_htcondor","title":"<code>def with_htcondor(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler that runs on a HTCondor cluster.</p> <p>This is useful for running on a HTCondor cluster. Uses dask_jobqueue.HTCondorCluster.</p> PARAMETER  DESCRIPTION <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A scheduler that will run on a HTCondor cluster.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_htcondor(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler that runs on a HTCondor cluster.\n\n    This is useful for running on a HTCondor cluster. Uses\n    [dask_jobqueue.HTCondorCluster][].\n\n    Args:\n        n_workers: The number of workers to start.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Returns:\n        A scheduler that will run on a HTCondor cluster.\n    \"\"\"\n    return cls.with_dask_jobqueue(\n        \"htcondor\",\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.with_dask_jobqueue","title":"<code>def with_dask_jobqueue(name, *, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a Scheduler with using <code>dask-jobqueue</code>.</p> <p>See <code>dask_jobqueue</code> for more details.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the jobqueue to use. This is the name of the class in <code>dask_jobqueue</code> to use. For example, to use <code>dask_jobqueue.SLURMCluster</code>, you would use <code>slurm</code>.</p> <p> TYPE: <code>DJQ_NAMES</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_workers</code> <p>The number of workers to start.</p> <p> TYPE: <code>int</code> </p> <code>submit_command</code> <p>Overwrite the command to submit a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the command to cancel a worker if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any additional keyword arguments to pass to the <code>dask_jobqueue</code> class.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If <code>dask-jobqueue</code> is not installed.</p> RETURNS DESCRIPTION <code>Self</code> <p>A new scheduler with a <code>dask_jobqueue</code> executor.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>@classmethod\ndef with_dask_jobqueue(\n    cls,\n    name: DJQ_NAMES,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; Self:\n    \"\"\"Create a Scheduler with using `dask-jobqueue`.\n\n    See [`dask_jobqueue`][dask_jobqueue] for more details.\n\n    [dask_jobqueue]: https://jobqueue.dask.org/en/latest/\n\n    Args:\n        name: The name of the jobqueue to use. This is the name of the\n            class in `dask_jobqueue` to use. For example, to use\n            `dask_jobqueue.SLURMCluster`, you would use `slurm`.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed\n            allocate all workers. This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers\n            specified.\n        n_workers: The number of workers to start.\n        submit_command: Overwrite the command to submit a worker if necessary.\n        cancel_command: Overwrite the command to cancel a worker if necessary.\n        kwargs: Any additional keyword arguments to pass to the\n            `dask_jobqueue` class.\n\n    Raises:\n        ImportError: If `dask-jobqueue` is not installed.\n\n    Returns:\n        A new scheduler with a `dask_jobqueue` executor.\n    \"\"\"\n    try:\n        from amltk.scheduling.executors.dask_jobqueue import DaskJobqueueExecutor\n\n    except ImportError as e:\n        raise ImportError(\n            f\"To use the {name} executor, you must install the \"\n            \"`dask-jobqueue` package.\",\n        ) from e\n\n    executor = DaskJobqueueExecutor.from_str(\n        name,\n        n_workers=n_workers,\n        adaptive=adaptive,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        **kwargs,\n    )\n    return cls(executor)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.empty","title":"<code>def empty()</code>","text":"<p>Check if the scheduler is empty.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if there are no tasks in the queue.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def empty(self) -&gt; bool:\n    \"\"\"Check if the scheduler is empty.\n\n    Returns:\n        True if there are no tasks in the queue.\n    \"\"\"\n    return len(self.queue) == 0\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.running","title":"<code>def running()</code>","text":"<p>Whether the scheduler is running and accepting tasks to dispatch.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the scheduler is running and accepting tasks.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def running(self) -&gt; bool:\n    \"\"\"Whether the scheduler is running and accepting tasks to dispatch.\n\n    Returns:\n        True if the scheduler is running and accepting tasks.\n    \"\"\"\n    return self._running_event.is_set()\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.submit","title":"<code>def submit(fn, /, *args, **kwargs)</code>","text":"<p>Submits a callable to be executed with the given arguments.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The callable to be executed as fn(args, *kwargs) that returns a Future instance representing the execution of the callable.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>args</code> <p>positional arguments to pass to the function</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>keyword arguments to pass to the function</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>SchedulerNotRunningError</code> <p>If the scheduler is not running. You can protect against this using, <code>scheduler.running()</code>.</p> RETURNS DESCRIPTION <code>Future[R]</code> <p>A Future representing the given call.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def submit(\n    self,\n    fn: Callable[P, R],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[R]:\n    \"\"\"Submits a callable to be executed with the given arguments.\n\n    Args:\n        fn: The callable to be executed as\n            fn(*args, **kwargs) that returns a Future instance representing\n            the execution of the callable.\n        args: positional arguments to pass to the function\n        kwargs: keyword arguments to pass to the function\n\n    Raises:\n        SchedulerNotRunningError: If the scheduler is not running.\n            You can protect against this using,\n            [`scheduler.running()`][amltk.scheduling.scheduler.Scheduler.running].\n\n    Returns:\n        A Future representing the given call.\n    \"\"\"\n    if not self.running():\n        msg = (\n            f\"Scheduler is not running, cannot submit task {fn}\"\n            f\" with {args=}, {kwargs=}\"\n        )\n        raise SchedulerNotRunningError(msg)\n\n    try:\n        sync_future = self.executor.submit(fn, *args, **kwargs)\n        future = asyncio.wrap_future(sync_future)\n    except Exception as e:\n        logger.exception(f\"Could not submit task {fn}\", exc_info=e)\n        raise e\n\n    self._register_future(future, fn, *args, **kwargs)\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.task","title":"<code>def task(function, *, plugins=(), init_plugins=True)</code>","text":"<p>Create a new task.</p> PARAMETER  DESCRIPTION <code>function</code> <p>The function to run using the scheduler.</p> <p> TYPE: <code>Callable[P, R] | Callable[Concatenate[Comm, P], R]</code> </p> <code>plugins</code> <p>The plugins to attach to the task.</p> <p> TYPE: <code>Plugin | Iterable[Plugin]</code> DEFAULT: <code>()</code> </p> <code>init_plugins</code> <p>Whether to initialize the plugins.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Task[P, R]</code> <p>A new task.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def task(\n    self,\n    function: Callable[P, R] | Callable[Concatenate[Comm, P], R],\n    *,\n    plugins: Plugin | Iterable[Plugin] = (),\n    init_plugins: bool = True,\n) -&gt; Task[P, R]:\n    \"\"\"Create a new task.\n\n    Args:\n        function: The function to run using the scheduler.\n        plugins: The plugins to attach to the task.\n        init_plugins: Whether to initialize the plugins.\n\n    Returns:\n        A new task.\n    \"\"\"\n    # HACK: Not that the type: ignore is due to the fact that we can't use type\n    # checking to enforce that\n    # A. `function` is a callable with the first arg being a Comm\n    # B. `plugins`\n    task = Task(function, self, plugins=plugins, init_plugins=init_plugins)  # type: ignore\n    self.add_renderable(task)\n    return task  # type: ignore\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.run","title":"<code>def run(*, timeout=None, end_on_empty=True, wait=True, on_exception='raise', asyncio_debug_mode=False, display=False)</code>","text":"<p>Run the scheduler.</p> PARAMETER  DESCRIPTION <code>timeout</code> <p>The maximum time to run the scheduler for in seconds. Defaults to <code>None</code> which means no timeout and it will end once the queue is empty if <code>end_on_empty=True</code>.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>end_on_empty</code> <p>Whether to end the scheduler when the queue becomes empty.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>wait</code> <p>Whether to wait for currently running compute to finish once the <code>Scheduler</code> is shutting down.</p> <ul> <li>If <code>True</code>, will wait for all currently running compute.</li> <li>If <code>False</code>, will attempt to cancel/terminate all currently     running compute and shutdown the executor. This may be useful     if you want to end the scheduler as quickly as possible or     respect the <code>timeout=</code> more precisely.</li> </ul> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>on_exception</code> <p>What to do when an exception occurs in the scheduler or callbacks (Does not apply to submitted compute!)</p> <ul> <li>If <code>\"raise\"</code>, the scheduler will stop and raise the     exception at the point where you called <code>run()</code>.</li> <li>If <code>\"ignore\"</code>, the scheduler will continue running,     ignoring the exception. This may be useful when requiring more     robust execution.</li> <li>If <code>\"end\"</code>, similar to <code>\"raise\"</code>, the scheduler     will stop but no exception will occur and the control flow     will return gracefully to the point where you called <code>run()</code>.</li> </ul> <p> TYPE: <code>Literal['raise', 'end', 'ignore']</code> DEFAULT: <code>'raise'</code> </p> <code>asyncio_debug_mode</code> <p>Whether to run the async loop in debug mode. Defaults to <code>False</code>. Please see asyncio.run for more.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>display</code> <p>Whether to display the scheduler live in the console.</p> <ul> <li>If <code>True</code>, will display the scheduler and all its tasks.</li> <li>If a <code>list[RenderableType]</code> , will display the scheduler     itself plus those renderables.</li> </ul> <p> TYPE: <code>bool | list[RenderableType]</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>ExitState</code> <p>The reason for the scheduler ending.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the scheduler is already running.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def run(\n    self,\n    *,\n    timeout: float | None = None,\n    end_on_empty: bool = True,\n    wait: bool = True,\n    on_exception: Literal[\"raise\", \"end\", \"ignore\"] = \"raise\",\n    asyncio_debug_mode: bool = False,\n    display: bool | list[RenderableType] = False,\n) -&gt; ExitState:\n    \"\"\"Run the scheduler.\n\n    Args:\n        timeout: The maximum time to run the scheduler for in\n            seconds. Defaults to `None` which means no timeout and it\n            will end once the queue is empty if `end_on_empty=True`.\n        end_on_empty: Whether to end the scheduler when the queue becomes empty.\n        wait: Whether to wait for currently running compute to finish once\n            the `Scheduler` is shutting down.\n\n            * If `#!python True`, will wait for all currently running compute.\n            * If `#!python False`, will attempt to cancel/terminate all currently\n                running compute and shutdown the executor. This may be useful\n                if you want to end the scheduler as quickly as possible or\n                respect the `timeout=` more precisely.\n        on_exception: What to do when an exception occurs in the scheduler\n            or callbacks (**Does not apply to submitted compute!**)\n\n            * If `#!python \"raise\"`, the scheduler will stop and raise the\n                exception at the point where you called `run()`.\n            * If `#!python \"ignore\"`, the scheduler will continue running,\n                ignoring the exception. This may be useful when requiring more\n                robust execution.\n            * If `#!python \"end\"`, similar to `#!python \"raise\"`, the scheduler\n                will stop but no exception will occur and the control flow\n                will return gracefully to the point where you called `run()`.\n        asyncio_debug_mode: Whether to run the async loop in debug mode.\n            Defaults to `False`. Please see [asyncio.run][] for more.\n        display: Whether to display the scheduler live in the console.\n\n            * If `#!python True`, will display the scheduler and all its tasks.\n            * If a `#!python list[RenderableType]` , will display the scheduler\n                itself plus those renderables.\n\n    Returns:\n        The reason for the scheduler ending.\n\n    Raises:\n        RuntimeError: If the scheduler is already running.\n    \"\"\"\n    return asyncio.run(\n        self.async_run(\n            timeout=timeout,\n            end_on_empty=end_on_empty,\n            wait=wait,\n            on_exception=on_exception,\n            display=display,\n        ),\n        debug=asyncio_debug_mode,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.async_run","title":"<code>def async_run(*, timeout=None, end_on_empty=True, wait=True, on_exception='raise', display=False)</code>   <code>async</code>","text":"<p>Async version of <code>run</code>.</p> <p>This can be useful if you are already running in an async context, such as in a web server or Jupyter notebook.</p> <p>Please see <code>run()</code> for more details.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>async def async_run(\n    self,\n    *,\n    timeout: float | None = None,\n    end_on_empty: bool = True,\n    wait: bool = True,\n    on_exception: Literal[\"raise\", \"end\", \"ignore\"] = \"raise\",\n    display: bool | list[RenderableType] = False,\n) -&gt; ExitState:\n    \"\"\"Async version of `run`.\n\n    This can be useful if you are already running in an async context,\n    such as in a web server or Jupyter notebook.\n\n    Please see [`run()`][amltk.Scheduler.run] for more details.\n    \"\"\"\n    if self.running():\n        raise RuntimeError(\"Scheduler already seems to be running\")\n\n    logger.debug(\"Starting scheduler\")\n\n    # Make sure flags are set\n    self._end_on_exception_flag.set(value=on_exception in (\"raise\", \"end\"))\n\n    # If the user has requested to have a live display,\n    # we will need to setup a `Live` instance to render to\n    if display:\n        from rich.live import Live\n\n        if isinstance(display, list):\n            self._extra_renderables = display\n\n        self._live_output = Live(\n            auto_refresh=False,\n            get_renderable=self.__rich__,\n        )\n\n    loop = asyncio.get_running_loop()\n\n    # Set the exception handler for asyncio\n    previous_exception_handler = None\n    if on_exception in (\"raise\", \"end\"):\n        previous_exception_handler = loop.get_exception_handler()\n\n        def custom_exception_handler(\n            loop: asyncio.AbstractEventLoop,\n            context: dict[str, Any],\n        ) -&gt; None:\n            exception = context.get(\"exception\")\n            message = context.get(\"message\")\n\n            # handle with previous handler\n            if previous_exception_handler:\n                previous_exception_handler(loop, context)\n            else:\n                loop.default_exception_handler(context)\n\n            self.stop(stop_msg=message, exception=exception)\n\n        loop.set_exception_handler(custom_exception_handler)\n\n    # Run the actual scheduling loop\n    result = await self._run_scheduler(\n        timeout=timeout,\n        end_on_empty=end_on_empty,\n        wait=wait,\n    )\n\n    # Reset variables back to its default\n    self._live_output = None\n    self._extra_renderables = None\n    self._end_on_exception_flag.reset()\n\n    if previous_exception_handler is not None:\n        loop.set_exception_handler(previous_exception_handler)\n\n    # If we were meant to end on an exception and the result\n    # we got back from the scheduler was an exception, raise it\n    if isinstance(result, BaseException):\n        if on_exception == \"raise\":\n            raise result\n\n        return ExitState(code=ExitState.Code.EXCEPTION, exception=result)\n\n    return ExitState(code=result)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.stop","title":"<code>def stop(*args, stop_msg=None, exception=None, **kwargs)</code>","text":"<p>Stop the scheduler.</p> <p>The scheduler will stop, finishing currently running tasks depending on the <code>wait=</code> parameter to <code>Scheduler.run</code>.</p> <p>The call signature is kept open with <code>*args, **kwargs</code> to make it easier to include in any callback.</p> PARAMETER  DESCRIPTION <code>*args</code> <p>Logged in a debug message</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Logged in a debug message</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <code>stop_msg</code> <p>The message to log when stopping the scheduler.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>exception</code> <p>The exception which incited <code>stop()</code> to be called. Will be used by the <code>Scheduler</code> to possibly raise the exception to the user.</p> <p> TYPE: <code>BaseException | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def stop(\n    self,\n    *args: Any,\n    stop_msg: str | None = None,\n    exception: BaseException | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Stop the scheduler.\n\n    The scheduler will stop, finishing currently running tasks depending\n    on the `wait=` parameter to [`Scheduler.run`][amltk.Scheduler.run].\n\n    The call signature is kept open with `*args, **kwargs` to make it\n    easier to include in any callback.\n\n    Args:\n        *args: Logged in a debug message\n        **kwargs: Logged in a debug message\n        stop_msg: The message to log when stopping the scheduler.\n        exception: The exception which incited `stop()` to be called.\n            Will be used by the `Scheduler` to possibly raise the exception\n            to the user.\n    \"\"\"\n    if not self.running():\n        return\n\n    assert self._stop_event is not None\n\n    msg = stop_msg if stop_msg is not None else \"scheduler.stop() was called.\"\n    logger.debug(f\"Stopping scheduler: {msg} {args=} {kwargs=}\")\n\n    self._stop_event.set(msg=msg, exception=exception)\n    self._running_event.clear()\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.call_later","title":"<code>def call_later(delay, fn, *args, **kwargs)</code>","text":"<p>Schedule a function to be run after a delay.</p> PARAMETER  DESCRIPTION <code>delay</code> <p>The delay in seconds.</p> <p> TYPE: <code>float</code> </p> <code>fn</code> <p>The function to run.</p> <p> TYPE: <code>Callable[P, Any]</code> </p> <code>args</code> <p>The positional arguments to pass to the function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>The keyword arguments to pass to the function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>TimerHandle</code> <p>A timer handle that can be used to cancel the function.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def call_later(\n    self,\n    delay: float,\n    fn: Callable[P, Any],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; asyncio.TimerHandle:\n    \"\"\"Schedule a function to be run after a delay.\n\n    Args:\n        delay: The delay in seconds.\n        fn: The function to run.\n        args: The positional arguments to pass to the function.\n        kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        A timer handle that can be used to cancel the function.\n    \"\"\"\n    if not self.running():\n        raise RuntimeError(\"Scheduler is not running!\")\n\n    _fn = partial(fn, *args, **kwargs)\n    loop = asyncio.get_running_loop()\n    return loop.call_later(delay, _fn)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.Scheduler.add_renderable","title":"<code>def add_renderable(renderable)</code>","text":"<p>Add a renderable object to the scheduler.</p> <p>This will be displayed whenever the scheduler is displayed.</p> Source code in <code>src/amltk/scheduling/scheduler.py</code> <pre><code>def add_renderable(self, renderable: RenderableType) -&gt; None:\n    \"\"\"Add a renderable object to the scheduler.\n\n    This will be displayed whenever the scheduler is displayed.\n    \"\"\"\n    self._renderables.append(renderable)\n</code></pre>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState","title":"<code>class ExitState</code>   <code>dataclass</code>","text":"<p>The exit state of a scheduler.</p> ATTRIBUTE DESCRIPTION <code>reason</code> <p>The reason for the exit.</p> <p> </p> <code>exception</code> <p>The exception that caused the exit, if any.</p> <p> TYPE: <code>BaseException | None</code> </p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code","title":"<code>class Code</code>","text":"<p>         Bases: <code>Enum</code></p> <p>The reason the scheduler ended.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.STOPPED","title":"<code>STOPPED</code>   <code>classvar</code> <code>attr</code>","text":"<p>The scheduler was stopped forcefully with <code>Scheduler.stop</code>.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.TIMEOUT","title":"<code>TIMEOUT</code>   <code>classvar</code> <code>attr</code>","text":"<p>The scheduler finished because of a timeout.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.EXHAUSTED","title":"<code>EXHAUSTED</code>   <code>classvar</code> <code>attr</code>","text":"<p>The scheduler finished because it exhausted its queue.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.CANCELLED","title":"<code>CANCELLED</code>   <code>classvar</code> <code>attr</code>","text":"<p>The scheduler was cancelled.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.UNKNOWN","title":"<code>UNKNOWN</code>   <code>classvar</code> <code>attr</code>","text":"<p>The scheduler finished for an unknown reason.</p>"},{"location":"api/amltk/scheduling/scheduler/#amltk.scheduling.scheduler.ExitState.Code.EXCEPTION","title":"<code>EXCEPTION</code>   <code>classvar</code> <code>attr</code>","text":"<p>The scheduler finished because of an exception.</p>"},{"location":"api/amltk/scheduling/task/","title":"Task","text":"<p>A <code>Task</code> is a unit of work that can be scheduled by the <code>Scheduler</code>.</p> <p>It is defined by its <code>function=</code> to call. Whenever a <code>Task</code> has its <code>submit()</code> method called, the function will be dispatched to run by a <code>Scheduler</code>.</p> <p>When a task has returned, either successfully, or with an exception, it will emit <code>@events</code> to indicate so. You can subscribe to these events with callbacks and act accordingly.</p> <code>@events</code> <p>Check out the <code>@events</code> reference for more on how to customize these callbacks. You can also take a look at the API of <code>on()</code> for more information.</p> <code>@on_result</code><code>@on_exception</code><code>@on_done</code><code>@on_submitted</code><code>@on_cancelled</code> <p>Called when a task has successfully returned a value. Comes with Future <pre><code>@task.on_result\ndef on_result(future: Future[R], result: R):\n    print(f\"Future {future} returned {result}\")\n</code></pre></p> <p>Called when a task failed to return anything but an exception. Comes with Future <pre><code>@task.on_exception\ndef on_exception(future: Future[R], error: BaseException):\n    print(f\"Future {future} exceptioned {error}\")\n</code></pre></p> <p>Called when a task is done running with a result or exception. <pre><code>@task.on_done\ndef on_done(future: Future[R]):\n    print(f\"Future {future} is done\")\n</code></pre></p> <p>An event that is emitted when a future is submitted to the scheduler. It will pass the future as the first argument with args and kwargs following.</p> <p>This is done before any callbacks are attached to the future. <pre><code>@task.on_submitted\ndef on_submitted(future: Future[R], *args, **kwargs):\n    print(f\"Future {future} was submitted with {args=} and {kwargs=}\")\n</code></pre></p> <p>Called when a task is cancelled. <pre><code>@task.on_cancelled\ndef on_cancelled(future: Future[R]):\n    print(f\"Future {future} was cancelled\")\n</code></pre></p> Usage <p>The usual way to create a task is with <code>Scheduler.task()</code>, where you provide the <code>function=</code> to call.</p> <p><pre><code>from amltk import Scheduler\n\ndef f(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(2)\ntask = scheduler.task(f)\n\n@scheduler.on_start\ndef on_start():\n    task.submit(1)\n\n@task.on_result\ndef on_result(future: Future[int], result: int):\n    print(f\"Task {future} returned {result}\")\n\nscheduler.run()\n</code></pre> <p>Task  returned 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def on_start() (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 1\n\u2503   @on_future_done 1\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 1\n\u2517\u2501\u2501 \u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 1                                                          \u2502\n    \u2502 @on_done 1                                                               \u2502\n    \u2502 @on_result 1                                                             \u2502\n    \u2502 \u2514\u2500\u2500 def on_result(future: 'Future[int]', result: 'int') (1)              \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 7qbmaiTD \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>If you'd like to simply just call the original function, without submitting it to the scheduler, you can always just call the task directly, i.e. <code>task(1)</code>.</p> <p>You can also provide <code>Plugins</code> to the task, to modify tasks, add functionality and add new events.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task","title":"<code>class Task(function, scheduler, *, plugins=(), init_plugins=True)</code>","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[P, R]</code></p> <p>The task class.</p> PARAMETER  DESCRIPTION <code>function</code> <p>The function of this task</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>scheduler</code> <p>The scheduler that this task is registered with.</p> <p> TYPE: <code>Scheduler</code> </p> <code>plugins</code> <p>The plugins to use for this task.</p> <p> TYPE: <code>Plugin | Iterable[Plugin]</code> DEFAULT: <code>()</code> </p> <code>init_plugins</code> <p>Whether to initialize the plugins or not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def __init__(\n    self: Self,\n    function: Callable[P, R],\n    scheduler: Scheduler,\n    *,\n    plugins: Plugin | Iterable[Plugin] = (),\n    init_plugins: bool = True,\n) -&gt; None:\n    \"\"\"Initialize a task.\n\n    Args:\n        function: The function of this task\n        scheduler: The scheduler that this task is registered with.\n        plugins: The plugins to use for this task.\n        init_plugins: Whether to initialize the plugins or not.\n    \"\"\"\n    super().__init__()\n    self.unique_ref = randuid(8)\n\n    self.emitter = Emitter()\n    self.event_counts = self.emitter.event_counts\n    self.plugins: list[Plugin] = (\n        [plugins] if isinstance(plugins, Plugin) else list(plugins)\n    )\n    self.function: Callable[P, R] = function\n    self.scheduler: Scheduler = scheduler\n    self.init_plugins: bool = init_plugins\n    self.queue: list[Future[R]] = []\n\n    # Set up subscription methods to events\n    self.on_submitted = self.emitter.subscriber(self.SUBMITTED)\n    self.on_done = self.emitter.subscriber(self.DONE)\n    self.on_result = self.emitter.subscriber(self.RESULT)\n    self.on_exception = self.emitter.subscriber(self.EXCEPTION)\n    self.on_cancelled = self.emitter.subscriber(self.CANCELLED)\n\n    if init_plugins:\n        for plugin in self.plugins:\n            plugin.attach_task(self)\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.unique_ref","title":"<code>unique_ref: str</code>   <code>attr</code>","text":"<p>A unique reference to this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.emitter","title":"<code>emitter: Emitter</code>   <code>attr</code>","text":"<p>The emitter for events of this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.plugins","title":"<code>plugins: list[Plugin]</code>   <code>attr</code>","text":"<p>The plugins to use for this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.function","title":"<code>function: Callable[P, R]</code>   <code>attr</code>","text":"<p>The function of this task</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.scheduler","title":"<code>scheduler: Scheduler</code>   <code>attr</code>","text":"<p>The scheduler that this task is registered with.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.init_plugins","title":"<code>init_plugins: bool</code>   <code>attr</code>","text":"<p>Whether to initialize the plugins or not.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.queue","title":"<code>queue: list[Future[R]]</code>   <code>attr</code>","text":"<p>The queue of futures for this task.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_submitted","title":"<code>on_submitted: Subscriber[Concatenate[Future[R], P]]</code>   <code>attr</code>","text":"<p>An event that is emitted when a future is submitted to the scheduler. It will pass the future as the first argument with args and kwargs following.</p> <p>This is done before any callbacks are attached to the future. <pre><code>@task.on_submitted\ndef on_submitted(future: Future[R], *args, **kwargs):\n    print(f\"Future {future} was submitted with {args=} and {kwargs=}\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_done","title":"<code>on_done: Subscriber[Future[R]]</code>   <code>attr</code>","text":"<p>Called when a task is done running with a result or exception. <pre><code>@task.on_done\ndef on_done(future: Future[R]):\n    print(f\"Future {future} is done\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_result","title":"<code>on_result: Subscriber[Future[R], R]</code>   <code>attr</code>","text":"<p>Called when a task has successfully returned a value. Comes with Future <pre><code>@task.on_result\ndef on_result(future: Future[R], result: R):\n    print(f\"Future {future} returned {result}\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_exception","title":"<code>on_exception: Subscriber[Future[R], BaseException]</code>   <code>attr</code>","text":"<p>Called when a task failed to return anything but an exception. Comes with Future <pre><code>@task.on_exception\ndef on_exception(future: Future[R], error: BaseException):\n    print(f\"Future {future} exceptioned {error}\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on_cancelled","title":"<code>on_cancelled: Subscriber[Future[R]]</code>   <code>attr</code>","text":"<p>Called when a task is cancelled. <pre><code>@task.on_cancelled\ndef on_cancelled(future: Future[R]):\n    print(f\"Future {future} was cancelled\")\n</code></pre></p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.n_running","title":"<code>n_running: int</code>   <code>prop</code>","text":"<p>Get the number of futures for this task that are currently running.</p>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.futures","title":"<code>def futures()</code>","text":"<p>Get the futures for this task.</p> RETURNS DESCRIPTION <code>list[Future[R]]</code> <p>A list of futures for this task.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def futures(self) -&gt; list[Future[R]]:\n    \"\"\"Get the futures for this task.\n\n    Returns:\n        A list of futures for this task.\n    \"\"\"\n    return self.queue\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.on","title":"<code>def on(event, callback=None, *, when=None, max_calls=None, repeat=1, every=1, hidden=False)</code>","text":"<p>Subscribe to an event.</p> PARAMETER  DESCRIPTION <code>event</code> <p>The event to subscribe to.</p> <p> TYPE: <code>Event[P2] | str</code> </p> <code>callback</code> <p>The callback to call when the event is emitted. If not specified, what is returned can be used as a decorator.</p> <p> TYPE: <code>Callable[P2, Any] | None</code> DEFAULT: <code>None</code> </p> <code>when</code> <p>A predicate to determine whether to call the callback.</p> <p> TYPE: <code>Callable[[], bool] | None</code> DEFAULT: <code>None</code> </p> <code>max_calls</code> <p>The maximum number of times to call the callback.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>repeat</code> <p>The number of times to repeat the subscription.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>every</code> <p>The number of times to wait between repeats.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>hidden</code> <p>Whether to hide the callback in visual output. This is mainly used to facilitate Plugins who act upon events but don't want to be seen, primarily as they are just book-keeping callbacks.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Subscriber[P2] | Subscriber[...] | None</code> <p>The subscriber if no callback was provided, otherwise <code>None</code>.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def on(\n    self,\n    event: Event[P2] | str,\n    callback: Callable[P2, Any] | None = None,\n    *,\n    when: Callable[[], bool] | None = None,\n    max_calls: int | None = None,\n    repeat: int = 1,\n    every: int = 1,\n    hidden: bool = False,\n) -&gt; Subscriber[P2] | Subscriber[...] | None:\n    \"\"\"Subscribe to an event.\n\n    Args:\n        event: The event to subscribe to.\n        callback: The callback to call when the event is emitted.\n            If not specified, what is returned can be used as a decorator.\n        when: A predicate to determine whether to call the callback.\n        max_calls: The maximum number of times to call the callback.\n        repeat: The number of times to repeat the subscription.\n        every: The number of times to wait between repeats.\n        hidden: Whether to hide the callback in visual output.\n            This is mainly used to facilitate Plugins who\n            act upon events but don't want to be seen, primarily\n            as they are just book-keeping callbacks.\n\n    Returns:\n        The subscriber if no callback was provided, otherwise `None`.\n    \"\"\"\n    if isinstance(event, str):\n        _e = first_true(self.emitter.events, None, lambda e: e.name == event)\n        if _e is None:\n            raise EventNotKnownError(\n                f\"{event=} is not a valid event.\"\n                f\"\\nKnown events are: {[e.name for e in self.emitter.events]}\",\n            )\n    else:\n        _e = event\n\n    subscriber = self.emitter.subscriber(\n        _e,  # type: ignore\n        when=when,\n        max_calls=max_calls,\n        repeat=repeat,\n        every=every,\n    )\n    if callback is None:\n        return subscriber\n\n    subscriber(callback, hidden=hidden)\n    return None\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.running","title":"<code>def running()</code>","text":"<p>Check if this task has any futures that are currently running.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def running(self) -&gt; bool:\n    \"\"\"Check if this task has any futures that are currently running.\"\"\"\n    return self.n_running &gt; 0\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.submit","title":"<code>def submit(*args, **kwargs)</code>","text":"<p>Dispatch this task.</p> PARAMETER  DESCRIPTION <code>*args</code> <p>The positional arguments to pass to the task.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to call the task with.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Future[R] | None</code> <p>The future of the task, or <code>None</code> if it was rejected for either reaching</p> <code>Future[R] | None</code> <p>some max call limit or a plugin prevented if from being submitted.</p> RAISES DESCRIPTION <code>SchedulerNotRunningError</code> <p>If the scheduler is not running. You can protect against this using, <code>scheduler.running()</code>.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def submit(self, *args: P.args, **kwargs: P.kwargs) -&gt; Future[R] | None:\n    \"\"\"Dispatch this task.\n\n    Args:\n        *args: The positional arguments to pass to the task.\n        **kwargs: The keyword arguments to call the task with.\n\n    Returns:\n        The future of the task, or `None` if it was rejected for either reaching\n        some max call limit or a plugin prevented if from being submitted.\n\n    Raises:\n        SchedulerNotRunningError: If the scheduler is not running.\n            You can protect against this using,\n            [`scheduler.running()`][amltk.scheduling.scheduler.Scheduler.running].\n    \"\"\"\n    # Inform all plugins that the task is about to be called\n    # They have chance to cancel submission based on their return\n    # value.\n    fn = self.function\n    for plugin in self.plugins:\n        items = plugin.pre_submit(fn, *args, **kwargs)\n        if items is None:\n            logger.debug(\n                f\"Plugin '{plugin.name}' prevented {self} from being submitted\"\n                f\" with {callstring(self.function, *args, **kwargs)}\",\n            )\n            return None\n\n        fn, args, kwargs = items  # type: ignore\n\n    try:\n        future = self.scheduler.submit(fn, *args, **kwargs)\n    except SchedulerNotRunningError as e:\n        logger.exception(\"Scheduler is not running\", exc_info=e)\n        raise e\n    except Exception as e:\n        logger.exception(\"Error submitting task\", exc_info=e)\n        raise e\n\n    self.queue.append(future)\n\n    # We have the function wrapped in something will\n    # attach tracebacks to errors, so we need to get the\n    # original function name.\n    msg = f\"Submitted {callstring(self.function, *args, **kwargs)} from {self}.\"\n    logger.debug(msg)\n    self.on_submitted.emit(future, *args, **kwargs)\n\n    # Process the task once it's completed\n    # NOTE: If the task is done super quickly or in the sequential mode,\n    # this will immediatly call `self._process_future`.\n    future.add_done_callback(self._process_future)\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.copy","title":"<code>def copy(*, init_plugins=True)</code>","text":"<p>Create a copy of this task.</p> <p>Will use the same scheduler and function, but will have a different event manager such that any events listend to on the old task will not trigger with the copied task.</p> PARAMETER  DESCRIPTION <code>init_plugins</code> <p>Whether to initialize the copied plugins on the copied task. Usually you will want to leave this as <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A copy of this task.</p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def copy(self, *, init_plugins: bool = True) -&gt; Self:\n    \"\"\"Create a copy of this task.\n\n    Will use the same scheduler and function, but will have a different\n    event manager such that any events listend to on the old task will\n    **not** trigger with the copied task.\n\n    Args:\n        init_plugins: Whether to initialize the copied plugins on the copied\n            task. Usually you will want to leave this as `True`.\n\n    Returns:\n        A copy of this task.\n    \"\"\"\n    return self.__class__(\n        self.function,\n        self.scheduler,\n        plugins=tuple(p.copy() for p in self.plugins),\n        init_plugins=init_plugins,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/task/#amltk.scheduling.task.Task.attach_plugin","title":"<code>def attach_plugin(plugin)</code>","text":"<p>Attach a plugin to this task.</p> PARAMETER  DESCRIPTION <code>plugin</code> <p>The plugin to attach.</p> <p> TYPE: <code>Plugin</code> </p> Source code in <code>src/amltk/scheduling/task.py</code> <pre><code>def attach_plugin(self, plugin: Plugin) -&gt; None:\n    \"\"\"Attach a plugin to this task.\n\n    Args:\n        plugin: The plugin to attach.\n    \"\"\"\n    self.plugins.append(plugin)\n    plugin.attach_task(self)\n</code></pre>"},{"location":"api/amltk/scheduling/termination_strategies/","title":"Termination strategies","text":"<p>This module is concerned with the termination of the workers of a scheduler.</p> <p>Most Executors on <code>shutdown(wait=False)</code> will not terminate their workers but just cancel pending futures. This means that currently running tasks will continue to run until they finish and have their result discarded as the program will have moved on.</p> <p>We provide some custom strategies for known executors.</p> Note <p>There is no known way in basic Python to forcibully terminate a thread that does not account for early terminiation explicitly.</p>"},{"location":"api/amltk/scheduling/termination_strategies/#amltk.scheduling.termination_strategies.termination_strategy","title":"<code>def termination_strategy(executor)</code>","text":"<p>Return a termination strategy for the given executor.</p> PARAMETER  DESCRIPTION <code>executor</code> <p>The executor to get a termination strategy for.</p> <p> TYPE: <code>_Executor</code> </p> RETURNS DESCRIPTION <code>Callable[[_Executor], None] | None</code> <p>A termination strategy for the given executor, or None if no</p> <code>Callable[[_Executor], None] | None</code> <p>termination strategy is available.</p> Source code in <code>src/amltk/scheduling/termination_strategies.py</code> <pre><code>def termination_strategy(executor: _Executor) -&gt; Callable[[_Executor], None] | None:\n    \"\"\"Return a termination strategy for the given executor.\n\n    Args:\n        executor: The executor to get a termination strategy for.\n\n    Returns:\n        A termination strategy for the given executor, or None if no\n        termination strategy is available.\n    \"\"\"\n    if isinstance(executor, ThreadPoolExecutor):\n        return None\n\n    if isinstance(executor, ProcessPoolExecutor):\n        return _terminate_with_psutil  # type: ignore\n\n    # Dask process based things seem pretty happy to close nicely and need\n    # no special treatment.\n\n    return None\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/","title":"Dask jobqueue","text":"<p>Dask Jobqueue Executors.</p> <p>These are essentially wrappers around the dask_jobqueue classes. We use them to provide a consistent interface for all the different jobqueue implementations and get access to their executors.</p> <p>Documentation from <code>dask_jobqueue</code></p> <p>See the dask jobqueue documentation specifically:</p> <ul> <li>Example deployments</li> <li>Tips and Tricks</li> <li>Debugging</li> </ul>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor","title":"<code>class DaskJobqueueExecutor(cluster, *, n_workers, adaptive=False, submit_command=None, cancel_command=None)</code>","text":"<p>         Bases: <code>Executor</code>, <code>Generic[_JQC]</code></p> <p>A concurrent.futures Executor that executes tasks on a dask_jobqueue cluster.</p> <p>Implementations</p> <p>Prefer to use the class methods to create an instance of this class.</p> <ul> <li><code>DaskJobqueueExecutor.SLURM()</code></li> <li><code>DaskJobqueueExecutor.HTCondor()</code></li> <li><code>DaskJobqueueExecutor.LSF()</code></li> <li><code>DaskJobqueueExecutor.OAR()</code></li> <li><code>DaskJobqueueExecutor.PBS()</code></li> <li><code>DaskJobqueueExecutor.SGE()</code></li> <li><code>DaskJobqueueExecutor.Moab()</code></li> </ul> PARAMETER  DESCRIPTION <code>cluster</code> <p>The implementation of a dask_jobqueue.JobQueueCluster.</p> <p> TYPE: <code>_JQC</code> </p> <code>n_workers</code> <p>The number of workers to maximally adapt to on the cluster.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>To overwrite the submission command if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>To overwrite the cancel command if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>def __init__(\n    self,\n    cluster: _JQC,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n):\n    \"\"\"Initialize a DaskJobqueueExecutor.\n\n\n    !!! note \"Implementations\"\n\n        Prefer to use the class methods to create an instance of this class.\n\n        * [`DaskJobqueueExecutor.SLURM()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SLURM]\n        * [`DaskJobqueueExecutor.HTCondor()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.HTCondor]\n        * [`DaskJobqueueExecutor.LSF()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.LSF]\n        * [`DaskJobqueueExecutor.OAR()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.OAR]\n        * [`DaskJobqueueExecutor.PBS()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.PBS]\n        * [`DaskJobqueueExecutor.SGE()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SGE]\n        * [`DaskJobqueueExecutor.Moab()`][amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.Moab]\n\n    Args:\n        cluster: The implementation of a\n            [dask_jobqueue.JobQueueCluster](https://jobqueue.dask.org/en/latest/api.html).\n        n_workers: The number of workers to maximally adapt to on the cluster.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed allocate all workers.\n            This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers specified.\n        submit_command: To overwrite the submission command if necessary.\n        cancel_command: To overwrite the cancel command if necessary.\n    \"\"\"\n    super().__init__()\n    self.cluster = cluster\n    self.adaptive = adaptive\n    if submit_command:\n        self.cluster.job_cls.submit_command = submit_command  # type: ignore\n\n    if cancel_command:\n        self.cluster.job_cls.cancel_command = cancel_command  # type: ignore\n\n    if adaptive:\n        self.cluster.adapt(minimum=0, maximum=n_workers)\n    else:\n        self.cluster.scale(n_workers)\n\n    self.n_workers = n_workers\n    self.executor: ClientExecutor = self.cluster.get_client().get_executor()\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.submit","title":"<code>def submit(fn, /, *args, **kwargs)</code>","text":"<p>See concurrent.futures.Executor.submit.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@override\ndef submit(\n    self,\n    fn: Callable[P, R],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[R]:\n    \"\"\"See [concurrent.futures.Executor.submit][].\"\"\"\n    future = self.executor.submit(fn, *args, **kwargs)\n    assert isinstance(future, Future)\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.map","title":"<code>def map(fn, *iterables, timeout=None, chunksize=1)</code>","text":"<p>See concurrent.futures.Executor.map.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@override\ndef map(\n    self,\n    fn: Callable[..., R],\n    *iterables: Iterable,\n    timeout: float | None = None,\n    chunksize: int = 1,\n) -&gt; Iterator[R]:\n    \"\"\"See [concurrent.futures.Executor.map][].\"\"\"\n    return self.executor.map(  # type: ignore\n        fn,\n        *iterables,\n        timeout=timeout,\n        chunksize=chunksize,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.shutdown","title":"<code>def shutdown(wait=True, **kwargs)</code>","text":"<p>See concurrent.futures.Executor.shutdown.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@override\ndef shutdown(\n    self,\n    wait: bool = True,  # noqa: FBT001, FBT002\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"See [concurrent.futures.Executor.shutdown][].\"\"\"\n    self.executor.shutdown(wait=wait, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SLURM","title":"<code>def SLURM(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor for a SLURM cluster.</p> <p>See the dask_jobqueue.SLURMCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef SLURM(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[SLURMCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a SLURM cluster.\n\n    See the [dask_jobqueue.SLURMCluster documentation][dask_jobqueue.SLURMCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        SLURMCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        n_workers=n_workers,\n        adaptive=adaptive,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.HTCondor","title":"<code>def HTCondor(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor for a HTCondor cluster.</p> <p>See the dask_jobqueue.HTCondorCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef HTCondor(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[HTCondorCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a HTCondor cluster.\n\n    See the [dask_jobqueue.HTCondorCluster documentation][dask_jobqueue.HTCondorCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        HTCondorCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.LSF","title":"<code>def LSF(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor for a LSF cluster.</p> <p>See the dask_jobqueue.LSFCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef LSF(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[LSFCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a LSF cluster.\n\n    See the [dask_jobqueue.LSFCluster documentation][dask_jobqueue.LSFCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        LSFCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.OAR","title":"<code>def OAR(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor for a OAR cluster.</p> <p>See the dask_jobqueue.OARCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef OAR(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[OARCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a OAR cluster.\n\n    See the [dask_jobqueue.OARCluster documentation][dask_jobqueue.OARCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        OARCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.PBS","title":"<code>def PBS(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor for a PBS cluster.</p> <p>See the dask_jobqueue.PBSCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef PBS(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[PBSCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a PBS cluster.\n\n    See the [dask_jobqueue.PBSCluster documentation][dask_jobqueue.PBSCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        PBSCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.SGE","title":"<code>def SGE(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor for a SGE cluster.</p> <p>See the dask_jobqueue.SGECluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef SGE(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[SGECluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a SGE cluster.\n\n    See the [dask_jobqueue.SGECluster documentation][dask_jobqueue.SGECluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        SGECluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.Moab","title":"<code>def Moab(*, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor for a Moab cluster.</p> <p>See the dask_jobqueue.MoabCluster documentation for more information on the available keyword arguments.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef Moab(\n    cls,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor[MoabCluster]:\n    \"\"\"Create a DaskJobqueueExecutor for a Moab cluster.\n\n    See the [dask_jobqueue.MoabCluster documentation][dask_jobqueue.MoabCluster] for\n    more information on the available keyword arguments.\n    \"\"\"\n    return cls(  # type: ignore\n        MoabCluster(**kwargs),\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        n_workers=n_workers,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/dask_jobqueue/#amltk.scheduling.executors.dask_jobqueue.DaskJobqueueExecutor.from_str","title":"<code>def from_str(name, *, n_workers, adaptive=False, submit_command=None, cancel_command=None, **kwargs)</code>   <code>classmethod</code>","text":"<p>Create a DaskJobqueueExecutor using a string lookup.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of cluster to create, must be one of [\"slurm\", \"htcondor\", \"lsf\", \"oar\", \"pbs\", \"sge\", \"moab\"].</p> <p> TYPE: <code>DJQ_NAMES</code> </p> <code>n_workers</code> <p>The number of workers to maximally adapt to on the cluster.</p> <p> TYPE: <code>int</code> </p> <code>adaptive</code> <p>Whether to use the adaptive scaling of the cluster or fixed allocate all workers. This will specifically use the dask_jobqueue.SLURMCluster.adapt method to dynamically scale the cluster to the number of workers specified.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>submit_command</code> <p>Overwrite the submit command of workers if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>cancel_command</code> <p>Overwrite the cancel command of workers if necessary.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>The keyword arguments to pass to the cluster constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>KeyError</code> <p>If <code>name</code> is not one of the supported cluster types.</p> RETURNS DESCRIPTION <code>DaskJobqueueExecutor</code> <p>A DaskJobqueueExecutor for the requested cluster type.</p> Source code in <code>src/amltk/scheduling/executors/dask_jobqueue.py</code> <pre><code>@classmethod\ndef from_str(\n    cls,\n    name: DJQ_NAMES,\n    *,\n    n_workers: int,\n    adaptive: bool = False,\n    submit_command: str | None = None,\n    cancel_command: str | None = None,\n    **kwargs: Any,\n) -&gt; DaskJobqueueExecutor:\n    \"\"\"Create a DaskJobqueueExecutor using a string lookup.\n\n    Args:\n        name: The name of cluster to create, must be one of\n            [\"slurm\", \"htcondor\", \"lsf\", \"oar\", \"pbs\", \"sge\", \"moab\"].\n        n_workers: The number of workers to maximally adapt to on the cluster.\n        adaptive: Whether to use the adaptive scaling of the cluster or fixed allocate all workers.\n            This will specifically use the\n            [dask_jobqueue.SLURMCluster.adapt](https://jobqueue.dask.org/en/latest/index.html?highlight=adapt#adaptivity)\n            method to dynamically scale the cluster to the number of workers specified.\n        submit_command: Overwrite the submit command of workers if necessary.\n        cancel_command: Overwrite the cancel command of workers if necessary.\n        kwargs: The keyword arguments to pass to the cluster constructor.\n\n    Raises:\n        KeyError: If `name` is not one of the supported cluster types.\n\n    Returns:\n        A DaskJobqueueExecutor for the requested cluster type.\n    \"\"\"\n    methods = {\n        \"slurm\": cls.SLURM,\n        \"htcondor\": cls.HTCondor,\n        \"lsf\": cls.LSF,\n        \"oar\": cls.OAR,\n        \"pbs\": cls.PBS,\n        \"sge\": cls.SGE,\n        \"moab\": cls.Moab,\n    }\n    method = methods.get(name.lower())\n    if method is None:\n        raise KeyError(\n            f\"Unknown cluster name: {name}, must be from {list(methods)}\",\n        )\n\n    return method(\n        n_workers=n_workers,\n        submit_command=submit_command,\n        cancel_command=cancel_command,\n        adaptive=adaptive,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/executors/sequential_executor/","title":"Sequential executor","text":"<p>A concurrent.futures.Executor interface that forces sequential execution.</p>"},{"location":"api/amltk/scheduling/executors/sequential_executor/#amltk.scheduling.executors.sequential_executor.SequentialExecutor","title":"<code>class SequentialExecutor</code>","text":"<p>         Bases: <code>Executor</code></p> <p>A Executor interface for sequential execution.</p>"},{"location":"api/amltk/scheduling/executors/sequential_executor/#amltk.scheduling.executors.sequential_executor.SequentialExecutor.submit","title":"<code>def submit(fn, /, *args, **kwargs)</code>","text":"<p>Submit a function to be executed.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The function to execute.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>*args</code> <p>The positional arguments to pass to the function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to pass to the function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Future[R]</code> <p>A future that is already resolved with the result/exception of the function.</p> Source code in <code>src/amltk/scheduling/executors/sequential_executor.py</code> <pre><code>@override\ndef submit(\n    self,\n    fn: Callable[P, R],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[R]:\n    \"\"\"Submit a function to be executed.\n\n    Args:\n        fn: The function to execute.\n        *args: The positional arguments to pass to the function.\n        **kwargs: The keyword arguments to pass to the function.\n\n    Returns:\n        A future that is already resolved with the result/exception of the function.\n    \"\"\"\n    # TODO: It would be good if we can somehow wrap this in some sort\n    # of async context such that it allows other callbacks to operate.\n    future: Future[R] = Future()\n    future.set_running_or_notify_cancel()\n\n    try:\n        future.set_result(fn(*args, **kwargs))\n    except BaseException as exc:  # noqa: BLE001\n        future.set_exception(exc)\n\n    return future\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/","title":"Comm","text":"<p>The <code>Comm.Plugin</code> enables two way-communication with running <code>Task</code>.</p> <p>The <code>Comm</code> provides an easy interface to communicate while the <code>Comm.Msg</code> encapsulates messages between the main process and the <code>Task</code>.</p> Usage <p>To setup a <code>Task</code> to work with a <code>Comm</code>, the <code>Task</code> must accept a <code>comm</code> as it's first argument.</p> <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef powers_of_two(comm: Comm, start: int, n: int) -&gt; None:\n    with comm.open():\n        for i in range(n):\n            comm.send(start ** (i+1))\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(powers_of_two, plugins=Comm.Plugin())\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(2, 5)\n\n@task.on(\"comm-open\")\ndef on_open(msg: Comm.Msg):\n    print(f\"Task has opened | {msg}\")\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>Task has opened | Comm.Msg(kind=&lt;Kind.OPEN: 'open'&gt;, data=None)\n[2, 4, 8, 16, 32]\n</code></pre> </p> <p>You can also block a worker, waiting for a response from the main process, allowing for the worker to <code>request()</code> data from the main process.</p> <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef my_worker(comm: Comm, n_tasks: int) -&gt; None:\n    with comm.open():\n        for task_number in range(n_tasks):\n            task = comm.request(task_number)\n            comm.send(f\"Task recieved {task} for {task_number}\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(my_worker, plugins=Comm.Plugin())\n\nitems = [\"A\", \"B\", \"C\"]\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(n_tasks=3)\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    task_number = msg.data\n    msg.respond(items[task_number])\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>['Task recieved A for 0', 'Task recieved B for 1', 'Task recieved C for 2']\n</code></pre> </p> <code>@events</code> <code>@comm-message</code><code>@comm-request</code><code>@comm-open</code><code>@comm-close</code> <p>A Task has sent a message to the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm, x: int) -&gt; int:\n    with comm.open():\n        comm.send(x + 1)\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@task.on(\"comm-message\")\ndef callback(msg: Comm.Msg):\n    print(msg.data)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(comm: 'Comm', x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message                                                                \u2502\n\u2502 \u2514\u2500\u2500 def callback(msg: 'Comm.Msg')                                            \u2502\n\u2502 @comm-request                                                                \u2502\n\u2502 @comm-open                                                                   \u2502\n\u2502 @comm-close                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: uX7FJqb6 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task has sent a request.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef greeter(comm: Comm, greeting: str) -&gt; None:\n    with comm.open():\n        name = comm.request()\n        comm.send(f\"{greeting} {name}!\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(greeter, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit(\"Hello\")\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    msg.respond(\"Alice\")\n\n@task.on(\"comm-message\")\ndef on_msg(msg: Comm.Msg):\n    print(msg.data)\n\nscheduler.run()\n</code></pre> <p>Hello Alice!  <pre>\n<code>\u256d\u2500 Task greeter(comm: 'Comm', greeting: 'str') -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message 1                                                              \u2502\n\u2502 \u2514\u2500\u2500 def on_msg(msg: 'Comm.Msg') (1)                                          \u2502\n\u2502 @comm-request 1                                                              \u2502\n\u2502 \u2514\u2500\u2500 def on_request(msg: 'Comm.Msg') (1)                                      \u2502\n\u2502 @comm-open 1                                                                 \u2502\n\u2502 @comm-close 1                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: W1LkagLs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The task has signalled it's open.</p> <p>```python exec=\"true\" source=\"material-block\" html=\"true\" hl_lines=\"5 15-17\"  from amltk.scheduling import Scheduler  from amltk.scheduling.plugins import Comm</p> <p>def fn(comm: Comm) -&gt; None:      with comm.open():          pass from amltk._doc import make_picklable; make_picklable(fn)  # markdown-exec: hide</p> <p>scheduler = Scheduler.with_processes(1)  task = scheduler.task(fn, plugins=Comm.Plugin())</p> <p>@scheduler.on_start  def on_start():      task.submit()</p> <p>@task.on(\"comm-open\")  def callback(msg: Comm.Msg):      print(\"Comm has just used comm.open()\")</p> <p>scheduler.run()  from amltk._doc import doc_print; doc_print(print, task)  # markdown-exec: hide  ```</p> <p>The task has signalled it's close.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm) -&gt; None:\n    with comm.open():\n        pass\n        # Will send a close signal to the main process as it exists this block\n\n    print(\"Done\")\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit()\n\n@task.on(\"comm-close\")\ndef on_close(msg: Comm.msg):\n    print(f\"Worker close with {msg}\")\n\nscheduler.run()\n</code></pre> <p>Worker close with Comm.Msg(kind=, data=None)  <pre>\n<code>\u256d\u2500 Task fn(comm: 'Comm') -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message                                                                \u2502\n\u2502 @comm-request                                                                \u2502\n\u2502 @comm-open 1                                                                 \u2502\n\u2502 @comm-close 1                                                                \u2502\n\u2502 \u2514\u2500\u2500 def on_close(msg: 'Comm.msg') (1)                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: GXQj0gc6 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> Supported Backends <p>The current implementation relies on <code>Pipe</code> which only works between processes on the same system/cluster. There is also limited support with <code>dask</code> backends.</p> <p>This could be extended to allow for web sockets or other forms of connections but requires time. Please let us know in the Github issues if this is something you are interested in!</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.AsyncComm","title":"<code>class AsyncComm</code>   <code>dataclass</code>","text":"<p>A async wrapper of a Comm.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.AsyncComm.request","title":"<code>def request(*, timeout=None)</code>   <code>async</code>","text":"<p>Recieve a message.</p> PARAMETER  DESCRIPTION <code>timeout</code> <p>The timeout in seconds to wait for a message, raises a <code>Comm.TimeoutError</code> if the timeout is reached. If <code>None</code>, will wait forever.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The message from the worker or the default value.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>async def request(\n    self,\n    *,\n    timeout: float | None = None,\n) -&gt; Any:\n    \"\"\"Recieve a message.\n\n    Args:\n        timeout: The timeout in seconds to wait for a message, raises\n            a [`Comm.TimeoutError`][amltk.scheduling.plugins.comm.Comm.TimeoutError]\n            if the timeout is reached.\n            If `None`, will wait forever.\n\n    Returns:\n        The message from the worker or the default value.\n    \"\"\"\n    connection = AsyncConnection(self.comm.connection)\n    try:\n        return await asyncio.wait_for(connection.recv(), timeout=timeout)\n    except asyncio.TimeoutError as e:\n        raise Comm.TimeoutError(\n            f\"Timed out waiting for response from {self.comm}\",\n        ) from e\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.AsyncComm.send","title":"<code>def send(obj)</code>   <code>async</code>","text":"<p>Send a message.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The message to send.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>async def send(self, obj: Any) -&gt; None:\n    \"\"\"Send a message.\n\n    Args:\n        obj: The message to send.\n    \"\"\"\n    return await AsyncConnection(self.comm.connection).send(obj)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm","title":"<code>class Comm(connection)</code>","text":"<p>A communication channel between a worker and scheduler.</p> <p>For duplex connections, such as returned by python's builtin <code>Pipe</code>, use the <code>create(duplex=...)</code> class method.</p> <p>Adds three new events to the task:</p> <ul> <li><code>@comm-message</code></li> <li><code>@comm-request</code></li> <li><code>@comm-close</code></li> <li><code>@comm-open</code></li> </ul> ATTRIBUTE DESCRIPTION <code>connection</code> <p>The underlying Connection</p> <p> </p> <code>id</code> <p>The id of the comm.</p> <p> TYPE: <code>CommID</code> </p> PARAMETER  DESCRIPTION <code>connection</code> <p>The underlying Connection</p> <p> TYPE: <code>Connection</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def __init__(self, connection: Connection) -&gt; None:\n    \"\"\"Initialize the Comm.\n\n    Args:\n        connection: The underlying Connection\n    \"\"\"\n    super().__init__()\n    self.connection = connection\n    self.id: CommID = id(self)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.MESSAGE","title":"<code>MESSAGE: Event[Comm.Msg]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A Task has sent a message to the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm, x: int) -&gt; int:\n    with comm.open():\n        comm.send(x + 1)\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@task.on(\"comm-message\")\ndef callback(msg: Comm.Msg):\n    print(msg.data)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(comm: 'Comm', x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message                                                                \u2502\n\u2502 \u2514\u2500\u2500 def callback(msg: 'Comm.Msg')                                            \u2502\n\u2502 @comm-request                                                                \u2502\n\u2502 @comm-open                                                                   \u2502\n\u2502 @comm-close                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: SWTJZ1rk \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.REQUEST","title":"<code>REQUEST: Event[Comm.Msg]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A Task has sent a request.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef greeter(comm: Comm, greeting: str) -&gt; None:\n    with comm.open():\n        name = comm.request()\n        comm.send(f\"{greeting} {name}!\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(greeter, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit(\"Hello\")\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    msg.respond(\"Alice\")\n\n@task.on(\"comm-message\")\ndef on_msg(msg: Comm.Msg):\n    print(msg.data)\n\nscheduler.run()\n</code></pre> <p>Hello Alice!  <pre>\n<code>\u256d\u2500 Task greeter(comm: 'Comm', greeting: 'str') -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message 1                                                              \u2502\n\u2502 \u2514\u2500\u2500 def on_msg(msg: 'Comm.Msg') (1)                                          \u2502\n\u2502 @comm-request 1                                                              \u2502\n\u2502 \u2514\u2500\u2500 def on_request(msg: 'Comm.Msg') (1)                                      \u2502\n\u2502 @comm-open 1                                                                 \u2502\n\u2502 @comm-close 1                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: bgoXRYSB \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.OPEN","title":"<code>OPEN: Event[Comm.Msg]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The task has signalled it's open.</p> <p>```python exec=\"true\" source=\"material-block\" html=\"true\" hl_lines=\"5 15-17\"  from amltk.scheduling import Scheduler  from amltk.scheduling.plugins import Comm</p> <p>def fn(comm: Comm) -&gt; None:      with comm.open():          pass from amltk._doc import make_picklable; make_picklable(fn)  # markdown-exec: hide</p> <p>scheduler = Scheduler.with_processes(1)  task = scheduler.task(fn, plugins=Comm.Plugin())</p> <p>@scheduler.on_start  def on_start():      task.submit()</p> <p>@task.on(\"comm-open\")  def callback(msg: Comm.Msg):      print(\"Comm has just used comm.open()\")</p> <p>scheduler.run()  from amltk._doc import doc_print; doc_print(print, task)  # markdown-exec: hide  ```</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.CLOSE","title":"<code>CLOSE: Event[Comm.Msg]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The task has signalled it's close.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm) -&gt; None:\n    with comm.open():\n        pass\n        # Will send a close signal to the main process as it exists this block\n\n    print(\"Done\")\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit()\n\n@task.on(\"comm-close\")\ndef on_close(msg: Comm.msg):\n    print(f\"Worker close with {msg}\")\n\nscheduler.run()\n</code></pre> <p>Worker close with Comm.Msg(kind=, data=None)  <pre>\n<code>\u256d\u2500 Task fn(comm: 'Comm') -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message                                                                \u2502\n\u2502 @comm-request                                                                \u2502\n\u2502 @comm-open 1                                                                 \u2502\n\u2502 @comm-close 1                                                                \u2502\n\u2502 \u2514\u2500\u2500 def on_close(msg: 'Comm.msg') (1)                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 3CBU1iWW \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.as_async","title":"<code>as_async: AsyncComm</code>   <code>prop</code>","text":"<p>Return an async version of this comm.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin","title":"<code>class Plugin(create_comms=None)</code>","text":"<p>         Bases: <code>Plugin</code></p> <p>A plugin that handles communication with a worker.</p> PARAMETER  DESCRIPTION <code>create_comms</code> <p>A function that creates a pair of communication channels. Defaults to <code>Comm.create</code>.</p> <p> TYPE: <code>Callable[[], tuple[Comm, Comm]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def __init__(\n    self,\n    create_comms: Callable[[], tuple[Comm, Comm]] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Args:\n        create_comms: A function that creates a pair of communication\n            channels. Defaults to `Comm.create`.\n    \"\"\"\n    super().__init__()\n    if create_comms is None:\n        create_comms = Comm.create\n\n    self.create_comms = create_comms\n    self.comms: dict[CommID, tuple[Comm, Comm]] = {}\n    self.communication_tasks: list[asyncio.Task] = []\n    self.task: Task\n    self.open_comms: set[CommID] = set()\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin.attach_task","title":"<code>def attach_task(task)</code>","text":"<p>Attach the plugin to a task.</p> <p>This method is called when the plugin is attached to a task. This is the place to subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> PARAMETER  DESCRIPTION <code>task</code> <p>The task the plugin is being attached to.</p> <p> TYPE: <code>Task</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\n\n    This method is called when the plugin is attached to a task. This\n    is the place to subscribe to events on the task, create new subscribers\n    for people to use or even store a reference to the task for later use.\n\n    Args:\n        task: The task the plugin is being attached to.\n    \"\"\"\n    self.task = task\n    task.emitter.add_event(Comm.MESSAGE, Comm.REQUEST, Comm.OPEN, Comm.CLOSE)\n    task.on_submitted(self._begin_listening, hidden=True)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin.pre_submit","title":"<code>def pre_submit(fn, *args, **kwargs)</code>","text":"<p>Pre-submit hook.</p> <p>This method is called before the task is submitted.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The task function.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>*args</code> <p>The arguments to the task function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to the task function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>A tuple of the task function, arguments and keyword arguments</p> <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>if the task should be submitted, or <code>None</code> if the task should</p> <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>not be submitted.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    This method is called before the task is submitted.\n\n    Args:\n        fn: The task function.\n        *args: The arguments to the task function.\n        **kwargs: The keyword arguments to the task function.\n\n    Returns:\n        A tuple of the task function, arguments and keyword arguments\n        if the task should be submitted, or `None` if the task should\n        not be submitted.\n    \"\"\"\n    host_comm, worker_comm = self.create_comms()\n\n    # We don't necessarily know if the future will be submitted. If so,\n    # we will use this index later to retrieve the host_comm\n    self.comms[worker_comm.id] = (host_comm, worker_comm)\n\n    # Make sure to include the Comm\n    return fn, (worker_comm, *args), kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Plugin.copy","title":"<code>def copy()</code>","text":"<p>Return a copy of the plugin.</p> <p>Please see <code>Plugin.copy()</code>.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@override\ndef copy(self) -&gt; Self:\n    \"\"\"Return a copy of the plugin.\n\n    Please see [`Plugin.copy()`][amltk.scheduling.Plugin.copy].\n    \"\"\"\n    return self.__class__(create_comms=self.create_comms)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Msg","title":"<code>class Msg</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Generic[T]</code></p> <p>A message sent over a communication channel.</p> ATTRIBUTE DESCRIPTION <code>task</code> <p>The task that sent the message.</p> <p> TYPE: <code>Task</code> </p> <code>comm</code> <p>The communication channel.</p> <p> TYPE: <code>Comm</code> </p> <code>future</code> <p>The future of the task.</p> <p> TYPE: <code>Future</code> </p> <code>data</code> <p>The data sent by the task.</p> <p> TYPE: <code>T</code> </p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Msg.Kind","title":"<code>class Kind</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>The kind of message.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.Msg.respond","title":"<code>def respond(response)</code>","text":"<p>Respond to the message.</p> PARAMETER  DESCRIPTION <code>response</code> <p>The response to send back to the task.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def respond(self, response: Any) -&gt; None:\n    \"\"\"Respond to the message.\n\n    Args:\n        response: The response to send back to the task.\n    \"\"\"\n    self.comm._send_pipe(response)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.TimeoutError","title":"<code>class TimeoutError</code>","text":"<p>         Bases: <code>TimeoutError</code></p> <p>A timeout error for communications.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.CloseRequestError","title":"<code>class CloseRequestError</code>","text":"<p>         Bases: <code>RuntimeError</code></p> <p>An exception happened in the main process and it send a response to the worker to raise this exception.</p>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.send","title":"<code>def send(obj)</code>","text":"<p>Send a message.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to send.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def send(self, obj: Any) -&gt; None:\n    \"\"\"Send a message.\n\n    Args:\n        obj: The object to send.\n    \"\"\"\n    self._send_pipe((Comm.Msg.Kind.MESSAGE, obj))\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.close","title":"<code>def close(msg=None, *, wait_for_ack=False, okay_if_broken_pipe=False, side='')</code>","text":"<p>Close the connection.</p> PARAMETER  DESCRIPTION <code>msg</code> <p>The message to send to the other end of the connection.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>wait_for_ack</code> <p>If <code>True</code>, wait for an acknowledgement from the other end before closing the connection.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>okay_if_broken_pipe</code> <p>If <code>True</code>, will not log an error if the connection is already closed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>side</code> <p>The side of the connection for naming purposes.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def close(  # noqa: PLR0912, C901\n    self,\n    msg: Any | None = None,\n    *,\n    wait_for_ack: bool = False,\n    okay_if_broken_pipe: bool = False,\n    side: str = \"\",\n) -&gt; None:\n    \"\"\"Close the connection.\n\n    Args:\n        msg: The message to send to the other end of the connection.\n        wait_for_ack: If `True`, wait for an acknowledgement from the\n            other end before closing the connection.\n        okay_if_broken_pipe: If `True`, will not log an error if the\n            connection is already closed.\n        side: The side of the connection for naming purposes.\n    \"\"\"\n    if not self.connection.closed:\n        kind = Comm.Msg.Kind.CLOSE_WITH_ACK if wait_for_ack else Comm.Msg.Kind.CLOSE\n        try:\n            self._send_pipe((kind, msg))\n        except BrokenPipeError as e:\n            if not okay_if_broken_pipe:\n                logger.error(f\"{side} - Error sending close signal: {type(e)}{e}\")\n        except Exception as e:  # noqa: BLE001\n            logger.error(f\"{side} - Error sending close signal: {type(e)}{e}\")\n        else:\n            if wait_for_ack:\n                logger.debug(f\"{side} - Waiting for ACK\")\n                try:\n                    recieved_msg = self.connection.recv()\n                except Exception as e:  # noqa: BLE001\n                    logger.error(\n                        f\"{side} - Error waiting for ACK, closing: {type(e)}{e}\",\n                    )\n                else:\n                    match recieved_msg:\n                        case Comm.Msg.Kind.WORKER_CLOSE_REQUEST:\n                            logger.error(\n                                f\"{side} - Worker recieved request to close!\",\n                            )\n                        case Comm.Msg.Kind.ACK:\n                            logger.debug(f\"{side} - Recieved ACK, closing\")\n                        case _:\n                            logger.warning(\n                                f\"{side} - Expected ACK but {recieved_msg=}\",\n                            )\n        finally:\n            try:\n                self.connection.close()\n            except OSError:\n                # It's possble that the connection was closed by the other end\n                # before we could close it.\n                pass\n            except Exception as e:  # noqa: BLE001\n                logger.error(f\"{side} - Error closing connection: {type(e)}{e}\")\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.create","title":"<code>def create(*, duplex=True)</code>   <code>classmethod</code>","text":"<p>Create a pair of communication channels.</p> <p>Wraps the output of <code>multiprocessing.Pipe(duplex=duplex)</code>.</p> PARAMETER  DESCRIPTION <code>duplex</code> <p>Whether to allow for two-way communication</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>tuple[Self, Self]</code> <p>A pair of communication channels.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@classmethod\ndef create(cls, *, duplex: bool = True) -&gt; tuple[Self, Self]:\n    \"\"\"Create a pair of communication channels.\n\n    Wraps the output of\n    [`multiprocessing.Pipe(duplex=duplex)`][multiprocessing.Pipe].\n\n    Args:\n        duplex: Whether to allow for two-way communication\n\n    Returns:\n        A pair of communication channels.\n    \"\"\"\n    reader, writer = Pipe(duplex=duplex)\n    return cls(reader), cls(writer)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.request","title":"<code>def request(msg=None, *, timeout=None)</code>","text":"<p>Receive a message.</p> PARAMETER  DESCRIPTION <code>msg</code> <p>The message to send to the other end of the connection. If left empty, will be <code>None</code>.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>timeout</code> <p>If float, will wait for that many seconds, raising an exception if exceeded. Otherwise, None will wait forever.</p> <p> TYPE: <code>None | float</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TimeoutError</code> <p>If the timeout is reached.</p> <code>CloseRequestError</code> <p>If the other end needs to abruptly end and can not fufill the request. If thise error is thrown, the worker should finish as soon as possible.</p> RETURNS DESCRIPTION <code>Any</code> <p>The received message or the default.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>def request(\n    self,\n    msg: Any | None = None,\n    *,\n    timeout: None | float = None,\n) -&gt; Any:\n    \"\"\"Receive a message.\n\n    Args:\n        msg: The message to send to the other end of the connection.\n            If left empty, will be `None`.\n        timeout: If float, will wait for that many seconds, raising an exception\n            if exceeded. Otherwise, None will wait forever.\n\n    Raises:\n        Comm.TimeoutError: If the timeout is reached.\n        Comm.CloseRequestError: If the other end needs to abruptly end and\n            can not fufill the request. If thise error is thrown, the worker\n            should finish as soon as possible.\n\n    Returns:\n        The received message or the default.\n    \"\"\"\n    self._send_pipe((Comm.Msg.Kind.REQUEST, msg))\n    if not self.connection.poll(timeout):\n        raise Comm.TimeoutError(f\"Timed out waiting for response for {msg}\")\n\n    response = self.connection.recv()\n    if response == Comm.Msg.Kind.WORKER_CLOSE_REQUEST:\n        logger.error(\"Worker recieved request to close!\")\n        raise Comm.CloseRequestError()\n\n    return response\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/comm/#amltk.scheduling.plugins.comm.Comm.open","title":"<code>def open(opening_msg=None, *, wait_for_ack=False, side='worker')</code>","text":"<p>Open the connection.</p> PARAMETER  DESCRIPTION <code>opening_msg</code> <p>The message to send to the main process when the connection is opened.</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>wait_for_ack</code> <p>If <code>True</code>, wait for an acknowledgement from the other end before closing the connection and exiting the context manager.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>side</code> <p>The side of the connection for naming purposes. Usually this is only done on the <code>\"worker\"</code> side.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'worker'</code> </p> YIELDS DESCRIPTION <code>Self</code> <p>The comm.</p> Source code in <code>src/amltk/scheduling/plugins/comm.py</code> <pre><code>@contextmanager\ndef open(\n    self,\n    opening_msg: Any | None = None,\n    *,\n    wait_for_ack: bool = False,\n    side: str = \"worker\",\n) -&gt; Iterator[Self]:\n    \"\"\"Open the connection.\n\n    Args:\n        opening_msg: The message to send to the main process\n            when the connection is opened.\n        wait_for_ack: If `True`, wait for an acknowledgement from the\n            other end before closing the connection and exiting the\n            context manager.\n        side: The side of the connection for naming purposes.\n            Usually this is only done on the `\"worker\"` side.\n\n    Yields:\n        The comm.\n    \"\"\"\n    self._send_pipe((Comm.Msg.Kind.OPEN, opening_msg))\n    yield self\n    self.close(wait_for_ack=wait_for_ack, side=side)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/","title":"Limiter","text":"<p>The <code>Limiter</code> can limit the number of times a function is called, how many concurrent instances of it can be running, or whether it can run while another task is running.</p> <p>The functionality of the <code>Limiter</code> could also be implemented without a plugin but it gives some nice utility.</p> Usage <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: s8VPja4H \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <code>@events</code> <code>@call-limit-reached</code><code>@concurrent-limit-reached</code><code>@disabled-due-to-running-task</code> <p>The event emitted when the task has reached its call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: wLrL5UIE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The event emitted when the task has reached its concurrent call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_concurrent=2)])\n\n@task.on(\"concurrent-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Concurrent 0/2                                                           \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 48C4gBD6 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The event emitter when the task was not submitted due to some other running task.</p> <p>Will call any subscribers with the task as first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\nother_task = scheduler.task(fn)\ntask = scheduler.task(fn, plugins=[Limiter(not_while_running=other_task)])\n\n@task.on(\"disabled-due-to-running-task\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Not While def fn(...) Ref: HAzJKSuy                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: vXz8pCha \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter","title":"<code>class Limiter(*, max_calls=None, max_concurrent=None, not_while_running=None)</code>","text":"<p>         Bases: <code>Plugin</code></p> <p>A plugin that limits the submission of a task.</p> <p>Adds three new events to the task:</p> <ul> <li><code>@call-limit-reached</code></li> <li><code>@concurrent-limit-reached</code></li> <li><code>@disabled-due-to-running-task</code></li> </ul> PARAMETER  DESCRIPTION <code>max_calls</code> <p>The maximum number of calls to the task.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>max_concurrent</code> <p>The maximum number of calls of this task that can be in the queue.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>not_while_running</code> <p>A task or iterable of tasks that if active, will prevent this task from being submitted.</p> <p> TYPE: <code>Task | Iterable[Task] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/limiter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    max_calls: int | None = None,\n    max_concurrent: int | None = None,\n    not_while_running: Task | Iterable[Task] | None = None,\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        max_calls: The maximum number of calls to the task.\n        max_concurrent: The maximum number of calls of this task that can\n            be in the queue.\n        not_while_running: A task or iterable of tasks that if active, will prevent\n            this task from being submitted.\n    \"\"\"\n    super().__init__()\n\n    if not_while_running is None:\n        not_while_running = []\n    elif isinstance(not_while_running, Iterable):\n        not_while_running = list(not_while_running)\n    else:\n        not_while_running = [not_while_running]\n\n    self.max_calls = max_calls\n    self.max_concurrent = max_concurrent\n    self.not_while_running = not_while_running\n    self.task: Task | None = None\n\n    if isinstance(max_calls, int) and not max_calls &gt; 0:\n        raise ValueError(\"max_calls must be greater than 0\")\n\n    if isinstance(max_concurrent, int) and not max_concurrent &gt; 0:\n        raise ValueError(\"max_concurrent must be greater than 0\")\n\n    self._calls = 0\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.CALL_LIMIT_REACHED","title":"<code>CALL_LIMIT_REACHED: Event[...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The event emitted when the task has reached its call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: AntWXSlu \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.CONCURRENT_LIMIT_REACHED","title":"<code>CONCURRENT_LIMIT_REACHED: Event[...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The event emitted when the task has reached its concurrent call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_concurrent=2)])\n\n@task.on(\"concurrent-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Concurrent 0/2                                                           \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: pks6WAkU \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.DISABLED_DUE_TO_RUNNING_TASK","title":"<code>DISABLED_DUE_TO_RUNNING_TASK: Event[...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The event emitter when the task was not submitted due to some other running task.</p> <p>Will call any subscribers with the task as first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\nother_task = scheduler.task(fn)\ntask = scheduler.task(fn, plugins=[Limiter(not_while_running=other_task)])\n\n@task.on(\"disabled-due-to-running-task\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Not While def fn(...) Ref: P1bDypwN                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: D4MUAgdy \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.n_running","title":"<code>n_running: int</code>   <code>prop</code>","text":"<p>Return the number of running tasks.</p>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.attach_task","title":"<code>def attach_task(task)</code>","text":"<p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/limiter.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n\n    if self.task in self.not_while_running:\n        raise ValueError(\n            f\"Task {self.task} was found in the {self.not_while_running=}\"\n            \" list. This is disabled but please raise an issue if you think this\"\n            \" has sufficient use case.\",\n        )\n\n    task.emitter.add_event(\n        self.CALL_LIMIT_REACHED,\n        self.CONCURRENT_LIMIT_REACHED,\n        self.DISABLED_DUE_TO_RUNNING_TASK,\n    )\n\n    # Make sure to increment the count when a task was submitted\n    task.on_submitted(self._increment_call_count, hidden=True)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.pre_submit","title":"<code>def pre_submit(fn, *args, **kwargs)</code>","text":"<p>Pre-submit hook.</p> <p>Prevents submission of the task if it exceeds any of the set limits.</p> Source code in <code>src/amltk/scheduling/plugins/limiter.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    Prevents submission of the task if it exceeds any of the set limits.\n    \"\"\"\n    assert self.task is not None\n\n    if self.max_calls is not None and self._calls &gt;= self.max_calls:\n        self.task.emitter.emit(self.CALL_LIMIT_REACHED, self.task, *args, **kwargs)\n        return None\n\n    if self.max_concurrent is not None and self.n_running &gt;= self.max_concurrent:\n        self.task.emitter.emit(\n            self.CONCURRENT_LIMIT_REACHED,\n            self.task,\n            *args,\n            **kwargs,\n        )\n        return None\n\n    for other_task in self.not_while_running:\n        if other_task.running():\n            self.task.emitter.emit(\n                self.DISABLED_DUE_TO_RUNNING_TASK,\n                other_task,\n                self.task,\n                *args,\n                **kwargs,\n            )\n            return None\n\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/limiter/#amltk.scheduling.plugins.limiter.Limiter.copy","title":"<code>def copy()</code>","text":"<p>Return a copy of the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/limiter.py</code> <pre><code>@override\ndef copy(self) -&gt; Self:\n    \"\"\"Return a copy of the plugin.\"\"\"\n    return self.__class__(\n        max_calls=self.max_calls,\n        max_concurrent=self.max_concurrent,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/plugin/","title":"Plugin","text":"<p>A plugin that can be attached to a Task.</p> <p>By inheriting from a <code>Plugin</code>, you can hook into a <code>Task</code>. A plugin can affect, modify and extend its behaviours. Please see the documentation of the methods for more information. Creating a plugin is only necesary if you need to modify actual behaviour of the task. For siply hooking into the lifecycle of a task, you can use the <code>@events</code> that a <code>Task</code> emits.</p> Creating a Plugin <p>For a full example of a simple plugin, see the <code>Limiter</code> plugin which prevents the task being submitted if for example, it has already been submitted too many times.</p> <p>The below example shows how to create a plugin that prints the task name before submitting it. It also emits an event when the task is submitted.</p> <p><pre><code>from __future__ import annotations\nfrom typing import Callable\n\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Plugin\nfrom amltk.scheduling.events import Event\n\n# A simple plugin that prints the task name before submitting\nclass Printer(Plugin):\n    name = \"my-plugin\"\n\n    # Define an event the plugin will emit\n    # Event[Task] indicates the callback for the event will be called with the task\n    PRINTED: Event[str] = Event(\"printer-msg\")\n\n    def __init__(self, greeting: str):\n        self.greeting = greeting\n        self.n_greetings = 0\n\n    def attach_task(self, task) -&gt; None:\n        self.task = task\n        # Register an event with the task, this lets the task know valid events\n        # people can subscribe to and helps it show up in visuals\n        task.emitter.add_event(self.PRINTED)\n        task.on_submitted(self._print_submitted, hidden=True)  # You can hide this callback from visuals\n\n    def pre_submit(self, fn, *args, **kwargs) -&gt; tuple[Callable, tuple, dict]:\n        print(f\"{self.greeting} for {self.task} {args} {kwargs}\")\n        self.n_greetings += 1\n        return fn, args, kwargs\n\n    def _print_submitted(self, future, *args, **kwargs) -&gt; None:\n        msg = f\"Task was submitted {self.task} {args} {kwargs}\"\n        self.task.emitter.emit(self.PRINTED, msg)  # Emit the event with a msg\n\n    def copy(self) -&gt; Printer:\n        # Plugins need to be able to copy themselves as if fresh\n        return self.__class__(self.greeting)\n\n    def __rich__(self):\n        # Custome how the plugin is displayed in rich (Optional)\n        # rich is an optional dependancy of amltk so we move the imports into here\n        from rich.panel import Panel\n\n        return Panel(\n            f\"Greeting: {self.greeting} ({self.n_greetings})\",\n            title=f\"Plugin {self.name}\"\n        )\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=[Printer(\"Hello\")])\n\n@scheduler.on_start\ndef on_start():\n    task.submit(15)\n\n@task.on(\"printer-msg\")\ndef callback(msg: str):\n    print(\"\\nmsg\")\n\nscheduler.run()\n</code></pre> <p>Hello for Task(unique_ref=JErgxEnv) (15,) {}  msg  <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin my-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Greeting: Hello (1)                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @printer-msg 1                                                               \u2502\n\u2502 \u2514\u2500\u2500 def callback(msg: 'str') (1)                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: JErgxEnv \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <p>All methods are optional, and you can choose to implement only the ones you need. Most plugins will likely need to implement the <code>attach_task()</code> method, which is called when the plugin is attached to a task. In this method, you can for example subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> <p>Plugins are also encouraged to utilize the events of a <code>Task</code> to further hook into the lifecycle of the task. For exampe, by saving a reference to the task in the <code>attach_task()</code> method, you can use the <code>emit()</code> method of the task to emit your own specialized events.</p>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin","title":"<code>class Plugin</code>","text":"<p>         Bases: <code>RichRenderable</code>, <code>ABC</code></p> <p>A plugin that can be attached to a Task.</p>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.name","title":"<code>name: str</code>   <code>classvar</code>","text":"<p>The name of the plugin.</p> <p>This is used to identify the plugin during logging.</p>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.attach_task","title":"<code>def attach_task(task)</code>","text":"<p>Attach the plugin to a task.</p> <p>This method is called when the plugin is attached to a task. This is the place to subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> PARAMETER  DESCRIPTION <code>task</code> <p>The task the plugin is being attached to.</p> <p> TYPE: <code>Task</code> </p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\n\n    This method is called when the plugin is attached to a task. This\n    is the place to subscribe to events on the task, create new subscribers\n    for people to use or even store a reference to the task for later use.\n\n    Args:\n        task: The task the plugin is being attached to.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.pre_submit","title":"<code>def pre_submit(fn, *args, **kwargs)</code>","text":"<p>Pre-submit hook.</p> <p>This method is called before the task is submitted.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The task function.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>*args</code> <p>The arguments to the task function.</p> <p> TYPE: <code>args</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>The keyword arguments to the task function.</p> <p> TYPE: <code>kwargs</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>A tuple of the task function, arguments and keyword arguments</p> <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>if the task should be submitted, or <code>None</code> if the task should</p> <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>not be submitted.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    This method is called before the task is submitted.\n\n    Args:\n        fn: The task function.\n        *args: The arguments to the task function.\n        **kwargs: The keyword arguments to the task function.\n\n    Returns:\n        A tuple of the task function, arguments and keyword arguments\n        if the task should be submitted, or `None` if the task should\n        not be submitted.\n    \"\"\"\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.events","title":"<code>def events()</code>","text":"<p>Return a list of events that this plugin emits.</p> <p>Likely no need to override this method, as it will automatically return all events defined on the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>def events(self) -&gt; list[Event]:\n    \"\"\"Return a list of events that this plugin emits.\n\n    Likely no need to override this method, as it will automatically\n    return all events defined on the plugin.\n    \"\"\"\n    inherited_attrs = chain.from_iterable(\n        vars(cls).values() for cls in self.__class__.__mro__\n    )\n    return [attr for attr in inherited_attrs if isinstance(attr, Event)]\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/plugin/#amltk.scheduling.plugins.plugin.Plugin.copy","title":"<code>def copy()</code>   <code>abstractmethod</code>","text":"<p>Return a copy of the plugin.</p> <p>This method is used to create a copy of the plugin when a task is copied. This is useful if the plugin stores a reference to the task it is attached to, as the copy will need to store a reference to the copy of the task.</p> Source code in <code>src/amltk/scheduling/plugins/plugin.py</code> <pre><code>@abstractmethod\ndef copy(self) -&gt; Self:\n    \"\"\"Return a copy of the plugin.\n\n    This method is used to create a copy of the plugin when a task is\n    copied. This is useful if the plugin stores a reference to the task\n    it is attached to, as the copy will need to store a reference to the\n    copy of the task.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/","title":"Pynisher","text":"<p>The <code>PynisherPlugin</code> uses pynisher to place memory, walltime and cputime constraints on processes, crashing them if these limits are reached. These default units are <code>bytes (\"B\")</code> and <code>seconds (\"s\")</code> but you can also use other units, please see the relevant API doc.</p> <p>It's best use is when used with <code>Scheduler.with_processes()</code> to have work performed in processes.</p> <p>Requirements</p> <p>This required <code>pynisher</code> which can be installed with:</p> <pre><code>pip install amltk[pynisher]\n\n# Or directly\npip install pynisher\n</code></pre> Usage <p><pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: hwGTKLDc \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <code>@events</code> <code>@pynisher-timeout</code><code>@pynisher-memory-limit</code><code>@pynisher-cputime-limit</code><code>@pynisher-walltime-limit</code> <p>A Task timed out, either due to the wall time or cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: wCfAorKe \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task was submitted but reached it's memory limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport numpy as np\n\ndef f(x: int) -&gt; int:\n    x = np.arange(100000000)\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(memory_limit=(1, \"KB\")))\n\n@task.on(\"pynisher-memory-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory     Wall Time  CPU Time                                          \u2502 \u2502\n\u2502 \u2502  (1, 'KB')  None       None                                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: uXywcedo \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task was submitted but reached it's cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    i = 0\n    while True:\n        # Keep busying computing the answer to everything\n        i += 1\n\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(cputime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-cputime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    None       (1, 's')                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: X8291hUL \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task was submitted but reached it's wall time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-walltime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: F9OnG9Ai \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> Scheduler Executor <p>This will place process limits on the task as soon as it starts running, whever it may be running. If you are using <code>Scheduler.with_sequential()</code> then this will place limits on the main process, likely not what you want. This also does not work with a <code>ThreadPoolExecutor</code>.</p> <p>If using this with something like [<code>dask-jobqueue</code>], then this will place limits on the workers it spawns. It would be better to place limits directly through dask job-queue then.</p> Platform Limitations (Mac, Windows) <p>Pynisher has some limitations with memory on Mac and Windows: automl/pynisher#features</p> <p>You can check this with <code>PynisherPlugin.supports(\"memory\")</code>, <code>PynisherPlugin.supports(\"cpu_time\")</code> and <code>PynisherPlugin.supports(\"wall_time\")</code>. See <code>PynisherPlugin.supports()</code></p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin","title":"<code>class PynisherPlugin(*, memory_limit=None, cputime_limit=None, walltime_limit=None, context=None)</code>","text":"<p>         Bases: <code>Plugin</code></p> <p>A plugin that wraps a task in a pynisher to enforce limits on it.</p> <p>This plugin wraps a task function in a <code>Pynisher</code> instance to enforce limits on the task. The limits are set by any of <code>memory_limit=</code>, <code>cpu_time_limit=</code> and <code>wall_time_limit=</code>.</p> <p>Adds four new events to the task</p> <ul> <li><code>@pynisher-timeout</code></li> <li><code>@pynisher-memory-limit</code></li> <li><code>@pynisher-cputime-limit</code></li> <li><code>@pynisher-walltime-limit</code></li> </ul> ATTRIBUTE DESCRIPTION <code>memory_limit</code> <p>The memory limit of the task.</p> <p> </p> <code>cpu_time_limit</code> <p>The cpu time limit of the task.</p> <p> </p> <code>wall_time_limit</code> <p>The wall time limit of the task.</p> <p> </p> PARAMETER  DESCRIPTION <code>memory_limit</code> <p>The memory limit to wrap the task in. Base unit is in bytes but you can specify <code>(value, unit)</code> where <code>unit</code> is one of <code>(\"B\", \"KB\", \"MB\", \"GB\")</code>. Defaults to <code>None</code></p> <p> TYPE: <code>int | tuple[int, str] | None</code> DEFAULT: <code>None</code> </p> <code>cputime_limit</code> <p>The cpu time limit to wrap the task in. Base unit is in seconds but you can specify <code>(value, unit)</code> where <code>unit</code> is one of <code>(\"s\", \"m\", \"h\")</code>. Defaults to <code>None</code></p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>walltime_limit</code> <p>The wall time limit for the task. Base unit is in seconds but you can specify <code>(value, unit)</code> where <code>unit</code> is one of <code>(\"s\", \"m\", \"h\")</code>. Defaults to <code>None</code>.</p> <p> TYPE: <code>int | tuple[float, str] | None</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>The context to use for multiprocessing. Defaults to <code>None</code>. See <code>multiprocessing.get_context()</code></p> <p> TYPE: <code>BaseContext | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>def __init__(\n    self,\n    *,\n    memory_limit: int | tuple[int, str] | None = None,\n    cputime_limit: int | tuple[float, str] | None = None,\n    walltime_limit: int | tuple[float, str] | None = None,\n    context: BaseContext | None = None,\n):\n    \"\"\"Initialize a `PynisherPlugin` instance.\n\n    Args:\n        memory_limit: The memory limit to wrap the task in. Base unit is in bytes\n            but you can specify `(value, unit)` where `unit` is one of\n            `(\"B\", \"KB\", \"MB\", \"GB\")`. Defaults to `None`\n        cputime_limit: The cpu time limit to wrap the task in. Base unit is in\n            seconds but you can specify `(value, unit)` where `unit` is one of\n            `(\"s\", \"m\", \"h\")`. Defaults to `None`\n        walltime_limit: The wall time limit for the task. Base unit is in seconds\n            but you can specify `(value, unit)` where `unit` is one of\n            `(\"s\", \"m\", \"h\")`. Defaults to `None`.\n        context: The context to use for multiprocessing. Defaults to `None`.\n            See [`multiprocessing.get_context()`][multiprocessing.get_context]\n    \"\"\"\n    super().__init__()\n    self.memory_limit = memory_limit\n    self.cputime_limit = cputime_limit\n    self.walltime_limit = walltime_limit\n    self.context = context\n\n    self.task: Task\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.TIMEOUT","title":"<code>TIMEOUT: Event[PynisherPlugin.TimeoutException]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A Task timed out, either due to the wall time or cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: iCHyJw2I \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.MEMORY_LIMIT_REACHED","title":"<code>MEMORY_LIMIT_REACHED: Event[pynisher.exceptions.MemoryLimitException]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A Task was submitted but reached it's memory limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport numpy as np\n\ndef f(x: int) -&gt; int:\n    x = np.arange(100000000)\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(memory_limit=(1, \"KB\")))\n\n@task.on(\"pynisher-memory-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory     Wall Time  CPU Time                                          \u2502 \u2502\n\u2502 \u2502  (1, 'KB')  None       None                                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: EVcQPrdK \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.CPU_TIME_LIMIT_REACHED","title":"<code>CPU_TIME_LIMIT_REACHED: Event[pynisher.exceptions.CpuTimeoutException]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A Task was submitted but reached it's cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    i = 0\n    while True:\n        # Keep busying computing the answer to everything\n        i += 1\n\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(cputime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-cputime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    None       (1, 's')                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 6chjzeZA \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.WALL_TIME_LIMIT_REACHED","title":"<code>WALL_TIME_LIMIT_REACHED: Event[pynisher.exceptions.WallTimeoutException]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A Task was submitted but reached it's wall time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-walltime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 1mu8Zhoc \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.TimeoutException","title":"<code>TimeoutException: TypeAlias</code>   <code>classvar</code> <code>attr</code>","text":"<p>The exception that is raised when a task times out.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.MemoryLimitException","title":"<code>MemoryLimitException: TypeAlias</code>   <code>classvar</code> <code>attr</code>","text":"<p>The exception that is raised when a task reaches it's memory limit.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.CpuTimeoutException","title":"<code>CpuTimeoutException: TypeAlias</code>   <code>classvar</code> <code>attr</code>","text":"<p>The exception that is raised when a task reaches it's cpu time limit.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.WallTimeoutException","title":"<code>WallTimeoutException: TypeAlias</code>   <code>classvar</code> <code>attr</code>","text":"<p>The exception that is raised when a task reaches it's wall time limit.</p>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.pre_submit","title":"<code>def pre_submit(fn, *args, **kwargs)</code>","text":"<p>Wrap a task function in a <code>Pynisher</code> instance.</p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict]:\n    \"\"\"Wrap a task function in a `Pynisher` instance.\"\"\"\n    # If any of our limits is set, we need to wrap it in Pynisher\n    # to enfore these limits.\n    if any(\n        limit is not None\n        for limit in (self.memory_limit, self.cputime_limit, self.walltime_limit)\n    ):\n        fn = pynisher.Pynisher(\n            fn,\n            memory=self.memory_limit,\n            cpu_time=self.cputime_limit,\n            wall_time=self.walltime_limit,\n            terminate_child_processes=True,\n            context=self.context,\n        )\n\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.attach_task","title":"<code>def attach_task(task)</code>","text":"<p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n    task.emitter.add_event(\n        self.TIMEOUT,\n        self.MEMORY_LIMIT_REACHED,\n        self.CPU_TIME_LIMIT_REACHED,\n        self.WALL_TIME_LIMIT_REACHED,\n    )\n\n    # Check the exception and emit pynisher specific ones too\n    task.on_exception(self._check_to_emit_pynisher_exception, hidden=True)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.copy","title":"<code>def copy()</code>","text":"<p>Return a copy of the plugin.</p> <p>Please see <code>Plugin.copy()</code>.</p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>@override\ndef copy(self) -&gt; Self:\n    \"\"\"Return a copy of the plugin.\n\n    Please see [`Plugin.copy()`][amltk.Plugin.copy].\n    \"\"\"\n    return self.__class__(\n        memory_limit=self.memory_limit,\n        cputime_limit=self.cputime_limit,\n        walltime_limit=self.walltime_limit,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/pynisher/#amltk.scheduling.plugins.pynisher.PynisherPlugin.supports","title":"<code>def supports(kind)</code>   <code>classmethod</code>","text":"<p>Check if the task is supported by the plugin.</p> PARAMETER  DESCRIPTION <code>kind</code> <p>The kind of limit to check.</p> <p> TYPE: <code>Literal['wall_time', 'cpu_time', 'memory']</code> </p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the limit is supported by the plugin for your os, else <code>False</code>.</p> Source code in <code>src/amltk/scheduling/plugins/pynisher.py</code> <pre><code>@classmethod\ndef supports(cls, kind: Literal[\"wall_time\", \"cpu_time\", \"memory\"]) -&gt; bool:\n    \"\"\"Check if the task is supported by the plugin.\n\n    Args:\n        kind: The kind of limit to check.\n\n    Returns:\n        `True` if the limit is supported by the plugin for your os, else `False`.\n    \"\"\"\n    return pynisher.supports(kind)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/","title":"Threadpoolctl","text":"<p>The <code>ThreadPoolCTLPlugin</code> if useful for parallel training of models. Without limiting with threadpoolctl, the number of threads used by a given model may oversubscribe to resources and cause significant slowdowns.</p> <p>This is the mechanism employed by scikit-learn to limit the number of threads used by a given model.</p> <p>See threadpoolctl documentation.</p> <p>Requirements</p> <p>This requires <code>threadpoolctl</code> which can be installed with:</p> <pre><code>pip install amltk[threadpoolctl]\n\n# Or directly\npip install threadpoolctl\n</code></pre> Usage <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\nscheduler = Scheduler.with_processes(1)\n\ndef f() -&gt; None:\n    # ... some task that respects the limits set by threadpoolctl\n    pass\n\ntask = scheduler.task(f, plugins=ThreadPoolCTLPlugin(max_threads=1))\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f() -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin threadpoolctl-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Max Threads  User-API                                                   \u2502 \u2502\n\u2502 \u2502  1            None                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 0dIvM1ji \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin","title":"<code>class ThreadPoolCTLPlugin(max_threads=None, user_api=None)</code>","text":"<p>         Bases: <code>Plugin</code></p> <p>A plugin that limits the usage of threads in a task.</p> <p>This plugin is used to make utilize threadpoolctl with tasks, useful for parallel training of models. Without limiting with threadpoolctl, the number of threads used by a given model may oversubscribe to resources and cause significant slowdowns.</p> ATTRIBUTE DESCRIPTION <code>max_calls</code> <p>The maximum number of calls to the task.</p> <p> </p> <code>max_concurrent</code> <p>The maximum number of calls of this task that can be in the queue.</p> <p> </p> PARAMETER  DESCRIPTION <code>max_threads</code> <p>The maximum number of threads to use.</p> <p> TYPE: <code>int | dict[str, int] | None</code> DEFAULT: <code>None</code> </p> <code>user_api</code> <p>The user API to limit.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/threadpoolctl.py</code> <pre><code>@override\ndef __init__(\n    self,\n    max_threads: int | dict[str, int] | None = None,\n    user_api: str | None = None,\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        max_threads: The maximum number of threads to use.\n        user_api: The user API to limit.\n    \"\"\"\n    super().__init__()\n    self.max_threads = max_threads\n    self.user_api = user_api\n    self.task: Task | None = None\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.attach_task","title":"<code>def attach_task(task)</code>","text":"<p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/threadpoolctl.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.pre_submit","title":"<code>def pre_submit(fn, *args, **kwargs)</code>","text":"<p>Pre-submit hook.</p> <p>Wrap the function in something that will activate threadpoolctl when the function is called.</p> Source code in <code>src/amltk/scheduling/plugins/threadpoolctl.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Pre-submit hook.\n\n    Wrap the function in something that will activate threadpoolctl\n    when the function is called.\n    \"\"\"\n    assert self.task is not None\n\n    fn = _ThreadPoolLimiter(\n        fn=fn,\n        max_threads=self.max_threads,\n        user_api=self.user_api,\n    )\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/threadpoolctl/#amltk.scheduling.plugins.threadpoolctl.ThreadPoolCTLPlugin.copy","title":"<code>def copy()</code>","text":"<p>Return a copy of the plugin.</p> <p>Please see <code>Plugin.copy()</code>.</p> Source code in <code>src/amltk/scheduling/plugins/threadpoolctl.py</code> <pre><code>@override\ndef copy(self) -&gt; Self:\n    \"\"\"Return a copy of the plugin.\n\n    Please see [`Plugin.copy()`][amltk.Plugin.copy].\n    \"\"\"\n    return self.__class__(max_threads=self.max_threads, user_api=self.user_api)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/","title":"Wandb","text":"<p>Wandb plugin.</p> <p>Todo</p> <p>This plugin is experimental and out of date.</p>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbParams","title":"<code>class WandbParams</code>   <code>dataclass</code>","text":"<p>Parameters for initializing a wandb run.</p> <p>This class is a dataclass that contains all the parameters that are used to initialize a wandb run. It is used by the <code>WandbPlugin</code> to initialize a run. It can be modified using the <code>modify()</code> method.</p> <p>Please refer to the documentation of the <code>wandb.init()</code> method for more information on the parameters.</p>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbParams.modify","title":"<code>def modify(**kwargs)</code>","text":"<p>Modify the parameters of this instance.</p> <p>This method returns a new instance of this class with the parameters modified. This is useful for example when you want to modify the parameters of a run to add tags or notes.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def modify(self, **kwargs: Any) -&gt; WandbParams:\n    \"\"\"Modify the parameters of this instance.\n\n    This method returns a new instance of this class with the parameters\n    modified. This is useful for example when you want to modify the\n    parameters of a run to add tags or notes.\n    \"\"\"\n    return replace(self, **kwargs)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbParams.run","title":"<code>def run(name, config=None)</code>","text":"<p>Initialize a wandb run.</p> <p>This method initializes a wandb run using the parameters of this instance. It returns the wandb run object.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the run.</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The configuration of the run.</p> <p> TYPE: <code>Mapping[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>WRun</code> <p>The wandb run object.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def run(\n    self,\n    name: str,\n    config: Mapping[str, Any] | None = None,\n) -&gt; WRun:\n    \"\"\"Initialize a wandb run.\n\n    This method initializes a wandb run using the parameters of this\n    instance. It returns the wandb run object.\n\n    Args:\n        name: The name of the run.\n        config: The configuration of the run.\n\n    Returns:\n        The wandb run object.\n    \"\"\"\n    run = wandb.init(\n        config=dict(config) if config else None,\n        name=name,\n        project=self.project,\n        group=self.group,\n        tags=self.tags,\n        entity=self.entity,\n        notes=self.notes,\n        reinit=self.reinit,\n        dir=self.dir,\n        config_exclude_keys=self.config_exclude_keys,\n        config_include_keys=self.config_include_keys,\n        mode=self.mode,\n        allow_val_change=self.allow_val_change,\n        force=self.force,\n    )\n    if run is None:\n        raise RuntimeError(\"Wandb run was not initialized\")\n\n    return run\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbLiveRunWrap","title":"<code>class WandbLiveRunWrap(params, fn, *, modify=None)</code>","text":"<p>         Bases: <code>Generic[P]</code></p> <p>Wrap a function to log the results to a wandb run.</p> <p>This class is used to wrap a function that returns a report to log the results to a wandb run. It is used by the <code>WandbTrialTracker</code> to wrap the target function.</p> PARAMETER  DESCRIPTION <code>params</code> <p>The parameters to initialize the wandb run.</p> <p> TYPE: <code>WandbParams</code> </p> <code>fn</code> <p>The function to wrap.</p> <p> TYPE: <code>Callable[Concatenate[Trial, P], Report]</code> </p> <code>modify</code> <p>A function that modifies the parameters of the wandb run before each trial.</p> <p> TYPE: <code>Callable[[Trial, WandbParams], WandbParams] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __init__(\n    self,\n    params: WandbParams,\n    fn: Callable[Concatenate[Trial, P], Trial.Report],\n    *,\n    modify: Callable[[Trial, WandbParams], WandbParams] | None = None,\n):\n    \"\"\"Initialize the wrapper.\n\n    Args:\n        params: The parameters to initialize the wandb run.\n        fn: The function to wrap.\n        modify: A function that modifies the parameters of the wandb run\n            before each trial.\n    \"\"\"\n    super().__init__()\n    self.params = params\n    self.fn = fn\n    self.modify = modify\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbLiveRunWrap.__call__","title":"<code>def __call__(trial, *args, **kwargs)</code>","text":"<p>Call the wrapped function and log the results to a wandb run.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __call__(self, trial: Trial, *args: P.args, **kwargs: P.kwargs) -&gt; Trial.Report:\n    \"\"\"Call the wrapped function and log the results to a wandb run.\"\"\"\n    params = self.params if self.modify is None else self.modify(trial, self.params)\n    with params.run(name=trial.name, config=trial.config) as run:\n        # Make sure the run is available from the trial\n        trial.extras[\"wandb\"] = run\n\n        report = self.fn(trial, *args, **kwargs)\n\n        report_df = report.df()\n        run.log({\"table\": wandb.Table(dataframe=report_df)})\n        wandb_summary = {\n            k: v\n            for k, v in report.summary.items()\n            if isinstance(v, int | float | np.number)\n        }\n        run.summary.update(wandb_summary)\n\n    wandb.finish()\n    return report\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker","title":"<code>class WandbTrialTracker(params, *, modify=None)</code>","text":"<p>         Bases: <code>Plugin</code></p> <p>Track trials using wandb.</p> <p>This class is a task plugin that tracks trials using wandb.</p> PARAMETER  DESCRIPTION <code>params</code> <p>The parameters to initialize the wandb run.</p> <p> TYPE: <code>WandbParams</code> </p> <code>modify</code> <p>A function that modifies the parameters of the wandb run before each trial.</p> <p> TYPE: <code>Callable[[Trial, WandbParams], WandbParams] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __init__(\n    self,\n    params: WandbParams,\n    *,\n    modify: Callable[[Trial, WandbParams], WandbParams] | None = None,\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        params: The parameters to initialize the wandb run.\n        modify: A function that modifies the parameters of the wandb run\n            before each trial.\n    \"\"\"\n    super().__init__()\n    self.params = params\n    self.modify = modify\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.name","title":"<code>name: str</code>   <code>classvar</code>","text":"<p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.attach_task","title":"<code>def attach_task(task)</code>","text":"<p>Use the task to register several callbacks.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Use the task to register several callbacks.\"\"\"\n    self._check_explicit_reinit_arg_with_executor(task.scheduler)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.pre_submit","title":"<code>def pre_submit(fn, *args, **kwargs)</code>","text":"<p>Wrap the target function to log the results to a wandb run.</p> <p>This method wraps the target function to log the results to a wandb run and returns the wrapped function.</p> PARAMETER  DESCRIPTION <code>fn</code> <p>The target function.</p> <p> TYPE: <code>Callable[P, R]</code> </p> <code>args</code> <p>The positional arguments of the target function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>The keyword arguments of the target function.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>The wrapped function, the positional arguments and the keyword</p> <code>tuple[Callable[P, R], tuple, dict] | None</code> <p>arguments.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: Any,\n    **kwargs: Any,\n) -&gt; tuple[Callable[P, R], tuple, dict] | None:\n    \"\"\"Wrap the target function to log the results to a wandb run.\n\n    This method wraps the target function to log the results to a wandb run\n    and returns the wrapped function.\n\n    Args:\n        fn: The target function.\n        args: The positional arguments of the target function.\n        kwargs: The keyword arguments of the target function.\n\n    Returns:\n        The wrapped function, the positional arguments and the keyword\n        arguments.\n    \"\"\"\n    fn = WandbLiveRunWrap(self.params, fn, modify=self.modify)  # type: ignore\n    return fn, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbTrialTracker.copy","title":"<code>def copy()</code>","text":"<p>Copy the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>@override\ndef copy(self) -&gt; Self:\n    \"\"\"Copy the plugin.\"\"\"\n    return self.__class__(modify=self.modify, params=replace(self.params))\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbPlugin","title":"<code>class WandbPlugin(*, project, group=None, entity=None, dir=None, mode='online')</code>","text":"<p>Log trials using wandb.</p> <p>This class is the entry point to log trials using wandb. It can be used to create a <code>trial_tracker()</code> to pass into a <code>Task(plugins=...)</code> or to create <code>wandb.Run</code>'s for custom purposes with <code>run()</code>.</p> PARAMETER  DESCRIPTION <code>project</code> <p>The name of the project.</p> <p> TYPE: <code>str</code> </p> <code>group</code> <p>The name of the group.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>entity</code> <p>The name of the entity.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>dir</code> <p>The directory to store the runs in.</p> <p> TYPE: <code>str | Path | None</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>The mode to use for the runs.</p> <p> TYPE: <code>Literal['online', 'offline', 'disabled']</code> DEFAULT: <code>'online'</code> </p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def __init__(\n    self,\n    *,\n    project: str,\n    group: str | None = None,\n    entity: str | None = None,\n    dir: str | Path | None = None,  # noqa: A002\n    mode: Literal[\"online\", \"offline\", \"disabled\"] = \"online\",\n):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        project: The name of the project.\n        group: The name of the group.\n        entity: The name of the entity.\n        dir: The directory to store the runs in.\n        mode: The mode to use for the runs.\n    \"\"\"\n    super().__init__()\n    _dir = Path(project) if dir is None else Path(dir)\n    _dir.mkdir(parents=True, exist_ok=True)\n\n    self.dir = _dir.resolve().absolute()\n    self.project = project\n    self.group = group\n    self.entity = entity\n    self.mode = mode\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbPlugin.trial_tracker","title":"<code>def trial_tracker(job_type='trial', *, modify=None)</code>","text":"<p>Create a live tracker.</p> PARAMETER  DESCRIPTION <code>job_type</code> <p>The job type to use for the runs.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'trial'</code> </p> <code>modify</code> <p>A function that modifies the parameters of the wandb run before each trial.</p> <p> TYPE: <code>Callable[[Trial, WandbParams], WandbParams] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>WandbTrialTracker</code> <p>A live tracker.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def trial_tracker(\n    self,\n    job_type: str = \"trial\",\n    *,\n    modify: Callable[[Trial, WandbParams], WandbParams] | None = None,\n) -&gt; WandbTrialTracker:\n    \"\"\"Create a live tracker.\n\n    Args:\n        job_type: The job type to use for the runs.\n        modify: A function that modifies the parameters of the wandb run\n            before each trial.\n\n    Returns:\n        A live tracker.\n    \"\"\"\n    params = WandbParams(\n        project=self.project,\n        entity=self.entity,\n        group=self.group,\n        dir=self.dir,\n        mode=self.mode,  # type: ignore\n        job_type=job_type,\n    )\n    return WandbTrialTracker(params, modify=modify)\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/wandb/#amltk.scheduling.plugins.wandb.WandbPlugin.run","title":"<code>def run(*, name, job_type=None, group=None, config=None, tags=None, resume=None, notes=None)</code>","text":"<p>Create a wandb run.</p> <p>See <code>wandb.init()</code> for more.</p> Source code in <code>src/amltk/scheduling/plugins/wandb.py</code> <pre><code>def run(\n    self,\n    *,\n    name: str,\n    job_type: str | None = None,\n    group: str | None = None,\n    config: Mapping[str, Any] | None = None,\n    tags: list[str] | None = None,\n    resume: bool | str | None = None,\n    notes: str | None = None,\n) -&gt; WRun:\n    \"\"\"Create a wandb run.\n\n    See [`wandb.init()`](https://docs.wandb.ai/ref/python/init) for more.\n    \"\"\"\n    return WandbParams(\n        project=self.project,\n        entity=self.entity,\n        group=group,\n        dir=self.dir,\n        mode=self.mode,  # type: ignore\n        job_type=job_type,\n        tags=tags,\n        resume=resume,\n        notes=notes,\n    ).run(\n        name=name,\n        config=config,\n    )\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/","title":"Warning filter","text":"<p>The <code>WarningFilter</code> if used to automatically filter out warnings from a <code>Task</code> as it runs.</p> <p>This wraps your function in context manager <code>warnings.catch_warnings()</code> and applies your arguments to <code>warnings.filterwarnings()</code>, as you would normally filter warnings in Python.</p> Usage <p><pre><code>import warnings\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import WarningFilter\n\ndef f() -&gt; None:\n    warnings.warn(\"This is a warning\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(f, plugins=WarningFilter(\"ignore\"))\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f() -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin warning-filter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Args         Kwargs                                                     \u2502 \u2502\n\u2502 \u2502  ('ignore',)  {}                                                         \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Bzdogs92 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter","title":"<code>class WarningFilter(*args, **kwargs)</code>","text":"<p>         Bases: <code>Plugin</code></p> <p>A plugin that disables warnings emitted from tasks.</p> PARAMETER  DESCRIPTION <code>*args</code> <p>arguments to pass to <code>warnings.filterwarnings</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>keyword arguments to pass to <code>warnings.filterwarnings</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/amltk/scheduling/plugins/warning_filter.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    \"\"\"Initialize the plugin.\n\n    Args:\n        *args: arguments to pass to\n            [`warnings.filterwarnings`][warnings.filterwarnings].\n        **kwargs: keyword arguments to pass to\n            [`warnings.filterwarnings`][warnings.filterwarnings].\n    \"\"\"\n    super().__init__()\n    self.task: Task | None = None\n    self.warning_args = args\n    self.warning_kwargs = kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the plugin.</p>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.attach_task","title":"<code>def attach_task(task)</code>","text":"<p>Attach the plugin to a task.</p> Source code in <code>src/amltk/scheduling/plugins/warning_filter.py</code> <pre><code>@override\ndef attach_task(self, task: Task) -&gt; None:\n    \"\"\"Attach the plugin to a task.\"\"\"\n    self.task = task\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.pre_submit","title":"<code>def pre_submit(fn, *args, **kwargs)</code>","text":"<p>Pre-submit hook.</p> <p>Wraps the function to ignore warnings.</p> Source code in <code>src/amltk/scheduling/plugins/warning_filter.py</code> <pre><code>@override\ndef pre_submit(\n    self,\n    fn: Callable[P, R],\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; tuple[Callable[P, R], tuple, dict]:\n    \"\"\"Pre-submit hook.\n\n    Wraps the function to ignore warnings.\n    \"\"\"\n    wrapped_f = _IgnoreWarningWrapper(fn, *self.warning_args, **self.warning_kwargs)\n    return wrapped_f, args, kwargs\n</code></pre>"},{"location":"api/amltk/scheduling/plugins/warning_filter/#amltk.scheduling.plugins.warning_filter.WarningFilter.copy","title":"<code>def copy()</code>","text":"<p>Return a copy of the plugin.</p> Source code in <code>src/amltk/scheduling/plugins/warning_filter.py</code> <pre><code>@override\ndef copy(self) -&gt; Self:\n    \"\"\"Return a copy of the plugin.\"\"\"\n    return self.__class__(*self.warning_args, **self.warning_kwargs)\n</code></pre>"},{"location":"api/amltk/sklearn/data/","title":"Data","text":"<p>Data utilities for scikit-learn.</p>"},{"location":"api/amltk/sklearn/data/#amltk.sklearn.data.split_data","title":"<code>def split_data(*items, splits, seed=None, shuffle=True, stratify=None)</code>","text":"<p>Split a set of items into multiple splits.</p> <pre><code>from amltk.sklearn.data import split\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\nsplits = split_data(x, y, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\ntrain_x, train_y = splits[\"train\"]\nval_x, val_y = splits[\"val\"]\ntest_x, test_y = splits[\"test\"]\n</code></pre> PARAMETER  DESCRIPTION <code>items</code> <p>The items to split. Must be indexible, like a list, np.ndarray, pandas dataframe/series or a tuple, etc...</p> <p> TYPE: <code>Sequence</code> DEFAULT: <code>()</code> </p> <code>splits</code> <p>A dictionary of split names and their percentage of the data. The percentages must sum to 1.</p> <p> TYPE: <code>dict[str, float]</code> </p> <code>seed</code> <p>The seed to use for the random state.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data before splitting. Passed forward to sklearn.model_selection.train_test_split.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>stratify</code> <p>The stratification to use for the split. This will be passed forward to sklearn.model_selection.train_test_split. We account for using the stratification for all splits, ensuring we split of the stratification values themselves.</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, tuple[Sequence, ...]]</code> <p>A dictionary of split names and their split items.</p> Source code in <code>src/amltk/sklearn/data.py</code> <pre><code>def split_data(\n    *items: Sequence,\n    splits: dict[str, float],\n    seed: Seed | None = None,\n    shuffle: bool = True,\n    stratify: Sequence | None = None,\n) -&gt; dict[str, tuple[Sequence, ...]]:\n    \"\"\"Split a set of items into multiple splits.\n\n    ```python\n    from amltk.sklearn.data import split\n\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n    splits = split_data(x, y, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\n    train_x, train_y = splits[\"train\"]\n    val_x, val_y = splits[\"val\"]\n    test_x, test_y = splits[\"test\"]\n    ```\n\n    Args:\n        items: The items to split. Must be indexible, like a list, np.ndarray,\n            pandas dataframe/series or a tuple, etc...\n        splits: A dictionary of split names and their percentage of the data.\n            The percentages must sum to 1.\n        seed: The seed to use for the random state.\n        shuffle: Whether to shuffle the data before splitting. Passed forward\n            to [sklearn.model_selection.train_test_split][].\n        stratify: The stratification to use for the split. This will be passed\n            forward to [sklearn.model_selection.train_test_split][]. We account\n            for using the stratification for all splits, ensuring we split of\n            the stratification values themselves.\n\n    Returns:\n        A dictionary of split names and their split items.\n    \"\"\"\n    if not all(0 &lt; s &lt; 1 for s in splits.values()):\n        raise ValueError(f\"Splits ({splits=}) must be between 0 and 1\")\n\n    if sum(splits.values()) != 1:\n        raise ValueError(f\"Splits ({splits=}) must sum to 1\")\n\n    if len(splits) &lt; 2:  # noqa: PLR2004\n        raise ValueError(f\"Splits ({splits=}) must have at least 2 splits\")\n\n    rng = as_int(seed) if seed is not None else None\n\n    # Store the results of each split, indexed by the split number\n    split_results: dict[str, list[Sequence]] = {}\n    remaining: list[Sequence] = list(items)\n\n    remaining_percentage = 1.0\n\n    # Enumerate up to the last split\n    for name, split_percentage in list(splits.items())[0:-1]:\n        # If we stratify, make sure to also include it in the splitting so\n        # further splits can be stratified correctly.\n        to_split = remaining if stratify is None else [*remaining, stratify]\n\n        # Calculate the percentage of the remaining data to split\n        percentage = split_percentage / remaining_percentage\n\n        splitted = train_test_split(\n            *to_split,\n            train_size=percentage,\n            random_state=rng,\n            shuffle=shuffle,\n            stratify=stratify,\n        )\n\n        # Update the remaining percentage\n        remaining_percentage -= split_percentage\n\n        # Splitted returns pairs of (train, test) for each item in items\n        # so we need to split them up\n        lefts = splitted[::2]\n        rights = splitted[1::2]\n\n        # If we had stratify, we need to remove the last item from splits\n        # as it was the stratified array, setting the stratification for\n        # the next split\n        if stratify is not None:\n            stratify = rights[-1]  # type: ignore\n\n            lefts = lefts[:-1]\n            rights = rights[:-1]\n\n        # Lastly, we insert the lefts into the split_results\n        # and set the remaining to the rights\n        split_results[name] = lefts  # type: ignore\n        remaining = rights  # type: ignore\n\n    # Since we enumerated up to the last split, we need to add the last\n    # split manually\n    last_name = last(splits.keys())\n    split_results[last_name] = remaining\n\n    return {name: tuple(split) for name, split in split_results.items()}\n</code></pre>"},{"location":"api/amltk/sklearn/data/#amltk.sklearn.data.train_val_test_split","title":"<code>def train_val_test_split(*items, splits, seed=None, shuffle=True, stratify=None)</code>","text":"<p>Split a set of items into train, val and test splits.</p> <pre><code>from amltk.sklearn.data import train_val_test_split\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\ntrain_x, train_y, val_x, val_y, test_x, test_y = train_val_test_split(\n    x, y, splits=(0.6, 0.2, 0.2),\n)\n</code></pre> PARAMETER  DESCRIPTION <code>items</code> <p>The items to split. Must be indexible, like a list, np.ndarray, pandas dataframe/series or a tuple, etc...</p> <p> TYPE: <code>Sequence</code> DEFAULT: <code>()</code> </p> <code>splits</code> <p>A tuple of the percentage of the data to use for the train, val and test splits. The percentages must sum to 1.</p> <p> TYPE: <code>tuple[float, float, float]</code> </p> <code>seed</code> <p>The seed to use for the random state.</p> <p> TYPE: <code>Seed | None</code> DEFAULT: <code>None</code> </p> <code>shuffle</code> <p>Whether to shuffle the data before splitting. Passed forward to sklearn.model_selection.train_test_split.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>stratify</code> <p>The stratification to use for the split. This will be passed forward to sklearn.model_selection.train_test_split. We account for using the stratification for all splits, ensuring we split of the stratification values themselves.</p> <p> TYPE: <code>Sequence | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Sequence, ...]</code> <p>A tuple containing the train, val and test splits.</p> Source code in <code>src/amltk/sklearn/data.py</code> <pre><code>def train_val_test_split(\n    *items: Sequence,\n    splits: tuple[float, float, float],\n    seed: Seed | None = None,\n    shuffle: bool = True,\n    stratify: Sequence | None = None,\n) -&gt; tuple[Sequence, ...]:\n    \"\"\"Split a set of items into train, val and test splits.\n\n    ```python\n    from amltk.sklearn.data import train_val_test_split\n\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n    train_x, train_y, val_x, val_y, test_x, test_y = train_val_test_split(\n        x, y, splits=(0.6, 0.2, 0.2),\n    )\n    ```\n\n    Args:\n        items: The items to split. Must be indexible, like a list, np.ndarray,\n            pandas dataframe/series or a tuple, etc...\n        splits: A tuple of the percentage of the data to use for the train,\n            val and test splits. The percentages must sum to 1.\n        seed: The seed to use for the random state.\n        shuffle: Whether to shuffle the data before splitting. Passed forward\n            to [sklearn.model_selection.train_test_split][].\n        stratify: The stratification to use for the split. This will be passed\n            forward to [sklearn.model_selection.train_test_split][]. We account\n            for using the stratification for all splits, ensuring we split of\n            the stratification values themselves.\n\n    Returns:\n        A tuple containing the train, val and test splits.\n    \"\"\"\n    results = split_data(\n        *items,\n        splits={\"train\": splits[0], \"val\": splits[1], \"test\": splits[2]},\n        seed=seed,\n        shuffle=shuffle,\n        stratify=stratify,\n    )\n    return tuple(chain(results[\"train\"], results[\"val\"], results[\"test\"]))\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/","title":"Estimators","text":"<p>Custom estimators for use with scikit-learn.</p>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionRegressor","title":"<code>class StoredPredictionRegressor(predictions)</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>A class that just uses precomputed values for regression.</p> PARAMETER  DESCRIPTION <code>predictions</code> <p>The precomputed predictions.</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def __init__(self, predictions: np.ndarray):\n    \"\"\"Initialize the estimator.\n\n    Args:\n        predictions: The precomputed predictions.\n    \"\"\"\n    super().__init__()\n    self.predictions = predictions\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionRegressor.fit","title":"<code>def fit(*_, **__)</code>","text":"<p>Fit the estimator. Doesn't do anything.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def fit(self, *_: Any, **__: Any) -&gt; Self:\n    \"\"\"Fit the estimator. Doesn't do anything.\"\"\"\n    return self\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionRegressor.predict","title":"<code>def predict(X, *_, **__)</code>","text":"<p>Predict the target values, returning the precomputed values.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def predict(self, X: Any, *_: Any, **__: Any) -&gt; np.ndarray:  # noqa: N803, ARG002\n    \"\"\"Predict the target values, returning the precomputed values.\"\"\"\n    return self.predictions\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier","title":"<code>class StoredPredictionClassifier(predictions=None, probabilities=None, classes=None)</code>","text":"<p>         Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A class that just uses precomputed values for classification.</p> PARAMETER  DESCRIPTION <code>predictions</code> <p>The precomputed predictions, if any.</p> <p> TYPE: <code>ndarray | None</code> DEFAULT: <code>None</code> </p> <code>probabilities</code> <p>The precomputed probabilities, if any.</p> <p> TYPE: <code>ndarray | None</code> DEFAULT: <code>None</code> </p> <code>classes</code> <p>The classes, if any.</p> <p> TYPE: <code>list[ndarray] | ndarray | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def __init__(\n    self,\n    predictions: np.ndarray | None = None,\n    probabilities: np.ndarray | None = None,\n    classes: list[np.ndarray] | np.ndarray | None = None,\n):\n    \"\"\"Initialize the estimator.\n\n    Args:\n        predictions: The precomputed predictions, if any.\n        probabilities: The precomputed probabilities, if any.\n        classes: The classes, if any.\n    \"\"\"\n    super().__init__()\n    self.predictions = predictions\n    self.probabilities = probabilities\n    self.classes = classes\n\n    # HACK: This is to enable sklearn-compatibility\n    # `clone` and other methods rely on this trailing underscore\n    # to indicate fitted attributes. We essentially declare it fitted\n    # at init for simplicity\n    self.classes_ = classes\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier.fit","title":"<code>def fit(*_, **__)</code>","text":"<p>Fit the estimator. Doesn't do anything.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def fit(self, *_: Any, **__: Any) -&gt; Self:\n    \"\"\"Fit the estimator. Doesn't do anything.\"\"\"\n    return self\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier.predict","title":"<code>def predict(X, *_, **__)</code>","text":"<p>Predict the target values, returning the precomputed values.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def predict(self, X: Any, *_: Any, **__: Any) -&gt; np.ndarray:  # noqa: N803, ARG002\n    \"\"\"Predict the target values, returning the precomputed values.\"\"\"\n    if self.predictions is None:\n        if self.probabilities is None:\n            raise RuntimeError(\n                \"No predictions or probabilities were provided during\",\n                \" construction, so this estimator cannot be used for\",\n                \" `predict()`.\",\n            )\n        if self.classes_ is None:\n            raise RuntimeError(\n                \"No classes were provided during construction, so it can't\"\n                \" be used for `predict()` from probabilities.\",\n            )\n\n        predictions = probabilities_to_classes(\n            self.probabilities,\n            classes=self.classes_,\n        )\n    else:\n        predictions = self.predictions\n\n    return predictions\n</code></pre>"},{"location":"api/amltk/sklearn/estimators/#amltk.sklearn.estimators.StoredPredictionClassifier.predict_proba","title":"<code>def predict_proba(X, *_, **__)</code>","text":"<p>Predict the probabilities, returning the precomputed values.</p> Source code in <code>src/amltk/sklearn/estimators.py</code> <pre><code>def predict_proba(\n    self,\n    X: Any,  # noqa: N803, ARG002\n    *_: Any,\n    **__: Any,\n) -&gt; np.ndarray:\n    \"\"\"Predict the probabilities, returning the precomputed values.\"\"\"\n    if self.probabilities is None:\n        raise RuntimeError(\n            \"No probabilities were provided during construction, so this\"\n            \" estimator cannot be used for `predict_proba()`.\",\n        )\n\n    return self.probabilities\n</code></pre>"},{"location":"api/amltk/sklearn/voting/","title":"Voting","text":"<p>Utilities for voting ensembles.</p>"},{"location":"api/amltk/sklearn/voting/#amltk.sklearn.voting.voting_with_preffited_estimators","title":"<code>def voting_with_preffited_estimators(estimators, weights=None, *, voter=None, **voting_kwargs)</code>","text":"<p>Create a voting ensemble with pre-fitted estimators.</p> PARAMETER  DESCRIPTION <code>estimators</code> <p>The estimators to use in the ensemble.</p> <p> TYPE: <code>Iterable[BaseEstimator]</code> </p> <code>weights</code> <p>The weights to use for the estimators. If None, will use uniform weights.</p> <p> TYPE: <code>Iterable[float] | None</code> DEFAULT: <code>None</code> </p> <code>voter</code> <p>The voting classifier or regressor to use. If None, will use the appropriate one based on the type of the first estimator.</p> <p> TYPE: <code>type[_Voter] | None</code> DEFAULT: <code>None</code> </p> <code>**voting_kwargs</code> <p>Additional arguments to pass to the voting classifier or regressor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>_Voter</code> <p>The voting classifier or regressor with the pre-fitted estimators.</p> Source code in <code>src/amltk/sklearn/voting.py</code> <pre><code>def voting_with_preffited_estimators(\n    estimators: Iterable[BaseEstimator],\n    weights: Iterable[float] | None = None,\n    *,\n    voter: type[_Voter] | None = None,\n    **voting_kwargs: Any,\n) -&gt; _Voter:\n    \"\"\"Create a voting ensemble with pre-fitted estimators.\n\n    Args:\n        estimators: The estimators to use in the ensemble.\n        weights: The weights to use for the estimators. If None,\n            will use uniform weights.\n        voter: The voting classifier or regressor to use.\n            If None, will use the appropriate one based on the type of the first\n            estimator.\n        **voting_kwargs: Additional arguments to pass to the voting classifier or\n            regressor.\n\n    Returns:\n        The voting classifier or regressor with the pre-fitted estimators.\n    \"\"\"\n    estimators = list(estimators)\n    est0 = estimators[0]\n    is_classification = voter is not None and issubclass(voter, VotingClassifier)\n\n    if voter is None:\n        if isinstance(est0, ClassifierMixin):\n            voter_cls = VotingClassifier\n            is_classification = True\n        elif isinstance(est0, ClassifierMixin):\n            voter_cls = VotingRegressor\n            is_classification = False\n        else:\n            raise ValueError(\n                f\"Could not infer voter type from estimator type: {type(est0)}.\"\n                \" Please specify the voter type explicitly.\",\n            )\n    else:\n        voter_cls = voter\n\n    if weights is None:\n        weights = np.ones(len(estimators)) / len(estimators)\n    else:\n        weights = list(weights)\n\n    named_estimators = [(str(i), e) for i, e in enumerate(estimators)]\n    _voter = voter_cls(named_estimators, weights=weights, **voting_kwargs)\n    _voter.estimators_ = [model for _, model in _voter.estimators]  # type: ignore\n\n    if is_classification:\n        est0_classes_ = est0.classes_  # type: ignore\n        _voter.classes_ = est0_classes_  # type: ignore\n        if np.ndim(est0_classes_) &gt; 1:\n            est0_classes_ = est0_classes_[0]\n            _voter.le_ = MultiLabelBinarizer().fit(est0_classes_)  # type: ignore\n        else:\n            _voter.le_ = LabelEncoder().fit(est0.classes_)  # type: ignore\n\n    _voter.named_estimators_ = Bunch()  # type: ignore\n\n    # Taken from Sklearn _BaseVoting.fit\n    # Uses 'drop' as placeholder for dropped estimators\n    est_iter = iter(_voter.estimators_)  # type: ignore\n    for name, est in _voter.estimators:  # type: ignore\n        current_est = est if est == \"drop\" else next(est_iter)\n        _voter.named_estimators_[name] = current_est  # type: ignore\n\n        if hasattr(current_est, \"feature_names_in_\"):\n            _voter.feature_names_in_ = current_est.feature_names_in_  # type: ignore\n\n    return _voter  # type: ignore\n</code></pre>"},{"location":"api/amltk/store/bucket/","title":"Bucket","text":"<p>Module containing the base definition of a bucket.</p> <p>A bucket is a collection of resources that can be accessed by a key of a given type. This lets you easily store and retrieve objects of varying types in a single location.</p> Concrete examples <ul> <li><code>PathBucket</code>.</li> </ul>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket","title":"<code>class Bucket</code>","text":"<p>         Bases: <code>ABC</code>, <code>MutableMapping[KeyT, Drop[LinkT]]</code>, <code>Generic[KeyT, LinkT]</code></p> <p>Definition of a bucket of resources, accessed by a Key.</p> <p>Indexing into a bucket returns a <code>Drop</code> that can be used to access the resource.</p> <p>The definition mostly follow that of MutableMapping, but with the change of <code>.keys()</code> and <code>.values()</code> to return iterators and <code>.items()</code> to return an iterator of tuples. The other change is that the <code>.values()</code> do not return the resources themselves, by rather a <code>Drop</code> which wraps the resource.</p>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__setitem__","title":"<code>def __setitem__(key, value)</code>   <code>abstractmethod</code>","text":"<p>Store a value in the bucket.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> <code>value</code> <p>The value to store in the bucket.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __setitem__(self, key: KeyT, value: Any) -&gt; None:\n    \"\"\"Store a value in the bucket.\n\n    Args:\n        key: The key to the resource.\n        value: The value to store in the bucket.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__getitem__","title":"<code>def __getitem__(key)</code>   <code>abstractmethod</code>","text":"<p>Get a drop for a resource in the bucket.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __getitem__(self, key: KeyT) -&gt; Drop[LinkT]:\n    \"\"\"Get a drop for a resource in the bucket.\n\n    Args:\n        key: The key to the resource.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__delitem__","title":"<code>def __delitem__(key)</code>   <code>abstractmethod</code>","text":"<p>Remove a resource from the bucket.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __delitem__(self, key: KeyT) -&gt; None:\n    \"\"\"Remove a resource from the bucket.\n\n    Args:\n        key: The key to the resource.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__iter__","title":"<code>def __iter__()</code>   <code>abstractmethod</code>","text":"<p>Iterate over the keys in the bucket.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\n@abstractmethod\ndef __iter__(self) -&gt; Iterator[KeyT]:\n    \"\"\"Iterate over the keys in the bucket.\"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.sub","title":"<code>def sub(key)</code>   <code>abstractmethod</code>","text":"<p>Create a subbucket of this bucket.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The name of the sub bucket.</p> <p> TYPE: <code>KeyT</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new bucket with the same loaders as the current bucket.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@abstractmethod\ndef sub(self, key: KeyT) -&gt; Self:\n    \"\"\"Create a subbucket of this bucket.\n\n    Args:\n        key: The name of the sub bucket.\n\n    Returns:\n        A new bucket with the same loaders as the current bucket.\n    \"\"\"\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__contains__","title":"<code>def __contains__(key)</code>","text":"<p>Check if a key is in the bucket.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to check for.</p> <p> TYPE: <code>object</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef __contains__(self, key: object) -&gt; bool:\n    \"\"\"Check if a key is in the bucket.\n\n    Args:\n        key: The key to check for.\n    \"\"\"\n    return key in self\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.__len__","title":"<code>def __len__()</code>","text":"<p>Get the number of keys in the bucket.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef __len__(self) -&gt; int:\n    \"\"\"Get the number of keys in the bucket.\"\"\"\n    return ilen(iter(self))\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.find","title":"<code>def find(pattern, *, multi_key=False)</code>","text":"<p>Find resources in the bucket.</p> <pre><code>found = bucket.find(r\"trial_(.+)_val_predictions.npy\")  # (1)!\nif found is None:\n    raise KeyError(\"No predictions found\")\n\nfor name, drop in found.items():\n    predictions = drop.get()\n    # Do something with the predictions\n    # ...\n</code></pre> <ol> <li>The <code>(.+)</code> is a capture group which will attempt to match anything <code>.</code>,     when there is one or more occurences <code>+</code>, and put it in a capure group <code>()</code>.     What is captured will be used as the key in the returned dict.</li> </ol> PARAMETER  DESCRIPTION <code>pattern</code> <p>The pattern to search for.</p> <p> TYPE: <code>str</code> </p> <code>multi_key</code> <p>Whether you have multiple capture groups in the pattern.</p> <p>Multiple capture groups with <code>()</code></p> <p>If using multiple capture groups, the returned dict will have tuples as keys. If there is only one capture group, the tuple will be expanded to a single value.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict[str, Drop[LinkT]] | dict[tuple[str, ...], Drop[LinkT]] | None</code> <p>A mapping of links to drops for the resources found.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def find(\n    self,\n    pattern: str,\n    *,\n    multi_key: bool = False,\n) -&gt; dict[str, Drop[LinkT]] | dict[tuple[str, ...], Drop[LinkT]] | None:\n    \"\"\"Find resources in the bucket.\n\n    ```python\n    found = bucket.find(r\"trial_(.+)_val_predictions.npy\")  # (1)!\n    if found is None:\n        raise KeyError(\"No predictions found\")\n\n    for name, drop in found.items():\n        predictions = drop.get()\n        # Do something with the predictions\n        # ...\n    ```\n\n    1. The `(.+)` is a **capture group** which will attempt to match anything `.`,\n        when there is one or more occurences `+`, and put it in a capure group `()`.\n        What is captured will be used as the key in the returned dict.\n\n    Args:\n        pattern: The pattern to search for.\n        multi_key: Whether you have multiple capture groups in the pattern.\n\n            !!! note \"Multiple capture groups with `()`\"\n\n                If using multiple capture groups, the returned dict will have\n                tuples as keys. If there is only one capture group, the tuple\n                will be expanded to a single value.\n\n    Returns:\n        A mapping of links to drops for the resources found.\n    \"\"\"\n    keys = [(key, match) for key in self if (match := re.search(pattern, str(key)))]\n    if not keys:\n        return None\n\n    matches = {match.groups(): self[key] for key, match in keys}\n\n    # If it's a tuple of length 1, we expand it\n    one_group = len(next(iter(matches.keys()))) == 1\n    if one_group:\n        if multi_key:\n            raise ValueError(\n                \"Use multi_key=True when the pattern has more than 1 capture group\",\n            )\n\n        return {key[0]: drop for key, drop in matches.items()}\n\n    # Here we have multi-groups =&gt; tuple keys\n    if not multi_key:\n        raise ValueError(\n            \"Use multi_key=False when the pattern has only 1 capture group\",\n        )\n\n    return matches\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.store","title":"<code>def store(other)</code>","text":"<p>Store items into the bucket with the given mapping.</p> PARAMETER  DESCRIPTION <code>other</code> <p>The mapping of items to store in the bucket.</p> <p> TYPE: <code>Mapping[KeyT, Any]</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def store(self, other: Mapping[KeyT, Any]) -&gt; None:\n    \"\"\"Store items into the bucket with the given mapping.\n\n    Args:\n        other: The mapping of items to store in the bucket.\n    \"\"\"\n    for key, value in other.items():\n        self[key] = value\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.fetch","title":"<code>def fetch(*keys, default=None)</code>","text":"<p>Fetch a resource from the bucket.</p> PARAMETER  DESCRIPTION <code>keys</code> <p>The keys to the resources.</p> <p> TYPE: <code>KeyT</code> DEFAULT: <code>()</code> </p> <code>default</code> <p>The default value to return if the key is not in the bucket. If a dict is passed, the default for each key will be the value in the dict for that key, using None if not present.</p> <p> TYPE: <code>None | Any | dict[KeyT, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[KeyT, Any]</code> <p>The resources stored in the bucket at the given keys.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def fetch(\n    self,\n    *keys: KeyT,\n    default: None | Any | dict[KeyT, Any] = None,\n) -&gt; dict[KeyT, Any]:\n    \"\"\"Fetch a resource from the bucket.\n\n    Args:\n        keys: The keys to the resources.\n        default: The default value to return if the key is not in the bucket.\n            If a dict is passed, the default for each key will be the value\n            in the dict for that key, using None if not present.\n\n    Returns:\n        The resources stored in the bucket at the given keys.\n    \"\"\"\n    default_dict = {} if not isinstance(default, dict) else default\n    return {\n        key: self[key].get(default=default_dict.get(key, default)) for key in keys\n    }\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.update","title":"<code>def update(items)</code>","text":"<p>Update the bucket with the given mapping.</p> PARAMETER  DESCRIPTION <code>items</code> <p>The mapping of items to store in the bucket.</p> <p> TYPE: <code>Mapping[KeyT, Any]</code> </p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>@override\ndef update(self, items: Mapping[KeyT, Any]) -&gt; None:  # type: ignore\n    \"\"\"Update the bucket with the given mapping.\n\n    Args:\n        items: The mapping of items to store in the bucket.\n    \"\"\"\n    for key, value in items.items():\n        self[key].put(value)\n</code></pre>"},{"location":"api/amltk/store/bucket/#amltk.store.bucket.Bucket.remove","title":"<code>def remove(keys, *, how=None)</code>","text":"<p>Remove resources from the bucket.</p> PARAMETER  DESCRIPTION <code>keys</code> <p>The keys to the resources.</p> <p> TYPE: <code>Iterable[KeyT]</code> </p> <code>how</code> <p>A function that removes the resource.</p> <p> TYPE: <code>Callable[[LinkT], bool] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[KeyT, bool]</code> <p>A mapping of keys to whether they were removed.</p> Source code in <code>src/amltk/store/bucket.py</code> <pre><code>def remove(\n    self,\n    keys: Iterable[KeyT],\n    *,\n    how: Callable[[LinkT], bool] | None = None,\n) -&gt; dict[KeyT, bool]:\n    \"\"\"Remove resources from the bucket.\n\n    Args:\n        keys: The keys to the resources.\n        how: A function that removes the resource.\n\n    Returns:\n        A mapping of keys to whether they were removed.\n    \"\"\"\n    return {key: self[key].remove(how=how) for key in keys}\n</code></pre>"},{"location":"api/amltk/store/drop/","title":"Drop","text":"<p>A <code>Drop</code> in a <code>Bucket</code> is a reference to a resource.</p>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop","title":"<code>class Drop</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Generic[KeyT]</code></p> <p>A drop is a reference to a resource in a bucket.</p> <p>You likely do not need to create these yourself and are a class used by <code>Bucket</code> to wrap access to a resource located at a give <code>key</code>.</p> <p>The main use of this class is to attempt to use different <code>loaders</code> to load a resource at a given <code>key</code>, using the <code>key</code> to try infer which loader to use. Each drop has a list of default loaders that it will try to use to load the resource.</p> <p>For flexibility, you can also specify a <code>how</code> when using any of <code>load</code>, <code>get</code> or <code>put</code> to override the default loaders. The <code>remove</code> and <code>exists</code> method also has a <code>how</code> incase the default methods are not sufficient.</p> <p>To support well typed code, you can also specify a <code>check</code> type which will be used to checked when loading objects, to make sure it is of the correct type. This is ignored if <code>how</code> is specified.</p> <p>The primary methods of interest are * <code>load</code> * <code>get</code> * <code>put</code> * <code>remove</code> * <code>exists</code> * <code>as_stored_value</code></p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to the resource.</p> <p> TYPE: <code>KeyT</code> </p> <code>loaders</code> <p>The loaders to use to load the resource.</p> <p> TYPE: <code>tuple[type[Loader[KeyT, Any]], ...]</code> DEFAULT: <code>field(repr=False)</code> </p>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.as_stored_value","title":"<code>def as_stored_value(read=None)</code>","text":"<p>Convert the drop to a <code>StoredValue</code>.</p> PARAMETER  DESCRIPTION <code>read</code> <p>The method to use to load the resource. If <code>None</code> then the first loader that can load the resource will be used.</p> <p> TYPE: <code>Callable[[KeyT], T] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>StoredValue[KeyT, T]</code> <p>The drop as a <code>StoredValue</code>.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def as_stored_value(\n    self,\n    read: Callable[[KeyT], T] | None = None,\n) -&gt; StoredValue[KeyT, T]:\n    \"\"\"Convert the drop to a [`StoredValue`][amltk.store.StoredValue].\n\n    Args:\n        read: The method to use to load the resource. If `None` then\n            the first loader that can load the resource will be used.\n\n    Returns:\n        The drop as a [`StoredValue`][amltk.store.StoredValue].\n    \"\"\"\n    if read is None:\n        loader = first(\n            (_l for _l in self.loaders if _l.can_load(self.key)),\n            default=None,\n        )\n\n        if loader is None:\n            raise ValueError(f\"Can't load {self.key=} from {self.loaders=}\")\n\n        read = loader.load\n\n    return StoredValue(self.key, read=read)\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.put","title":"<code>def put(obj, *, how=None)</code>","text":"<p>Put an object into the bucket.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to put into the bucket.</p> <p> TYPE: <code>T</code> </p> <code>how</code> <p>The function to use to put the object into the bucket. If <code>None</code> then the first loader that can put the object will be used.</p> <p> TYPE: <code>Callable[[T], None] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def put(\n    self,\n    obj: T,\n    *,\n    how: Callable[[T], None] | None = None,\n) -&gt; None:\n    \"\"\"Put an object into the bucket.\n\n    Args:\n        obj: The object to put into the bucket.\n        how: The function to use to put the object into the bucket.\n            If `None` then the first loader that can put the object\n            will be used.\n    \"\"\"\n    if how:\n        how(obj)\n        return\n\n    loader = first(\n        (_l for _l in self.loaders if _l.can_save(obj, self.key)),\n        default=None,\n    )\n    if not loader:\n        msg = (\n            f\"No default way to handle {type(obj)=} objects.\"\n            \" Please provide a `how` function that will save\"\n            f\" the object to {self.key}.\"\n        )\n        raise ValueError(msg)\n\n    loader.save(obj, self.key)\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.load","title":"<code>def load(*, check=None, how=None)</code>","text":"<p>Load the resource.</p> PARAMETER  DESCRIPTION <code>check</code> <p>By specifying a <code>type</code> we check the loaded object of that type, to enable correctly typed checked code.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> <code>how</code> <p>The function to use to load the resource.</p> <p> TYPE: <code>Callable[[KeyT], T] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>T | Any</code> <p>The loaded resource.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def load(\n    self,\n    *,\n    check: type[T] | None = None,\n    how: Callable[[KeyT], T] | None = None,\n) -&gt; T | Any:\n    \"\"\"Load the resource.\n\n    Args:\n        check: By specifying a `type` we check the loaded object of that type, to\n            enable correctly typed checked code.\n        how: The function to use to load the resource.\n\n    Returns:\n        The loaded resource.\n    \"\"\"\n    if not isinstance(how, type) and callable(how):\n        value = how(self.key)\n        loader_name = funcname(how)\n    else:\n        loader = first(\n            (_l for _l in self.loaders if _l.can_load(self.key)),\n            default=None,\n        )\n        if loader is None:\n            raise ValueError(f\"Can't load {self.key=} from {self.loaders=}\")\n\n        value = loader.load(self.key)\n        loader_name = loader.name\n\n    if check is not None and not isinstance(value, check):\n        msg = (\n            f\"Value {value=} loaded by {loader_name=} is not of type {check=},\"\n            f\" but is of type {type(value)=}.\"\n        )\n        raise TypeError(msg)\n\n    return value\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.get","title":"<code>def get(default=None, *, check=None, how=None)</code>","text":"<p>Load the resource, or return the default if it can't be loaded.</p> <p>See <code>load</code> for more details.</p> Note <p>This function makes no distinction for the reason it fails to load, namely if it's of the incorrect type or the resource does not exist at the key.</p> PARAMETER  DESCRIPTION <code>default</code> <p>The default value to return if the resource can't be loaded.</p> <p> TYPE: <code>Default | None</code> DEFAULT: <code>None</code> </p> <code>check</code> <p>By specifying a <code>type</code> we check the loaded object of that type, to enable correctly typed checked code. If the default value should be returned because the resource can't be loaded, then the default value is not checked.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> <code>how</code> <p>The function to use to load the resource.</p> <p> TYPE: <code>Callable[[KeyT], T] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Default | T | None</code> <p>The loaded resource or the default value if it cant be loaded.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def get(\n    self,\n    default: Default | None = None,\n    *,\n    check: type[T] | None = None,\n    how: Callable[[KeyT], T] | None = None,\n) -&gt; Default | T | None:\n    \"\"\"Load the resource, or return the default if it can't be loaded.\n\n    See [`load`][amltk.store.drop.Drop.load] for more details.\n\n    Note:\n        This function makes no distinction for the reason it fails to load,\n        namely if it's of the incorrect type or the resource does not exist\n        at the key.\n\n    Args:\n        default: The default value to return if the resource can't be loaded.\n        check: By specifying a `type` we check the loaded object of that type, to\n            enable correctly typed checked code. If the default value should\n            be returned because the resource can't be loaded, then the default\n            value is **not** checked.\n        how: The function to use to load the resource.\n\n    Returns:\n        The loaded resource or the default value if it cant be loaded.\n    \"\"\"\n    try:\n        return self.load(check=check, how=how)\n    except TypeError as e:\n        raise e\n    except FileNotFoundError:\n        return default\n    except Exception as e:  # noqa: BLE001\n        logger.warning(\n            f\"Failed to load {self.key=} from {self.loaders=}: {e}\",\n            exc_info=True,\n        )\n\n    return None\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.remove","title":"<code>def remove(*, how=None)</code>","text":"<p>Remove the resource from the bucket.</p> PARAMETER  DESCRIPTION <code>how</code> <p>The function to use to remove the resource. Returns <code>True</code> if the resource no longer exists after the removal, <code>False</code> otherwise.</p> <p>Non-existent resources</p> <p>If the resource does not exist, then the function will <code>True</code>.</p> <p> TYPE: <code>Callable[[KeyT], bool] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def remove(self, *, how: Callable[[KeyT], bool] | None = None) -&gt; bool:\n    \"\"\"Remove the resource from the bucket.\n\n    Args:\n        how: The function to use to remove the resource. Returns `True` if\n            the resource no longer exists after the removal, `False` otherwise.\n\n            !!! note \"Non-existent resources\"\n\n                If the resource does not exist, then the function will `True`.\n    \"\"\"\n    logger.debug(f\"Removing {self.key=}\")\n    if how:\n        return how(self.key)\n\n    return self._remove(self.key)\n</code></pre>"},{"location":"api/amltk/store/drop/#amltk.store.drop.Drop.exists","title":"<code>def exists(*, how=None)</code>","text":"<p>Check if the resource exists.</p> RETURNS DESCRIPTION <code>bool</code> <p><code>True</code> if the resource exists, <code>False</code> otherwise.</p> Source code in <code>src/amltk/store/drop.py</code> <pre><code>def exists(self, *, how: Callable[[KeyT], bool] | None = None) -&gt; bool:\n    \"\"\"Check if the resource exists.\n\n    Returns:\n        `True` if the resource exists, `False` otherwise.\n    \"\"\"\n    if how:\n        return how(self.key)\n    return self._exists(self.key)\n</code></pre>"},{"location":"api/amltk/store/loader/","title":"Loader","text":"<p>Module containing the base protocol of a loader.</p> <p>For concrete implementations based on the <code>key</code> being a <code>Path</code> see the <code>path_loaders</code> module.</p>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader","title":"<code>class Loader</code>","text":"<p>         Bases: <code>ABC</code>, <code>Generic[KeyT_contra, T]</code></p> <p>The base definition of a Loader.</p> <p>A Loader is a class that can save and load objects to and from a bucket. The Loader is responsible for knowing how to save and load objects of a particular type at a given key.</p>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.name","title":"<code>name: str</code>   <code>classvar</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>KeyT_contra</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_load(cls, key: KeyT_contra, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.can_save","title":"<code>def can_save(obj, key)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>KeyT_contra</code> </p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: KeyT_contra, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.save","title":"<code>def save(obj, key)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>KeyT_contra</code> </p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: KeyT_contra, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/loader/#amltk.store.loader.Loader.load","title":"<code>def load(key)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>KeyT_contra</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/loader.py</code> <pre><code>@classmethod\n@abstractmethod\ndef load(cls, key: KeyT_contra, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/stored_value/","title":"Stored value","text":"<p>A value that is stored on disk and loaded lazily.</p> <p>This is useful for transmitting large objects between processes.</p> StoredValue<pre><code>from amltk.store import StoredValue\nimport pandas as pd\nfrom pathlib import Path\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\npath = Path(\"df.csv\")\ndf.to_csv(path)\n\nstored_value = StoredValue(path, read=pd.read_csv)\n\n# Somewhere in a processes\ndf = stored_value.value()\nprint(df)\n\npath.unlink()\n</code></pre> <pre><code>   Unnamed: 0  a  b\n0           0  1  4\n1           1  2  5\n2           2  3  6\n</code></pre> <p>You can quickly obtain these from buckets if you require StoredValue from bucket<pre><code>from amltk import PathBucket\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nbucket = PathBucket(\"bucket_path\")\nbucket.update({\"df.csv\": df})\n\nstored_value = bucket[\"df.csv\"].as_stored_value()\n\n# Somewhere in a processes\ndf = stored_value.value()\nprint(df)\n\nbucket.rmdir()\n</code></pre> <pre><code>       a  b\nindex      \n0      1  4\n1      2  5\n2      3  6\n</code></pre> </p>"},{"location":"api/amltk/store/stored_value/#amltk.store.stored_value.StoredValue","title":"<code>class StoredValue</code>   <code>dataclass</code>","text":"<p>         Bases: <code>Generic[K, V]</code></p> <p>A value that is stored on disk and can be loaded when needed.</p>"},{"location":"api/amltk/store/stored_value/#amltk.store.stored_value.StoredValue.value","title":"<code>def value()</code>","text":"<p>Get the value.</p> Source code in <code>src/amltk/store/stored_value.py</code> <pre><code>def value(self) -&gt; V:\n    \"\"\"Get the value.\"\"\"\n    if self._value is None:\n        self._value = self.read(self.key)\n\n    return self._value\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/","title":"Path bucket","text":"<p>A module containing a concreate implementation of a <code>Bucket</code> that uses the Path API to store objects.</p>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket","title":"<code>class PathBucket(path, *, loaders=None, create=True, clean=False, exists_ok=True)</code>","text":"<p>         Bases: <code>Bucket[str, Path]</code></p> <p>A bucket that uses the Path API to store objects.</p> <p>This bucket is a key-value lookup backed up by some filesystem. By assinging to the bucket, you store the object to the filesystem. However the values you get back are instead a <code>Drop</code> that can be used to perform operations on the stores object, such as <code>load</code>, <code>get</code> and <code>remove</code>.</p> Drop methods <ul> <li><code>Drop.load</code> - Load the object from the bucket.</li> <li><code>Drop.get</code> - Load the object from the bucket     with a default if something fails.</li> <li><code>Drop.put</code> - Store an object in the bucket.</li> <li><code>Drop.remove</code> - Remove the object from the     bucket.</li> <li><code>Drop.exists</code> - Check if the object exists     in the bucket.</li> </ul> <pre><code>from amltk.store.paths import PathBucket\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nbucket = PathBucket(\"path/to/bucket\")\n\narray = np.array([1, 2, 3])\ndataframe = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nmodel = LinearRegression()\n\n# Store things\nbucket[\"myarray.npy\"] = array # (1)!\nbucket[\"df.csv\"] = dataframe  # (2)!\nbucket[\"model.pkl\"].put(model)\n\nbucket[\"config.json\"] = {\"hello\": \"world\"}\nassert bucket[\"config.json\"].exists()\nbucket[\"config.json\"].remove()\n\n# Load things\narray = bucket[\"myarray.npy\"].load()\nmaybe_df = bucket[\"df.csv\"].get()  # (3)!\nmodel: LinearRegression = bucket[\"model.pkl\"].get(check=LinearRegression)  # (4)!\n\n# Create subdirectories\nmodel_bucket = bucket / \"my_model\" # (5)!\nmodel_bucket[\"model.pkl\"] = model\nmodel_bucket[\"predictions.npy\"] = model.predict(X)\n\n# Acts like a mapping\nassert \"myarray.npy\" in bucket\nassert len(bucket) == 3\nfor key, item in bucket.items():\n    print(key, item.load())\ndel bucket[\"model.pkl\"]\n</code></pre> <ol> <li>The <code>=</code> is a shortcut for <code>bucket[\"myarray.npy\"].put(array)</code></li> <li>The extension is used to determine which     <code>PathLoader</code> to use     and how to save it.</li> <li>The <code>get</code> method acts like the <code>dict.load</code> method.</li> <li>The <code>get</code> method can be used to check the type of the loaded object.     If the type does not match, a <code>TypeError</code> is raised.</li> <li>Uses the familiar <code>Path</code> API to create subdirectories.</li> </ol> PARAMETER  DESCRIPTION <code>path</code> <p>The path to the bucket.</p> <p> TYPE: <code>Path | str</code> </p> <code>loaders</code> <p>A sequence of loaders to use when loading objects. These will be prepended to the default loaders and attempted to be used first.</p> <p> TYPE: <code>Sequence[type[PathLoader]] | None</code> DEFAULT: <code>None</code> </p> <code>create</code> <p>If True, the base path will be created if it does not exist.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>clean</code> <p>If True, the base path will be deleted if it exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>exists_ok</code> <p>If False, an error will be raised if the base path already exists.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def __init__(\n    self,\n    path: Path | str,\n    *,\n    loaders: Sequence[type[PathLoader]] | None = None,\n    create: bool = True,\n    clean: bool = False,\n    exists_ok: bool = True,\n) -&gt; None:\n    \"\"\"Create a new PathBucket.\n\n    Args:\n        path: The path to the bucket.\n        loaders: A sequence of loaders to use when loading objects.\n            These will be prepended to the default loaders and attempted\n            to be used first.\n        create: If True, the base path will be created if it does not\n            exist.\n        clean: If True, the base path will be deleted if it exists.\n        exists_ok: If False, an error will be raised if the base path\n            already exists.\n    \"\"\"\n    super().__init__()\n    _loaders = DEFAULT_LOADERS\n    if loaders is not None:\n        _loaders = tuple(chain(loaders, DEFAULT_LOADERS))\n\n    if isinstance(path, str):\n        path = Path(path)\n\n    if clean and path.exists():\n        shutil.rmtree(path, ignore_errors=True)\n\n    if not exists_ok and path.exists():\n        raise FileExistsError(f\"File/Directory already exists at {path}\")\n\n    if create:\n        path.mkdir(parents=True, exist_ok=True)\n\n    self._create = create\n    self.path = path\n    self.loaders = _loaders\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.sizes","title":"<code>def sizes()</code>","text":"<p>Get the sizes of all the files in the bucket.</p> <p>Files only</p> <p>This method only returns the sizes of the files in the bucket. It does not include directories, their sizes, or their contents.</p> RETURNS DESCRIPTION <code>dict[str, int]</code> <p>A dictionary mapping the keys to the sizes of the files.</p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def sizes(self) -&gt; dict[str, int]:\n    \"\"\"Get the sizes of all the files in the bucket.\n\n    !!! warning \"Files only\"\n\n        This method only returns the sizes of the files in the bucket.\n        It does not include directories, their sizes, or their contents.\n\n    Returns:\n        A dictionary mapping the keys to the sizes of the files.\n    \"\"\"\n    return {str(path.name): path.stat().st_size for path in self.path.iterdir()}\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.add_loader","title":"<code>def add_loader(loader)</code>","text":"<p>Add a loader to the bucket.</p> PARAMETER  DESCRIPTION <code>loader</code> <p>The loader to add.</p> <p> TYPE: <code>type[PathLoader]</code> </p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def add_loader(self, loader: type[PathLoader]) -&gt; None:\n    \"\"\"Add a loader to the bucket.\n\n    Args:\n        loader: The loader to add.\n    \"\"\"\n    self.loaders = (loader, *self.loaders)\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.sub","title":"<code>def sub(key, *, create=None)</code>","text":"<p>Create a subdirectory of the bucket.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The name of the subdirectory.</p> <p> TYPE: <code>str</code> </p> <code>create</code> <p>Whether the subdirectory will be created if it does not exist. If None, the default, the value of <code>create</code> passed to the constructor will be used.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>A new bucket with the same loaders as the current bucket.</p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>@override\ndef sub(self, key: str, *, create: bool | None = None) -&gt; Self:\n    \"\"\"Create a subdirectory of the bucket.\n\n    Args:\n        key: The name of the subdirectory.\n        create: Whether the subdirectory will be created if it does not\n            exist. If None, the default, the value of `create` passed to\n            the constructor will be used.\n\n    Returns:\n        A new bucket with the same loaders as the current bucket.\n    \"\"\"\n    return self.__class__(\n        self.path / key,\n        loaders=self.loaders,\n        create=self._create if create is None else create,\n        clean=False,\n    )\n</code></pre>"},{"location":"api/amltk/store/paths/path_bucket/#amltk.store.paths.path_bucket.PathBucket.rmdir","title":"<code>def rmdir()</code>","text":"<p>Delete the bucket.</p> Source code in <code>src/amltk/store/paths/path_bucket.py</code> <pre><code>def rmdir(self) -&gt; None:\n    \"\"\"Delete the bucket.\"\"\"\n    shutil.rmtree(self.path)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/","title":"Path loaders","text":"<p>Loaders for <code>PathBucket</code>s.</p> <p>The <code>Loader</code>s in this module are used to load and save objects identified by a unique <code>Path</code>. For saving objects, these loaders rely on checking the type of the object for <code>can_save</code> and <code>save</code> methods. For loading objects, these loaders rely on checking the file extension of the path for <code>can_load</code> and <code>load</code> methods.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader","title":"<code>class PathLoader</code>","text":"<p>         Bases: <code>Loader[Path, T]</code></p> <p>A <code>Loader</code> for loading and saving objects indentified by a <code>Path</code>.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.name","title":"<code>name: str</code>   <code>classvar</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.save","title":"<code>def save(obj, key)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PathLoader.load","title":"<code>def load(key)</code>   <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader","title":"<code>class NPYLoader</code>","text":"<p>         Bases: <code>PathLoader[ndarray]</code></p> <p>A <code>Loader</code> for loading and saving <code>np.ndarray</code>s.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".npy\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>np.ndarray</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, np.ndarray) and key.suffix in {\".npy\"}\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in {\".npy\"} and check in (np.ndarray, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.load","title":"<code>def load(key)</code>   <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; np.ndarray:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    item = np.load(key, allow_pickle=False)\n    if not isinstance(item, np.ndarray):\n        msg = f\"Expected `np.ndarray` from {key=} but got `{type(item).__name__}`.\"\n        raise TypeError(msg)\n\n    return item\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.NPYLoader.save","title":"<code>def save(obj, key)</code>   <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: np.ndarray, key: Path, /) -&gt; None:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    np.save(key, obj, allow_pickle=False)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader","title":"<code>class PDLoader</code>","text":"<p>         Bases: <code>PathLoader[DataFrame | Series]</code></p> <p>A <code>Loader</code> for loading and saving <code>pd.DataFrame</code>s.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".csv\"</code></li> <li><code>\".parquet\"</code></li> <li><code>\".pdpickle\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>pd.DataFrame</code></li> <li><code>pd.Series</code> - Only to <code>\".pdpickle\"</code> files</li> </ul> Multiindex support <p>There is currently no multi-index support as we explicitly use <code>index_col=0</code> when loading a <code>\".csv\"</code> file. This is because we assume that the first column is the index to prevent Unamed columns from being created.</p> Series support <p>There is currently limited support for pandas series as once written to csv/parquet, they are converted to a dataframe with a single column. See this issue</p> <p>Please consider using <code>\".pdpickle\"</code> instead.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    if key.suffix in (\".pdpickle\", None):\n        return check in (pd.Series, pd.DataFrame, None)\n\n    if key.suffix in (\".csv\", \".parquet\"):\n        return check in (pd.DataFrame, None)\n\n    return False\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    if key.suffix == \".pdpickle\":\n        return isinstance(obj, pd.Series | pd.DataFrame)\n\n    if key.suffix == \".parquet\":\n        return isinstance(obj, pd.DataFrame)\n\n    if key.suffix == \".csv\":\n        # TODO: https://github.com/automl/amltk/issues/4\n        return isinstance(obj, pd.DataFrame) and obj.index.nlevels == 1\n\n    return False\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.load","title":"<code>def load(key)</code>   <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; pd.DataFrame | pd.Series:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    if key.suffix == \".csv\":\n        return pd.read_csv(key, index_col=0)\n\n    if key.suffix == \".parquet\":\n        return pd.read_parquet(key)\n\n    if key.suffix == \".pdpickle\":\n        obj = pd.read_pickle(key)  # noqa: S301\n        if not isinstance(obj, pd.Series | pd.DataFrame):\n            msg = (\n                f\"Expected `pd.Series | pd.DataFrame` from {key=}\"\n                f\" but got `{type(obj).__name__}`.\"\n            )\n            raise TypeError(msg)\n\n        return obj\n\n    raise ValueError(f\"Unknown file extension {key.suffix}\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PDLoader.save","title":"<code>def save(obj, key)</code>   <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: pd.Series | pd.DataFrame, key: Path, /) -&gt; None:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    # Most pandas methods only seem to support dataframes\n    logger.debug(f\"Saving {key=}\")\n\n    if key.suffix == \".pdpickle\":\n        obj.to_pickle(key)\n        return\n\n    if key.suffix == \".csv\":\n        if obj.index.name is None and obj.index.nlevels == 1:\n            obj.index.name = \"index\"\n\n        obj.to_csv(key, index=True)\n        return\n\n    if key.suffix == \".parquet\":\n        obj.to_parquet(key)\n        return\n\n    raise ValueError(f\"Unknown extension {key.suffix=}\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader","title":"<code>class JSONLoader</code>","text":"<p>         Bases: <code>PathLoader[dict | list]</code></p> <p>A <code>Loader</code> for loading and saving <code>dict</code>s and <code>list</code>s to JSON.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".json\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>dict</code></li> <li><code>list</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix == \".json\" and check in (dict, list, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, dict | list) and key.suffix == \".json\"\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.load","title":"<code>def load(key)</code>   <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; dict | list:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"r\") as f:\n        item = json.load(f)\n\n    if not isinstance(item, dict | list):\n        msg = f\"Expected `dict | list` from {key=} but got `{type(item).__name__}`\"\n        raise TypeError(msg)\n\n    return item\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.JSONLoader.save","title":"<code>def save(obj, key)</code>   <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: dict | list, key: Path, /) -&gt; None:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"w\") as f:\n        json.dump(obj, f)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader","title":"<code>class YAMLLoader</code>","text":"<p>         Bases: <code>PathLoader[dict | list]</code></p> <p>A <code>Loader</code> for loading and saving <code>dict</code>s and <code>list</code>s to YAML.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".yaml\"</code></li> <li><code>\".yml\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>dict</code></li> <li><code>list</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".yaml\", \".yml\") and check in (dict, list, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, dict | list) and key.suffix in (\".yaml\", \".yml\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.load","title":"<code>def load(key)</code>   <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; dict | list:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    if yaml is None:\n        raise ModuleNotFoundError(\"PyYAML is not installed\")\n\n    with key.open(\"r\") as f:\n        item = yaml.safe_load(f)\n\n    if not isinstance(item, dict | list):\n        msg = f\"Expected `dict | list` from {key=} but got `{type(item).__name__}`\"\n        raise TypeError(msg)\n\n    return item\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.YAMLLoader.save","title":"<code>def save(obj, key)</code>   <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: dict | list, key: Path, /) -&gt; None:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    if yaml is None:\n        raise ModuleNotFoundError(\"PyYAML is not installed\")\n\n    with key.open(\"w\") as f:\n        yaml.dump(obj, f)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader","title":"<code>class PickleLoader</code>","text":"<p>         Bases: <code>PathLoader[Any]</code></p> <p>A <code>Loader</code> for loading and saving any object to a pickle file.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".pkl\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li>object</li> </ul> Picklability <p>This loader uses Python's built-in <code>pickle</code> module to save and load objects. This means that the object must be picklable in order to be saved and loaded. If the object is not picklable, then an error will be raised when attempting to save or load the object.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(\n    cls,\n    key: Path,\n    /,\n    *,\n    check: type | None = None,\n) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".pkl\", \".pickle\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return True  # Anything can be attempted to be pickled\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.load","title":"<code>def load(key)</code>   <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; Any:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"rb\") as f:\n        return pickle.load(f)  # noqa: S301\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.PickleLoader.save","title":"<code>def save(obj, key)</code>   <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"wb\") as f:\n        pickle.dump(obj, f)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader","title":"<code>class TxtLoader</code>","text":"<p>         Bases: <code>PathLoader[str]</code></p> <p>A <code>Loader</code> for loading and saving <code>str</code>s to text files.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".text\"</code></li> <li><code>\".txt\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>str</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.name","title":"<code>name: ClassVar</code>   <code>classvar</code> <code>attr</code>","text":"<p>The name of the loader.</p>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".text\", \".txt\") and check in (str, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, str) and key.suffix in (\".text\", \".txt\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.load","title":"<code>def load(key)</code>   <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; str:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"r\") as f:\n        return f.read()\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.TxtLoader.save","title":"<code>def save(obj, key)</code>   <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: str, key: Path, /) -&gt; None:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"w\") as f:\n        f.write(obj)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader","title":"<code>class ByteLoader</code>","text":"<p>         Bases: <code>PathLoader[bytes]</code></p> <p>A <code>Loader</code> for loading and saving <code>bytes</code> to binary files.</p> <p>This loader supports the following file extensions:</p> <ul> <li><code>\".bin\"</code></li> <li><code>\".bytes\"</code></li> </ul> <p>This loader supports the following types:</p> <ul> <li><code>bytes</code></li> </ul>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.can_load","title":"<code>def can_load(key, /, *, check=None)</code>   <code>classmethod</code>","text":"<p>Return True if this loader supports the resource at key.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> <code>check</code> <p>If the loader can support loading a specific type of object.</p> <p> TYPE: <code>type[T] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_load(cls, key: Path, /, *, check: type[T] | None = None) -&gt; bool:\n    \"\"\"Return True if this loader supports the resource at key.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        key: The key used to identify the resource\n        check: If the loader can support loading a specific type\n            of object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_load(cls, key: Path, /, *, check: type | None = None) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_load\"\"\"  # noqa: D415\n    return key.suffix in (\".bin\", \".bytes\") and check in (bytes, None)\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.can_save","title":"<code>def can_save(obj, key)</code>   <code>classmethod</code>","text":"<p>Return True if this loader can save this object.</p> <p>This is used to determine which loader to use when loading a resource from a key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key used to identify the resource</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"Return True if this loader can save this object.\n\n    This is used to determine which loader to use when loading a\n    resource from a key.\n\n    Args:\n        obj: The object to save.\n        key: The key used to identify the resource\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef can_save(cls, obj: Any, key: Path, /) -&gt; bool:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.can_save\"\"\"  # noqa: D415\n    return isinstance(obj, dict | list) and key.suffix in (\".bin\", \".bytes\")\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.load","title":"<code>def load(key)</code>   <code>classmethod</code>","text":"<p>Load an object from the given key.</p> PARAMETER  DESCRIPTION <code>key</code> <p>The key to load the object from.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The loaded object.</p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef load(cls, key: Path, /) -&gt; T:\n    \"\"\"Load an object from the given key.\n\n    Args:\n        key: The key to load the object from.\n\n    Returns:\n        The loaded object.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef load(cls, key: Path, /) -&gt; bytes:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.load\"\"\"  # noqa: D415\n    logger.debug(f\"Loading {key=}\")\n    with key.open(\"rb\") as f:\n        return f.read()\n</code></pre>"},{"location":"api/amltk/store/paths/path_loaders/#amltk.store.paths.path_loaders.ByteLoader.save","title":"<code>def save(obj, key)</code>   <code>classmethod</code>","text":"<p>Save an object to under the given key.</p> PARAMETER  DESCRIPTION <code>obj</code> <p>The object to save.</p> <p> TYPE: <code>Any</code> </p> <code>key</code> <p>The key to save the object under.</p> <p> TYPE: <code>Path</code> </p> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\n@abstractmethod\ndef save(cls, obj: Any, key: Path, /) -&gt; None:\n    \"\"\"Save an object to under the given key.\n\n    Args:\n        obj: The object to save.\n        key: The key to save the object under.\n    \"\"\"\n    ...\n</code></pre> Source code in <code>src/amltk/store/paths/path_loaders.py</code> <pre><code>@override\n@classmethod\ndef save(cls, obj: bytes, key: Path, /) -&gt; None:\n    \"\"\"::: amltk.store.paths.path_loaders.PathLoader.save\"\"\"  # noqa: D415\n    logger.debug(f\"Saving {key=}\")\n    with key.open(\"wb\") as f:\n        f.write(obj)\n</code></pre>"},{"location":"examples/","title":"Index","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>This houses the examples for the project. Use the navigation bar to the left to view more.</p>"},{"location":"examples/dask-jobqueue/","title":"Using the Scheduler with SLURM (dask-jobqueue)","text":"Expand to copy <code>examples/dask-jobqueue.py</code>  (top right) <pre><code>from typing import Any\n\nimport openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Component, Node, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn import split_data\nfrom amltk.store import PathBucket\n\nN_WORKERS = 32\nscheduler = Scheduler.with_slurm(\n    n_workers=N_WORKERS,  # Number of workers to launch\n    queue=\"the-name-of-the-partition/queue\",  # Name of the queue to submit to\n    cores=1,  # Number of cores per worker\n    memory=\"4 GB\",  # Memory per worker\n    walltime=\"00:20:00\",  # Walltime per worker\n    # submit_command=\"sbatch --extra-arguments\",  # Sometimes you need extra arguments to the launch command\n)\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                OneHotEncoder(drop=\"first\"),\n            ],\n            \"numerical\": Component(\n                SimpleImputer,\n                space={\"strategy\": [\"mean\", \"median\"]},\n            ),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\n\ndef target_function(trial: Trial, _pipeline: Node) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            trial.bucket[\"X_train.csv\"].load(),\n            trial.bucket[\"X_val.csv\"].load(),\n            trial.bucket[\"X_test.csv\"].load(),\n            trial.bucket[\"y_train.npy\"].load(),\n            trial.bucket[\"y_val.npy\"].load(),\n            trial.bucket[\"y_test.npy\"].load(),\n        )\n\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        sklearn_pipeline.fit(X_train, y_train)\n\n    if trial.exception:\n        trial.store({\"exception.txt\": f\"{trial.exception}\\n {trial.traceback}\"})\n        return trial.fail()\n\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_acc)\n\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\nbucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  # &lt;!&gt; (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\ntask = scheduler.task(target_function)\n\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline)\n\n\ntrial_history = History()\n\n\n@task.on_result\ndef process_result_and_launc(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n    optimizer.tell(report)\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline)\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    raise RuntimeError(\"Scheduler cancelled a worker!\")\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=60)\n\n    history_df = trial_history.df()\n    print(history_df)\n    print(len(history_df))\n</code></pre>"},{"location":"examples/dask-jobqueue/#description","title":"Description","text":"<p>The point of this example is to show how to set up <code>dask-jobqueue</code> with a realistic workload.</p> <p>Dependencies</p> <p>Requires the following integrations and dependencies:</p> <ul> <li><code>pip install openml amltk[smac, sklearn, dask-jobqueue]</code></li> </ul> <p>This example shows how to use <code>dask-jobqueue</code> to run HPO on a <code>RandomForestClassifier</code> with SMAC. This workload is borrowed from the HPO example.</p> <p>SMAC can not handle fast updates and seems to be quite efficient for this workload with ~32 cores.</p> <pre><code>from typing import Any\n\nimport openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Component, Node, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn import split_data\nfrom amltk.store import PathBucket\n\nN_WORKERS = 32\nscheduler = Scheduler.with_slurm(\n    n_workers=N_WORKERS,  # Number of workers to launch\n    queue=\"the-name-of-the-partition/queue\",  # Name of the queue to submit to\n    cores=1,  # Number of cores per worker\n    memory=\"4 GB\",  # Memory per worker\n    walltime=\"00:20:00\",  # Walltime per worker\n    # submit_command=\"sbatch --extra-arguments\",  # Sometimes you need extra arguments to the launch command\n)\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                OneHotEncoder(drop=\"first\"),\n            ],\n            \"numerical\": Component(\n                SimpleImputer,\n                space={\"strategy\": [\"mean\", \"median\"]},\n            ),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\n\ndef target_function(trial: Trial, _pipeline: Node) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            trial.bucket[\"X_train.csv\"].load(),\n            trial.bucket[\"X_val.csv\"].load(),\n            trial.bucket[\"X_test.csv\"].load(),\n            trial.bucket[\"y_train.npy\"].load(),\n            trial.bucket[\"y_val.npy\"].load(),\n            trial.bucket[\"y_test.npy\"].load(),\n        )\n\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        sklearn_pipeline.fit(X_train, y_train)\n\n    if trial.exception:\n        trial.store({\"exception.txt\": f\"{trial.exception}\\n {trial.traceback}\"})\n        return trial.fail()\n\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_acc)\n\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\nbucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  #  (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\ntask = scheduler.task(target_function)\n\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline)\n\n\ntrial_history = History()\n\n\n@task.on_result\ndef process_result_and_launc(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n    optimizer.tell(report)\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline)\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    raise RuntimeError(\"Scheduler cancelled a worker!\")\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=60)\n\n    history_df = trial_history.df()\n    print(history_df)\n    print(len(history_df))\n</code></pre>"},{"location":"examples/hpo/","title":"HPO","text":"Expand to copy <code>examples/hpo.py</code>  (top right) <pre><code>from typing import Any\n\nimport openml\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom amltk.sklearn import split_data\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom amltk.pipeline import Component, Node, Sequential, Split\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")],\n            \"numerical\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n\nfrom sklearn.metrics import accuracy_score\n\nfrom amltk.optimization import Trial\n\n\ndef target_function(trial: Trial, _pipeline: Node) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    # Load in data\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            trial.bucket[\"X_train.csv\"].load(),\n            trial.bucket[\"X_val.csv\"].load(),\n            trial.bucket[\"X_test.csv\"].load(),\n            trial.bucket[\"y_train.npy\"].load(),\n            trial.bucket[\"y_val.npy\"].load(),\n            trial.bucket[\"y_test.npy\"].load(),\n        )\n\n    # Configure the pipeline with the trial config before building it.\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    # Fit the pipeline, indicating when you want to start the trial timing and error\n    # catchnig.\n    with trial.begin():\n        sklearn_pipeline.fit(X_train, y_train)\n\n    # If an exception happened, we use `trial.fail` to indicate that the\n    # trial failed\n    if trial.exception:\n        trial.store({\"exception.txt\": f\"{trial.exception}\\n {trial.traceback}\"})\n        return trial.fail()\n\n    # Make our predictions with the model\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    # Save the scores to the summary of the trial\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    # Save all of this to the file system\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    # Finally report the success\n    return trial.success(accuracy=val_acc)\n\n\nfrom amltk.store import PathBucket\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\nbucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nprint(bucket)\nprint(dict(bucket))\nfrom amltk.optimization import Metric\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  # &lt;!&gt; (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\n\n# 1. You can also explicitly pass in the space of hyperparameters to optimize.\n#   ```python\n#   space = pipeline.search_space(\"configspace\")\n#   # or\n#   space = pipeline.search_space(SMACOptimizer.preffered_parser())\n#   ```\ntask = scheduler.task(target_function)\n\nprint(task)\n\n\n@scheduler.on_start\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline)\n\n\n\n\n@task.on_result\ndef tell_optimizer(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n\n\nfrom amltk.optimization import History\n\ntrial_history = History()\n\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n\n\n\n@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline)\n\n\n\n\n@task.on_exception\ndef stop_scheduler_on_exception(*_: Any) -&gt; None:\n    scheduler.stop()\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    scheduler.stop()\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=5)\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n</code></pre>"},{"location":"examples/hpo/#description","title":"Description","text":"<p>Dependencies</p> <p>Requires the following integrations and dependencies:</p> <ul> <li><code>pip install openml amltk[smac, sklearn]</code></li> </ul> <p>This example shows the basic of setting up a simple HPO loop around a <code>RandomForestClassifier</code>. We will use the OpenML to get a dataset and also use some static preprocessing as part of our pipeline definition.</p> <p>You can fine the pipeline guide here and the optimization guide here to learn more.</p> <p>You can skip the imports sections and go straight to the pipeline definition.</p>"},{"location":"examples/hpo/#dataset","title":"Dataset","text":"<p>Below is just a small function to help us get the dataset from OpenML and encode the labels.</p> <pre><code>from typing import Any\n\nimport openml\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom amltk.sklearn import split_data\n\n\ndef get_dataset(\n    dataset_id: str | int,\n    *,\n    seed: int,\n    splits: dict[str, float],\n) -&gt; dict[str, Any]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n\n    target_name = dataset.default_target_attribute\n    X, y, _, _ = dataset.get_data(dataset_format=\"dataframe\", target=target_name)\n    _y = LabelEncoder().fit_transform(y)\n\n    return split_data(X, _y, splits=splits, seed=seed)  # type: ignore\n</code></pre>"},{"location":"examples/hpo/#pipeline-definition","title":"Pipeline Definition","text":"<p>Here we define a pipeline which splits categoricals and numericals down two different paths, and then combines them back together before passing them to the <code>RandomForestClassifier</code>.</p> <p>For more on definitions of pipelines, see the Pipeline guide. <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom amltk.pipeline import Component, Node, Sequential, Split\n\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categorical\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")],\n            \"numerical\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n        },\n        name=\"feature_preprocessing\",\n    )\n    &gt;&gt; Component(\n        RandomForestClassifier,\n        space={\n            \"n_estimators\": (10, 100),\n            \"max_features\": (0.0, 1.0),\n            \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n        },\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n</code></pre></p>"},{"location":"examples/hpo/#target-function","title":"Target Function","text":"<p>The function we will optimize must take in a <code>Trial</code> and return a <code>Trial.Report</code>. We also pass in a <code>PathBucket</code> which is a dict-like view of the file system, where we have our dataset stored.</p> <p>We also pass in our pipeline, which we will use to build our sklearn pipeline with a specific <code>trial.config</code> suggested by the <code>Optimizer</code>. <pre><code>from sklearn.metrics import accuracy_score\n\nfrom amltk.optimization import Trial\n\n\ndef target_function(trial: Trial, _pipeline: Node) -&gt; Trial.Report:\n    trial.store({\"config.json\": trial.config})\n    # Load in data\n    with trial.profile(\"data-loading\"):\n        X_train, X_val, X_test, y_train, y_val, y_test = (\n            trial.bucket[\"X_train.csv\"].load(),\n            trial.bucket[\"X_val.csv\"].load(),\n            trial.bucket[\"X_test.csv\"].load(),\n            trial.bucket[\"y_train.npy\"].load(),\n            trial.bucket[\"y_val.npy\"].load(),\n            trial.bucket[\"y_test.npy\"].load(),\n        )\n\n    # Configure the pipeline with the trial config before building it.\n    sklearn_pipeline = _pipeline.configure(trial.config).build(\"sklearn\")\n\n    # Fit the pipeline, indicating when you want to start the trial timing and error\n    # catchnig.\n    with trial.begin():\n        sklearn_pipeline.fit(X_train, y_train)\n\n    # If an exception happened, we use `trial.fail` to indicate that the\n    # trial failed\n    if trial.exception:\n        trial.store({\"exception.txt\": f\"{trial.exception}\\n {trial.traceback}\"})\n        return trial.fail()\n\n    # Make our predictions with the model\n    with trial.profile(\"predictions\"):\n        train_predictions = sklearn_pipeline.predict(X_train)\n        val_predictions = sklearn_pipeline.predict(X_val)\n        test_predictions = sklearn_pipeline.predict(X_test)\n\n    with trial.profile(\"probabilities\"):\n        val_probabilites = sklearn_pipeline.predict_proba(X_val)\n\n    # Save the scores to the summary of the trial\n    with trial.profile(\"scoring\"):\n        train_acc = float(accuracy_score(train_predictions, y_train))\n        val_acc = float(accuracy_score(val_predictions, y_val))\n        test_acc = float(accuracy_score(test_predictions, y_test))\n\n    trial.summary[\"train/acc\"] = train_acc\n    trial.summary[\"val/acc\"] = val_acc\n    trial.summary[\"test/acc\"] = test_acc\n\n    # Save all of this to the file system\n    trial.store(\n        {\n            \"model.pkl\": sklearn_pipeline,\n            \"val_probabilities.npy\": val_probabilites,\n            \"val_predictions.npy\": val_predictions,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    # Finally report the success\n    return trial.success(accuracy=val_acc)\n</code></pre></p>"},{"location":"examples/hpo/#running-the-whole-thing","title":"Running the Whole Thing","text":"<p>Now we can run the whole thing. We will use the <code>Scheduler</code> to run the optimization, and the <code>SMACOptimizer</code> to optimize the pipeline.</p>"},{"location":"examples/hpo/#getting-and-storing-data","title":"Getting and storing data","text":"<p>We use a <code>PathBucket</code> to store the data. This is a dict-like view of the file system. <pre><code>from amltk.store import PathBucket\n\nseed = 42\ndata = get_dataset(31, seed=seed, splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2})\n\nX_train, y_train = data[\"train\"]\nX_val, y_val = data[\"val\"]\nX_test, y_test = data[\"test\"]\n\nbucket = PathBucket(\"example-hpo\", clean=True, create=True)\nbucket.store(\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nprint(bucket)\nprint(dict(bucket))\n</code></pre></p>"},{"location":"examples/hpo/#setting-up-the-scheduler-task-and-optimizer","title":"Setting up the Scheduler, Task and Optimizer","text":"<p>We use the <code>Scheduler.with_processes</code> method to create a <code>Scheduler</code> that will run the optimization.</p> <p>Please check out the full guides to learn more!</p> <p>We then create an <code>SMACOptimizer</code> which will optimize the pipeline. We pass in pipeline, and SMAC the optimizer will parser out the space of hyperparameters to optimize. <pre><code>from amltk.optimization import Metric\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\n\noptimizer = SMACOptimizer.create(\n    space=pipeline,  #  (1)!\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0)),\n    bucket=bucket,\n    seed=seed,\n)\n</code></pre></p> <ol> <li>You can also explicitly pass in the space of hyperparameters to optimize.   <pre><code>space = pipeline.search_space(\"configspace\")\n# or\nspace = pipeline.search_space(SMACOptimizer.preffered_parser())\n</code></pre></li> </ol> <p>Next we create a <code>Task</code>, passing in the function we want to run and the scheduler we will run it in. <pre><code>task = scheduler.task(target_function)\n\nprint(task)\n</code></pre> We use the callback decorators of the <code>Scheduler</code> and the <code>Task</code> to add callbacks that get called during events that happen during the running of the scheduler. Using this, we can control the flow of how things run. Check out the task guide for more.</p> <p>This one here asks the optimizer for a new trial when the scheduler starts and launches the task we created earlier with this trial. <pre><code>@scheduler.on_start\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, _pipeline=pipeline)\n</code></pre> When a <code>Task</code> returns and we get a report, i.e. with <code>task.success()</code> or <code>task.fail()</code>, the <code>task</code> will fire off the callbacks registered with <code>@on_result</code>. We can use these to add callbacks that get called when these events happen.</p> <p>Here we use it to update the optimizer with the report we got. <pre><code>@task.on_result\ndef tell_optimizer(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n</code></pre> We can use the <code>History</code> class to store the reports we get from the <code>Task</code>. We can then use this to analyze the results of the optimization afterwords. <pre><code>from amltk.optimization import History\n\ntrial_history = History()\n\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n</code></pre> We launch a new task when the scheduler is empty, i.e. when all the tasks have finished. This will keep going until we hit the timeout we set on the scheduler.</p> <p>If you want to run the optimization in parallel, you can use the <code>@task.on_result</code> callback to launch a new task when you get a report. This will launch a new task as soon as one finishes. <pre><code>@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    if scheduler.running():\n        trial = optimizer.ask()\n        task.submit(trial, _pipeline=pipeline)\n</code></pre> If something goes wrong, we likely want to stop the scheduler. <pre><code>@task.on_exception\ndef stop_scheduler_on_exception(*_: Any) -&gt; None:\n    scheduler.stop()\n\n\n@task.on_cancelled\ndef stop_scheduler_on_cancelled(_: Any) -&gt; None:\n    scheduler.stop()\n</code></pre></p>"},{"location":"examples/hpo/#setting-the-system-to-run","title":"Setting the system to run","text":"<p>Lastly we use <code>Scheduler.run</code> to run the scheduler. We pass in a timeout of 20 seconds. <pre><code>if __name__ == \"__main__\":\n    scheduler.run(timeout=5)\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n</code></pre></p>"},{"location":"examples/hpo_with_ensembling/","title":"Performing HPO with Post-Hoc Ensembling.","text":"Expand to copy <code>examples/hpo_with_ensembling.py</code>  (top right) <pre><code>from __future__ import annotations\n\nimport shutil\nfrom asyncio import Future\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport openml\nfrom sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    MinMaxScaler,\n    OneHotEncoder,\n    RobustScaler,\n    StandardScaler,\n)\nfrom sklearn.svm import SVC\n\nfrom amltk.data.conversions import probabilities_to_classes\nfrom amltk.ensembling.weighted_ensemble_caruana import weighted_ensemble_caruana\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Choice, Component, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn.data import split_data\nfrom amltk.store import PathBucket\n\n\n\ndef get_dataset(seed: int) -&gt; tuple[np.ndarray, ...]:\n    dataset = openml.datasets.get_dataset(\n        31,\n        download_qualities=False,\n        download_features_meta_data=False,\n        download_data=True,\n    )\n    X, y, _, _ = dataset.get_data(\n        dataset_format=\"dataframe\",\n        target=dataset.default_target_attribute,\n    )\n    _y = LabelEncoder().fit_transform(y)\n    splits = split_data(  # &lt;!&gt; (1)!\n        X,  # &lt;!&gt;\n        _y,  # &lt;!&gt;\n        splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2},  # &lt;!&gt;\n        seed=seed,  # &lt;!&gt;\n    )  # &lt;!&gt;\n\n    x_train, y_train = splits[\"train\"]\n    x_val, y_val = splits[\"val\"]\n    x_test, y_test = splits[\"test\"]\n    return x_train, x_val, x_test, y_train, y_val, y_test  # type: ignore\n\n\n# 1. We use the [`split_data()`][amltk.sklearn.data.split_data] function from the\n#   to split the data into a custom amount of splits, in this case\n#   `#!python \"train\", \"val\", \"test\"`. You could also use the\n#   dedicated [`train_val_test_split()`][amltk.sklearn.data.train_val_test_split]\n#   function instead.\npipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categories\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                Component(\n                    OneHotEncoder,\n                    space={\n                        \"min_frequency\": (0.01, 0.1),\n                        \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n                    },\n                    config={\"drop\": \"first\"},\n                ),\n            ],\n            \"numbers\": [\n                Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n                Component(VarianceThreshold, space={\"threshold\": (0.0, 0.2)}),\n                Choice(StandardScaler, MinMaxScaler, RobustScaler, name=\"scaler\"),\n            ],\n        },\n        name=\"feature_preprocessing\",\n        config={\n            \"categories\": make_column_selector(dtype_include=object),\n            \"numbers\": make_column_selector(dtype_include=np.number),\n        },\n    )\n    &gt;&gt; Choice(  # &lt;!&gt; (1)!\n        Component(SVC, space={\"C\": (0.1, 10.0)}, config={\"probability\": True}),\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n        ),\n        Component(\n            MLPClassifier,\n            space={\n                \"activation\": [\"identity\", \"logistic\", \"relu\"],\n                \"alpha\": (0.0001, 0.1),\n                \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n            },\n        ),\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n\n# 1. Here we define a choice of algorithms to use where each entry is a possible\n#   algorithm to use. Each algorithm is defined by a step, which is a\n#   configuration of a sklearn estimator. The space parameter is a dictionary\n#   of hyperparameters to optimize over, and the config parameter is a\n#   dictionary of fixed parameters to set on the estimator.\n# 2. Here we gropu the numerical preprocessing steps to use. Each step is a\n#  scaler to use. Each scaler is defined by a step, which is a configuration\n#  of the preprocessor. The space parameter is a dictionary of\n#  hyperparameters to optimize over, and the config parameter is a dictionary\n#  of fixed parameters to set on the preprocessing step.\n# 3. Here we group the categorical preprocessing steps to use.\n#   Each step is given a space, which is a dictionary of hyperparameters to\n#   optimize over, and a config, which is a dictionary of fixed parameters to\n#   set on the preprocessing step.\n\n\ndef target_function(\n    trial: Trial,\n    bucket: PathBucket,\n    pipeline: Sequential,\n) -&gt; Trial.Report:\n    X_train, X_val, X_test, y_train, y_val, y_test = (  # (1)!\n        bucket[\"X_train.csv\"].load(),\n        bucket[\"X_val.csv\"].load(),\n        bucket[\"X_test.csv\"].load(),\n        bucket[\"y_train.npy\"].load(),\n        bucket[\"y_val.npy\"].load(),\n        bucket[\"y_test.npy\"].load(),\n    )\n    pipeline = pipeline.configure(trial.config)  # &lt;!&gt; (2)!\n    sklearn_pipeline = pipeline.build(\"sklearn\")  # &lt;!&gt;\n\n    with trial.begin():  # &lt;!&gt; (3)!\n        sklearn_pipeline.fit(X_train, y_train)\n\n    if trial.exception:\n        trial.store(\n            {\n                \"exception.txt\": str(trial.exception),\n                \"config.json\": dict(trial.config),\n                \"traceback.txt\": str(trial.traceback),\n            },\n        )\n\n        return trial.fail()  # &lt;!&gt; (4)!\n\n    # Make our predictions with the model\n    train_predictions = sklearn_pipeline.predict(X_train)\n    val_predictions = sklearn_pipeline.predict(X_val)\n    test_predictions = sklearn_pipeline.predict(X_test)\n\n    val_probabilites = sklearn_pipeline.predict_proba(X_val)\n    val_accuracy = float(accuracy_score(val_predictions, y_val))\n\n    # Save the scores to the summary of the trial\n    trial.summary[\"train_accuracy\"] = float(accuracy_score(train_predictions, y_train))\n    trial.summary[\"validation_accuracy\"] = val_accuracy\n    trial.summary[\"test_accuracy\"] = float(accuracy_score(test_predictions, y_test))\n\n    # Save all of this to the file system\n    trial.store(  # (5)!\n        {\n            \"config.json\": dict(trial.config),\n            \"scores.json\": trial.summary,\n            \"model.pkl\": sklearn_pipeline,\n            \"val_predictions.npy\": val_predictions,\n            \"val_probabilities.npy\": val_probabilites,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_accuracy)  # &lt;!&gt; (6)!\n\n\n# 1. We can easily load data from a [`PathBucket`][amltk.store.PathBucket]\n#   using the `load` method.\n# 2. We configure the pipeline with a specific set of hyperparameters suggested\n#  by the optimizer through the [`Trial`][amltk.optimization.Trial] object.\n# 3. We begin the trial by timing the execution of the target function and capturing\n#  any potential exceptions.\n# 4. If the trial failed, we return a failed report with a cost of infinity.\n# 5. We save the results of the trial using\n#   [`Trial.store`][amltk.optimization.Trial.store], creating a subdirectory\n#   for this trial.\n# 6. We return a successful report with the cost of the trial, which is the\n# inverse of the validation accuracy.\n\n\n@dataclass\nclass Ensemble:\n    weights: dict[str, float]\n    trajectory: list[tuple[str, float]]\n    configs: dict[str, dict[str, Any]]\n\n\ndef create_ensemble(\n    history: History,\n    bucket: PathBucket,\n    /,\n    size: int = 5,\n    seed: int = 42,\n) -&gt; Ensemble:\n    if len(history) == 0:\n        return Ensemble({}, [], {})\n\n    validation_predictions = {\n        report.name: report.retrieve(\"val_probabilities.npy\", where=bucket)\n        for report in history\n    }\n    targets = bucket[\"y_val.npy\"].load()\n\n    accuracy: Callable[[np.ndarray, np.ndarray], float] = accuracy_score  # type: ignore\n\n    def _score(_targets: np.ndarray, ensembled_probabilities: np.ndarray) -&gt; float:\n        predictions = probabilities_to_classes(ensembled_probabilities, classes=[0, 1])\n        return accuracy(_targets, predictions)\n\n    weights, trajectory, final_probabilities = weighted_ensemble_caruana(  # &lt;!&gt;\n        model_predictions=validation_predictions,  # &lt;!&gt;\n        targets=targets,  # &lt;!&gt;\n        size=size,  # &lt;!&gt;\n        metric=_score,  # &lt;!&gt;\n        select=max,  # &lt;!&gt;\n        seed=seed,  # &lt;!&gt;\n    )  # &lt;!&gt;\n\n    configs = {\n        name: history.find(name).retrieve(\"config.json\", where=bucket)\n        for name in weights\n    }\n    return Ensemble(weights=weights, trajectory=trajectory, configs=configs)\n\n\nseed = 42\n\nX_train, X_val, X_test, y_train, y_val, y_test = get_dataset(seed)  # (1)!\n\npath = Path(\"example-hpo-with-ensembling\")\nif path.exists():\n    shutil.rmtree(path)\n\nbucket = PathBucket(path)\nbucket.store(  # (2)!\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nscheduler = Scheduler.with_processes()  # (3)!\noptimizer = SMACOptimizer.create(\n    space=pipeline,\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0, 1)),\n    bucket=path,\n    seed=seed,\n)  # (4)!\n\ntask = scheduler.task(target_function)  # (6)!\nensemble_task = scheduler.task(create_ensemble)  # (7)!\n\ntrial_history = History()\nensembles: list[Ensemble] = []\n\n\n@scheduler.on_start  # (8)!\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@task.on_result\ndef tell_optimizer(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n\n\n@task.on_result\ndef add_to_history(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n\n\n@task.on_result\ndef launch_ensemble_task(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When a task successfully completes, launch an ensemble task.\"\"\"\n    if report.status is Trial.Status.SUCCESS:\n        ensemble_task.submit(trial_history, bucket)\n\n\n@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@ensemble_task.on_result\ndef save_ensemble(future: Future, ensemble: Ensemble) -&gt; None:\n    \"\"\"When an ensemble task returns, save it.\"\"\"\n    ensembles.append(ensemble)\n\n\n@ensemble_task.on_exception\ndef print_ensemble_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@task.on_exception\ndef print_task_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@scheduler.on_timeout\ndef run_last_ensemble_task() -&gt; None:\n    \"\"\"When the scheduler is empty, run the last ensemble task.\"\"\"\n    ensemble_task.submit(trial_history, bucket)\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=5, wait=True)  # (9)!\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n\n    best_ensemble = max(ensembles, key=lambda e: e.trajectory[-1])\n\n    print(\"Best ensemble:\")\n    print(best_ensemble)\n# 1. We use `#!python get_dataset()` defined earlier to load the\n#  dataset.\n# 2. We use [`store()`][amltk.store.Bucket.store] to store the data in the bucket, with\n# each key being the name of the file and the value being the data.\n# 3. We use [`Scheduler.with_processes()`][amltk.scheduling.Scheduler.with_processes]\n#  create a [`Scheduler`][amltk.scheduling.Scheduler] that runs everything\n#  in a different process. You can of course use a different backend if you want.\n# 4. We use [`SMACOptimizer.create()`][amltk.optimization.optimizers.smac.SMACOptimizer.create] to create a\n#  [`SMACOptimizer`][amltk.optimization.optimizers.smac.SMACOptimizer] given the space from the pipeline\n#  to optimize over.\n# 6. We create a [`Task`][amltk.scheduling.Task] that will run our objective, passing\n#   in the function to run and the scheduler for where to run it\n# 7. We use [`task()`][amltk.scheduling.Task] to create a\n#   [`Task`][amltk.scheduling.Task]\n#   for the `create_ensemble` method above. This will also run in parallel with the hpo\n#   trials if using a non-sequential scheduling mode.\n# 8. We use `@scheduler.on_start()` hook to register a\n#  callback that will be called when the scheduler starts. We can use the\n#  `repeat` argument to make sure it's called many times if we want.\n# 9. We use [`Scheduler.run()`][amltk.scheduling.Scheduler.run] to run the scheduler.\n#  Here we set it to run briefly for 5 seconds and wait for remaining tasks to finish\n#  before continuing.\n</code></pre>"},{"location":"examples/hpo_with_ensembling/#description","title":"Description","text":"<p>Dependencies</p> <p>Requires the following integrations and dependencies:</p> <ul> <li><code>pip install openml amltk[smac, sklearn]</code></li> </ul> <p>This example performs Hyperparameter optimization on a fairly default data-preprocessing + model sklearn pipeline, using a dataset pulled from OpenML.</p> <p>After the HPO is complete, we use the validation predictions from each trial to create an ensemble using the Weighted Ensemble algorithm from Caruana et al. (2004).</p> Reference: Ensemble selection from libraries of models <p>Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew and Alex Ksikes</p> <p>ICML 2004</p> <p>dl.acm.org/doi/10.1145/1015330.1015432</p> <p>www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf</p> <p>This makes heavy use of the pipelines and the optimization faculties of amltk. You can fine the pipeline guide here and the optimization guide here to learn more.</p> <p>You can skip the imports sections and go straight to the pipeline definition.</p>"},{"location":"examples/hpo_with_ensembling/#imports","title":"Imports","text":"<p><pre><code>from __future__ import annotations\n\nimport shutil\nfrom asyncio import Future\nfrom collections.abc import Callable\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport openml\nfrom sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    MinMaxScaler,\n    OneHotEncoder,\n    RobustScaler,\n    StandardScaler,\n)\nfrom sklearn.svm import SVC\n\nfrom amltk.data.conversions import probabilities_to_classes\nfrom amltk.ensembling.weighted_ensemble_caruana import weighted_ensemble_caruana\nfrom amltk.optimization import History, Metric, Trial\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.pipeline import Choice, Component, Sequential, Split\nfrom amltk.scheduling import Scheduler\nfrom amltk.sklearn.data import split_data\nfrom amltk.store import PathBucket\n</code></pre> Below is just a small function to help us get the dataset from OpenML and encode the labels. <pre><code>def get_dataset(seed: int) -&gt; tuple[np.ndarray, ...]:\n    dataset = openml.datasets.get_dataset(\n        31,\n        download_qualities=False,\n        download_features_meta_data=False,\n        download_data=True,\n    )\n    X, y, _, _ = dataset.get_data(\n        dataset_format=\"dataframe\",\n        target=dataset.default_target_attribute,\n    )\n    _y = LabelEncoder().fit_transform(y)\n    splits = split_data(  #  (1)!\n        X,  \n        _y,  \n        splits={\"train\": 0.6, \"val\": 0.2, \"test\": 0.2},  \n        seed=seed,  \n    )  \n\n    x_train, y_train = splits[\"train\"]\n    x_val, y_val = splits[\"val\"]\n    x_test, y_test = splits[\"test\"]\n    return x_train, x_val, x_test, y_train, y_val, y_test  # type: ignore\n</code></pre></p> <ol> <li>We use the <code>split_data()</code> function from the   to split the data into a custom amount of splits, in this case   <code>\"train\", \"val\", \"test\"</code>. You could also use the   dedicated <code>train_val_test_split()</code>   function instead.</li> </ol>"},{"location":"examples/hpo_with_ensembling/#pipeline-definition","title":"Pipeline Definition","text":"<p>Here we define a pipeline which splits categoricals and numericals down two different paths, and then combines them back together before passing them to a choice of classifier between a Random Forest, Support Vector Machine, and Multi-Layer Perceptron.</p> <p>For more on definitions of pipelines, see the Pipeline guide. <pre><code>pipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Split(\n        {\n            \"categories\": [\n                SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n                Component(\n                    OneHotEncoder,\n                    space={\n                        \"min_frequency\": (0.01, 0.1),\n                        \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n                    },\n                    config={\"drop\": \"first\"},\n                ),\n            ],\n            \"numbers\": [\n                Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n                Component(VarianceThreshold, space={\"threshold\": (0.0, 0.2)}),\n                Choice(StandardScaler, MinMaxScaler, RobustScaler, name=\"scaler\"),\n            ],\n        },\n        name=\"feature_preprocessing\",\n        config={\n            \"categories\": make_column_selector(dtype_include=object),\n            \"numbers\": make_column_selector(dtype_include=np.number),\n        },\n    )\n    &gt;&gt; Choice(  #  (1)!\n        Component(SVC, space={\"C\": (0.1, 10.0)}, config={\"probability\": True}),\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n        ),\n        Component(\n            MLPClassifier,\n            space={\n                \"activation\": [\"identity\", \"logistic\", \"relu\"],\n                \"alpha\": (0.0001, 0.1),\n                \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n            },\n        ),\n    )\n)\n\nprint(pipeline)\nprint(pipeline.search_space(\"configspace\"))\n</code></pre></p> <ol> <li>Here we define a choice of algorithms to use where each entry is a possible   algorithm to use. Each algorithm is defined by a step, which is a   configuration of a sklearn estimator. The space parameter is a dictionary   of hyperparameters to optimize over, and the config parameter is a   dictionary of fixed parameters to set on the estimator.</li> <li>Here we gropu the numerical preprocessing steps to use. Each step is a  scaler to use. Each scaler is defined by a step, which is a configuration  of the preprocessor. The space parameter is a dictionary of  hyperparameters to optimize over, and the config parameter is a dictionary  of fixed parameters to set on the preprocessing step.</li> <li>Here we group the categorical preprocessing steps to use.   Each step is given a space, which is a dictionary of hyperparameters to   optimize over, and a config, which is a dictionary of fixed parameters to   set on the preprocessing step.</li> </ol>"},{"location":"examples/hpo_with_ensembling/#target-function","title":"Target Function","text":"<p>Next we establish the actual target function we wish to evaluate, that is, the function we wish to optimize. In this case, we are optimizing the accuracy of the model on the validation set.</p> <p>The target function takes a <code>Trial</code> object, which has the configuration of the pipeline to evaluate and provides utility to time, and return the results of the evaluation, whether it be a success or failure.</p> <p>We make use of a <code>PathBucket</code> to store and load the data, and the <code>Pipeline</code> we defined above to configure the pipeline with the hyperparameters we are optimizing over.</p> <p>For more details, please check out the Optimization guide for more details. <pre><code>def target_function(\n    trial: Trial,\n    bucket: PathBucket,\n    pipeline: Sequential,\n) -&gt; Trial.Report:\n    X_train, X_val, X_test, y_train, y_val, y_test = (  # (1)!\n        bucket[\"X_train.csv\"].load(),\n        bucket[\"X_val.csv\"].load(),\n        bucket[\"X_test.csv\"].load(),\n        bucket[\"y_train.npy\"].load(),\n        bucket[\"y_val.npy\"].load(),\n        bucket[\"y_test.npy\"].load(),\n    )\n    pipeline = pipeline.configure(trial.config)  #  (2)!\n    sklearn_pipeline = pipeline.build(\"sklearn\")  \n\n    with trial.begin():  #  (3)!\n        sklearn_pipeline.fit(X_train, y_train)\n\n    if trial.exception:\n        trial.store(\n            {\n                \"exception.txt\": str(trial.exception),\n                \"config.json\": dict(trial.config),\n                \"traceback.txt\": str(trial.traceback),\n            },\n        )\n\n        return trial.fail()  #  (4)!\n\n    # Make our predictions with the model\n    train_predictions = sklearn_pipeline.predict(X_train)\n    val_predictions = sklearn_pipeline.predict(X_val)\n    test_predictions = sklearn_pipeline.predict(X_test)\n\n    val_probabilites = sklearn_pipeline.predict_proba(X_val)\n    val_accuracy = float(accuracy_score(val_predictions, y_val))\n\n    # Save the scores to the summary of the trial\n    trial.summary[\"train_accuracy\"] = float(accuracy_score(train_predictions, y_train))\n    trial.summary[\"validation_accuracy\"] = val_accuracy\n    trial.summary[\"test_accuracy\"] = float(accuracy_score(test_predictions, y_test))\n\n    # Save all of this to the file system\n    trial.store(  # (5)!\n        {\n            \"config.json\": dict(trial.config),\n            \"scores.json\": trial.summary,\n            \"model.pkl\": sklearn_pipeline,\n            \"val_predictions.npy\": val_predictions,\n            \"val_probabilities.npy\": val_probabilites,\n            \"test_predictions.npy\": test_predictions,\n        },\n    )\n\n    return trial.success(accuracy=val_accuracy)  #  (6)!\n</code></pre></p> <ol> <li>We can easily load data from a <code>PathBucket</code>   using the <code>load</code> method.</li> <li>We configure the pipeline with a specific set of hyperparameters suggested  by the optimizer through the <code>Trial</code> object.</li> <li>We begin the trial by timing the execution of the target function and capturing  any potential exceptions.</li> <li>If the trial failed, we return a failed report with a cost of infinity.</li> <li>We save the results of the trial using   <code>Trial.store</code>, creating a subdirectory   for this trial.</li> <li>We return a successful report with the cost of the trial, which is the inverse of the validation accuracy.</li> </ol> <p>Next we define a simple <code>@dataclass</code> to store the our definition of an Esemble, which is simply a collection of the models trial names to their weight in the ensemble. We also store the trajectory of the ensemble, which is a list of tuples of the trial name and the weight of the trial at that point in the trajectory. Finally, we store the configuration of each trial in the ensemble.</p> <p>We could of course add extra functionality to the Ensemble, give it references to the <code>PathBucket</code> and the pipeline objects, and even add methods to train the ensemble, but for the sake of simplicity we will leave it as is. <pre><code>@dataclass\nclass Ensemble:\n    weights: dict[str, float]\n    trajectory: list[tuple[str, float]]\n    configs: dict[str, dict[str, Any]]\n\n\ndef create_ensemble(\n    history: History,\n    bucket: PathBucket,\n    /,\n    size: int = 5,\n    seed: int = 42,\n) -&gt; Ensemble:\n    if len(history) == 0:\n        return Ensemble({}, [], {})\n\n    validation_predictions = {\n        report.name: report.retrieve(\"val_probabilities.npy\", where=bucket)\n        for report in history\n    }\n    targets = bucket[\"y_val.npy\"].load()\n\n    accuracy: Callable[[np.ndarray, np.ndarray], float] = accuracy_score  # type: ignore\n\n    def _score(_targets: np.ndarray, ensembled_probabilities: np.ndarray) -&gt; float:\n        predictions = probabilities_to_classes(ensembled_probabilities, classes=[0, 1])\n        return accuracy(_targets, predictions)\n\n    weights, trajectory, final_probabilities = weighted_ensemble_caruana(  \n        model_predictions=validation_predictions,  \n        targets=targets,  \n        size=size,  \n        metric=_score,  \n        select=max,  \n        seed=seed,  \n    )  \n\n    configs = {\n        name: history.find(name).retrieve(\"config.json\", where=bucket)\n        for name in weights\n    }\n    return Ensemble(weights=weights, trajectory=trajectory, configs=configs)\n</code></pre></p>"},{"location":"examples/hpo_with_ensembling/#main","title":"Main","text":"<p>Finally we come to the main script that runs everything. <pre><code>seed = 42\n\nX_train, X_val, X_test, y_train, y_val, y_test = get_dataset(seed)  # (1)!\n\npath = Path(\"example-hpo-with-ensembling\")\nif path.exists():\n    shutil.rmtree(path)\n\nbucket = PathBucket(path)\nbucket.store(  # (2)!\n    {\n        \"X_train.csv\": X_train,\n        \"X_val.csv\": X_val,\n        \"X_test.csv\": X_test,\n        \"y_train.npy\": y_train,\n        \"y_val.npy\": y_val,\n        \"y_test.npy\": y_test,\n    },\n)\n\nscheduler = Scheduler.with_processes()  # (3)!\noptimizer = SMACOptimizer.create(\n    space=pipeline,\n    metrics=Metric(\"accuracy\", minimize=False, bounds=(0, 1)),\n    bucket=path,\n    seed=seed,\n)  # (4)!\n\ntask = scheduler.task(target_function)  # (6)!\nensemble_task = scheduler.task(create_ensemble)  # (7)!\n\ntrial_history = History()\nensembles: list[Ensemble] = []\n\n\n@scheduler.on_start  # (8)!\ndef launch_initial_tasks() -&gt; None:\n    \"\"\"When we start, launch `n_workers` tasks.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@task.on_result\ndef tell_optimizer(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, tell the optimizer.\"\"\"\n    optimizer.tell(report)\n\n\n@task.on_result\ndef add_to_history(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When we get a report, print it.\"\"\"\n    trial_history.add(report)\n\n\n@task.on_result\ndef launch_ensemble_task(future: Future, report: Trial.Report) -&gt; None:\n    \"\"\"When a task successfully completes, launch an ensemble task.\"\"\"\n    if report.status is Trial.Status.SUCCESS:\n        ensemble_task.submit(trial_history, bucket)\n\n\n@task.on_result\ndef launch_another_task(*_: Any) -&gt; None:\n    \"\"\"When we get a report, evaluate another trial.\"\"\"\n    trial = optimizer.ask()\n    task.submit(trial, bucket=bucket, pipeline=pipeline)\n\n\n@ensemble_task.on_result\ndef save_ensemble(future: Future, ensemble: Ensemble) -&gt; None:\n    \"\"\"When an ensemble task returns, save it.\"\"\"\n    ensembles.append(ensemble)\n\n\n@ensemble_task.on_exception\ndef print_ensemble_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@task.on_exception\ndef print_task_exception(future: Future[Any], exception: BaseException) -&gt; None:\n    \"\"\"When an exception occurs, log it and stop.\"\"\"\n    print(exception)\n    scheduler.stop()\n\n\n@scheduler.on_timeout\ndef run_last_ensemble_task() -&gt; None:\n    \"\"\"When the scheduler is empty, run the last ensemble task.\"\"\"\n    ensemble_task.submit(trial_history, bucket)\n\n\nif __name__ == \"__main__\":\n    scheduler.run(timeout=5, wait=True)  # (9)!\n\n    print(\"Trial history:\")\n    history_df = trial_history.df()\n    print(history_df)\n\n    best_ensemble = max(ensembles, key=lambda e: e.trajectory[-1])\n\n    print(\"Best ensemble:\")\n    print(best_ensemble)\n</code></pre></p> <ol> <li>We use <code>get_dataset()</code> defined earlier to load the  dataset.</li> <li>We use <code>store()</code> to store the data in the bucket, with each key being the name of the file and the value being the data.</li> <li>We use <code>Scheduler.with_processes()</code>  create a <code>Scheduler</code> that runs everything  in a different process. You can of course use a different backend if you want.</li> <li>We use <code>SMACOptimizer.create()</code> to create a  <code>SMACOptimizer</code> given the space from the pipeline  to optimize over.</li> <li>We create a <code>Task</code> that will run our objective, passing   in the function to run and the scheduler for where to run it</li> <li>We use <code>task()</code> to create a   <code>Task</code>   for the <code>create_ensemble</code> method above. This will also run in parallel with the hpo   trials if using a non-sequential scheduling mode.</li> <li>We use <code>@scheduler.on_start()</code> hook to register a  callback that will be called when the scheduler starts. We can use the  <code>repeat</code> argument to make sure it's called many times if we want.</li> <li>We use <code>Scheduler.run()</code> to run the scheduler.  Here we set it to run briefly for 5 seconds and wait for remaining tasks to finish  before continuing.</li> </ol>"},{"location":"guides/","title":"Index","text":"<p>The guides here serve as a well-structured introduction to the capabilities of AutoML-Toolkit. Notably, we have three core concepts at the heart of AutoML-Toolkit, with supporting types and auxiliary functionality to enable these concepts.</p> <p>These take the form of a scheduling, a pipeline construction and optimization. By combining these concepts, we provide an extensive framework from which to do AutoML research, utilize AutoML for you task or build brand new AutoML systems.</p> <ul> <li> <p>Scheduling</p> <p>Dealing with multiple processes and simultaneous compute, can be both difficult in terms of understanding and utilization. Often a prototype script just doesn't work when you need to run larger experiments.</p> <p>We provide an event-driven system with a flexible backend, to help you write code that scales from just a few more cores on your machine to utilizing an entire cluster.</p> <p>This guide introduces <code>Task</code>s and the <code>Scheduler</code> in which they run, as well as <code>@events</code> which you can subscribe callbacks to. Define what should run, when it should run and simply define a callback to say what should happen when it's done.</p> <p>This framework allows you to write code that simply scales, with as little code change required as possible. Go from a single local process to an entire cluster with the same script and 5 lines of code.</p> <p>Checkout the Scheduling guide! for the full guide. We also cover some of these topics in brief detail in the reference pages.</p> <p>Notable Features</p> <ul> <li>A system that allows incremental and encapsulated feature addition.</li> <li>An <code>@event</code> system with easy to use callbacks.</li> <li>Place constraints and modify your <code>Task</code>     with <code>Plugins</code></li> <li>Integrations for different backends for where to run your tasks.</li> <li>A wide set of events to plug into.</li> <li>An easy way to extend the functionality provided with your own set of domain or task     specific events.</li> </ul> </li> </ul> <ul> <li> <p>Pipelines</p> <p>Optimizer require some search space to optimize, yet provide no utility to actually define these search space. When scaling beyond a simple single model, these search space become harder to define, difficult to extend and are often disjoint from the actual pipeline creation. When you want to create search spaces that can have choices between models, parametrized pre-processing and a method to quickly change these setups, it can often feel tedious and error-prone</p> <p>By piecing together <code>Node</code>s of a pipeline, utilizing a set of different building blocks such as a <code>Component</code>, <code>Sequential</code>, <code>Choice</code>es and more, you can abstractly define your entire pipeline. Once you're done, we'll stitch together the entire <code>search_space()</code>, allow you to easily <code>configure()</code> it and finally <code>build()</code> it into a concrete object you can use, all in the same place.</p> <p>Checkout the Pipeline guide! We also cover some of these topics in brief detail in the reference pages.</p> <p>Notable Features</p> <ul> <li>An easy, declaritive pipeline structure, allowing for rapid addition, deletion and   modification during experimentation.</li> <li>A flexible pipeline capable of handling complex structures and subpipelines.</li> <li>Mutliple component types to help you define your pipeline.</li> <li>Exporting of pipelines into concrete implementations like an sklearn.pipeline.Pipeline   for use in your downstream tasks.</li> <li>Extensible to add your own component types and <code>builder=</code>s to use.</li> </ul> </li> </ul> <ul> <li> <p>Optimization</p> <p>An optimizer is the backbone behind many AutoML systems and the quickest way to improve the performance of your current pipelines. However optimizer's vary in terms of how they expect you to write code, they vary in how much control they take of your code and can be quite difficult to interact with other than their <code>run()</code> function.</p> <p>By setting a simple expectation on an <code>Optimizer</code>, e.g. that it should have an <code>ask()</code> and <code>tell()</code>, you are placed get back in terms of defining the loop, define what happens, when and you can store what you'd like to record and put it where you'd like to put it.</p> <p>By unifying their suggestions as a <code>Trial</code> and a convenient <code>Report</code> to hand back to them, you can switch between optimizers with minimal changes required. We have added a load of utility to the <code>Trial</code>'s, such that you can easily profile sections, add extra summary information, store artifacts and export DataFrames.</p> <p>Checkout the Optimization guide. We recommend reading the previous two guides to fully understand the possibilities with optimization. We also cover some of these topics in brief detail in the reference pages.</p> <p>Notable Features</p> <ul> <li>An assortment of different optimizers for you to swap in an out with relative ease through a unified interface.</li> <li>A suite of utilities to help you record that data you want from your HPO experiments.</li> <li>Full control of how you interact with it, allowing for easy warm-starting, complex swapping mechanisms or custom stopping criterion.</li> <li>A simple interface to integrate in your own optimizer.</li> </ul> </li> </ul>"},{"location":"guides/optimization/","title":"Optimization Guide","text":"<p>One of the core tasks of any AutoML system is to optimize some objective, whether it be some pipeline, a black-box or even a toy function. In the context of AMLTK, this means defining some <code>Metric(s)</code> to optimize and creating an <code>Optimizer</code> to optimize them.</p> <p>You can check out the integrated optimizers in our optimizer reference</p> <p>This guide relies lightly on topics covered in the Pipeline Guide for creating a pipeline but also the Scheduling guide for creating a <code>Scheduler</code> and a <code>Task</code>. These aren't required but if something is not clear or you'd like to know how something works, please refer to these guides or the reference!</p>"},{"location":"guides/optimization/#optimizing-a-1-d-function","title":"Optimizing a 1-D function","text":"<p>We'll start with a simple example of maximizing a polynomial function The first thing to do is define the function we want to optimize.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef poly(x):\n    return (x**2 + 4*x + 3) / x\n\nfig, ax = plt.subplots()\nx = np.linspace(-10, 10, 100)\nax.plot(x, poly(x))\n</code></pre> <p> 2023-12-12T03:40:58.708501 image/svg+xml Matplotlib v3.8.2, https://matplotlib.org/ </p> <p>Our next step is to define the search range over which we want to optimize, in this case, the range of values <code>x</code> can take. Here we use a simple <code>Searchable</code>, however we can reprsent entire machine learning pipelines, with conditonality and much more complex ranges. (Pipeline guide)</p> <p>Vocab...</p> <p>When dealing with such functions, one might call the <code>x</code> just a parameter. However in the context of Machine Learning, if this <code>poly()</code> function was more like <code>train_model()</code>, then we would refer to <code>x</code> as a hyperparameter with it's range as it's search space.</p> <pre><code>from amltk.pipeline import Searchable\n\ndef poly(x: float) -&gt; float:\n    return (x**2 + 4*x + 3) / x\n\ns = Searchable(\n    {\"x\": (-10.0, 10.0)},\n    name=\"my-searchable\"\n)\nfrom amltk._doc import doc_print; doc_print(print, s)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Searchable(my-searchable) \u2500\u256e\n\u2502 space {'x': (-10.0, 10.0)}  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"guides/optimization/#creating-an-optimizer","title":"Creating an Optimizer","text":"<p>We'll utilize SMAC here for optimization as an example but you can find other available optimizers here.</p> Requirements <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[smac]\n\n# Or directly\npip install smac\n</code></pre> <p>The first thing we'll need to do is create a <code>Metric</code> a definition of some value we want to optimize.</p> <pre><code>from amltk.optimization import Metric\n\nmetric = Metric(\"score\", minimize=False)\nprint(metric)\n</code></pre> <pre><code>score (maximize)\n</code></pre> <p>The next step is to actually create an optimizer, you'll have to refer to their reference documentation. However, for most integrated optimizers, we expose a helpful <code>create()</code>.</p> <pre><code>from amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.optimization import Metric\nfrom amltk.pipeline import Searchable\n\ndef poly(x: float) -&gt; float:\n    return (x**2 + 4*x + 3) / x\n\nmetric = Metric(\"score\", minimize=False)\nspace = Searchable(space={\"x\": (-10.0, 10.0)}, name=\"my-searchable\")\n\noptimizer = SMACOptimizer.create(space=space, metrics=metric, seed=42)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"guides/optimization/#running-an-optimizer","title":"Running an Optimizer","text":"<p>At this point, we can begin optimizing our function, using the <code>ask</code> to get <code>Trial</code>s and <code>tell</code> methods with <code>Trial.Report</code>s.</p> <pre><code>from amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.optimization import Metric, History, Trial\nfrom amltk.pipeline import Searchable\n\ndef poly(x: float) -&gt; float:\n    return (x**2 + 4*x + 3) / x\n\nmetric = Metric(\"score\", minimize=False)\nspace = Searchable(space={\"x\": (-10.0, 10.0)}, name=\"my-searchable\")\n\noptimizer = SMACOptimizer.create(space=space, metrics=metric, seed=42)\n\nhistory = History()\nfor _ in range(10):\n    # Get a trial from an Optimizer\n    trial: Trial = optimizer.ask()\n    print(f\"Evaluating trial {trial.name} with config {trial.config}\")\n\n    # Access the the trial's config\n    x = trial.config[\"my-searchable:x\"]\n\n    # Begin the trial\n    with trial.begin():\n        score = poly(x)\n\n    if trial.exception is None:\n        # Generate a success report\n        report: Trial.Report = trial.success(score=score)\n    else:\n        # Generate a failed report (i.e. poly(x) raised divide by zero exception with x=0)\n        report: Trial.Report = trial.fail()\n\n    # Store artifacts with the trial, using file extensions to infer how to store it\n    trial.store({ \"config.json\": trial.config, \"array.npy\": [1, 2, 3] })\n\n    # Tell the Optimizer about the report\n    optimizer.tell(report)\n\n    # Add the report to the history\n    history.add(report)\n</code></pre> <pre><code>Evaluating trial config_id=1_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 5.9014238975942135}\nEvaluating trial config_id=2_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -2.0745517686009407}\nEvaluating trial config_id=3_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -8.257772866636515}\nEvaluating trial config_id=4_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 4.430919848382473}\nEvaluating trial config_id=5_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 0.24310464039444923}\nEvaluating trial config_id=6_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -6.413793563842773}\nEvaluating trial config_id=7_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -2.58980056270957}\nEvaluating trial config_id=8_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 8.760508447885513}\nEvaluating trial config_id=9_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': 8.428955599665642}\nEvaluating trial config_id=10_seed=1608637542_budget=None_instance=None with config {'my-searchable:x': -4.599663596600294}\n</code></pre> <p>And we can use the <code>History</code> to see the history of the optimization process</p> <pre><code>df = history.df()\nprint(df)\n</code></pre> <pre><code>                                                     status  ...  time:unit\nname                                                         ...           \nconfig_id=1_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=2_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=3_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=4_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=5_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=6_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=7_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=8_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=9_seed=1608637542_budget=None_instanc...  success  ...    seconds\nconfig_id=10_seed=1608637542_budget=None_instan...  success  ...    seconds\n\n[10 rows x 19 columns]\n</code></pre> <p>Okay so there are a few things introduced all at once here, let's go over them bit by bit.</p>"},{"location":"guides/optimization/#the-trial-object","title":"The <code>Trial</code> object","text":"<p>The <code>Trial</code> object is the main object that you'll be interacting with when optimizing. It contains a load of useful properties and functionality to help you during optimization.</p> <p>The <code>.config</code> will contained name spaced parameters, in this case, <code>my-searchable:x</code>, based on the pipeline/search space you specified.</p> <p>We also wrap the actual evaluation of the function in a <code>with trial.begin():</code> which will time and profile the evaluation of the function and handle any exceptions that occur within the block.</p> <p>If an exception occured in the <code>with trial.begin():</code> block, any exception/traceback that occured will be attached to <code>.exception</code> and <code>.traceback</code>.</p> <p>It's also quite typical to store artifacts with the trial, a common feature of things like TensorBoard, MLFlow, etc. We provide a primitive way to store artifacts with the trial using <code>.store()</code> which takes a dictionary of file names to file contents. The file extension is used to infer how to store the file, for example, <code>.json</code> files will be stored as JSON, <code>.npy</code> files will be stored as numpy arrays. You are of course still free to use your other favourite logging tools in conjunction with AMLTK!</p> <p>Lastly, we use <code>trial.success()</code> or <code>trial.fail()</code> which generates a <code>Trial.Report</code> for us, that we can give back to the optimizer.</p> <p>Feel free to explore the full API.</p>"},{"location":"guides/optimization/#the-history-object","title":"The <code>History</code> object","text":"<p>You may have noticed that we also created a <code>History</code> object to store our reports in. This is a simple container to store the reports together and get a dataframe out of. We may extend this with future utility such as plotting or other export formats but for now, we can use it primarily for getting our results together in one place.</p> <p>We'll create a simple example where we create our own trials and record some results on them, getting out a dataframe at the end.</p> <pre><code>from amltk.optimization import History, Trial, Metric\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"my-bucket\")\nmetric = Metric(\"score\", minimize=False, bounds=(0, 5))\nhistory = History()\n\ntrials = [\n    Trial(name=\"trial-1\", config={\"x\": 1.0}, bucket=bucket, metrics=[metric]),\n    Trial(name=\"trial-2\", config={\"x\": 2.0}, bucket=bucket, metrics=[metric]),\n    Trial(name=\"trial-3\", config={\"x\": 3.0}, bucket=bucket, metrics=[metric]),\n]\n\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        if x &gt;= 2:\n            report = trial.fail()\n        else:\n            report = trial.success(score=x)\n\n    history.add(report)\n\ndf = history.df()\nprint(df)\n</code></pre> <pre><code>          status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                    ...                                   \ntrial-1  success        &lt;NA&gt;        NA  ...      0.000022      wall    seconds\ntrial-2     fail        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial-3     fail        &lt;NA&gt;        NA  ...      0.000016      wall    seconds\n\n[3 rows x 19 columns]\n</code></pre> <p>You can use the <code>History.df()</code> method to get a dataframe of the history and use your favourite dataframe tools to analyze the results.</p>"},{"location":"guides/optimization/#optimizing-an-sklearn-pipeline","title":"Optimizing an Sklearn-Pipeline","text":"<p>To give a more concrete example, we will optimize a simple sklearn pipeline. You'll likely want to refer to the pipeline guide for more information on pipelines, but the example should be clear enough without it.</p> <p>We start with defining our pipeline.</p> <pre><code>from typing import Any\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.neural_network import MLPClassifier\n\nfrom amltk.pipeline import Sequential, Choice, Component\n\ndef dims_to_hidden_layer(config: dict[str, Any], _):\n    config = dict(config)\n    config[\"hidden_layer_sizes\"] = (config.pop(\"dim1\"), config.pop(\"dim2\"))\n    return config\n\n# A pipeline with a choice of scalers and a parametrized MLP\nmy_pipeline = (\n    Sequential(name=\"my-pipeline\")\n    &gt;&gt; Choice(\n        StandardScaler,\n        MinMaxScaler,\n        Component(RobustScaler, space={\"with_scaling\": [True, False], \"unit_variance\": [True, False]}),\n        name=\"scaler\",\n    )\n    &gt;&gt; Component(\n        MLPClassifier,\n        space={\n            \"dim1\": (1, 10),\n            \"dim2\": (1, 10),\n            \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n        },\n        config_transform=dims_to_hidden_layer,\n    )\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my-pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Choice(scaler) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(MinMaxSc\u2500\u256e \u256d\u2500 Component(RobustSc\u2500\u256e \u256d\u2500 Component(StandardS\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item class          \u2502 \u2502 item  class         \u2502 \u2502 item class           \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      MinMaxScaler(\u2026 \u2502 \u2502       RobustScaler\u2026 \u2502 \u2502      StandardScaler\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 space {             \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502                         \u2502           'with_sc\u2026 \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502       [             \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               True, \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               False \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502           ],        \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502           'unit_va\u2026 \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502       [             \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               True, \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502               False \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502           ]         \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2502       }             \u2502                          \u2502 \u2502\n\u2502 \u2502                         \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                      \u2193                                       \u2502\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                 \u2502\n\u2502 \u2502 item      class MLPClassifier(...)                       \u2502                 \u2502\n\u2502 \u2502 space     {                                              \u2502                 \u2502\n\u2502 \u2502               'dim1': (1, 10),                           \u2502                 \u2502\n\u2502 \u2502               'dim2': (1, 10),                           \u2502                 \u2502\n\u2502 \u2502               'activation': [                            \u2502                 \u2502\n\u2502 \u2502                   'relu',                                \u2502                 \u2502\n\u2502 \u2502                   'tanh',                                \u2502                 \u2502\n\u2502 \u2502                   'logistic'                             \u2502                 \u2502\n\u2502 \u2502               ]                                          \u2502                 \u2502\n\u2502 \u2502           }                                              \u2502                 \u2502\n\u2502 \u2502 transform def dims_to_hidden_layer(...)                  \u2502                 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Next up, we need to define a simple target function we want to evaluate on. With that, we'll also store our data, so that on each evaluate call, we load it in. This doesn't make much sense for a single in-process call but when scaling up to using multiple processes or remote compute, this is a good practice to follow. For this we use a <code>PathBucket</code> and get a <code>StoredValue</code> from it, basically a reference to some object we can load back in later.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_validate\nfrom amltk.optimization import Trial\nfrom amltk.store import PathBucket, StoredValue\nimport numpy as np\n\n# Load in our data\n_X, _y = load_iris(return_X_y=True)\n\n# Store our data in a bucket\nbucket = PathBucket(\"my-bucket\")\nbucket.update({\"X.npy\": _X, \"y.npy\": _y})\n\ndef evaluate(\n    trial: Trial,\n    pipeline: Sequential,\n    X: StoredValue[str, np.ndarray],\n    y: StoredValue[str, np.ndarray],\n) -&gt; Trial.Report:\n    # Configure our pipeline and build it\n    sklearn_pipeline = (\n        pipeline\n        .configure(trial.config)\n        .build(\"sklearn\")\n    )\n\n    # Load in our data\n    X = X.value()\n    y = y.value()\n\n    # Use sklearns.cross_validate as our evaluator\n    with trial.begin():\n        results = cross_validate(sklearn_pipeline, X, y, scoring=\"accuracy\", cv=3, return_estimator=True)\n\n    test_scores = results[\"test_score\"]\n    estimators = results[\"estimator\"]  # You can store these if you like (you'll likely want to use the `.pkl` suffix for the filename)\n\n    # Report the mean test score\n    mean_test_score = np.mean(test_scores)\n    return trial.success(acc=mean_test_score)\n</code></pre> <pre><code>\n</code></pre> <p>Lastly, we'll create our optimizer and run it. In this example, we'll use the <code>SMACOptimizer</code> but you can refer to the optimizer reference for other optimizers. For basic use cases, you should be able to swap in and out the optimizer and it should work without any changes.</p> <pre><code>from amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.optimization import Metric, History\n\nmetric = Metric(\"acc\", minimize=False, bounds=(0, 1))\nbucket = PathBucket(\"my-bucket\")\noptimizer = SMACOptimizer.create(\n    space=my_pipeline,  # Let it know what to optimize\n    metrics=metric,  # And let it know what to expect\n    bucket=bucket,  # And where to store artifacts for trials and optimizer output\n)\n\nhistory = History()\nstored_X = bucket[\"X.npy\"].as_stored_value()\nstored_y = bucket[\"y.npy\"].as_stored_value()\n\nfor _ in range(10):\n    # Get a trial from the optimizer\n    trial = optimizer.ask()\n\n    # Evaluate the trial\n    report = evaluate(trial=trial, pipeline=my_pipeline, X=stored_X, y=stored_y)\n\n    # Tell the optimizer about the report\n    optimizer.tell(report)\n\n    # Add the report to the history\n    history.add(report)\n\ndf = history.df()\n</code></pre> <pre><code>                                                     status  ...  config:my-pipeline:scaler:RobustScaler:with_scaling\nname                                                         ...                                                     \nconfig_id=1_seed=250294882_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=2_seed=250294882_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=3_seed=250294882_budget=None_instance...  success  ...                                               True  \nconfig_id=4_seed=250294882_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=5_seed=250294882_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=6_seed=250294882_budget=None_instance...  success  ...                                               True  \nconfig_id=7_seed=250294882_budget=None_instance...  success  ...                                               True  \nconfig_id=8_seed=250294882_budget=None_instance...  success  ...                                               &lt;NA&gt;  \nconfig_id=9_seed=250294882_budget=None_instance...  success  ...                                              False  \nconfig_id=10_seed=250294882_budget=None_instanc...  success  ...                                               &lt;NA&gt;  \n\n[10 rows x 24 columns]\n</code></pre>"},{"location":"guides/pipelines/","title":"Pipelines Guide","text":"<p>AutoML-toolkit was built to support future development of AutoML systems and a central part of an AutoML system is its pipeline. The purpose of this guide is to help you understand all the utility AutoML-toolkit can provide to help you define your pipeline. We will do this by introducing concepts from the ground up, rather than top down. Please see the reference if you just want to quickly look something up.</p>"},{"location":"guides/pipelines/#introduction","title":"Introduction","text":"<p>The kinds of pipelines that exist in an AutoML system come in many different forms. For example, one might be an sklearn.pipeline.Pipeline, other's might be some deep-learning pipeline while some might even stand for some real life machinery process and the settings of these machines.</p> <p>To accomodate this, what AutoML-Toolkit provides is an abstract representation of a pipeline, to help you define its search space and also to build concrete objects in code if possible (see builders.</p> <p>We categorize this into 4 steps:</p> <ol> <li> <p>Parametrize your pipeline using the various components,     including the kinds of items in the pipeline, the search spaces and any additional configuration.     Each of the various types of components give a syntactic meaning when performing the next steps.</p> </li> <li> <p><code>pipeline.search_space(parser=...)</code>,     Get a useable search space out of the pipeline. This can then be passed to an     <code>Optimizer</code>.</p> </li> <li> <p><code>pipeline.configure(config=...)</code>,     Configure your pipeline, either manually or using a configuration suggested by     an optimizers.</p> </li> <li> <p><code>pipeline.build(builder=)</code>,     Build your configured pipeline definition into something useable, i.e.     an <code>sklearn.pipeline.Pipeline</code> or a     <code>torch.nn.Module</code> (todo).</p> </li> </ol> <p>At the core of these definitions is the many <code>Nodes</code> it consists of. By combining these together, you can define a directed acyclic graph (DAG), that represents the structure of your pipeline. Here is one such sklearn example that we will build up towards.</p> ResultSource <p> <pre>\n<code>\u256d\u2500 Sequential(Classy Pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Split(preprocessing) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 config {                                                                 \u2502 \u2502\n\u2502 \u2502            'categoricals':                                               \u2502 \u2502\n\u2502 \u2502        &lt;sklearn.compose._column_transformer.make_column_selector object  \u2502 \u2502\n\u2502 \u2502        at 0x7f24987c0100&gt;,                                               \u2502 \u2502\n\u2502 \u2502            'numerics':                                                   \u2502 \u2502\n\u2502 \u2502        &lt;sklearn.compose._column_transformer.make_column_selector object  \u2502 \u2502\n\u2502 \u2502        at 0x7f24987c0370&gt;                                                \u2502 \u2502\n\u2502 \u2502        }                                                                 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Sequential(categoricals) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerics) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u256e \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 item SimpleImputer(fill_va\u2026 \u2502 \u2502 \u2502 \u2502 item  class                  \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502      strategy='constant')   \u2502 \u2502 \u2502 \u2502       SimpleImputer(...)     \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502 space {                      \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502                \u2193                \u2502 \u2502 \u2502           'strategy': [      \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'mean',        \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2502 item OneHotEncoder(drop='f\u2026 \u2502 \u2502 \u2502 \u2502               'median'       \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           ]                  \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502       }                      \u2502 \u2502 \u2502 \u2502\n\u2502 \u2502                                     \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502\n\u2502 \u2502                                     \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                      \u2193                                       \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e     \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...)                              \u2502     \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100), 'criterion': ['gini', 'log_loss']} \u2502     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> Pipeline<pre><code>from sklearn.compose import make_column_selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nfrom amltk.pipeline import Component, Split, Sequential\n\nfeature_preprocessing = Split(\n    {\n        \"categoricals\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")],\n        \"numerics\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n    },\n    config={\n        \"categoricals\": make_column_selector(dtype_include=object),\n        \"numerics\": make_column_selector(dtype_include=np.number),\n    },\n    name=\"preprocessing\",\n)\n\npipeline = Sequential(\n    feature_preprocessing,\n    Component(RandomForestClassifier, space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]}),\n    name=\"Classy Pipeline\",\n)\n</code></pre> <code>rich</code> printing <p>To get the same output locally (terminal or Notebook), you can either call <code>thing.__rich()__</code>, use <code>from rich import print; print(thing)</code> or in a Notebook, simply leave it as the last object of a cell.</p> <p>Once we have our pipeline definition, extracting a search space, configuring it and building it into something useful can be done with the methods.</p> <p>Guide Requirements</p> <p>For this guide, we will be using <code>ConfigSpace</code> and <code>scikit-learn</code>, you can install them manually or as so:</p> <pre><code>pip install \"amltk[sklearn, configspace]\"\n</code></pre>"},{"location":"guides/pipelines/#component","title":"Component","text":"<p>A pipeline consists of building blocks which we can combine together to create a DAG. We will start by introducing the <code>Component</code>, the common operations, and then show how to combine them together.</p> <p>A <code>Component</code> is the most common kind of node a pipeline. Like all parts of the pipeline, they subclass <code>Node</code> but a <code>Component</code> signifies this is some concrete object, with a possible <code>.space</code> and <code>.config</code>.</p>"},{"location":"guides/pipelines/#definition","title":"Definition","text":"Naming Nodes <p>By default, a <code>Component</code> (or any <code>Node</code> for that matter), will use the function/classname for the <code>.name</code> of the <code>Node</code>. You can explicitly pass a <code>name=</code> as a keyword argument when constructing these.</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk.pipeline import Component\n\n@dataclass\nclass MyModel:\n    f: float\n    i: int\n    c: str\n\nmy_component = Component(\n    MyModel,\n    space={\"f\": (0.0, 1.0), \"i\": (0, 10), \"c\": [\"red\", \"green\", \"blue\"]},\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class MyModel(...)                                             \u2502\n\u2502 space {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>You can also use a function instead of a class if that is preferred.</p> <pre><code>def myfunc(f: float, i: int, c: str) -&gt; MyModel:\n    if f &lt; 0.5:\n        c = \"red\"\n    return MyModel(f=f, i=i, c=c)\n\ncomponent_with_function = Component(\n    myfunc,\n    space={\"f\": (0.0, 1.0), \"i\": (0, 10), \"c\": [\"red\", \"green\", \"blue\"]},\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(function) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  def myfunc(...)                                                \u2502\n\u2502 space {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"guides/pipelines/#search-space","title":"Search Space","text":"<p>If interacting with an <code>Optimizer</code>, you'll often require some search space object to pass to it. To extract a search space from a <code>Component</code>, we can call <code>search_space(parser=)</code>, passing in the kind of search space you'd like to get out of it.</p> <pre><code>space = my_component.search_space(\"configspace\")\nprint(space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MyModel:c, Type: Categorical, Choices: {red, green, blue}, Default: red\n    MyModel:f, Type: UniformFloat, Range: [0.0, 1.0], Default: 0.5\n    MyModel:i, Type: UniformInteger, Range: [0, 10], Default: 5\n</code></pre> <p>Available Search Spaces</p> <p>Please see the spaces reference</p> <p>Depending on what you pass as the <code>parser=</code> to <code>search_space(parser=...)</code>, we'll attempt to give you a valid search space. In this case, we specified <code>\"configspace\"</code> and  so we get a <code>ConfigSpace</code> implementation.</p> <p>You may also define your own <code>parser=</code> and use that if desired.</p>"},{"location":"guides/pipelines/#configure","title":"Configure","text":"<p>Pretty straight forward but what do we do with this <code>config</code>? Well we can <code>configure(config=...)</code> the component with it.</p> <pre><code>config = space.sample_configuration()\nconfigured_component = my_component.configure(config)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)                                             \u2502\n\u2502 config {'c': 'blue', 'f': 0.8579909763244906, 'i': 1}                 \u2502\n\u2502 space  {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>You'll notice that each variable in the space has been set to some value. We could also manually define a config and pass that in. You are not obliged to fully specify this either.</p> <pre><code>manually_configured_component = my_component.configure({\"f\": 0.5, \"i\": 1})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)                                             \u2502\n\u2502 config {'f': 0.5, 'i': 1}                                             \u2502\n\u2502 space  {'f': (0.0, 1.0), 'i': (0, 10), 'c': ['red', 'green', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Immutable methods!</p> <p>One thing you may have noticed is that we assigned the result of <code>configure(config=...)</code> to a new variable. This is because we do not mutate the original <code>my_component</code> and instead return a copy with all of the <code>config</code> variables set.</p>"},{"location":"guides/pipelines/#build","title":"Build","text":"<p>To build the individual item of a <code>Component</code> we can use <code>build_item()</code> and it simply calls the <code>.item</code> with the config we have set.</p> <pre><code># Same as if we did `configured_component.item(**configured_component.config)`\nthe_built_model = configured_component.build_item()\nprint(the_built_model)\n</code></pre> <pre><code>MyModel(f=0.8579909763244906, i=1, c='blue')\n</code></pre> <p>However, as we'll see later, we often have multiple steps of a pipeline joined together and so we need some way to get a full object out of it that takes into account all of these items joined together. We can do this with <code>build(builder=...)</code>.</p> <pre><code>the_built_model = configured_component.build(builder=\"sklearn\")\nprint(the_built_model)\n</code></pre> <pre><code>Pipeline(steps=[('MyModel', MyModel(f=0.8579909763244906, i=1, c='blue'))])\n</code></pre> <p>For a look at the available arguments to pass to <code>builder=</code>, see the builder reference</p>"},{"location":"guides/pipelines/#fixed","title":"Fixed","text":"<p>Sometimes we just have some part of the pipeline with no search space and no configuration required, i.e. just some prebuilt thing. We can use the <code>Fixed</code> node type to signify this.</p> <pre><code>from amltk.pipeline import Fixed\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrozen_rf = Fixed(RandomForestClassifier(n_estimators=5))\n</code></pre> <pre><code>&lt;pre style=\"font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace;font-size:0.75rem\"&gt;\n&lt;code style=\"font-family:inherit\"&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u256d\u2500 &lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e; font-weight: bold\"&gt;Fixed&lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;(&lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e; font-style: italic\"&gt;RandomForestClassifier&lt;/span&gt;&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e&lt;/span&gt;\n&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u2502&lt;/span&gt; &lt;span style=\"color: #000000; text-decoration-color: #000000\"&gt;item &lt;/span&gt;&lt;span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"&gt;RandomForestClassifier&lt;/span&gt;&lt;span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"&gt;(&lt;/span&gt;&lt;span style=\"color: #808000; text-decoration-color: #808000\"&gt;n_estimators&lt;/span&gt;&lt;span style=\"color: #000000; text-decoration-color: #000000\"&gt;=&lt;/span&gt;&lt;span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"&gt;5&lt;/span&gt;&lt;span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"&gt;)&lt;/span&gt; &lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u2502&lt;/span&gt;\n&lt;span style=\"color: #56351e; text-decoration-color: #56351e\"&gt;\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f&lt;/span&gt;\n&lt;/code&gt;\n&lt;/pre&gt;\n</code></pre>"},{"location":"guides/pipelines/#parameter-requests","title":"Parameter Requests","text":"<p>Sometimes you may wish to explicitly specify some value should be added to the <code>.config</code> during <code>configure()</code> which would be difficult to include in the <code>config</code> directly, for example the <code>random_state</code> of an sklearn estimator. You can pass these extra parameters into <code>configure(params={...})</code>, which do not require any namespace prefixing.</p> <p>For this reason, we introduce the concept of a <code>request()</code>, allowing you to specify that a certain parameter should be added to the config during <code>configure()</code>.</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk import Component, request\n\n@dataclass\nclass MyModel:\n    f: float\n    random_state: int\n\nmy_component = Component(\n    MyModel,\n    space={\"f\": (0.0, 1.0)},\n    config={\"random_state\": request(\"seed\", default=42)}\n)\n\n# Without passing the params\nconfigured_component_no_seed = my_component.configure({\"f\": 0.5})\n\n# With passing the params\nconfigured_component_with_seed = my_component.configure({\"f\": 0.5}, params={\"seed\": 1337})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)             \u2502\n\u2502 config {'random_state': 42, 'f': 0.5} \u2502\n\u2502 space  {'f': (0.0, 1.0)}              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class MyModel(...)               \u2502\n\u2502 config {'random_state': 1337, 'f': 0.5} \u2502\n\u2502 space  {'f': (0.0, 1.0)}                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>If you explicitly require a parameter to be set, just do not set a <code>default=</code>.</p> <pre><code>my_component = Component(\n    MyModel,\n    space={\"f\": (0.0, 1.0)},\n    config={\"random_state\": request(\"seed\")}\n)\n\nmy_component.configure({\"f\": 0.5}, params={\"seed\": 5})  # All good\n\ntry:\n    my_component.configure({\"f\": 0.5})  # Missing required parameter\nexcept ValueError as e:\n    print(e)\n</code></pre> <pre><code>Missing request=ParamRequest(key='seed', default=&lt;object object at 0x7f24a0596a50&gt;) for Component(name='MyModel', item=&lt;class 'MyModel'&gt;, nodes=(), config={'random_state': ParamRequest(key='seed', default=&lt;object object at 0x7f24a0596a50&gt;)}, space={'f': (0.0, 1.0)}, fidelities=None, config_transform=None, meta=None).\nparams=None\n</code></pre>"},{"location":"guides/pipelines/#config-transform","title":"Config Transform","text":"<p>Some search space and optimizers may have limitations in terms of the kinds of parameters they can support, one notable example is tuple parameters. To get around this, we can pass a <code>config_transform=</code> to <code>component</code> which will transform the config before it is passed to the <code>.item</code> during <code>build()</code>.</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk import Component\n\n@dataclass\nclass MyModel:\n    dimensions: tuple[int, int]\n\ndef config_transform(config: dict, _) -&gt; dict:\n    \"\"\"Convert \"dim1\" and \"dim2\" into a tuple.\"\"\"\n    dim1 = config.pop(\"dim1\")\n    dim2 = config.pop(\"dim2\")\n    config[\"dimensions\"] = (dim1, dim2)\n    return config\n\nmy_component = Component(\n    MyModel,\n    space={\"dim1\": (1, 10), \"dim2\": (1, 10)},\n    config_transform=config_transform,\n)\n\nconfigured_component = my_component.configure({\"dim1\": 5, \"dim2\": 5})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(MyModel) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item      class MyModel(...)                 \u2502\n\u2502 config    {'dimensions': (5, 5)}             \u2502\n\u2502 space     {'dim1': (1, 10), 'dim2': (1, 10)} \u2502\n\u2502 transform def config_transform(...)          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Transform Context</p> <p>There may be times where you may have some additional context which you may only know at configuration time, you may pass this to <code>configure(..., transform_context=...)</code> which will be forwarded as the second argument to your <code>.config_transform</code>.</p>"},{"location":"guides/pipelines/#sequential","title":"Sequential","text":"<p>A single component might be enough for some basic definitions but generally we need to combine multiple components together. AutoML-Toolkit is designed for large and more complex structures which can be made from simple atomic <code>Node</code>s.</p>"},{"location":"guides/pipelines/#chaining-together-nodes","title":"Chaining Together Nodes","text":"<p>We'll begin by creating two components that wrap scikit-learn estimators.</p> <pre><code>from sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom amltk.pipeline import Component\n\nimputer = Component(SimpleImputer, space={\"strategy\": [\"median\", \"mean\"]})\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class SimpleImputer(...)         \u2502\n\u2502 space {'strategy': ['median', 'mean']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class RandomForestClassifier(...) \u2502\n\u2502 space {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Infix <code>&gt;&gt;</code></p> <p>To join these two components together, we can either use the infix notation using <code>&gt;&gt;</code>, or passing them directly to a <code>Sequential</code>. However a random name will be given.</p> <pre><code>joined_components = imputer &gt;&gt; rf\n</code></pre> <pre><code>from amltk.pipeline import Sequential\npipeline = Sequential(imputer, rf, name=\"My Pipeline\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(My Pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e  \u2502\n\u2502 \u2502 item  class SimpleImputer(...)         \u2502  \u2502\n\u2502 \u2502 space {'strategy': ['median', 'mean']} \u2502  \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"guides/pipelines/#operations","title":"Operations","text":"<p>You can perform much of the same operations as we did for the individual node but now taking into account everything in the pipeline.</p> <pre><code>space = pipeline.search_space(\"configspace\")\nconfig = space.sample_configuration()\nconfigured_pipeline = pipeline.configure(config)\n</code></pre> <p> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    My Pipeline:RandomForestClassifier:n_estimators, Type: UniformInteger, \nRange: [10, 100], Default: 55\n    My Pipeline:SimpleImputer:strategy, Type: Categorical, Choices: {median, \nmean}, Default: median\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'My Pipeline:RandomForestClassifier:n_estimators': 60,\n  'My Pipeline:SimpleImputer:strategy': 'median',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Sequential(My Pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e  \u2502\n\u2502 \u2502 item   class SimpleImputer(...)         \u2502  \u2502\n\u2502 \u2502 config {'strategy': 'median'}           \u2502  \u2502\n\u2502 \u2502 space  {'strategy': ['median', 'mean']} \u2502  \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502\n\u2502                      \u2193                       \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item   class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 config {'n_estimators': 60}              \u2502 \u2502\n\u2502 \u2502 space  {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>To build a pipeline of nodes, we simply call <code>build(builder=)</code>. We explicitly pass the builder we want to use, which informs <code>build()</code> how to go from the abstract pipeline definition you've defined to something concrete you can use. You can find the available builders here.</p> <pre><code>from sklearn.pipeline import Pipeline as SklearnPipeline\n\nbuilt_pipeline = configured_pipeline.build(\"sklearn\")\nassert isinstance(built_pipeline, SklearnPipeline)\n</code></pre> <p><pre>Pipeline(steps=[('SimpleImputer', SimpleImputer(strategy='median')),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=60))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('SimpleImputer', SimpleImputer(strategy='median')),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=60))])</pre>SimpleImputer<pre>SimpleImputer(strategy='median')</pre>RandomForestClassifier<pre>RandomForestClassifier(n_estimators=60)</pre> </p>"},{"location":"guides/pipelines/#other-building-blocks","title":"Other Building blocks","text":"<p>We saw the basic building block of a <code>Component</code> but AutoML-Toolkit also provides support for some other kinds of building blocks. These building blocks can be attached and joined together just like a <code>Component</code> can and allow for much more complex pipeline structures.</p>"},{"location":"guides/pipelines/#choice","title":"Choice","text":"<p>A <code>Choice</code> is a way to define a choice between multiple components. This is useful when you want to search over multiple algorithms, which may each have their own hyperparameters.</p> <p>We'll start again by creating two nodes:</p> <pre><code>from dataclasses import dataclass\n\nfrom amltk.pipeline import Component\n\n@dataclass\nclass ModelA:\n    i: int\n\n@dataclass\nclass ModelB:\n    c: str\n\nmodel_a = Component(ModelA, space={\"i\": (0, 100)})\nmodel_b = Component(ModelB, space={\"c\": [\"red\", \"blue\"]})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(ModelA) \u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class ModelA(...) \u2502\n\u2502 space {'i': (0, 100)}   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(ModelB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class ModelB(...)      \u2502\n\u2502 space {'c': ['red', 'blue']} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Now combining them into a choice is rather straight forward:</p> <pre><code>from amltk.pipeline import Choice\n\nmodel_choice = Choice(model_a, model_b, name=\"estimator\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(ModelA) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(ModelB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class ModelA(...) \u2502 \u2502 item  class ModelB(...)      \u2502 \u2502\n\u2502 \u2502 space {'i': (0, 100)}   \u2502 \u2502 space {'c': ['red', 'blue']} \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> Conditionals and Search Spaces <p>Not all search space implementations support conditionals and so some <code>parser=</code> may not be able to handle this. In this case, there won't be any conditionality in the search space.</p> <p>Check out the parser reference for more information.</p> <p>Just as we did with a <code>Component</code>, we can also get a <code>search_space()</code> from the choice.</p> <pre><code>space = model_choice.search_space(\"configspace\")\n</code></pre> <p> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    estimator:ModelA:i, Type: UniformInteger, Range: [0, 100], Default: 50\n    estimator:ModelB:c, Type: Categorical, Choices: {red, blue}, Default: red\n    estimator:__choice__, Type: Categorical, Choices: {ModelA, ModelB}, Default:\nModelA\n  Conditions:\n    estimator:ModelA:i | estimator:__choice__ == 'ModelA'\n    estimator:ModelB:c | estimator:__choice__ == 'ModelB'\n\n</code>\n</pre> </p> <p>When we <code>configure()</code> a choice, we will collapse it down to a single component. This is done according to what is set in the config.</p> <p><pre><code>config = space.sample_configuration()\nconfigured_choice = model_choice.configure(config)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {'__choice__': 'ModelB'}                               \u2502\n\u2502 \u256d\u2500 Component(ModelA) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(ModelB) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class ModelA(...) \u2502 \u2502 item   class ModelB(...)      \u2502 \u2502\n\u2502 \u2502 space {'i': (0, 100)}   \u2502 \u2502 config {'c': 'blue'}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 space  {'c': ['red', 'blue']} \u2502 \u2502\n\u2502                             \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>  You'll notice that it set the <code>.config</code> of the <code>Choice</code> to <code>{\"__choice__\": \"model_a\"}</code> or <code>{\"__choice__\": \"model_b\"}</code>. This lets a builder know which of these two to build.</p>"},{"location":"guides/pipelines/#split","title":"Split","text":"<p>A <code>Split</code> is a way to signify a split in the dataflow of a pipeline. This <code>Split</code> by itself will not do anything but it informs the builder about what to do. Each builder will have if it's own specific strategy for dealing with one.</p> <p>Let's go ahead with a scikit-learn example, where we'll split the data into categorical and numerical features and then perform some preprocessing on each of them.</p> <pre><code>from sklearn.compose import make_column_selector\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\n\nfrom amltk.pipeline import Component, Split\n\nselect_categories = make_column_selector(dtype_include=object)\nselect_numerical = make_column_selector(dtype_include=np.number)\n\npreprocessor = Split(\n    {\n        \"categories\": [SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), OneHotEncoder(drop=\"first\")],\n        \"numerics\": Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]}),\n    },\n    config={\"categories\": select_categories, \"numerics\": select_numerical},\n    name=\"feature_preprocessing\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Split(feature_preprocessing) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f2498843790&gt;,                                                      \u2502\n\u2502            'numerics':                                                       \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f24987c2440&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerics) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502 space {                        \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           'strategy': [        \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502               'mean',          \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'median'         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502           ]                    \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502       }                        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>An important thing to note here is that first, we passed a <code>dict</code> to <code>Split</code>, such that we can name the individual paths. This is important because we need some name to refer to them when configuring the <code>Split</code>. It does this by simply wrapping each of the paths in a <code>Sequential</code>.</p> <p>The second thing is that the parameters set for the <code>.config</code> matches those of the paths. This let's the <code>Split</code> know which data should be sent where. Each <code>builder=</code> will have it's own way of how to set up a <code>Split</code> and you should refer to the builders reference for more information.</p> <p>Our last step is just to convert this into a useable object and so once again we use <code>build()</code>.</p> <pre><code>built_pipeline = preprocessor.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('feature_preprocessing',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2498843790&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24987c2440&gt;)]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('feature_preprocessing',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2498843790&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24987c2440&gt;)]))])</pre>feature_preprocessing: ColumnTransformer<pre>ColumnTransformer(transformers=[('categories',\n                                 Pipeline(steps=[('SimpleImputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('OneHotEncoder',\n                                                  OneHotEncoder(drop='first'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2498843790&gt;),\n                                ('SimpleImputer', SimpleImputer(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24987c2440&gt;)])</pre>categories<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f2498843790&gt;</pre>SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre>OneHotEncoder<pre>OneHotEncoder(drop='first')</pre>SimpleImputer<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24987c2440&gt;</pre>SimpleImputer<pre>SimpleImputer()</pre> </p>"},{"location":"guides/pipelines/#join","title":"Join","text":"<p>TODO</p> <p>TODO</p>"},{"location":"guides/pipelines/#searchable","title":"Searchable","text":"<p>TODO</p> <p>TODO</p>"},{"location":"guides/pipelines/#option","title":"Option","text":"<p>TODO</p> <p>Please feel free to provide a contribution!</p>"},{"location":"guides/scheduling/","title":"Scheduling","text":"<p>AutoML-toolkit was designed to make offloading computation away from the main process easy, to foster increased ability for interact-ability deployment and control. At the same time, we wanted to have an event based system to manage the complexity that comes with AutoML systems, all while making the API intuitive and extensible.</p> <p>By the end of this guide, we hope that the following code, its options and its inner working become easy to understand.</p> SourceResult Scheduler<pre><code>from amltk import Scheduler\n\n# Some function to offload to compute\ndef collatz(n: int) -&gt; int:\n    is_even = (n % 2 == 0)\n    return int(n / 2) if is_even else int(3 * n + 1)\n\n# Setup the scheduler and create a \"task\"\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(collatz)\n\nanswers = []\n\n# Tell the scheduler what to do when it starts\n@scheduler.on_start\ndef start_computing() -&gt; None:\n    answers.append(12)\n    task.submit(12)  # Launch the task with the argument 12\n\n# Tell the scheduler what to do when the task returns\n@task.on_result\ndef compute_next(_, next_n: int) -&gt; None:\n    answers.append(next_n)\n    if scheduler.running() and next_n != 1:\n        task.submit(next_n)\n\n# Run the scheduler\nscheduler.run(timeout=1)  # One second timeout\nprint(answers)\n</code></pre> <p>[12, 6, 3, 10, 5, 16, 8, 4, 2, 1]  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def start_computing() -&gt; 'None' (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 9\n\u2503   @on_future_done 9\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 9\n\u2517\u2501\u2501 \u256d\u2500 Task collatz(n: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 9                                                          \u2502\n    \u2502 @on_done 9                                                               \u2502\n    \u2502 @on_result 9                                                             \u2502\n    \u2502 \u2514\u2500\u2500 def compute_next(_, next_n: 'int') -&gt; 'None' (9)                     \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: fwMNpx0p \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>We start by introducing the engine, the <code>Scheduler</code> and how this interacts with python's built-in <code>Executor</code> interface to offload compute to processes, cluster nodes, or even cloud resources.</p> <p>However, the <code>Scheduler</code> is rather useless without some fuel. For this, we present <code>Tasks</code>, the computational task to perform with the <code>Scheduler</code> and start the system's gears turning.</p> <code>rich</code> printing <p>To get the same output locally (terminal or Notebook), you can either call <code>thing.__rich()__</code>, use <code>from rich import print; print(thing)</code> or in a Notebook, simply leave it as the last object of a cell.</p> <p>You'll have to install with <code>amltk[jupyter]</code> or <code>pip install rich[jupyter]</code> manually.k</p>"},{"location":"guides/scheduling/#scheduler","title":"Scheduler","text":"<p>The core engine of the AutoML-Toolkit is the <code>Scheduler</code>. It purpose it to allow you to create workflows in an event driven manner. It does this by allowing you to <code>submit()</code> functions with arguments to be computed in the background, while the main process can continue to do other work. Once this computation has completed, you can react with various callbacks, most likely to submit more computations.</p> Sounds like <code>asyncio</code>? <p>If you're familiar with pythons <code>await/async</code> syntax, then this description might sound similar. The <code>Scheduler</code> is powered by an asynchronous event loop but hides this complexity in it's API. We do have an asynchronous API which we will discuss later.</p>"},{"location":"guides/scheduling/#backend","title":"Backend","text":"<p>The first thing to do is define where this computation should happen. A <code>Scheduler</code> builds upon, an <code>Executor</code>, an interface provided by python's <code>concurrent.futures</code> module. This interface is used to abstract away the details of how the computation is actually performed. This allows us to easily switch between different backends, such as threads, processes, clusters, cloud resources, or even custom backends.</p> <p>Available Executors</p> <p>You can find a list of these in our executor reference.</p> <p>The simplest one is a <code>ProcessPoolExecutor</code> which will create a pool of processes to run the compute in parallel. We provide a convenience function for this as <code>Scheduler.with_processes()</code> well as some other builder</p> <pre><code>from concurrent.futures import ProcessPoolExecutor\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_start\n    @on_finishing\n    @on_finished\n    @on_stop\n    @on_timeout\n    @on_empty\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p>"},{"location":"guides/scheduling/#running-the-scheduler","title":"Running the Scheduler","text":"<p>You may have noticed from the above example that there are many events the shceduler will emit, such as <code>@on_start</code> or <code>@on_future_done</code>. One particularly important one is <code>@on_start</code>, an event to signal the scheduler has started and ready to accept tasks.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello() -&gt; None:\n    print(\"hello\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_start\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None'\n    @on_finishing\n    @on_finished\n    @on_stop\n    @on_timeout\n    @on_empty\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> <p>From the output, we can see that the <code>print_hello()</code> function was registered to the event <code>@on_start</code>, but it was never called and no <code>\"hello\"</code> printed.</p> <p>For this to happen, we actually have to <code>run()</code> the scheduler.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.run()\n</code></pre> <p>hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> <p>Now the output will show a little yellow number next to the <code>@on_start</code> and the <code>print_hello()</code>, indicating that event was triggered and the callback was called.</p> <p>You can subscribe multiple callbacks to the same event and they will each be called in the order they were registered.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello_1() -&gt; None:\n    print(\"hello 1\")\n\ndef print_hello_2() -&gt; None:\n    print(\"hello 2\")\n\nscheduler.on_start(print_hello_2)  # You can also register without a decorator\n\nscheduler.run()\n</code></pre> <p>hello 1 hello 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u251c\u2500\u2500 def print_hello_1() -&gt; 'None' (1)\n    \u2514\u2500\u2500 def print_hello_2() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> <p>Determinism</p> <p>It's worth noting that even though we are using an event based system, we are still guaranteed deterministic execution of the callbacks for any given event. The source of indeterminism is the order in which events are emitted, this is determined entirely by your compute functions themselves.</p>"},{"location":"guides/scheduling/#submitting-compute","title":"Submitting Compute","text":"<p>The <code>Scheduler</code> exposes a simple <code>submit()</code> method which allows you to submit compute to be performed while the scheduler is running.</p> <p>While we will later visit the <code>Task</code> class for defining these units of compute, is benficial to see how the <code>Scheduler</code> operates directly with <code>submit()</code>, without abstractions.</p> <p>In the below example, we will use the <code>@on_future_result</code> event to submit more compute once the previous computation has returned a result.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\ndef expensive_function(x: int) -&gt; int:\n    return 2 ** x\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, 2)  # Submit compute\n\n# Called when the submitted function is done\n@scheduler.on_future_result\ndef print_result(_, result: int) -&gt; None:\n    print(result)\n    if result &lt; 10:\n        scheduler.submit(expensive_function, result)\n\nscheduler.run()\n</code></pre> <p>4 16  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 2\n    @on_future_done 2\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 2\n    \u2514\u2500\u2500 def print_result(_, result: 'int') -&gt; 'None' (2)\n</code>\n</pre> </p> What's a <code>Future</code>? <p>A <code>Future</code> is a special object which represents the result of an asynchronous computation. It's an object that can be queried for its result/exception of some computation which may not have completed yet.</p>"},{"location":"guides/scheduling/#scheduler-events","title":"Scheduler Events","text":"<p>Here are some of the possible <code>@events</code> a <code>Scheduler</code> can emit, but please visit the scheduler reference for a complete list.</p> <p><code>@events</code></p> <code>@on_start</code><code>@on_future_result</code><code>@on_future_exception</code><code>@on_future_submitted</code><code>@on_future_done</code><code>@on_future_cancelled</code><code>@on_timeout</code><code>@on_stop</code><code>@on_finishing</code><code>@on_finished</code><code>@on_empty</code> <p>A <code>Subscriber</code> which is called when the scheduler starts. This is the first event emitted by the scheduler and one of the only ways to submit the initial compute to the scheduler.</p> <pre><code>@scheduler.on_start\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future returned with a result, no exception raise.</p> <pre><code>@scheduler.on_future_result\ndef my_callback(future: Future, result: Any):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute raised an uncaught exception.</p> <pre><code>@scheduler.on_future_exception\ndef my_callback(future: Future, exception: BaseException):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is submitted.</p> <pre><code>@scheduler.on_future_submitted\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is done, regardless of whether it was successful or not.</p> <pre><code>@scheduler.on_future_done\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future is cancelled. This usually occurs due to the underlying Scheduler, and is not something we do directly, other than when shutting down the scheduler.</p> <pre><code>@scheduler.on_future_cancelled\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler reaches the timeout.</p> <pre><code>@scheduler.on_timeout\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is has been stopped due to the <code>stop()</code> method being called.</p> <pre><code>@scheduler.on_stop\ndef my_callback(stop_msg: str, exception: BaseException | None):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finishing up. This occurs right before the scheduler shuts down the executor.</p> <pre><code>@scheduler.on_finishing\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finished, has shutdown the executor and possibly terminated any remaining compute.</p> <pre><code>@scheduler.on_finished\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the queue is empty. This can be useful to re-fill the queue and prevent the scheduler from exiting.</p> <pre><code>@scheduler.on_empty\ndef my_callback():\n    ...\n</code></pre> <p>We can access all the counts of all events through the <code>scheduler.event_counts</code> property. This is a <code>dict</code> which has the events as keys and the amount of times it was emitted as the values.</p>"},{"location":"guides/scheduling/#controlling-callbacks","title":"Controlling Callbacks","text":"<p>There's a few parameters you can pass to any event subscriber such as <code>@on_start</code> or <code>@on_future_result</code>. These control the behavior of what happens when its event is fired and can be used to control the flow of your system.</p> <p>These are covered more extensively in our events reference.</p> <code>repeat=</code><code>max_calls=</code><code>when=</code><code>every=</code> <p>Repeat the callback a certain number of times, every time the event is emitted.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n# Print \"hello\" 3 times when the scheduler starts\n@scheduler.on_start(repeat=3)\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.run()\n</code></pre> <p>hello hello hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (3)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p>Limit the number of times a callback can be called, after which, the callback will be ignored.</p> <p><pre><code>from asyncio import Future\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\ndef expensive_function(x: int) -&gt; int:\n    return x ** 2\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\n@scheduler.on_future_result(max_calls=3)\ndef print_result(future, result) -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\nscheduler.run()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 4\n    @on_future_done 4\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 4\n    \u2514\u2500\u2500 def print_result(future, result) -&gt; 'None' (3)\n</code>\n</pre> </p> </p> <p>A callable which takes no arguments and returns a <code>bool</code>. The callback will only be called when the <code>when</code> callable returns <code>True</code>.</p> <p>Below is a rather contrived example, but it shows how we can use the <code>when</code> parameter to control when the callback is called.</p> <p><pre><code>import random\nfrom amltk.scheduling import Scheduler\n\nLOCALE = random.choice([\"English\", \"German\"])\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start(when=lambda: LOCALE == \"English\")\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n@scheduler.on_start(when=lambda: LOCALE == \"German\")\ndef print_guten_tag() -&gt; None:\n    print(\"guten tag\")\n\nscheduler.run()\n</code></pre> <p>hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u251c\u2500\u2500 def print_hello() -&gt; 'None' (1)\n    \u2514\u2500\u2500 def print_guten_tag() -&gt; 'None'\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p>Only call the callback every <code>every</code> times the event is emitted. This includes the first time it's called.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n# Print \"hello\" only every 2 times the scheduler starts.\n@scheduler.on_start(every=2)\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n# Run the scheduler 5 times\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\n</code></pre> <p>hello hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 5\n    @on_start 5\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (2)\n    @on_finishing 5\n    @on_finished 5\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p>"},{"location":"guides/scheduling/#stopping-the-scheduler","title":"Stopping the Scheduler","text":"<p>There are a few ways the <code>Scheduler</code> will stop. The one we have implicitly been using this whole time is when the <code>Scheduler</code> has run out of events to process with no compute left to perform. This is the default behavior but can be controlled with <code>run(end_on_empty=False)</code>.</p> <p>However there are more explicit methods.</p> <code>scheduler.stop()</code><code>scheduler.run(timeout=...)</code> <p>You can explicitly call <code>stop()</code> from aywhere on the <code>Scheduler</code> to stop it. By default this will wait for any currently running compute to finish but you can inform the scheduler to stop immediately with <code>run(wait=False)</code>.</p> <p>You'll notice this in the event count of the Scheduler where the event <code>@on_future_cancelled</code> was fired.</p> <p><pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function(sleep_for: int) -&gt; None:\n    time.sleep(sleep_for)\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, sleep_for=10)\n\n@scheduler.on_future_submitted\ndef stop_the_scheduler(_) -&gt; None:\n    scheduler.stop()\n\nscheduler.run(wait=False)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop 1\n    @on_timeout\n    @on_future_submitted 1\n    \u2514\u2500\u2500 def stop_the_scheduler(_) -&gt; 'None' (1)\n    @on_future_done\n    @on_future_cancelled 1\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p>You can also tell the <code>Scheduler</code> to stop after a certain amount of time with the <code>timeout=</code> argument to <code>run()</code>.</p> <p>This will also trigger the <code>@on_timeout</code> event as seen in the <code>Scheduler</code> output.</p> <p><pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; None:\n    time.sleep(0.1)\n    return 42\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\n# The will endlessly loop the scheduler\n@scheduler.on_future_done\ndef submit_again(future: Future) -&gt; None:\n    if scheduler.running():\n        scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1)  # End after 1 second\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 10\n    @on_future_done 10\n    \u2514\u2500\u2500 def submit_again(future: 'Future') -&gt; 'None' (10)\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 10\n</code>\n</pre> </p> </p>"},{"location":"guides/scheduling/#exceptions","title":"Exceptions","text":"<p>Dealing with exceptions is an important part of any AutoML system. It is important to clarify that there's two kinds of exceptions that can occur within the Scheduler.</p> <p>The 1st kind that can happen is within some function submitted with <code>submit()</code>. When this happens, the <code>@on_future_exception</code> will be emitted, passing the exception to the callback.</p> <p>By default, the <code>Scheduler</code> will then raise the exception that occured up to your program and end it's computations. This is done by setting <code>run(on_exception=\"raise\")</code>, the default, but it also takes two other possibilities:</p> <ul> <li><code>\"ignore\"</code> - Just ignore the exception and keep running.</li> <li><code>\"end\"</code> - Stop the scheduler but don't raise it.</li> </ul> <p>One example is to just <code>stop()</code> the scheduler when some exception occurs.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef failing_compute_function(err_msg: str) -&gt; None:\n    raise ValueError(err_msg)\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(failing_compute_function, \"Failed!\")\n\n@scheduler.on_future_exception\ndef stop_the_scheduler(future: Future, exception: Exception) -&gt; None:\n    print(f\"Got exception {exception}\")\n    scheduler.stop()  # You can optionally pass `exception=` for logging purposes.\n\nscheduler.run(on_exception=\"ignore\")  # Scheduler will not stop because of the error\n</code></pre> <p>Got exception Failed!  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop 1\n    @on_timeout\n    @on_future_submitted 1\n    @on_future_done 1\n    @on_future_cancelled\n    @on_future_exception 1\n    \u2514\u2500\u2500 def stop_the_scheduler(future: 'Future', exception: 'Exception') -&gt; \n        'None' (1)\n    @on_future_result\n</code>\n</pre> </p> <p>The second kind of exception that can happen is one that happens in the main process. For example this could happen in one of your callbacks or in the <code>Scheduler</code> itself (please raise an issue if this occurs!). By default when you call <code>run()</code> it will set <code>run(on_exception=\"raise\")</code> and raise the exception that occurred, with its traceback. This is to help you debug your program.</p> <p>You may also use <code>run(on_exception=\"end\")</code> which will just end the <code>Scheduler</code> and raise no exception, or use <code>run(on_exception=\"ignore\")</code>, in which case the <code>Scheduler</code> will continue on with whatever events are next to process.</p>"},{"location":"guides/scheduling/#tasks","title":"Tasks","text":"<p>Now that we have seen how the <code>Scheduler</code> works, we can look at the <code>Task</code>, a wrapper around a function that you'll want to submit to the <code>Scheduler</code>. The preferred way to create one of these <code>Tasks</code> is to use <code>scheduler.task(function)</code>.</p>"},{"location":"guides/scheduling/#running-a-task","title":"Running a task","text":"<p>In the following example, we will create a task for the scheduler and attempt to call it. This task will be run by the backend specified.</p> <pre><code>from amltk import Scheduler\n\n# Some function to offload to compute\ndef collatz(n: int) -&gt; int:\n    is_even = (n % 2 == 0)\n    return int(n / 2) if is_even else int(3 * n + 1)\n\nscheduler = Scheduler.with_processes(1)\n\n# Creating a \"task\"\ncollatz_task = scheduler.task(collatz)\n\ntry:\n    collatz_task.submit(5)\nexcept Exception as e:\n    print(f\"{type(e)}: {e}\")\n</code></pre> <pre><code>&lt;class 'amltk.exceptions.SchedulerNotRunningError'&gt;: Scheduler is not running, cannot submit task &lt;function collatz at 0x7f24985a25f0&gt; with args=(5,), kwargs={}\n</code></pre> <p>As you can see, we can not submit tasks before the scheduler is running. This is because the backend that it's running on usually has to setup and teardown when <code>scheduler.run()</code> is called.</p> <p>The proper approach would be to do the following:</p> <pre><code>from amltk import Scheduler\n\n# Some function to offload to compute\ndef collatz(n: int) -&gt; int:\n    is_even = (n % 2 == 0)\n    return int(n / 2) if is_even else int(3 * n + 1)\n\n# Setup the scheduler and create a \"task\"\nscheduler = Scheduler.with_processes(1)\ncollatz_task = scheduler.task(collatz)\n\n@scheduler.on_start\ndef launch_initial_task() -&gt; None:\n    collatz_task.submit(5)\n\nscheduler.run()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def launch_initial_task() -&gt; 'None' (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 1\n\u2503   @on_future_done 1\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 1\n\u2517\u2501\u2501 \u256d\u2500 Task collatz(n: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 1                                                          \u2502\n    \u2502 @on_done 1                                                               \u2502\n    \u2502 @on_result 1                                                             \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: BJCDwTqJ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"guides/scheduling/#task-specific-events","title":"Task Specific Events","text":"<p>As you may have noticed, we can see the <code>Task</code> itself in the <code>Scheduler</code> as well as the events it defines. This allows us to react to certain tasks themselves, and not generally everything that may pass through the <code>Scheduler</code>.</p> <p>In the below example, we'll do two things. First is we'll create a <code>Task</code> and react to it's events, but also use the <code>Scheduler</code> directly and use <code>submit()</code>. Then we'll see how the callbacks reacted to different events.</p> <pre><code>from amltk import Scheduler\n\ndef echo(msg: str) -&gt; str:\n    return msg\n\nscheduler = Scheduler.with_processes(1)\necho_task = scheduler.task(echo)\n\n# Launch the task and do a raw `submit()` with the Scheduler\n@scheduler.on_start\ndef launch_initial_task() -&gt; None:\n    echo_task.submit(\"hello\")\n    scheduler.submit(echo, \"hi\")\n\n# Callback for anything result from the scheduler\n@scheduler.on_future_result\ndef from_scheduler(_, msg: str) -&gt; None:\n    print(f\"result_from_scheduler {msg}\")\n\n# Callback for specifically results from the `echo_task`\n@echo_task.on_result\ndef from_task(_, msg: str) -&gt; None:\n    print(f\"result_from_scheduler {msg}\")\n\nscheduler.run()\n</code></pre> <p>result_from_scheduler hello result_from_scheduler hello result_from_scheduler hi  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def launch_initial_task() -&gt; 'None' (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 2\n\u2503   @on_future_done 2\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 2\n\u2503   \u2514\u2500\u2500 def from_scheduler(_, msg: 'str') -&gt; 'None' (2)\n\u2517\u2501\u2501 \u256d\u2500 Task echo(msg: 'str') -&gt; 'str' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 1                                                          \u2502\n    \u2502 @on_done 1                                                               \u2502\n    \u2502 @on_result 1                                                             \u2502\n    \u2502 \u2514\u2500\u2500 def from_task(_, msg: 'str') -&gt; 'None' (1)                           \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 0Faaxt7X \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>We can see in the output of the above code that the <code>@scheduler.on_future_result</code> was called twice, meaning our callback <code>def from_scheduler()</code> was called twice, one for the result of <code>echo_task.submit(\"hello\")</code> and the other from <code>scheduler.submit(echo, \"hi\")</code>. On the other hand, the event <code>@task.on_result</code> was only called once, meaning our callback <code>def from_task()</code> was only called once.</p> <p>In practice, you will likely need to define a variety of tasks for your AutoML System and having dedicated code to respond to individual tasks is of critical importance. This can even allow you to chain the results of one task into another, and define more complex workflows.</p> <p>The below example shows how you can define two tasks with the scheduler and have certain callbacks for different tasks, or even share callbacks between them!</p> <pre><code>from amltk import Scheduler\n\ndef expensive_thing_1(x: int) -&gt; int:\n    return x * 2\n\ndef expensive_thing_2(x: int) -&gt; int:\n    return x ** 2\n\n# Create a scheduler and 2 tasks\nscheduler = Scheduler.with_processes(1)\ntask_1 = scheduler.task(expensive_thing_1)\ntask_2 = scheduler.task(expensive_thing_1)\n\n# A list of things we want to compute\nitems = iter([1, 2, 3])\n\n@scheduler.on_start\ndef submit_initial() -&gt; None:\n    next_item = next(items)\n    task_1.submit(next_item)\n\n@task_1.on_result\ndef submit_task_2_with_results_of_task_1(_, result: int) -&gt; None:\n    \"\"\"When task_1 returns, send the result to task_2\"\"\"\n    task_2.submit(result)\n\n@task_1.on_result\ndef submit_task_1_with_next_item(_, result: int) -&gt; None:\n    \"\"\"When task_1 returns, launch it again with the next items\"\"\"\n    next_item = next(items, None)\n    if next_item is not None:\n        task_1.submit(next_item)\n        return\n\n    print(\"Done!\")\n\n# You may share callbacks for the two tasks\n@task_1.on_exception\n@task_2.on_exception\ndef handle_task_exception(_, exception: BaseException) -&gt; None:\n    print(f\"A task errored! {exception}\")\n\nscheduler.run()\n</code></pre> <p>Done!  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def submit_initial() -&gt; 'None' (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 6\n\u2503   @on_future_done 6\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 6\n\u2523\u2501\u2501 \u256d\u2500 Task expensive_thing_1(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2503   \u2502 @on_submitted 3                                                          \u2502\n\u2503   \u2502 @on_done 3                                                               \u2502\n\u2503   \u2502 @on_result 3                                                             \u2502\n\u2503   \u2502 \u251c\u2500\u2500 def submit_task_2_with_results_of_task_1(_, result: 'int') -&gt; 'None' \u2502\n\u2503   \u2502 \u2502   (3)                                                                  \u2502\n\u2503   \u2502 \u2514\u2500\u2500 def submit_task_1_with_next_item(_, result: 'int') -&gt; 'None' (3)     \u2502\n\u2503   \u2502 @on_exception                                                            \u2502\n\u2503   \u2502 \u2514\u2500\u2500 def handle_task_exception(_, exception: 'BaseException') -&gt; 'None'   \u2502\n\u2503   \u2502 @on_cancelled                                                            \u2502\n\u2503   \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: HJikumcd \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 \u256d\u2500 Task expensive_thing_1(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 3                                                          \u2502\n    \u2502 @on_done 3                                                               \u2502\n    \u2502 @on_result 3                                                             \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 \u2514\u2500\u2500 def handle_task_exception(_, exception: 'BaseException') -&gt; 'None'   \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 7f6GS5Kg \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"guides/scheduling/#task-plugins","title":"Task Plugins","text":"<p>Another benefit of <code>Task</code> objects is that we can attach a <code>Plugin</code> to them. These plugins can automate control behaviour of tasks, either through preventing their execution, modifying the function and its arguments or even attaching plugin specific events!</p> <p>For a complete reference, please see the plugin reference page.</p>"},{"location":"guides/scheduling/#call-limiter","title":"Call Limiter","text":"<p>Perhaps one of the more useful plugins, at least when designing an AutoML System is the <code>Limiter</code> plugin. This can help you control both it's concurrency or the absolute limit of how many times a certain task can be successfully submitted.</p> <p>In the following contrived example, we will setup a <code>Scheduler</code> with 2 workers and attempt to submit a <code>Task</code> 4 times in rapid succession. However we have the constraint that we only ever want 2 of these tasks running at a given time. Let's see how we could achieve that.</p> <pre><code>from amltk.scheduling import Scheduler, Limiter\n\ndef my_func(x: int) -&gt; int:\n    return x\n\nscheduler = Scheduler.with_processes(2)\n\n# Specify a concurrency limit of 2\ntask = scheduler.task(my_func, plugins=Limiter(max_concurrent=2))\n\n# A list of 10 things we want to compute\nitems = iter(range(10))\nresults = []\n\n@scheduler.on_start(repeat=4)  # Repeat callback 4 times\ndef submit() -&gt; None:\n    next_item = next(items)\n    task.submit(next_item)\n\n@task.on_result\ndef record_result(_, result: int) -&gt; None:\n    results.append(result)\n\n@task.on_result\ndef launch_another(_, result: int) -&gt; None:\n    next_item = next(items, None)\n    if next_item is not None:\n        task.submit(next_item)\n\nscheduler.run()\nprint(results)\n</code></pre> <p>[1, 0, 4, 5, 6, 7, 8, 9]  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def submit() -&gt; 'None' (4)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 8\n\u2503   @on_future_done 8\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 8\n\u2517\u2501\u2501 \u256d\u2500 Task my_func(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n    \u2502 \u2502 Concurrent 0/2                                                       \u2502 \u2502\n    \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n    \u2502 @on_submitted 8                                                          \u2502\n    \u2502 @on_done 8                                                               \u2502\n    \u2502 @on_result 8                                                             \u2502\n    \u2502 \u251c\u2500\u2500 def record_result(_, result: 'int') -&gt; 'None' (8)                    \u2502\n    \u2502 \u2514\u2500\u2500 def launch_another(_, result: 'int') -&gt; 'None' (8)                   \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2502 @call-limit-reached                                                      \u2502\n    \u2502 @concurrent-limit-reached 2                                              \u2502\n    \u2502 @disabled-due-to-running-task                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: V0pl8ZPU \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>You can notice that this limiting worked, given the numbers <code>2</code> and <code>3</code> were skipped and not printed. As expected, we successfully launched the task with both  <code>0</code> and <code>1</code> but as these tasks were not done processing, the <code>Limiter</code> kicks in and prevents the other two.</p> <p>A natural extension to ask is then, \"how do we requeue these?\". Well lets take a look at the above output. The plugin has added three new events to <code>Task</code>, namely <code>@call-limit-reached</code>, <code>@concurrent-limit-reached</code> and <code>@disabled-due-to-running-task</code>.</p> <p>To subscribe to these extra events (or any for that matter), we can use the <code>task.on()</code> method. Below is the same example except here we respond to <code>@call-limit-reached</code> and requeue the submissions that failed.</p> <pre><code>from amltk.scheduling import Scheduler, Limiter, Task\nfrom amltk.types import Requeue\n\ndef my_func(x: int) -&gt; int:\n    return x\n\nscheduler = Scheduler.with_processes(2)\ntask = scheduler.task(my_func, plugins=Limiter(max_concurrent=2))\n\n# A list of 10 things we want to compute\nitems = Requeue(range(10))  # A convenience type that you can requeue/append to\nresults = []\n\n@scheduler.on_start(repeat=4)  # Repeat callback 4 times\ndef submit() -&gt; None:\n    next_item = next(items)\n    task.submit(next_item)\n\n@task.on(\"concurrent-limit-reached\")\ndef add_back_to_queue(task: Task, x: int) -&gt; None:\n    items.requeue(x)  # Put x back at the start of the queue\n\n@task.on_result\ndef record_result(_, result: int) -&gt; None:\n    results.append(result)\n\n@task.on_result\ndef launch_another(_, result: int) -&gt; None:\n    next_item = next(items, None)\n    if next_item is not None:\n        task.submit(next_item)\n\nscheduler.run()\nprint(results)\n</code></pre> <p>[1, 0, 2, 3, 4, 5, 6, 7, 8, 9]  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def submit() -&gt; 'None' (4)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 10\n\u2503   @on_future_done 10\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 10\n\u2517\u2501\u2501 \u256d\u2500 Task my_func(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n    \u2502 \u2502 Concurrent 0/2                                                       \u2502 \u2502\n    \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n    \u2502 @on_submitted 10                                                         \u2502\n    \u2502 @on_done 10                                                              \u2502\n    \u2502 @on_result 10                                                            \u2502\n    \u2502 \u251c\u2500\u2500 def record_result(_, result: 'int') -&gt; 'None' (10)                   \u2502\n    \u2502 \u2514\u2500\u2500 def launch_another(_, result: 'int') -&gt; 'None' (10)                  \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2502 @call-limit-reached                                                      \u2502\n    \u2502 @concurrent-limit-reached 2                                              \u2502\n    \u2502 \u2514\u2500\u2500 def add_back_to_queue(task: 'Task', x: 'int') -&gt; 'None' (2)          \u2502\n    \u2502 @disabled-due-to-running-task                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: odMHq7zw \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"guides/scheduling/#under-construction","title":"Under Construction","text":"<p>Please see the following reference pages in the meantime:</p> <ul> <li>scheduler reference - A slighltly     more condensed version of how to use the <code>Scheduler</code>.</li> <li>task reference - A more comprehensive     explanation of <code>Task</code>s and their <code>@events</code>.</li> <li>plugin reference - An intro to plugins     and how to create your own.</li> <li>executors reference - A list of     executors and how to use them.</li> <li>events reference - A more comprehensive     look at the event system in AutoML-Toolkit and how to work with them or extend them.</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Here you'll find a non-exhaustive but quick reference to many of the core types and utilities available to AMLTK. Please use the Table of Contents on the left to browse them.</p> <p>If you're looking for a more in depth understanding of how automl-toolkit works, please take a look at the guides section.</p> <p>If you're looking for signatures and specific function documentation, check out the API docs.</p>"},{"location":"reference/data/","title":"Data","text":"<p>AutoML-Toolkit provides some utility for manipulating data containers, specifically <code>pd.DataFrame</code>, <code>pd.Series</code>, <code>np.ndarray</code>.</p>"},{"location":"reference/data/#reducing-the-size-of-your-data-in-memory","title":"Reducing the size of your data in memory","text":"<p>Often times, the defaults of <code>numpy</code> and <code>pandas</code> is to use large dtypes that are suited for most tasks. However sometimes this can be prohibitive, especially in low memory compute regimes.</p> <p>To measure the memory consumption of a data container, we can use <code>byte_size()</code>. While independant methods exist for each of these structures, we wrap them together in a single function for convenience.</p> ref-data-bytesize<pre><code>from amltk.data import byte_size\n\nimport pandas as pd\nimport numpy as np\n\nx = np.arange(100)\ny = pd.Series(np.linspace(1, 100, 100))\nz = pd.DataFrame({\"a\": np.arange(100), \"b\": pd.Series(np.linspace(1, 100, 100))})\n\nprint(\"x: \", byte_size(x))\nprint(\"y: \", byte_size(y))\nprint(\"z: \", byte_size(z))\n\nprint(\"combined: \", byte_size([x, y, z]))\n</code></pre> <pre><code>x:  800\ny:  928\nz:  1728\ncombined:  3456\n</code></pre> <p>Now that we can measure the size of our data, we can use the <code>reduce_dtypes()</code> function to reduce the memory of our data by:</p> <ul> <li>Find the smallest <code>int</code> dtype that can represent integer data</li> <li>Reduce the percision of floating point data by one step. i.e. <code>float64</code> -&gt; <code>float32</code></li> </ul> ref-data-reducedtypes<pre><code>from amltk.data import reduce_dtypes, byte_size\n\nimport pandas as pd\nimport numpy as np\n\nx = np.arange(100)\ny = pd.Series(np.linspace(1, 100, 100))\nz = pd.DataFrame({\"a\": np.arange(100), \"b\": pd.Series(np.linspace(1, 100, 100))})\n\nprint(f\"x: {x.dtype}\")\nprint(f\"y: {y.dtype}\")\nprint(f\"z: {z.dtypes}\")\n\nprint(\"combined memory: \", byte_size([x, y, z]))\n\nx, y, z = [reduce_dtypes(d) for d in [x, y, z]]\n\nprint(f\"x: {x.dtype}\")\nprint(f\"y: {y.dtype}\")\nprint(f\"z: {z.dtypes}\")\n\nprint(\"combined memory: \", byte_size([x, y, z]))\n</code></pre> <pre><code>x: int64\ny: float64\nz: a      int64\nb    float64\ndtype: object\ncombined memory:  3456\nx: uint8\ny: UInt8\nz: a    UInt8\nb    UInt8\ndtype: object\ncombined memory:  956\n</code></pre>"},{"location":"reference/data/buckets/","title":"Buckets","text":"<p>A bucket is a collection of dict-like view of resources that can be accessed by a key of a given type. This lets you easily store and retrieve objects of varying types in a single location.</p> <p>The main implementation we provide is the <code>PathBucket</code>, which is a dict-like view over a directory to quickly store many files of different types and also retrieve them.</p> <pre><code>from amltk.store.paths import PathBucket\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nbucket = PathBucket(\"./path/to/bucket\")\n\narray = np.array([1, 2, 3])\ndataframe = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nmodel = LinearRegression()\n\n# Store things\nbucket[\"myarray.npy\"] = array # (1)!\nbucket[\"df.csv\"] = dataframe  # (2)!\nbucket[\"model.pkl\"].put(model)\n\nbucket[\"config.json\"] = {\"hello\": \"world\"}\nassert bucket[\"config.json\"].exists()\nbucket[\"config.json\"].remove()\n\n# Store multiple at once\nbucket.store(\n    {\n        \"myarray.npy\": array,\n        \"df.csv\": dataframe,\n        \"model.pkl\": model,\n        \"config.json\": {\"hello\": \"world\"}\n    }\n)\n\n# Load things\narray = bucket[\"myarray.npy\"].load()\nmaybe_df = bucket[\"df.csv\"].get()  # (3)!\nmodel: LinearRegression = bucket[\"model.pkl\"].get(check=LinearRegression)  # (4)!\n\n# Load multiple at once\nitems = bucket.fetch(\"myarray.npy\", \"df.csv\", \"model.pkl\", \"config.json\")\narray = items[\"myarray.npy\"]\ndf = items[\"df.csv\"]\nmodel = items[\"model.pkl\"]\nconfig = items[\"config.json\"]\n\n# Create subdirectories\nmodel_bucket = bucket / \"my_model\" # (5)!\nmodel_bucket[\"model.pkl\"] = model\nmodel_bucket[\"predictions.npy\"] = model.predict(X)\n\n# Acts like a mapping\nassert \"myarray.npy\" in bucket\nassert len(bucket) == 3\nfor key, item in bucket.items():\n    print(key, item.load())\ndel bucket[\"model.pkl\"]\n</code></pre> <ol> <li>The <code>=</code> is a shortcut for <code>bucket[\"myarray.npy\"].put(array)</code></li> <li>The extension is used to determine which     <code>PathLoader</code> to use     and how to save it.</li> <li>The <code>get</code> method acts like the <code>dict.load</code> method.</li> <li>The <code>get</code> method can be used to check the type of the loaded object.     If the type does not match, a <code>TypeError</code> is raised.</li> <li>Uses the familiar <code>Path</code> API to create subdirectories.</li> </ol>"},{"location":"reference/metalearning/","title":"Metalearning","text":"<p>An important part of AutoML systems is to perform well on new unseen data. There are a variety of methods to do so but we provide some building blocks to help implement these methods.</p> <p>API</p> <p>The meta-learning features have not been extensively used yet and such no solid API has been developed yet. We will deprecate any API subject to change before changing them.</p>"},{"location":"reference/metalearning/#metafeatures","title":"MetaFeatures","text":"<p>A <code>MetaFeature</code> is some statistic about a dataset/task, that can be used to make datasets or tasks more comparable, thus enabling meta-learning methods.</p> <p>Calculating meta-features of a dataset is quite straight foward.</p> Metafeatures<pre><code>import openml\nfrom amltk.metalearning import compute_metafeatures\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nmfs = compute_metafeatures(X, y)\n\nprint(mfs)\n</code></pre> <pre><code>instance_count                                           1000.000000\nlog_instance_count                                          6.907755\nnumber_of_classes                                           2.000000\nnumber_of_features                                         20.000000\nlog_number_of_features                                      2.995732\npercentage_missing_values                                   0.000000\npercentage_of_instances_with_missing_values                 0.000000\npercentage_of_features_with_missing_values                  0.000000\npercentage_of_categorical_columns_with_missing_values       0.000000\npercentage_of_categorical_values_with_missing_values        0.000000\npercentage_of_numeric_columns_with_missing_values           0.000000\npercentage_of_numeric_values_with_missing_values            0.000000\nnumber_of_numeric_features                                  7.000000\nnumber_of_categorical_features                             13.000000\nratio_numerical_features                                    0.350000\nratio_categorical_features                                  0.650000\nratio_features_to_instances                                 0.020000\nminority_class_imbalance                                    0.200000\nmajority_class_imbalance                                    0.200000\nclass_imbalance                                             0.400000\nmean_categorical_imbalance                                  0.500500\nstd_categorical_imbalance                                   0.234994\nskewness_mean                                               0.920379\nskewness_std                                                0.904952\nskewness_min                                               -0.531348\nskewness_max                                                1.949628\nkurtosis_mean                                               0.924278\nkurtosis_std                                                1.785467\nkurtosis_min                                               -1.381449\nkurtosis_max                                                4.292590\ndtype: float64\n</code></pre> <p>By default <code>compute_metafeatures()</code> will calculate all the <code>MetaFeature</code> implemented, iterating through their subclasses to do so. You can pass an explicit list as well to <code>compute_metafeatures(X, y, features=[...])</code>.</p> <p>To implement your own is also quite straight forward:</p> Create Metafeature<pre><code>from amltk.metalearning import MetaFeature, compute_metafeatures\nimport openml\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nclass TotalValues(MetaFeature):\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; int:\n        return int(x.shape[0] * x.shape[1])\n\nmfs = compute_metafeatures(X, y, features=[TotalValues])\nprint(mfs)\n</code></pre> <pre><code>total_values    20000\ndtype: int64\n</code></pre> <p>As many metafeatures rely on pre-computed dataset statistics, and they do not need to be calculated more than once, you can specify the dependancies of a meta feature. When a metafeature would return something other than a single value, i.e. a <code>dict</code> or a <code>pd.DataFrame</code>, we instead call those a <code>DatasetStatistic</code>. These will not be included in the result of <code>compute_metafeatures()</code>. These <code>DatasetStatistic</code>s will only be calculated once on a call to <code>compute_metafeatures()</code> so they can be re-used across all <code>MetaFeature</code>s that require that dependancy.</p> Metafeature Dependancy<pre><code>from amltk.metalearning import MetaFeature, DatasetStatistic, compute_metafeatures\nimport openml\n\ndataset = openml.datasets.get_dataset(\n    31,  # credit-g\n    download_data=True,\n    download_features_meta_data=False,\n    download_qualities=False,\n)\nX, y, _, _ = dataset.get_data(\n    dataset_format=\"dataframe\",\n    target=dataset.default_target_attribute,\n)\n\nclass NAValues(DatasetStatistic):\n    \"\"\"A mask of all NA values in a dataset\"\"\"\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; pd.DataFrame:\n        return x.isna()\n\n\nclass PercentageNA(MetaFeature):\n    \"\"\"The percentage of values missing\"\"\"\n\n    dependencies = (NAValues,)\n\n    @classmethod\n    def compute(\n        cls,\n        x: pd.DataFrame,\n        y: pd.Series | pd.DataFrame,\n        dependancy_values: dict,\n    ) -&gt; int:\n        na_values = dependancy_values[NAValues]\n        n_na = na_values.sum().sum()\n        n_values = int(x.shape[0] * x.shape[1])\n        return float(n_na / n_values)\n\nmfs = compute_metafeatures(X, y, features=[PercentageNA])\nprint(mfs)\n</code></pre> <pre><code>percentage_n_a    0.0\ndtype: float64\n</code></pre> <p>To view the description of a particular <code>MetaFeature</code>, you can call <code>.description()</code> on it. Otherwise you can access all of them in the following way:</p> SourceResult Metafeature Descriptions<pre><code>from pprint import pprint\nfrom amltk.metalearning import metafeature_descriptions\n\ndescriptions = metafeature_descriptions()\nfor name, description in descriptions.items():\n    print(\"---\")\n    print(name)\n    print(\"---\")\n    print(\" * \" + description)\n</code></pre> <pre><code>---\ninstance_count\n---\n * Number of instances in the dataset.\n---\nlog_instance_count\n---\n * Logarithm of the number of instances in the dataset.\n---\nnumber_of_classes\n---\n * Number of classes in the dataset.\n---\nnumber_of_features\n---\n * Number of features in the dataset.\n---\nlog_number_of_features\n---\n * Logarithm of the number of features in the dataset.\n---\npercentage_missing_values\n---\n * Percentage of missing values in the dataset.\n---\npercentage_of_instances_with_missing_values\n---\n * Percentage of instances with missing values.\n---\npercentage_of_features_with_missing_values\n---\n * Percentage of features with missing values.\n---\npercentage_of_categorical_columns_with_missing_values\n---\n * Percentage of categorical columns with missing values.\n---\npercentage_of_categorical_values_with_missing_values\n---\n * Percentage of categorical values with missing values.\n---\npercentage_of_numeric_columns_with_missing_values\n---\n * Percentage of numeric columns with missing values.\n---\npercentage_of_numeric_values_with_missing_values\n---\n * Percentage of numeric values with missing values.\n---\nnumber_of_numeric_features\n---\n * Number of numeric features in the dataset.\n---\nnumber_of_categorical_features\n---\n * Number of categorical features in the dataset.\n---\nratio_numerical_features\n---\n * Ratio of numerical features to total features in the dataset.\n---\nratio_categorical_features\n---\n * Ratio of categoricals features to total features in the dataset.\n---\nratio_features_to_instances\n---\n * Ratio of features to instances in the dataset.\n---\nminority_class_imbalance\n---\n * Imbalance of the minority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.\n---\nmajority_class_imbalance\n---\n * Imbalance of the majority class in the dataset. 0 =&gt; Balanced. 1 imbalanced.\n---\nclass_imbalance\n---\n * Mean Target Imbalance of the classes in general.\n\n    0 =&gt; Balanced. 1 Imbalanced.\n\n---\nmean_categorical_imbalance\n---\n * The mean imbalance of categorical features.\n---\nstd_categorical_imbalance\n---\n * The std imbalance of categorical features.\n---\nskewness_mean\n---\n * The mean skewness of numerical features.\n---\nskewness_std\n---\n * The std skewness of numerical features.\n---\nskewness_min\n---\n * The min skewness of numerical features.\n---\nskewness_max\n---\n * The max skewness of numerical features.\n---\nkurtosis_mean\n---\n * The mean kurtosis of numerical features.\n---\nkurtosis_std\n---\n * The std kurtosis of numerical features.\n---\nkurtosis_min\n---\n * The min kurtosis of numerical features.\n---\nkurtosis_max\n---\n * The max kurtosis of numerical features.\n---\ntotal_values\n---\n * \n---\npercentage_n_a\n---\n * The percentage of values missing\n</code></pre>"},{"location":"reference/metalearning/#dataset-distances","title":"Dataset Distances","text":"<p>One common way to define how similar two datasets are is to compute some \"similarity\" between them. This notion of \"similarity\" requires computing some features of a dataset (metafeatures) first, such that we can numerically compute some distance function.</p> <p>Let's see how we can quickly compute the distance between some datasets with <code>dataset_distance()</code>!</p> Dataset Distances P.1<pre><code>import pandas as pd\nimport openml\n\nfrom amltk.metalearning import compute_metafeatures\n\ndef get_dataset(dataset_id: int) -&gt; tuple[pd.DataFrame, pd.Series]:\n    dataset = openml.datasets.get_dataset(\n        dataset_id,\n        download_data=True,\n        download_features_meta_data=False,\n        download_qualities=False,\n    )\n    X, y, _, _ = dataset.get_data(\n        dataset_format=\"dataframe\",\n        target=dataset.default_target_attribute,\n    )\n    return X, y\n\nd31 = get_dataset(31)\nd3 = get_dataset(3)\nd4 = get_dataset(4)\n\nmetafeatures_dict = {\n    \"dataset_31\": compute_metafeatures(*d31),\n    \"dataset_3\": compute_metafeatures(*d3),\n    \"dataset_4\": compute_metafeatures(*d4),\n}\n\nmetafeatures = pd.DataFrame(metafeatures_dict)\nprint(metafeatures)\n</code></pre> <pre><code>                                                     dataset_31  ...  dataset_4\ninstance_count                                      1000.000000  ...  57.000000\nlog_instance_count                                     6.907755  ...   4.043051\nnumber_of_classes                                      2.000000  ...   2.000000\nnumber_of_features                                    20.000000  ...  16.000000\nlog_number_of_features                                 2.995732  ...   2.772589\npercentage_missing_values                              0.000000  ...   0.357456\npercentage_of_instances_with_missing_values            0.000000  ...   0.982456\npercentage_of_features_with_missing_values             0.000000  ...   1.000000\npercentage_of_categorical_columns_with_missing_...     0.000000  ...   1.000000\npercentage_of_categorical_values_with_missing_v...     0.000000  ...   0.410088\npercentage_of_numeric_columns_with_missing_values      0.000000  ...   1.000000\npercentage_of_numeric_values_with_missing_values       0.000000  ...   0.304825\nnumber_of_numeric_features                             7.000000  ...   8.000000\nnumber_of_categorical_features                        13.000000  ...   8.000000\nratio_numerical_features                               0.350000  ...   0.500000\nratio_categorical_features                             0.650000  ...   0.500000\nratio_features_to_instances                            0.020000  ...   0.280702\nminority_class_imbalance                               0.200000  ...   0.149123\nmajority_class_imbalance                               0.200000  ...   0.149123\nclass_imbalance                                        0.400000  ...   0.298246\nmean_categorical_imbalance                             0.500500  ...   0.308063\nstd_categorical_imbalance                              0.234994  ...   0.228906\nskewness_mean                                          0.920379  ...   0.255076\nskewness_std                                           0.904952  ...   1.420729\nskewness_min                                          -0.531348  ...  -2.007217\nskewness_max                                           1.949628  ...   3.318064\nkurtosis_mean                                          0.924278  ...   2.046258\nkurtosis_std                                           1.785467  ...   4.890029\nkurtosis_min                                          -1.381449  ...  -2.035406\nkurtosis_max                                           4.292590  ...  13.193069\n\n[30 rows x 3 columns]\n</code></pre> <p>Now we want to know which one of <code>\"dataset_3\"</code> or <code>\"dataset_4\"</code> is more similar to <code>\"dataset_31\"</code>.</p> Dataset Distances P.2<pre><code>from amltk.metalearning import dataset_distance\n\ntarget = metafeatures_dict.pop(\"dataset_31\")\nothers = metafeatures_dict\n\ndistances = dataset_distance(target, others, distance_metric=\"l2\")\nprint(distances)\n</code></pre> <pre><code>dataset_4     943.079572\ndataset_3    2196.197231\nName: l2, dtype: float64\n</code></pre> <p>Seems like <code>\"dataset_3\"</code> is some notion of closer to <code>\"dataset_31\"</code> than <code>\"dataset_4\"</code>. However the scale of the metafeatures are not exactly all close. For example, many lie between <code>(0, 1)</code> but some like <code>instance_count</code> can completely dominate the show.</p> <p>Lets repeat the computation but specify that we should apply a <code>\"minmax\"</code> scaling across the rows.</p> Dataset Distances P.3<pre><code>distances = dataset_distance(\n    target,\n    others,\n    distance_metric=\"l2\",\n    scaler=\"minmax\"\n)\nprint(distances)\n</code></pre> <pre><code>dataset_3    3.293831\ndataset_4    3.480296\nName: l2, dtype: float64\n</code></pre> <p>Now <code>\"dataset_3\"</code> is considered more similar but the difference between the two is a lot less dramatic. In general, applying some scaling to values of different scales is required for metalearning.</p> <p>You can also use an sklearn.preprocessing.MinMaxScaler or anything other scaler from scikit-learn for that matter.</p> Dataset Distances P.3<pre><code>from sklearn.preprocessing import MinMaxScaler\n\ndistances = dataset_distance(\n    target,\n    others,\n    distance_metric=\"l2\",\n    scaler=MinMaxScaler()\n)\nprint(distances)\n</code></pre> <pre><code>dataset_3    3.293831\ndataset_4    3.480296\nName: l2, dtype: float64\n</code></pre>"},{"location":"reference/metalearning/#portfolio-selection","title":"Portfolio Selection","text":"<p>A portfolio in meta-learning is to a set (ordered or not) of configurations that maximize some notion of coverage across datasets or tasks. The intuition here is that this also means that any new dataset is also covered!</p> <p>Suppose we have the given performances of some configurations across some datasets. Initial Portfolio<pre><code>import pandas as pd\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\nprint(portfolio)\n</code></pre> <pre><code>           c1  c2  c3  c4\ndataset_1  90  20  10  90\ndataset_2  60  10  20  10\ndataset_3  20  90  40  10\ndataset_4  10  20  90  10\n</code></pre> </p> <p>If we could only choose <code>k=3</code> of these configurations on some new given dataset, which ones would you choose and in what priority? Here is where we can apply <code>portfolio_selection()</code>!</p> <p>The idea is that we pick a subset of these algorithms that maximise some value of utility for the portfolio. We do this by adding a single configuration from the entire set, 1-by-1 until we reach <code>k</code>, beginning with the empty portfolio.</p> <p>Let's see this in action!</p> Portfolio Selection<pre><code>import pandas as pd\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\"\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre> <p>The trajectory tells us which configuration was added at each time stamp along with the utility of the portfolio with that configuration added. However we havn't specified how exactly we defined the utility of a given portfolio. We could define our own function to do so:</p> Portfolio Selection Custom<pre><code>import pandas as pd\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\ndef my_function(p: pd.DataFrame) -&gt; float:\n    # Take the maximum score for each dataset and then take the mean across them.\n    return p.max(axis=1).mean()\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\",\n    portfolio_value=my_function,\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre> <p>This notion of reducing across all configurations for a dataset and then aggregating these is common enough that we can also directly just define these operations and we will perform the rest.</p> Portfolio Selection With Reduction<pre><code>import pandas as pd\nimport numpy as np\nfrom amltk.metalearning import portfolio_selection\n\nperformances = {\n    \"c1\": [90, 60, 20, 10],\n    \"c2\": [20, 10, 90, 20],\n    \"c3\": [10, 20, 40, 90],\n    \"c4\": [90, 10, 10, 10],\n}\nportfolio = pd.DataFrame(performances, index=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n\nselected_portfolio, trajectory = portfolio_selection(\n    portfolio,\n    k=3,\n    scaler=\"minmax\",\n    row_reducer=np.max,  # This is actually the default\n    aggregator=np.mean,  # This is actually the default\n)\n\nprint(selected_portfolio)\nprint()\nprint(trajectory)\n</code></pre> <pre><code>              c1     c3     c2\ndataset_1  1.000  0.000  0.125\ndataset_2  1.000  0.200  0.000\ndataset_3  0.125  0.375  1.000\ndataset_4  0.000  1.000  0.125\n\nc1    0.53125\nc3    0.84375\nc2    1.00000\ndtype: float64\n</code></pre>"},{"location":"reference/optimization/history/","title":"History","text":"API links <ul> <li><code>History</code></li> <li><code>Report</code></li> </ul>"},{"location":"reference/optimization/history/#basic-usage","title":"Basic Usage","text":"<p>The <code>History</code> class is used to store <code>Report</code>s from <code>Trial</code>s.</p> <p>In it's most simple usage, you can simply <code>add()</code> a <code>Report</code> as you recieve them and then use the <code>df()</code> method to get a <code>pandas.DataFrame</code> of the history.</p> Reference History<pre><code>from amltk.optimization import Trial, History, Metric\n\nloss = Metric(\"loss\", minimize=True)\n\ndef quadratic(x):\n    return x**2\n\nhistory = History()\ntrials = [\n    Trial(name=f\"trial_{count}\", config={\"x\": i}, metrics=[loss])\n    for count, i in enumerate(range(-5, 5))\n]\n\nreports = []\nfor trial in trials:\n    with trial.begin():\n        x = trial.config[\"x\"]\n        report = trial.success(loss=quadratic(x))\n        history.add(report)\n\nprint(history.df())\n</code></pre> <pre><code>          status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                    ...                                   \ntrial_0  success        &lt;NA&gt;        NA  ...      0.000051      wall    seconds\ntrial_1  success        &lt;NA&gt;        NA  ...      0.000025      wall    seconds\ntrial_2  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_3  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_4  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_5  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_6  success        &lt;NA&gt;        NA  ...      0.000021      wall    seconds\ntrial_7  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_8  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\ntrial_9  success        &lt;NA&gt;        NA  ...       0.00002      wall    seconds\n\n[10 rows x 19 columns]\n</code></pre> <p>Typically, to use this inside of an optimization run, you would add the reports inside of a callback from your <code>Task</code>s. Please see the optimization guide for more details.</p> With an Optimizer and Scheduler <pre><code>from amltk.optimization import Trial, History, Metric\nfrom amltk.scheduling import Scheduler\nfrom amltk.pipeline import Searchable\n\nsearchable = Searchable(\"quad\", space={\"x\": (-5, 5)})\nn_workers = 2\n\ndef quadratic(x):\n    return x**2\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    with trial.begin():\n        cost = quadratic(x)\n        return trial.success(cost=cost)\n\noptimizer = SMACOptimizer(space=searchable, metrics=Metric(\"cost\", minimize=True), seed=42)\n\nscheduler = Scheduler.with_processes(2)\ntask = scheduler.task(quadratic)\n\n@scheduler.on_start(repeat=n_workers)\ndef launch_trial():\n    trial = optimizer.ask()\n    task(trial)\n\n@task.on_result\ndef add_to_history(report):\n    history.add(report)\n\n@task.on_done\ndef launch_another(_):\n    trial = optimizer.ask()\n    task(trial)\n\nscheduler.run(timeout=3)\n</code></pre>"},{"location":"reference/optimization/history/#querying","title":"Querying","text":"<p>The <code>History</code> can be queried by either an index or by the trial name.</p> History Querying [str]<pre><code>last_report = history[-1]\nprint(last_report)\nprint(history[last_report.name])\n</code></pre> <pre><code>Trial.Report(trial=Trial(name='trial_9', config={'x': 4}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='loss', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'loss': 16.0}, metric_values=(Metric.Value(metric=Metric(name='loss', minimize=True, bounds=None), value=16.0),), metric_defs={'loss': Metric(name='loss', minimize=True, bounds=None)}, metric_names=('loss',))\nTrial.Report(trial=Trial(name='trial_9', config={'x': 4}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='loss', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'loss': 16.0}, metric_values=(Metric.Value(metric=Metric(name='loss', minimize=True, bounds=None), value=16.0),), metric_defs={'loss': Metric(name='loss', minimize=True, bounds=None)}, metric_names=('loss',))\n</code></pre> <pre><code>for report in history:\n    print(report.name, f\"loss = {report.metrics['loss']}\")\n</code></pre> <pre><code>trial_0 loss = 25.0\ntrial_1 loss = 16.0\ntrial_2 loss = 9.0\ntrial_3 loss = 4.0\ntrial_4 loss = 1.0\ntrial_5 loss = 0.0\ntrial_6 loss = 1.0\ntrial_7 loss = 4.0\ntrial_8 loss = 9.0\ntrial_9 loss = 16.0\n</code></pre> <pre><code>sorted_history = history.sortby(\"loss\")\nprint(sorted_history[0])\n</code></pre> <pre><code>Trial.Report(trial=Trial(name='trial_5', config={'x': 0}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='loss', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'loss': 0.0}, metric_values=(Metric.Value(metric=Metric(name='loss', minimize=True, bounds=None), value=0.0),), metric_defs={'loss': Metric(name='loss', minimize=True, bounds=None)}, metric_names=('loss',))\n</code></pre>"},{"location":"reference/optimization/history/#filtering","title":"Filtering","text":"<p>You can filter the history by using the <code>filter()</code> method. This method takes a <code>Callable[[Trial.Report], bool]</code> and returns a new <code>History</code> with only the <code>Report</code>s that return <code>True</code> from the given function.</p> Filtering<pre><code>def is_even(report):\n    return report.config[\"x\"] % 2 == 0\n\neven_history = history.filter(is_even)\neven_history_df = even_history.df(profiles=False)\nprint(even_history_df)\n</code></pre> <pre><code>          status  trial_seed  ... metric:loss (minimize) config:x\nname                          ...                                \ntrial_1  success        &lt;NA&gt;  ...                     16       -4\ntrial_3  success        &lt;NA&gt;  ...                      4       -2\ntrial_5  success        &lt;NA&gt;  ...                      0        0\ntrial_7  success        &lt;NA&gt;  ...                      4        2\ntrial_9  success        &lt;NA&gt;  ...                     16        4\n\n[5 rows x 7 columns]\n</code></pre>"},{"location":"reference/optimization/metrics/","title":"Metrics","text":""},{"location":"reference/optimization/metrics/#metric","title":"Metric","text":"<p>A <code>Metric</code> to let optimizers know how to handle numeric values properly.</p> <p>A <code>Metric</code> is defined by a <code>.name: str</code> and whether it is better to <code>.minimize: bool</code> the metric. Further, you can specify <code>.bounds: tuple[lower, upper]</code> which can help optimizers and other code know how to treat metrics.</p> <p>To easily convert between <code>loss</code>, <code>score</code> of a a value in a <code>Metric.Value</code> object.</p> <p>If the metric is bounded, you can also get the <code>distance_to_optimal</code> which is the distance to the optimal value.</p> <pre><code>from amltk.optimization import Metric\n\nacc = Metric(\"accuracy\", minimize=False, bounds=(0.0, 1.0))\n\nacc_value = acc.as_value(0.9)\nprint(f\"Cost: {acc_value.distance_to_optimal}\")  # Distance to optimal.\nprint(f\"Loss: {acc_value.loss}\")  # Something that can be minimized\nprint(f\"Score: {acc_value.score}\")  # Something that can be maximized\n</code></pre> <pre><code>Cost: 0.09999999999999998\nLoss: -0.9\nScore: 0.9\n</code></pre>"},{"location":"reference/optimization/optimizers/","title":"Optimizers","text":""},{"location":"reference/optimization/optimizers/#optimizers","title":"Optimizers","text":"<p>An <code>Optimizer</code>'s goal is to achieve the optimal value for a given <code>Metric</code> or <code>Metrics</code> using repeated <code>Trials</code>.</p> <p>What differentiates AMLTK from other optimization libraries is that we rely solely on optimizers that support an \"Ask-and-Tell\" interface. This means we can \"Ask\" and optimizer for its next suggested <code>Trial</code>, and we can \"Tell\" it a <code>Report</code> when we have one. In fact, here's the required interface.</p> <pre><code>class Optimizer:\n\n    def tell(self, report: Trial.Report) -&gt; None: ...\n\n    def ask(self) -&gt; Trial: ...\n</code></pre> <p>Now we do require optimizers to implement these <code>ask()</code> and <code>tell()</code> methods, correctly filling in a <code>Trial</code> with appropriate parsing out results from the <code>Report</code>, as this will be different for every optimizer.</p> Why only Ask and Tell Optimizers? <ol> <li> <p>Easy Parallelization: Many optimizers handle running the function to optimize and hence     roll out their own parallelization schemes and store data in all various different ways. By taking     this repsonsiblity away from an optimzer and giving it to the user, we can easily parallelize how     we wish</p> </li> <li> <p>API maintenance: Many optimziers are research code and hence a bit unstable with resepct to their     API so wrapping around them can be difficult. By requiring this \"Ask-and-Tell\" interface,     we reduce the complexity of what is required of both the \"Optimizer\" and wrapping it.</p> </li> <li> <p>Full Integration: We can fully hook into the life cycle of a running optimizer. We are not relying     on the optimizer to support callbacks at every step of their hot-loop and as such, we     can fully leverage all the other systems of AutoML-toolkit</p> </li> <li> <p>Easy Integration: it makes developing and integrating new optimizers easy. You only have     to worry that the internal state of the optimizer is updated accordingly to these     two \"Ask\" and \"Tell\" events and that's it.</p> </li> </ol> <p>For a reference on implementing an optimizer you can refer to any of the following:</p>"},{"location":"reference/optimization/optimizers/#smac","title":"SMAC","text":"<p>The <code>SMACOptimizer</code>, is a wrapper around the <code>smac</code> optimizer.</p> <p>Requirements</p> <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[smac]\n\n# Or directly\npip install smac\n</code></pre> <p>This uses <code>ConfigSpace</code> as its <code>search_space()</code> to optimize.</p> <p>Users should report results using <code>trial.success()</code>.</p> <p>Visit their documentation for what you can pass to <code>SMACOptimizer.create()</code>.</p> <p>The below example shows how you can use SMAC to optimize an sklearn pipeline.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.smac import SMACOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component, Node\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Node) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        return trial.success(accuracy=accuracy)\n\n    return trial.fail()\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100), \"max_samples\": (0.1, 0.9)})\n\nmetric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = SMACOptimizer.create(space=pipeline, metrics=metric, bucket=\"smac-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\n</code></pre> <pre><code>                                                     status  ...  time:unit\nname                                                         ...           \nconfig_id=2_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=1_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=3_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=4_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=6_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=5_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=8_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=7_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=10_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=9_seed=1787025900_budget=None_instanc...  success  ...    seconds\nconfig_id=12_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=11_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=14_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=13_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=16_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=15_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=18_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=17_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=20_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=19_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=21_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=22_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=24_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=25_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=26_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=23_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=27_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=28_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=29_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=30_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=31_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=32_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=33_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=34_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=35_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=36_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=37_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=39_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=38_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=41_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=40_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=42_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=43_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=44_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=45_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=46_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=47_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=48_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=49_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=50_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=52_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=51_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=53_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=55_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=54_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=56_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=57_seed=1787025900_budget=None_instan...  success  ...    seconds\nconfig_id=58_seed=1787025900_budget=None_instan...  success  ...    seconds\n\n[58 rows x 20 columns]\n</code></pre>"},{"location":"reference/optimization/optimizers/#neps","title":"NePs","text":"<p>The <code>NEPSOptimizer</code>, is a wrapper around the <code>NePs</code> optimizer.</p> <p>Requirements</p> <p>This requires <code>smac</code> which can be installed with:</p> <pre><code>pip install amltk[neps]\n\n# Or directly\npip install neural-pipeline-search\n</code></pre> <p>NePs is still in development</p> <p>NePs is still in development and is not yet stable. There are likely going to be issues. Please report any issues to NePs or in AMLTK.</p> <p>This uses <code>ConfigSpace</code> as its <code>search_space()</code> to optimize.</p> <p>Users should report results using <code>trial.success(loss=...)</code> where <code>loss=</code> is a scaler value to minimize. Optionally, you can also return a <code>cost=</code> which is used for more budget aware algorithms. Again, please see NeP's documentation for more.</p> <p>Conditionals in ConfigSpace</p> <p>NePs does not support conditionals in its search space. This is account for when using the <code>preferred_parser()</code>. during search space creation. In this case, it will simply remove all conditionals from the search space, which may not be ideal for the given problem at hand.</p> <p>Visit their documentation for what you can pass to <code>NEPSOptimizer.create()</code>.</p> <p>The below example shows how you can use neps to optimize an sklearn pipeline.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.neps import NEPSOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Pipeline) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        loss = 1 - accuracy\n        return trial.success(loss=loss, accuracy=accuracy)\n\n    return trial.fail()\nfrom amltk._doc import make_picklable; make_picklable(target_function)  # markdown-exec: hide\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\nmetric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = NEPSOptimizer.create(space=pipeline, metrics=metric, bucket=\"neps-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\noptimizer.bucket.rmdir()  # markdown-exec: hide\n</code></pre> <p>Deep Learning</p> <p>Write an example demonstrating NEPS with continuations</p> <p>Graph Search Spaces</p> <p>Write an example demonstrating NEPS with its graph search spaces</p>"},{"location":"reference/optimization/optimizers/#optuna","title":"Optuna","text":"<p>Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.</p> <p>Requirements</p> <p>This requires <code>Optuna</code> which can be installed with:</p> <pre><code>pip install amltk[optuna]\n\n# Or directly\npip install optuna\n</code></pre> <p>We provide a thin wrapper called <code>OptunaOptimizer</code> from which you can integrate <code>Optuna</code> into your workflow.</p> <p>This uses an Optuna-like <code>search_space()</code> for its optimization.</p> <p>Users should report results using <code>trial.success()</code> with either <code>cost=</code> or <code>values=</code> depending on any optimization directions given to the underyling optimizer created. Please see their documentation for more.</p> <p>Visit their documentation for what you can pass to <code>OptunaOptimizer.create()</code>, which is forward to <code>optun.create_study()</code>.</p> <pre><code>from __future__ import annotations\n\nimport logging\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom amltk.optimization.optimizers.optuna import OptunaOptimizer\nfrom amltk.scheduling import Scheduler\nfrom amltk.optimization import History, Trial, Metric\nfrom amltk.pipeline import Component\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef target_function(trial: Trial, pipeline: Pipeline) -&gt; Trial.Report:\n    X, y = load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    clf = pipeline.configure(trial.config).build(\"sklearn\")\n\n    with trial.begin():\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        return trial.success(accuracy=accuracy_score(y_test, y_pred))\n\n    return trial.fail()\n\npipeline = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\naccuracy_metric = Metric(\"accuracy\", minimize=False, bounds=(0, 1))\noptimizer = OptunaOptimizer.create(space=pipeline, metrics=accuracy_metric, bucket=\"optuna-doc-example\")\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(target_function)\n\nhistory = History()\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    trial = optimizer.ask()\n    task.submit(trial, pipeline)\n\n@task.on_result\ndef tell_and_launch_trial(_, report: Trial.Report):\n    if scheduler.running():\n        optimizer.tell(report)\n        trial = optimizer.ask()\n        task.submit(trial, pipeline)\n\n\n@task.on_result\ndef add_to_history(_, report: Trial.Report):\n    history.add(report)\n\nscheduler.run(timeout=3, wait=False)\n\nprint(history.df())\n</code></pre> <pre><code>                  status  trial_seed  ... time:kind time:unit\nname                                  ...                    \ntrial_number=1   success   328279307  ...      wall   seconds\ntrial_number=0   success   328279307  ...      wall   seconds\ntrial_number=2   success   328279307  ...      wall   seconds\ntrial_number=3   success   328279307  ...      wall   seconds\ntrial_number=5   success   328279307  ...      wall   seconds\n...                  ...         ...  ...       ...       ...\ntrial_number=69  success   328279307  ...      wall   seconds\ntrial_number=71  success   328279307  ...      wall   seconds\ntrial_number=72  success   328279307  ...      wall   seconds\ntrial_number=74  success   328279307  ...      wall   seconds\ntrial_number=73  success   328279307  ...      wall   seconds\n\n[75 rows x 19 columns]\n</code></pre> <p>Some more documentation</p> <p>Sorry!</p>"},{"location":"reference/optimization/optimizers/#integrating-your-own","title":"Integrating your own","text":"<p>The base <code>Optimizer</code> class, defines the API we require optimizers to implement.</p> <ul> <li><code>ask()</code> - Ask the optimizer for a     new <code>Trial</code> to evaluate.</li> <li><code>tell()</code> - Tell the optimizer     the result of the sampled config. This comes in the form of a     <code>Trial.Report</code>.</li> </ul> <p>Additionally, to aid users from switching between optimizers, the <code>preferred_parser()</code> method should return either a <code>parser</code> function or a string that can be used with <code>node.search_space(parser=..._)</code> to extract the search space for the optimizer.</p>"},{"location":"reference/optimization/profiling/","title":"Profiling","text":""},{"location":"reference/optimization/profiling/#profiling","title":"Profiling","text":"<p>:: amltk.profiling.profiler     options:         members: False</p>"},{"location":"reference/optimization/trials/","title":"Trials","text":""},{"location":"reference/optimization/trials/#trial","title":"Trial","text":"<p>A <code>Trial</code> is typically the output of <code>Optimizer.ask()</code>, indicating what the optimizer would like to evaluate next. We provide a host of convenience methods attached to the <code>Trial</code> to make it easy to save results, store artifacts, and more.</p> <p>Paired with the <code>Trial</code> is the <code>Trial.Report</code>, class, providing an easy way to report back to the optimizer's <code>tell()</code> with a simple <code>trial.success(cost=...)</code> or <code>trial.fail(cost=...)</code> call..</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial--trial","title":"Trial","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I]</code></p> <p>A <code>Trial</code> encapsulates some configuration that needs to be evaluated. Typically this is what is generated by an <code>Optimizer.ask()</code> call.</p> Usage <p>To begin a trial, you can use the <code>trial.begin()</code>, which will catch exceptions/traceback and profile the block of code.</p> <p>If all went smooth, your trial was successful and you can use <code>trial.success()</code> to generate a success <code>Report</code>, typically passing what your chosen optimizer expects, e.g. <code>\"loss\"</code> or <code>\"cost\"</code>.</p> <p>If your trial failed, you can instead use the <code>trial.fail()</code> to generate a failure <code>Report</code>, where any caught exception will be attached to it. Each <code>Optimizer</code> will take care of what to do from here.</p> <p><pre><code>from amltk.optimization import Trial, Metric\nfrom amltk.store import PathBucket\n\ncost = Metric(\"cost\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n\n    with trial.begin():\n        cost = x**2 - y\n\n    if trial.exception:\n        return trial.fail()\n\n    return trial.success(cost=cost)\n\n# ... usually obtained from an optimizer\ntrial = Trial(name=\"some-unique-name\", config={\"x\": 1, \"y\": 2}, metrics=[cost])\n\nreport = target_function(trial)\nprint(report.df())\n</code></pre> <p>                   status  trial_seed  ... time:kind time:unit name                                   ...                     some-unique-name  success          ...      wall   seconds  [1 rows x 20 columns]  <p>What you can return with <code>trial.success()</code> or <code>trial.fail()</code> depends on the <code>metrics</code> of the trial. Typically an optimizer will provide the trial with the list of metrics.</p> Metrics <p>A metric with a given name, optimal direction, and possible bounds.</p> <p>Some important properties is that they have a unique <code>.name</code> given the optimization run, a candidate <code>.config</code>' to evaluate, a possible <code>.seed</code> to use, and an <code>.info</code> object which is the optimizer specific information, if required by you.</p> <p>If using <code>Plugins</code>, they may insert some extra objects in the <code>.extra</code> dict.</p> <p>To profile your trial, you can wrap the logic you'd like to check with <code>trial.begin()</code>, which will automatically catch any errors, record the traceback, and profile the block of code, in terms of time and memory.</p> <p>You can access the profiled time and memory using the <code>.time</code> and <code>.memory</code> attributes. If you've <code>profile()</code>'ed any other intervals, you can access them by name through <code>trial.profiles</code>. Please see the <code>Profiler</code> for more.</p> Profiling with a trial. <p>profile<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"some-unique-name\", config={})\n\n# ... somewhere where you've begun your trial.\nwith trial.profile(\"some_interval\"):\n    for work in range(100):\n        pass\n\nprint(trial.profiler.df())\n</code></pre> <pre><code>               memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nsome_interval      2.083082e+09      2083082240  ...       wall    seconds\n\n[1 rows x 12 columns]\n</code></pre> </p> <p>You can also record anything you'd like into the <code>.summary</code>, a plain <code>dict</code> or use <code>trial.store()</code> to store artifacts related to the trial.</p> What to put in <code>.summary</code>? <p>For large items, e.g. predictions or models, these are highly advised to <code>.store()</code> to disk, especially if using a <code>Task</code> for multiprocessing.</p> <p>Further, if serializing the report using the <code>report.df()</code>, returning a single row, or a <code>History</code> with <code>history.df()</code> for a dataframe consisting of many of the reports, then you'd likely only want to store things that are scalar and can be serialised to disk by a pandas DataFrame.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial--report","title":"Report","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I2]</code></p> <p>The <code>Trial.Report</code> encapsulates a <code>Trial</code>, its status and any metrics/exceptions that may have occured.</p> <p>Typically you will not create these yourself, but instead use <code>trial.success()</code> or <code>trial.fail()</code> to generate them.</p> <pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True)\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\nwith trial.begin():\n    # Do some work\n    # ...\n    report: Trial.Report = trial.success(loss=1)\n\nprint(report.df())\n</code></pre> <pre><code>        status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                  ...                                   \ntrial  success        &lt;NA&gt;        NA  ...      0.000024      wall    seconds\n\n[1 rows x 19 columns]\n</code></pre> <p>These reports are used to report back metrics to an <code>Optimizer</code> with <code>Optimizer.tell()</code> but can also be stored for your own uses.</p> <p>You can access the original trial with the <code>.trial</code> attribute, and the <code>Status</code> of the trial with the <code>.status</code> attribute.</p> <p>You may also want to check out the <code>History</code> class for storing a collection of <code>Report</code>s, allowing for an easier time to convert them to a dataframe or perform some common Hyperparameter optimization parsing of metrics.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial","title":"<code>class Trial</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I]</code></p> <p>A <code>Trial</code> encapsulates some configuration that needs to be evaluated. Typically this is what is generated by an <code>Optimizer.ask()</code> call.</p> Usage <p>To begin a trial, you can use the <code>trial.begin()</code>, which will catch exceptions/traceback and profile the block of code.</p> <p>If all went smooth, your trial was successful and you can use <code>trial.success()</code> to generate a success <code>Report</code>, typically passing what your chosen optimizer expects, e.g. <code>\"loss\"</code> or <code>\"cost\"</code>.</p> <p>If your trial failed, you can instead use the <code>trial.fail()</code> to generate a failure <code>Report</code>, where any caught exception will be attached to it. Each <code>Optimizer</code> will take care of what to do from here.</p> <p><pre><code>from amltk.optimization import Trial, Metric\nfrom amltk.store import PathBucket\n\ncost = Metric(\"cost\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n\n    with trial.begin():\n        cost = x**2 - y\n\n    if trial.exception:\n        return trial.fail()\n\n    return trial.success(cost=cost)\n\n# ... usually obtained from an optimizer\ntrial = Trial(name=\"some-unique-name\", config={\"x\": 1, \"y\": 2}, metrics=[cost])\n\nreport = target_function(trial)\nprint(report.df())\n</code></pre> <p>                   status  trial_seed  ... time:kind time:unit name                                   ...                     some-unique-name  success          ...      wall   seconds  [1 rows x 20 columns]  <p>What you can return with <code>trial.success()</code> or <code>trial.fail()</code> depends on the <code>metrics</code> of the trial. Typically an optimizer will provide the trial with the list of metrics.</p> Metrics <p>A metric with a given name, optimal direction, and possible bounds.</p> <p>Some important properties is that they have a unique <code>.name</code> given the optimization run, a candidate <code>.config</code>' to evaluate, a possible <code>.seed</code> to use, and an <code>.info</code> object which is the optimizer specific information, if required by you.</p> <p>If using <code>Plugins</code>, they may insert some extra objects in the <code>.extra</code> dict.</p> <p>To profile your trial, you can wrap the logic you'd like to check with <code>trial.begin()</code>, which will automatically catch any errors, record the traceback, and profile the block of code, in terms of time and memory.</p> <p>You can access the profiled time and memory using the <code>.time</code> and <code>.memory</code> attributes. If you've <code>profile()</code>'ed any other intervals, you can access them by name through <code>trial.profiles</code>. Please see the <code>Profiler</code> for more.</p> Profiling with a trial. <p>profile<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"some-unique-name\", config={})\n\n# ... somewhere where you've begun your trial.\nwith trial.profile(\"some_interval\"):\n    for work in range(100):\n        pass\n\nprint(trial.profiler.df())\n</code></pre> <pre><code>               memory:start_vms  memory:end_vms  ...  time:kind  time:unit\nsome_interval      2.083082e+09      2083082240  ...       wall    seconds\n\n[1 rows x 12 columns]\n</code></pre> </p> <p>You can also record anything you'd like into the <code>.summary</code>, a plain <code>dict</code> or use <code>trial.store()</code> to store artifacts related to the trial.</p> What to put in <code>.summary</code>? <p>For large items, e.g. predictions or models, these are highly advised to <code>.store()</code> to disk, especially if using a <code>Task</code> for multiprocessing.</p> <p>Further, if serializing the report using the <code>report.df()</code>, returning a single row, or a <code>History</code> with <code>history.df()</code> for a dataframe consisting of many of the reports, then you'd likely only want to store things that are scalar and can be serialised to disk by a pandas DataFrame.</p> <p>options:         members: False</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.name","title":"<code>name: str</code>   <code>attr</code>","text":"<p>The unique name of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.config","title":"<code>config: Mapping[str, Any]</code>   <code>attr</code>","text":"<p>The config of the trial provided by the optimizer.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.bucket","title":"<code>bucket: PathBucket</code>   <code>classvar</code> <code>attr</code>","text":"<p>The bucket to store trial related output to.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.info","title":"<code>info: I | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The info of the trial provided by the optimizer.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.metrics","title":"<code>metrics: Sequence[Metric]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The metrics associated with the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.seed","title":"<code>seed: int | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The seed to use if suggested by the optimizer.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.fidelities","title":"<code>fidelities: dict[str, Any] | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The fidelities at which to evaluate the trial, if any.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.time","title":"<code>time: Timer.Interval</code>   <code>classvar</code> <code>attr</code>","text":"<p>The time taken by the trial, once ended.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.memory","title":"<code>memory: Memory.Interval</code>   <code>classvar</code> <code>attr</code>","text":"<p>The memory used by the trial, once ended.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.profiler","title":"<code>profiler: Profiler</code>   <code>classvar</code> <code>attr</code>","text":"<p>A profiler for this trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.summary","title":"<code>summary: dict[str, Any]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The summary of the trial. These are for summary statistics of a trial and are single values.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.exception","title":"<code>exception: BaseException | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The exception raised by the trial, if any.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.traceback","title":"<code>traceback: str | None</code>   <code>classvar</code> <code>attr</code>","text":"<p>The traceback of the exception, if any.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.storage","title":"<code>storage: set[Any]</code>   <code>classvar</code> <code>attr</code>","text":"<p>Anything stored in the trial, the elements of the list are keys that can be used to retrieve them later, such as a Path.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.extras","title":"<code>extras: dict[str, Any]</code>   <code>classvar</code> <code>attr</code>","text":"<p>Any extras attached to the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.profiles","title":"<code>profiles: Mapping[str, Profile.Interval]</code>   <code>prop</code>","text":"<p>The profiles of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Status","title":"<code>class Status</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>The status of a trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Status.SUCCESS","title":"<code>SUCCESS</code>   <code>classvar</code> <code>attr</code>","text":"<p>The trial was successful.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Status.FAIL","title":"<code>FAIL</code>   <code>classvar</code> <code>attr</code>","text":"<p>The trial failed.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Status.CRASHED","title":"<code>CRASHED</code>   <code>classvar</code> <code>attr</code>","text":"<p>The trial crashed.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Status.UNKNOWN","title":"<code>UNKNOWN</code>   <code>classvar</code> <code>attr</code>","text":"<p>The status of the trial is unknown.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report","title":"<code>class Report</code>   <code>dataclass</code>","text":"<p>         Bases: <code>RichRenderable</code>, <code>Generic[I2]</code></p> <p>The <code>Trial.Report</code> encapsulates a <code>Trial</code>, its status and any metrics/exceptions that may have occured.</p> <p>Typically you will not create these yourself, but instead use <code>trial.success()</code> or <code>trial.fail()</code> to generate them.</p> <pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True)\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\nwith trial.begin():\n    # Do some work\n    # ...\n    report: Trial.Report = trial.success(loss=1)\n\nprint(report.df())\n</code></pre> <pre><code>        status  trial_seed exception  ... time:duration time:kind  time:unit\nname                                  ...                                   \ntrial  success        &lt;NA&gt;        NA  ...      0.000022      wall    seconds\n\n[1 rows x 19 columns]\n</code></pre> <p>These reports are used to report back metrics to an <code>Optimizer</code> with <code>Optimizer.tell()</code> but can also be stored for your own uses.</p> <p>You can access the original trial with the <code>.trial</code> attribute, and the <code>Status</code> of the trial with the <code>.status</code> attribute.</p> <p>You may also want to check out the <code>History</code> class for storing a collection of <code>Report</code>s, allowing for an easier time to convert them to a dataframe or perform some common Hyperparameter optimization parsing of metrics.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.trial","title":"<code>trial: Trial[I2]</code>   <code>attr</code>","text":"<p>The trial that was run.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.status","title":"<code>status: Trial.Status</code>   <code>attr</code>","text":"<p>The status of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.metrics","title":"<code>metrics: dict[str, float]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The metric values of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.metric_values","title":"<code>metric_values: tuple[Metric.Value, ...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The metrics of the trial, linked to the metrics.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.metric_defs","title":"<code>metric_defs: dict[str, Metric]</code>   <code>classvar</code> <code>attr</code>","text":"<p>A lookup to the metric definitions</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.metric_names","title":"<code>metric_names: tuple[str, ...]</code>   <code>classvar</code> <code>attr</code>","text":"<p>The names of the metrics.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.exception","title":"<code>exception: BaseException | None</code>   <code>prop</code>","text":"<p>The exception of the trial, if any.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.traceback","title":"<code>traceback: str | None</code>   <code>prop</code>","text":"<p>The traceback of the trial, if any.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.name","title":"<code>name: str</code>   <code>prop</code>","text":"<p>The name of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.config","title":"<code>config: Mapping[str, Any]</code>   <code>prop</code>","text":"<p>The config of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.profiles","title":"<code>profiles: Mapping[str, Profile.Interval]</code>   <code>prop</code>","text":"<p>The profiles of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.summary","title":"<code>summary: dict[str, Any]</code>   <code>prop</code>","text":"<p>The summary of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.storage","title":"<code>storage: set[str]</code>   <code>prop</code>","text":"<p>The storage of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.time","title":"<code>time: Timer.Interval</code>   <code>prop</code>","text":"<p>The time of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.memory","title":"<code>memory: Memory.Interval</code>   <code>prop</code>","text":"<p>The memory of the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.bucket","title":"<code>bucket: PathBucket</code>   <code>prop</code>","text":"<p>The bucket attached to the trial.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.info","title":"<code>info: I2 | None</code>   <code>prop</code>","text":"<p>The info of the trial, specific to the optimizer that issued it.</p>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.df","title":"<code>def df(*, profiles=True, configs=True, summary=True, metrics=True)</code>","text":"<p>Get a dataframe of the trial.</p> <p>Prefixes</p> <ul> <li><code>summary</code>: Entries will be prefixed with <code>\"summary:\"</code></li> <li><code>config</code>: Entries will be prefixed with <code>\"config:\"</code></li> <li><code>storage</code>: Entries will be prefixed with <code>\"storage:\"</code></li> <li><code>metrics</code>: Entries will be prefixed with <code>\"metrics:\"</code></li> <li><code>profile:&lt;name&gt;</code>: Entries will be prefixed with     <code>\"profile:&lt;name&gt;:\"</code></li> </ul> PARAMETER  DESCRIPTION <code>profiles</code> <p>Whether to include the profiles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>configs</code> <p>Whether to include the configs.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>summary</code> <p>Whether to include the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>metrics</code> <p>Whether to include the metrics.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def df(\n    self,\n    *,\n    profiles: bool = True,\n    configs: bool = True,\n    summary: bool = True,\n    metrics: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Get a dataframe of the trial.\n\n    !!! note \"Prefixes\"\n\n        * `summary`: Entries will be prefixed with `#!python \"summary:\"`\n        * `config`: Entries will be prefixed with `#!python \"config:\"`\n        * `storage`: Entries will be prefixed with `#!python \"storage:\"`\n        * `metrics`: Entries will be prefixed with `#!python \"metrics:\"`\n        * `profile:&lt;name&gt;`: Entries will be prefixed with\n            `#!python \"profile:&lt;name&gt;:\"`\n\n    Args:\n        profiles: Whether to include the profiles.\n        configs: Whether to include the configs.\n        summary: Whether to include the summary.\n        metrics: Whether to include the metrics.\n    \"\"\"\n    items = {\n        \"name\": self.name,\n        \"status\": str(self.status),\n        \"trial_seed\": self.trial.seed if self.trial.seed else np.nan,\n        \"exception\": str(self.exception) if self.exception else \"NA\",\n        \"traceback\": str(self.traceback) if self.traceback else \"NA\",\n        \"bucket\": str(self.bucket.path),\n    }\n    if metrics:\n        for value in self.metric_values:\n            items[f\"metric:{value.metric}\"] = value.value\n    if summary:\n        items.update(**prefix_keys(self.trial.summary, \"summary:\"))\n    if configs:\n        items.update(**prefix_keys(self.trial.config, \"config:\"))\n    if profiles:\n        for name, profile in sorted(self.profiles.items(), key=lambda x: x[0]):\n            # We log this one seperatly\n            if name == \"trial\":\n                items.update(profile.to_dict())\n            else:\n                items.update(profile.to_dict(prefix=f\"profile:{name}\"))\n\n    return pd.DataFrame(items, index=[0]).convert_dtypes().set_index(\"name\")\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.retrieve","title":"<code>def retrieve(key, *, where=None, check=None)</code>","text":"<p>Retrieve items related to the trial.</p> <p>Same argument for <code>where=</code></p> <p>Use the same argument for <code>where=</code> as you did for <code>store()</code>.</p> retrieve<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\nwith trial.begin():\n    report = trial.success()\n\nconfig = report.retrieve(\"config.json\")\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> <p>You could also create a Bucket and use that instead.</p> retrieve-bucket<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\n\nwith trial.begin():\n    report = trial.success()\n\nconfig = report.retrieve(\"config.json\")\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>The key of the item to retrieve as said in <code>.storage</code>.</p> <p> TYPE: <code>str</code> </p> <code>check</code> <p>If provided, will check that the retrieved item is of the provided type. If not, will raise a <code>TypeError</code>. This is only used if <code>where=</code> is a <code>str</code>, <code>Path</code> or <code>Bucket</code>.</p> <p> TYPE: <code>type[R] | None</code> DEFAULT: <code>None</code> </p> <code>where</code> <p>Where to retrieve the items from.</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will store a bucket will be created at the path, and the items will be retrieved from a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will retrieve the items from a sub-bucket with the name of the trial.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R | Any</code> <p>The retrieved item.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>check=</code> is provided and  the retrieved item is not of the provided type.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def retrieve(\n    self,\n    key: str,\n    *,\n    where: str | Path | Bucket[str, Any] | None = None,\n    check: type[R] | None = None,\n) -&gt; R | Any:\n    \"\"\"Retrieve items related to the trial.\n\n    !!! note \"Same argument for `where=`\"\n\n         Use the same argument for `where=` as you did for `store()`.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve\" hl_lines=\"7\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    with trial.begin():\n        report = trial.success()\n\n    config = report.retrieve(\"config.json\")\n    print(config)\n    ```\n\n    You could also create a Bucket and use that instead.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve-bucket\" hl_lines=\"11\"\n\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n\n    with trial.begin():\n        report = trial.success()\n\n    config = report.retrieve(\"config.json\")\n    print(config)\n    ```\n\n    Args:\n        key: The key of the item to retrieve as said in `.storage`.\n        check: If provided, will check that the retrieved item is of the\n            provided type. If not, will raise a `TypeError`. This\n            is only used if `where=` is a `str`, `Path` or `Bucket`.\n        where: Where to retrieve the items from.\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will store\n            a bucket will be created at the path, and the items will be\n            retrieved from a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will retrieve the items from a sub-bucket with the\n            name of the trial.\n\n    Returns:\n        The retrieved item.\n\n    Raises:\n        TypeError: If `check=` is provided and  the retrieved item is not of the provided\n            type.\n    \"\"\"  # noqa: E501\n    return self.trial.retrieve(key, where=where, check=check)\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.store","title":"<code>def store(items, *, where=None)</code>","text":"<p>Store items related to the trial.</p> <p>See: <code>Trial.store()</code></p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def store(\n    self,\n    items: Mapping[str, T],\n    *,\n    where: (\n        str | Path | Bucket | Callable[[str, Mapping[str, T]], None] | None\n    ) = None,\n) -&gt; None:\n    \"\"\"Store items related to the trial.\n\n    See: [`Trial.store()`][amltk.optimization.trial.Trial.store]\n    \"\"\"\n    self.trial.store(items, where=where)\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.from_df","title":"<code>def from_df(df)</code>   <code>classmethod</code>","text":"<p>Create a report from a dataframe.</p> See Also <ul> <li><code>.from_dict()</code></li> </ul> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@classmethod\ndef from_df(cls, df: pd.DataFrame | pd.Series) -&gt; Trial.Report:\n    \"\"\"Create a report from a dataframe.\n\n    See Also:\n        * [`.from_dict()`][amltk.optimization.Trial.Report.from_dict]\n    \"\"\"\n    if isinstance(df, pd.DataFrame):\n        if len(df) != 1:\n            raise ValueError(\n                f\"Expected a dataframe with one row, got {len(df)} rows.\",\n            )\n        series = df.iloc[0]\n    else:\n        series = df\n\n    data_dict = {\"name\": series.name, **series.to_dict()}\n    return cls.from_dict(data_dict)\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.from_dict","title":"<code>def from_dict(d)</code>   <code>classmethod</code>","text":"<p>Create a report from a dictionary.</p> <p>Prefixes</p> <p>Please see <code>.df()</code> for information on what the prefixes should be for certain fields.</p> PARAMETER  DESCRIPTION <code>d</code> <p>The dictionary to create the report from.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Report</code> <p>The created report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: Mapping[str, Any]) -&gt; Trial.Report:\n    \"\"\"Create a report from a dictionary.\n\n    !!! note \"Prefixes\"\n\n        Please see [`.df()`][amltk.optimization.Trial.Report.df]\n        for information on what the prefixes should be for certain fields.\n\n    Args:\n        d: The dictionary to create the report from.\n\n    Returns:\n        The created report.\n    \"\"\"\n    prof_dict = mapping_select(d, \"profile:\")\n    if any(prof_dict):\n        profile_names = sorted(\n            {name.rsplit(\":\", maxsplit=2)[0] for name in prof_dict},\n        )\n        profiles = {\n            name: Profile.from_dict(mapping_select(prof_dict, f\"{name}:\"))\n            for name in profile_names\n        }\n    else:\n        profiles = {}\n\n    # NOTE: We assume the order of the objectives are in the right\n    # order in the dict. If we attempt to force a sort-order, we may\n    # deserialize incorrectly. By not having a sort order, we rely\n    # on serialization to keep the order, which is not ideal either.\n    # May revisit this if we need to\n    raw_metrics: dict[str, float] = mapping_select(d, \"metric:\")\n    _intermediate = {\n        Metric.from_str(name): value for name, value in raw_metrics.items()\n    }\n    metrics: dict[Metric, Metric.Value] = {\n        metric: metric.as_value(value)\n        for metric, value in _intermediate.items()\n    }\n\n    _trial_profile_items = {\n        k: v for k, v in d.items() if k.startswith((\"memory:\", \"time:\"))\n    }\n    if any(_trial_profile_items):\n        trial_profile = Profile.from_dict(_trial_profile_items)\n        profiles[\"trial\"] = trial_profile\n    else:\n        trial_profile = Profile.na()\n\n    exception = d.get(\"exception\")\n    traceback = d.get(\"traceback\")\n    trial_seed = d.get(\"trial_seed\")\n    if pd.isna(exception) or exception == \"NA\":  # type: ignore\n        exception = None\n    if pd.isna(traceback) or traceback == \"NA\":  # type: ignore\n        traceback = None\n    if pd.isna(trial_seed):  # type: ignore\n        trial_seed = None\n\n    if (_bucket := d.get(\"bucket\")) is not None:\n        bucket = PathBucket(_bucket)\n    else:\n        bucket = PathBucket(f\"uknown_trial_bucket-{datetime.now().isoformat()}\")\n\n    trial: Trial[None] = Trial(\n        name=d[\"name\"],\n        config=mapping_select(d, \"config:\"),\n        info=None,  # We don't save this to disk so we load it back as None\n        bucket=bucket,\n        seed=trial_seed,\n        fidelities=mapping_select(d, \"fidelities:\"),\n        time=trial_profile.time,\n        memory=trial_profile.memory,\n        profiler=Profiler(profiles=profiles),\n        metrics=list(metrics.keys()),\n        summary=mapping_select(d, \"summary:\"),\n        exception=exception,\n        traceback=traceback,\n    )\n    status = Trial.Status(dict_get_not_none(d, \"status\", \"unknown\"))\n    _values: dict[str, float] = {m.name: r.value for m, r in metrics.items()}\n    if status == Trial.Status.SUCCESS:\n        return trial.success(**_values)\n\n    if status == Trial.Status.FAIL:\n        return trial.fail(**_values)\n\n    if status == Trial.Status.CRASHED:\n        return trial.crashed(\n            exception=Exception(\"Unknown status.\")\n            if trial.exception is None\n            else None,\n        )\n\n    return trial.crashed(exception=Exception(\"Unknown status.\"))\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.Report.rich_renderables","title":"<code>def rich_renderables()</code>","text":"<p>The renderables for rich for this report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def rich_renderables(self) -&gt; Iterable[RenderableType]:\n    \"\"\"The renderables for rich for this report.\"\"\"\n    from rich.pretty import Pretty\n    from rich.text import Text\n\n    yield Text.assemble(\n        (\"Status\", \"bold\"),\n        (\"(\", \"default\"),\n        self.status.__rich__(),\n        (\")\", \"default\"),\n    )\n    yield Pretty(self.metrics)\n    yield from self.trial.rich_renderables()\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.begin","title":"<code>def begin(time=None, memory_unit=None)</code>","text":"<p>Begin the trial with a <code>contextmanager</code>.</p> <p>Will begin timing the trial in the <code>with</code> block, attaching the profiled time and memory to the trial once completed, under <code>.profile.time</code> and <code>.profile.memory</code> attributes.</p> <p>If an exception is raised, it will be attached to the trial under <code>.exception</code> with the traceback attached to the actual error message, such that it can be pickled and sent back to the main process loop.</p> begin<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\n\nwith trial.begin():\n    # Do some work\n    pass\n\nprint(trial.memory)\nprint(trial.time)\n</code></pre> <pre><code>Memory.Interval(start_vms=2083082240.0, start_rss=413958144.0, end_vms=2083082240, end_rss=413958144, unit=bytes)\nTimer.Interval(start=1702352470.8504946, end=1702352470.8505044, kind=wall, unit=seconds)\n</code></pre> begin-fail<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"trial\", config={\"x\": -1})\n\nwith trial.begin():\n    raise ValueError(\"x must be positive\")\n\nprint(trial.exception)\nprint(trial.traceback)\nprint(trial.memory)\nprint(trial.time)\n</code></pre> <pre><code>x must be positive\nTraceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/amltk/optimization/trial.py\", line 301, in begin\n    yield\n  File \"&lt;code block: n173; title begin-fail&gt;\", line 6, in &lt;module&gt;\nValueError: x must be positive\n\nMemory.Interval(start_vms=2083082240.0, start_rss=413958144.0, end_vms=2083082240, end_rss=413958144, unit=bytes)\nTimer.Interval(start=1702352470.8543284, end=1702352470.8544264, kind=wall, unit=seconds)\n</code></pre> PARAMETER  DESCRIPTION <code>time</code> <p>The timer kind to use for the trial. Defaults to the default timer kind of the profiler.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> <code>memory_unit</code> <p>The memory unit to use for the trial. Defaults to the default memory unit of the profiler.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@contextmanager\ndef begin(\n    self,\n    time: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n) -&gt; Iterator[None]:\n    \"\"\"Begin the trial with a `contextmanager`.\n\n    Will begin timing the trial in the `with` block, attaching the profiled time and memory\n    to the trial once completed, under `.profile.time` and `.profile.memory` attributes.\n\n    If an exception is raised, it will be attached to the trial under `.exception`\n    with the traceback attached to the actual error message, such that it can\n    be pickled and sent back to the main process loop.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"begin\" hl_lines=\"5\"\n    from amltk.optimization import Trial\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n\n    with trial.begin():\n        # Do some work\n        pass\n\n    print(trial.memory)\n    print(trial.time)\n    ```\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"begin-fail\" hl_lines=\"5\"\n    from amltk.optimization import Trial\n\n    trial = Trial(name=\"trial\", config={\"x\": -1})\n\n    with trial.begin():\n        raise ValueError(\"x must be positive\")\n\n    print(trial.exception)\n    print(trial.traceback)\n    print(trial.memory)\n    print(trial.time)\n    ```\n\n    Args:\n        time: The timer kind to use for the trial. Defaults to the default\n            timer kind of the profiler.\n        memory_unit: The memory unit to use for the trial. Defaults to the\n            default memory unit of the profiler.\n    \"\"\"  # noqa: E501\n    with self.profiler(name=\"trial\", memory_unit=memory_unit, time_kind=time):\n        try:\n            yield\n        except Exception as error:  # noqa: BLE001\n            self.exception = error\n            self.traceback = traceback.format_exc()\n        finally:\n            self.time = self.profiler[\"trial\"].time\n            self.memory = self.profiler[\"trial\"].memory\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.profile","title":"<code>def profile(name, *, time=None, memory_unit=None, summary=False)</code>","text":"<p>Measure some interval in the trial.</p> <p>The results of the profiling will be available in the <code>.summary</code> attribute with the name of the interval as the key.</p> profile<pre><code>from amltk.optimization import Trial\nimport time\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\n\nwith trial.profile(\"some_interval\"):\n    # Do some work\n    time.sleep(1)\n\nprint(trial.profiler[\"some_interval\"].time)\n</code></pre> <pre><code>Timer.Interval(start=1702352470.865767, end=1702352471.8668308, kind=wall, unit=seconds)\n</code></pre> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the interval.</p> <p> TYPE: <code>str</code> </p> <code>time</code> <p>The timer kind to use for the trial. Defaults to the default timer kind of the profiler.</p> <p> TYPE: <code>Kind | Literal['wall', 'cpu', 'process'] | None</code> DEFAULT: <code>None</code> </p> <code>memory_unit</code> <p>The memory unit to use for the trial. Defaults to the default memory unit of the profiler.</p> <p> TYPE: <code>Unit | Literal['B', 'KB', 'MB', 'GB'] | None</code> DEFAULT: <code>None</code> </p> <code>summary</code> <p>Whether to add the interval to the summary.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>Iterator[None]</code> <p>The interval measured. Values will be nan until the with block is finished.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>@contextmanager\ndef profile(\n    self,\n    name: str,\n    *,\n    time: Timer.Kind | Literal[\"wall\", \"cpu\", \"process\"] | None = None,\n    memory_unit: Memory.Unit | Literal[\"B\", \"KB\", \"MB\", \"GB\"] | None = None,\n    summary: bool = False,\n) -&gt; Iterator[None]:\n    \"\"\"Measure some interval in the trial.\n\n    The results of the profiling will be available in the `.summary` attribute\n    with the name of the interval as the key.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"profile\"\n    from amltk.optimization import Trial\n    import time\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n\n    with trial.profile(\"some_interval\"):\n        # Do some work\n        time.sleep(1)\n\n    print(trial.profiler[\"some_interval\"].time)\n    ```\n\n    Args:\n        name: The name of the interval.\n        time: The timer kind to use for the trial. Defaults to the default\n            timer kind of the profiler.\n        memory_unit: The memory unit to use for the trial. Defaults to the\n            default memory unit of the profiler.\n        summary: Whether to add the interval to the summary.\n\n    Yields:\n        The interval measured. Values will be nan until the with block is finished.\n    \"\"\"\n    with self.profiler(name=name, memory_unit=memory_unit, time_kind=time):\n        yield\n\n    if summary:\n        profile = self.profiler[name]\n        self.summary.update(profile.to_dict(prefix=name))\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.success","title":"<code>def success(**metrics)</code>","text":"<p>Generate a success report.</p> success<pre><code>from amltk.optimization import Trial, Metric\n\nloss_metric = Metric(\"loss\", minimize=True)\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss_metric])\n\nwith trial.begin():\n    # Do some work\n    report = trial.success(loss=1)\n\nprint(report)\n</code></pre> <pre><code>Trial.Report(trial=Trial(name='trial', config={'x': 1}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='loss', minimize=True, bounds=None)], seed=None, fidelities=None, summary={}, exception=None, storage=set(), extras={}), status=&lt;Status.SUCCESS: 'success'&gt;, metrics={'loss': 1.0}, metric_values=(Metric.Value(metric=Metric(name='loss', minimize=True, bounds=None), value=1.0),), metric_defs={'loss': Metric(name='loss', minimize=True, bounds=None)}, metric_names=('loss',))\n</code></pre> PARAMETER  DESCRIPTION <code>**metrics</code> <p>The metrics of the trial, where the key is the name of the metrics and the value is the metric.</p> <p> TYPE: <code>float | int</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Report[I]</code> <p>The report of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def success(self, **metrics: float | int) -&gt; Trial.Report[I]:\n    \"\"\"Generate a success report.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"success\" hl_lines=\"7\"\n    from amltk.optimization import Trial, Metric\n\n    loss_metric = Metric(\"loss\", minimize=True)\n\n    trial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss_metric])\n\n    with trial.begin():\n        # Do some work\n        report = trial.success(loss=1)\n\n    print(report)\n    ```\n\n    Args:\n        **metrics: The metrics of the trial, where the key is the name of the\n            metrics and the value is the metric.\n\n    Returns:\n        The report of the trial.\n    \"\"\"  # noqa: E501\n    _recorded_values: list[Metric.Value] = []\n    for _metric in self.metrics:\n        if (raw_value := metrics.get(_metric.name)) is not None:\n            _recorded_values.append(_metric.as_value(raw_value))\n        else:\n            raise ValueError(\n                f\"Cannot report success without {self.metrics=}.\"\n                f\" Please provide a value for the metric.\",\n            )\n\n    # Need to check if anything extra was reported!\n    extra = set(metrics.keys()) - {metric.name for metric in self.metrics}\n    if extra:\n        raise ValueError(\n            f\"Cannot report success with extra metrics: {extra=}.\"\n            f\"\\nOnly {self.metrics=} are allowed.\",\n        )\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.SUCCESS,\n        metric_values=tuple(_recorded_values),\n    )\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.fail","title":"<code>def fail(**metrics)</code>","text":"<p>Generate a failure report.</p> <p>Non specifed metrics</p> <p>If you do not specify metrics, this will use the <code>.metrics</code> to determine the <code>.worst</code> value of the metric, using that as the reported result</p> fail<pre><code>from amltk.optimization import Trial, Metric\n\nloss = Metric(\"loss\", minimize=True, bounds=(0, 1_000))\ntrial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\nwith trial.begin():\n    raise ValueError(\"This is an error\")  # Something went wrong\n\nif trial.exception: # You can check for an exception of the trial here\n    report = trial.fail()\n\nprint(report.metrics)\nprint(report)\n</code></pre> <pre><code>{'loss': 1000.0}\nTrial.Report(trial=Trial(name='trial', config={'x': 1}, bucket=PathBucket(PosixPath('unknown-trial-bucket')), metrics=[Metric(name='loss', minimize=True, bounds=(0.0, 1000.0))], seed=None, fidelities=None, summary={}, exception=ValueError('This is an error'), storage=set(), extras={}), status=&lt;Status.FAIL: 'fail'&gt;, metrics={'loss': 1000.0}, metric_values=(Metric.Value(metric=Metric(name='loss', minimize=True, bounds=(0.0, 1000.0)), value=1000.0),), metric_defs={'loss': Metric(name='loss', minimize=True, bounds=(0.0, 1000.0))}, metric_names=('loss',))\n</code></pre> RETURNS DESCRIPTION <code>Report[I]</code> <p>The result of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def fail(self, **metrics: float | int) -&gt; Trial.Report[I]:\n    \"\"\"Generate a failure report.\n\n    !!! note \"Non specifed metrics\"\n\n        If you do not specify metrics, this will use\n        the [`.metrics`][amltk.optimization.Trial.metrics] to determine\n        the [`.worst`][amltk.optimization.Metric.worst] value of the metric,\n        using that as the reported result\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"fail\"\n    from amltk.optimization import Trial, Metric\n\n    loss = Metric(\"loss\", minimize=True, bounds=(0, 1_000))\n    trial = Trial(name=\"trial\", config={\"x\": 1}, metrics=[loss])\n\n    with trial.begin():\n        raise ValueError(\"This is an error\")  # Something went wrong\n\n    if trial.exception: # You can check for an exception of the trial here\n        report = trial.fail()\n\n    print(report.metrics)\n    print(report)\n    ```\n\n    Returns:\n        The result of the trial.\n    \"\"\"\n    _recorded_values: list[Metric.Value] = []\n    for _metric in self.metrics:\n        if (raw_value := metrics.get(_metric.name)) is not None:\n            _recorded_values.append(_metric.as_value(raw_value))\n        else:\n            _recorded_values.append(_metric.worst)\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.FAIL,\n        metric_values=tuple(_recorded_values),\n    )\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.crashed","title":"<code>def crashed(exception=None, traceback=None)</code>","text":"<p>Generate a crash report.</p> <p>Note</p> <p>You will typically not create these manually, but instead if we don't recieve a report from a target function evaluation, but only an error, we assume something crashed and generate a crash report for you.</p> <p>Non specifed metrics</p> <p>We will use the <code>.metrics</code> to determine the <code>.worst</code> value of the metric, using that as the reported metrics</p> PARAMETER  DESCRIPTION <code>exception</code> <p>The exception that caused the crash. If not provided, the exception will be taken from the trial. If this is still <code>None</code>, a <code>RuntimeError</code> will be raised.</p> <p> TYPE: <code>BaseException | None</code> DEFAULT: <code>None</code> </p> <code>traceback</code> <p>The traceback of the exception. If not provided, the traceback will be taken from the trial if there is one there.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Report[I]</code> <p>The report of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def crashed(\n    self,\n    exception: BaseException | None = None,\n    traceback: str | None = None,\n) -&gt; Trial.Report[I]:\n    \"\"\"Generate a crash report.\n\n    !!! note\n\n        You will typically not create these manually, but instead if we don't\n        recieve a report from a target function evaluation, but only an error,\n        we assume something crashed and generate a crash report for you.\n\n    !!! note \"Non specifed metrics\"\n\n        We will use the [`.metrics`][amltk.optimization.Trial.metrics] to determine\n        the [`.worst`][amltk.optimization.Metric.worst] value of the metric,\n        using that as the reported metrics\n\n    Args:\n        exception: The exception that caused the crash. If not provided, the\n            exception will be taken from the trial. If this is still `None`,\n            a `RuntimeError` will be raised.\n        traceback: The traceback of the exception. If not provided, the\n            traceback will be taken from the trial if there is one there.\n\n    Returns:\n        The report of the trial.\n    \"\"\"\n    if exception is None and self.exception is None:\n        raise RuntimeError(\n            \"Cannot generate a crash report without an exception.\"\n            \" Please provide an exception or use `with trial.begin():` to start\"\n            \" the trial.\",\n        )\n\n    self.exception = exception if exception else self.exception\n    self.traceback = traceback if traceback else self.traceback\n\n    return Trial.Report(\n        trial=self,\n        status=Trial.Status.CRASHED,\n        metric_values=tuple(metric.worst for metric in self.metrics),\n    )\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.store","title":"<code>def store(items, *, where=None)</code>","text":"<p>Store items related to the trial.</p> store<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=PathBucket(\"results\"))\ntrial.store({\"config.json\": trial.config})\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> <p>You could also specify <code>where=</code> exactly to store the thing</p> store-bucket<pre><code>from amltk.optimization import Trial\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\ntrial.store({\"config.json\": trial.config}, where=\"./results\")\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> PARAMETER  DESCRIPTION <code>items</code> <p>The items to store, a dict from the key to store it under to the item itself.If using a <code>str</code>, <code>Path</code> or <code>PathBucket</code>, the keys of the items should be a valid filename, including the correct extension. e.g. <code>{\"config.json\": trial.config}</code></p> <p> TYPE: <code>Mapping[str, T]</code> </p> <code>where</code> <p>Where to store the items.</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will store a bucket will be created at the path, and the items will be stored in a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will store the items in a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Callable</code>, will call the callable with the name of the trial and the key-valued pair of items to store.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket | Callable[[str, Mapping[str, T]], None] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def store(\n    self,\n    items: Mapping[str, T],\n    *,\n    where: (\n        str | Path | Bucket | Callable[[str, Mapping[str, T]], None] | None\n    ) = None,\n) -&gt; None:\n    \"\"\"Store items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"store\" hl_lines=\"5\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=PathBucket(\"results\"))\n    trial.store({\"config.json\": trial.config})\n\n    print(trial.storage)\n    ```\n\n    You could also specify `where=` exactly to store the thing\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"store-bucket\" hl_lines=\"7\"\n    from amltk.optimization import Trial\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n    trial.store({\"config.json\": trial.config}, where=\"./results\")\n\n    print(trial.storage)\n    ```\n\n    Args:\n        items: The items to store, a dict from the key to store it under\n            to the item itself.If using a `str`, `Path` or `PathBucket`,\n            the keys of the items should be a valid filename, including\n            the correct extension. e.g. `#!python {\"config.json\": trial.config}`\n\n        where: Where to store the items.\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will store\n            a bucket will be created at the path, and the items will be\n            stored in a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will store the items **in a sub-bucket** with the\n            name of the trial.\n\n            * If a `Callable`, will call the callable with the name of the\n            trial and the key-valued pair of items to store.\n    \"\"\"  # noqa: E501\n    method: Bucket\n    match where:\n        case None:\n            method = self.bucket\n            method.sub(self.name).store(items)\n        case str() | Path():\n            method = PathBucket(where, create=True)\n            method.sub(self.name).store(items)\n        case Bucket():\n            method = where\n            method.sub(self.name).store(items)\n        case _:\n            # Leave it up to supplied method\n            where(self.name, items)\n\n    # Add the keys to storage\n    self.storage.update(items.keys())\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.delete_from_storage","title":"<code>def delete_from_storage(items, *, where=None)</code>","text":"<p>Delete items related to the trial.</p> delete-storage<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\ntrial = Trial(name=\"trial\", config={\"x\": 1}, info={}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\ntrial.delete_from_storage(items=[\"config.json\"])\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> <p>You could also create a Bucket and use that instead.</p> delete-storage-bucket<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\ntrial.delete_from_storage(items=[\"config.json\"])\n\nprint(trial.storage)\n</code></pre> <pre><code>{'config.json'}\n</code></pre> PARAMETER  DESCRIPTION <code>items</code> <p>The items to delete, an iterable of keys</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>where</code> <p>Where the items are stored</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will lookup a bucket at the path, and the items will be deleted from a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will delete the items in a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Callable</code>, will call the callable with the name of the trial and the keys of the items to delete. Should a mapping from the key to whether it was deleted or not.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket | Callable[[str, Iterable[str]], dict[str, bool]] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, bool]</code> <p>A dict from the key to whether it was deleted or not.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def delete_from_storage(\n    self,\n    items: Iterable[str],\n    *,\n    where: (\n        str | Path | Bucket | Callable[[str, Iterable[str]], dict[str, bool]] | None\n    ) = None,\n) -&gt; dict[str, bool]:\n    \"\"\"Delete items related to the trial.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"delete-storage\" hl_lines=\"6\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n    trial = Trial(name=\"trial\", config={\"x\": 1}, info={}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    trial.delete_from_storage(items=[\"config.json\"])\n\n    print(trial.storage)\n    ```\n\n    You could also create a Bucket and use that instead.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"delete-storage-bucket\" hl_lines=\"9\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    trial.delete_from_storage(items=[\"config.json\"])\n\n    print(trial.storage)\n    ```\n\n    Args:\n        items: The items to delete, an iterable of keys\n        where: Where the items are stored\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will lookup a bucket at the path,\n            and the items will be deleted from a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will delete the items in a sub-bucket with the\n            name of the trial.\n\n            * If a `Callable`, will call the callable with the name of the\n            trial and the keys of the items to delete. Should a mapping from\n            the key to whether it was deleted or not.\n\n    Returns:\n        A dict from the key to whether it was deleted or not.\n    \"\"\"  # noqa: E501\n    # If not a Callable, we convert to a path bucket\n    method: Bucket\n    match where:\n        case None:\n            method = self.bucket\n        case str() | Path():\n            method = PathBucket(where, create=False)\n        case Bucket():\n            method = where\n        case _:\n            # Leave it up to supplied method\n            return where(self.name, items)\n\n    sub_bucket = method.sub(self.name)\n    return sub_bucket.remove(items)\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.copy","title":"<code>def copy()</code>","text":"<p>Create a copy of the trial.</p> RETURNS DESCRIPTION <code>Self</code> <p>The copy of the trial.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def copy(self) -&gt; Self:\n    \"\"\"Create a copy of the trial.\n\n    Returns:\n        The copy of the trial.\n    \"\"\"\n    return copy.deepcopy(self)\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.retrieve","title":"<code>def retrieve(key, *, where=None, check=None)</code>","text":"<p>Retrieve items related to the trial.</p> <p>Same argument for <code>where=</code></p> <p>Use the same argument for <code>where=</code> as you did for <code>store()</code>.</p> retrieve<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\nbucket = PathBucket(\"results\")\n\n# Create a trial, normally done by an optimizer\ntrial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\ntrial.store({\"config.json\": trial.config})\nconfig = trial.retrieve(\"config.json\")\n\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> <p>You could also manually specify where something get's stored and retrieved</p> retrieve-bucket<pre><code>from amltk.optimization import Trial\nfrom amltk.store import PathBucket\n\npath = \"./config_path\"\n\ntrial = Trial(name=\"trial\", config={\"x\": 1})\n\ntrial.store({\"config.json\": trial.config}, where=path)\n\nconfig = trial.retrieve(\"config.json\", where=path)\nprint(config)\n</code></pre> <pre><code>{'x': 1}\n</code></pre> PARAMETER  DESCRIPTION <code>key</code> <p>The key of the item to retrieve as said in <code>.storage</code>.</p> <p> TYPE: <code>str</code> </p> <code>check</code> <p>If provided, will check that the retrieved item is of the provided type. If not, will raise a <code>TypeError</code>. This is only used if <code>where=</code> is a <code>str</code>, <code>Path</code> or <code>Bucket</code>.</p> <p> TYPE: <code>type[R] | None</code> DEFAULT: <code>None</code> </p> <code>where</code> <p>Where to retrieve the items from.</p> <ul> <li> <p>If <code>None</code>, will use the bucket attached to the <code>Trial</code> if any,     otherwise it will raise an error.</p> </li> <li> <p>If a <code>str</code> or <code>Path</code>, will store a bucket will be created at the path, and the items will be retrieved from a sub-bucket with the name of the trial.</p> </li> <li> <p>If a <code>Bucket</code>, will retrieve the items from a sub-bucket with the name of the trial.</p> </li> </ul> <p> TYPE: <code>str | Path | Bucket[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>R | Any</code> <p>The retrieved item.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If <code>check=</code> is provided and  the retrieved item is not of the provided type.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def retrieve(\n    self,\n    key: str,\n    *,\n    where: str | Path | Bucket[str, Any] | None = None,\n    check: type[R] | None = None,\n) -&gt; R | Any:\n    \"\"\"Retrieve items related to the trial.\n\n    !!! note \"Same argument for `where=`\"\n\n         Use the same argument for `where=` as you did for `store()`.\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve\" hl_lines=\"7\"\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    bucket = PathBucket(\"results\")\n\n    # Create a trial, normally done by an optimizer\n    trial = Trial(name=\"trial\", config={\"x\": 1}, bucket=bucket)\n\n    trial.store({\"config.json\": trial.config})\n    config = trial.retrieve(\"config.json\")\n\n    print(config)\n    ```\n\n    You could also manually specify where something get's stored and retrieved\n\n    ```python exec=\"true\" source=\"material-block\" result=\"python\" title=\"retrieve-bucket\" hl_lines=\"11\"\n\n    from amltk.optimization import Trial\n    from amltk.store import PathBucket\n\n    path = \"./config_path\"\n\n    trial = Trial(name=\"trial\", config={\"x\": 1})\n\n    trial.store({\"config.json\": trial.config}, where=path)\n\n    config = trial.retrieve(\"config.json\", where=path)\n    print(config)\n    import shutil; shutil.rmtree(path)  # markdown-exec: hide\n    ```\n\n    Args:\n        key: The key of the item to retrieve as said in `.storage`.\n        check: If provided, will check that the retrieved item is of the\n            provided type. If not, will raise a `TypeError`. This\n            is only used if `where=` is a `str`, `Path` or `Bucket`.\n\n        where: Where to retrieve the items from.\n\n            * If `None`, will use the bucket attached to the `Trial` if any,\n                otherwise it will raise an error.\n\n            * If a `str` or `Path`, will store\n            a bucket will be created at the path, and the items will be\n            retrieved from a sub-bucket with the name of the trial.\n\n            * If a `Bucket`, will retrieve the items from a sub-bucket with the\n            name of the trial.\n\n    Returns:\n        The retrieved item.\n\n    Raises:\n        TypeError: If `check=` is provided and  the retrieved item is not of the provided\n            type.\n    \"\"\"  # noqa: E501\n    # If not a Callable, we convert to a path bucket\n    method: Bucket[str, Any]\n    match where:\n        case None:\n            method = self.bucket\n        case str():\n            method = PathBucket(where, create=True)\n        case Path():\n            method = PathBucket(where, create=True)\n        case Bucket():\n            method = where\n\n    # Store in a sub-bucket\n    return method.sub(self.name)[key].load(check=check)\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.attach_extra","title":"<code>def attach_extra(name, plugin_item)</code>","text":"<p>Attach a plugin item to the trial.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the plugin item.</p> <p> TYPE: <code>str</code> </p> <code>plugin_item</code> <p>The plugin item.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def attach_extra(self, name: str, plugin_item: Any) -&gt; None:\n    \"\"\"Attach a plugin item to the trial.\n\n    Args:\n        name: The name of the plugin item.\n        plugin_item: The plugin item.\n    \"\"\"\n    self.extras[name] = plugin_item\n</code></pre>"},{"location":"reference/optimization/trials/#amltk.optimization.trial.Trial.rich_renderables","title":"<code>def rich_renderables()</code>","text":"<p>The renderables for rich for this report.</p> Source code in <code>src/amltk/optimization/trial.py</code> <pre><code>def rich_renderables(self) -&gt; Iterable[RenderableType]:  # noqa: C901\n    \"\"\"The renderables for rich for this report.\"\"\"\n    from rich.panel import Panel\n    from rich.pretty import Pretty\n    from rich.table import Table\n    from rich.text import Text\n\n    items: list[RenderableType] = []\n    table = Table.grid(padding=(0, 1), expand=False)\n\n    # Predfined things\n    table.add_row(\"config\", Pretty(self.config))\n\n    if self.fidelities:\n        table.add_row(\"fidelities\", Pretty(self.fidelities))\n\n    if any(self.extras):\n        table.add_row(\"extras\", Pretty(self.extras))\n\n    if self.seed:\n        table.add_row(\"seed\", Pretty(self.seed))\n\n    if self.bucket:\n        table.add_row(\"bucket\", Pretty(self.bucket))\n\n    if self.metrics:\n        items.append(\n            Panel(Pretty(self.metrics), title=\"Metrics\", title_align=\"left\"),\n        )\n\n    # Dynamic things\n    if self.summary:\n        table.add_row(\"summary\", Pretty(self.summary))\n\n    if any(self.storage):\n        table.add_row(\"storage\", Pretty(self.storage))\n\n    if self.exception:\n        table.add_row(\"exception\", Text(str(self.exception), style=\"bold red\"))\n\n    if self.traceback:\n        table.add_row(\"traceback\", Text(self.traceback, style=\"bold red\"))\n\n    for name, profile in self.profiles.items():\n        table.add_row(\"profile:\" + name, Pretty(profile))\n\n    items.append(table)\n\n    yield from items\n</code></pre>"},{"location":"reference/optimization/trials/#history","title":"History","text":"<p>The <code>History</code> is used to keep a structured record of what occured with <code>Trial</code>s and their associated <code>Report</code>s.</p> Usage <p><pre><code>from amltk.optimization import Trial, History, Metric\nfrom amltk.store import PathBucket\n\nloss = Metric(\"loss\", minimize=True)\n\ndef target_function(trial: Trial) -&gt; Trial.Report:\n    x = trial.config[\"x\"]\n    y = trial.config[\"y\"]\n    trial.store({\"config.json\": trial.config})\n\n    with trial.begin():\n        loss = x**2 - y\n\n    if trial.exception:\n        return trial.fail()\n\n    return trial.success(loss=loss)\n\n# ... usually obtained from an optimizer\nbucket = PathBucket(\"all-trial-results\")\nhistory = History()\n\nfor x, y in zip([1, 2, 3], [4, 5, 6]):\n    trial = Trial(name=\"some-unique-name\", config={\"x\": x, \"y\": y}, bucket=bucket, metrics=[loss])\n    report = target_function(trial)\n    history.add(report)\n\nprint(history.df())\nbucket.rmdir()  # markdon-exec: hide\n</code></pre> <p>                   status  trial_seed  ... time:kind time:unit name                                   ...                     some-unique-name  success          ...      wall   seconds some-unique-name  success          ...      wall   seconds some-unique-name  success          ...      wall   seconds  [3 rows x 20 columns]  <p>You'll often need to perform some operations on a <code>History</code> so we provide some utility functions here:</p> <ul> <li><code>filter(key=...)</code> - Filters the history by some     predicate, e.g. <code>history.filter(lambda report: report.status == \"success\")</code></li> <li><code>groupby(key=...)</code> - Groups the history by some     key, e.g. <code>history.groupby(lambda report: report.config[\"x\"] &lt; 5)</code></li> <li><code>sortby(key=...)</code> - Sorts the history by some     key, e.g. <code>history.sortby(lambda report: report.time.end)</code></li> </ul> <p>There is also some serialization capabilities built in, to allow you to store your reports and load them back in later:</p> <ul> <li><code>df(...)</code> - Output a <code>pd.DataFrame</code> of all  the information available.</li> <li><code>from_df(...)</code> - Create a <code>History</code> from     a <code>pd.DataFrame</code>.</li> </ul> <p>You can also retrieve individual reports from the history by using their name, e.g. <code>history[\"some-unique-name\"]</code> or iterate through the history with <code>for report in history: ...</code>.</p>"},{"location":"reference/pipelines/builders/","title":"Builders","text":""},{"location":"reference/pipelines/builders/#builders","title":"Builders","text":"<p>A pipeline of <code>Node</code>s is just an abstract representation of some implementation of a pipeline that will actually do things, for example an sklearn <code>Pipeline</code> or a Pytorch <code>Sequential</code>.</p> <p>To facilitate custom builders and to allow you to customize building, there is a explicit argument <code>builder=</code> required when calling <code>.build(builder=...)</code> on your pipeline.</p> <p>Each builder gives the various kinds of components an actual meaning, for example the <code>Split</code> with the sklearn <code>builder()</code>, translates to a <code>ColumnTransformer</code> and a <code>Sequential</code> translates to an sklearn <code>Pipeline</code>.</p>"},{"location":"reference/pipelines/builders/#scikit-learn","title":"Scikit-learn","text":"<p>The sklearn <code>builder()</code>, converts a pipeline made of <code>Node</code>s into a sklearn <code>Pipeline</code>.</p> <p>Requirements</p> <p>This requires <code>sklearn</code> which can be installed with:</p> <pre><code>pip install \"amltk[scikit-learn]\"\n\n# Or directly\npip install scikit-learn\n</code></pre> <p>Each kind of node corresponds to a different part of the end pipeline:</p> <code>Fixed</code><code>Component</code><code>Sequential</code><code>Split</code><code>Join</code><code>Choice</code> <p><code>Fixed</code> - The estimator will simply be cloned, allowing you to directly configure some object in a pipeline.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Fixed\n\nest = Fixed(RandomForestClassifier(n_estimators=25))\nbuilt_pipeline = est.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> </p> </p> <p><code>Component</code> - The estimator will be built from the component's config. This is mostly useful to allow a space to be defined for the component.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom amltk.pipeline import Component\n\nest = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n\n# ... Likely get the configuration through an optimizer or sampling\nconfigured_est = est.configure({\"n_estimators\": 25})\n\nbuilt_pipeline = configured_est.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> </p> </p> <p><code>Sequential</code> - The sequential will be converted into a <code>Pipeline</code>, building whatever nodes are contained within in.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom amltk.pipeline import Component, Sequential\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, config={\"n_estimators\": 25})\n)\nbuilt_pipeline = pipeline.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier',\n                 RandomForestClassifier(n_estimators=25))])</pre>PCA<pre>PCA(n_components=3)</pre>RandomForestClassifier<pre>RandomForestClassifier(n_estimators=25)</pre> </p> </p> <p><code>Split</code> - The split will be converted into a <code>ColumnTransformer</code>, where each path and the data that should go through it is specified by the split's config. You can provide a <code>ColumnTransformer</code> directly as the item to the <code>Split</code>, or otherwise if left blank, it will default to the standard sklearn one.</p> <p>You can use a <code>Fixed</code> with the special keyword <code>\"passthrough\"</code> as you might normally do with a <code>ColumnTransformer</code>.</p> <p>By default, we provide two special keywords you can provide to a <code>Split</code>, namely <code>\"categorical\"</code> and <code>\"numerical\"</code>, which will automatically configure a <code>ColumnTransorfmer</code> to pass the appropraite columns of a data-frame to the given paths.</p> <p><pre><code>from amltk.pipeline import Split, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    Component(\n        OneHotEncoder,\n        space={\n            \"min_frequency\": (0.01, 0.1),\n            \"handle_unknown\": [\"ignore\", \"infrequent_if_exist\"],\n        },\n        config={\"drop\": \"first\"},\n    ),\n]\nnumerical_pipeline = [SimpleImputer(strategy=\"median\"), StandardScaler()]\n\nsplit = Split(\n    {\n        \"categorical\": categorical_pipeline,\n        \"numerical\": numerical_pipeline\n    }\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Split(Split-EeZgUbYu) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(categorical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item SimpleImputer(strategy='\u2026 \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502                 \u2193                  \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u256d\u2500 Fixed(StandardScaler) \u2500\u256e        \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Component(OneHotEncoder) \u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502 item StandardScaler()   \u2502        \u2502 \u2502\n\u2502 \u2502 \u2502 item   class                  \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f        \u2502 \u2502\n\u2502 \u2502 \u2502        OneHotEncoder(...)     \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 \u2502 \u2502 config {'drop': 'first'}      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502 space  {                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'min_frequency': ( \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.01,          \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                0.1            \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ),                 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            'handle_unknown':  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        [                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'ignore',      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502                'infrequent_i\u2026 \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502            ]                  \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2502        }                      \u2502 \u2502                                        \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502                                        \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <p>You can manually specify the column selectors if you prefer.</p> <pre><code>split = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numbers\": numerical_pipeline,\n    },\n    config={\n        \"categories\": make_column_selector(dtype_include=object),\n        \"numbers\": make_column_selector(dtype_include=np.number),\n    },\n)\n</code></pre> <p><code>Join</code> - The join will be converted into a <code>FeatureUnion</code>.</p> <p><pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\njoin = Join(PCA(n_components=2), SelectKBest(k=3), name=\"my_feature_union\")\n\npipeline = join.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                                                ('SelectKBest',\n                                                 SelectKBest(k=3))]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA(n_components=2)),\n                               ('SelectKBest', SelectKBest(k=3))])</pre>PCAPCA<pre>PCA(n_components=2)</pre>SelectKBestSelectKBest<pre>SelectKBest(k=3)</pre> </p> </p> <p><code>Choice</code> - The estimator will be built from the chosen component's config. This is very similar to <code>Component</code>.</p> <p><pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom amltk.pipeline import Choice\n\n# The choice here is usually provided during the `.configure()` step.\nestimator_choice = Choice(\n    RandomForestClassifier(),\n    MLPClassifier(),\n    config={\"__choice__\": \"RandomForestClassifier\"}\n)\n\nbuilt_pipeline = estimator_choice.build(\"sklearn\")\n</code></pre> <p><pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('RandomForestClassifier', RandomForestClassifier())])</pre>RandomForestClassifier<pre>RandomForestClassifier()</pre> </p> </p>"},{"location":"reference/pipelines/builders/#pytorch","title":"PyTorch","text":"Planned <p>If anyone has good knowledge of building pytorch networks in a more functional manner and would like to contribute, please feel free to reach out!</p> <p>At the moment, we do not provide any native support for <code>torch</code>. You can however make use of <code>skorch</code> to convert your networks to a scikit-learn interface, using the scikit-learn builder instead.</p>"},{"location":"reference/pipelines/pipeline/","title":"Pipeline","text":""},{"location":"reference/pipelines/pipeline/#pieces-of-a-pipeline","title":"Pieces of a Pipeline","text":"<p>A pipeline is a collection of <code>Node</code>s that are connected together to form a directed acylic graph, where the nodes follow a parent-child relation ship. The purpose of these is to form some abstract representation of what you want to search over/optimize and then build into a concrete object.</p> <p>These <code>Node</code>s allow you to specific the function/object that will be used there, it's search space and any configuration you want to explicitly apply. There are various components listed below which gives these nodes extract syntatic meaning, e.g. a <code>Choice</code> which represents some choice between it's children while a <code>Sequential</code> indicates that each child follows one after the other.</p> <p>Once a pipeline is created, you can perform 3 very critical operations on it:</p> <ul> <li><code>search_space(parser=...)</code> - This will return the   search space of the pipeline, as defined by it's nodes. You can find the reference to   the available parsers and search spaces here.</li> <li><code>configure(config=...)</code> - This will return a   new pipeline where each node is configured correctly.</li> <li><code>build(builder=...)</code> - This will return some     concrete object from a configured pipeline. You can find the reference to     the available builders here.</li> </ul>"},{"location":"reference/pipelines/pipeline/#components","title":"Components","text":"<p>You can use the various different node types to build a pipeline.</p> <p>You can connect these nodes together using either the constructors explicitly, as shown in the examples. We also provide some index operators:</p> <ul> <li><code>&gt;&gt;</code> - Connect nodes together to form a <code>Sequential</code></li> <li><code>&amp;</code> - Connect nodes together to form a <code>Join</code></li> <li><code>|</code> - Connect nodes together to form a <code>Choice</code></li> </ul> <p>There is also another short-hand that you may find useful to know:</p> <ul> <li><code>{comp1, comp2, comp3}</code> - This will automatically be converted into a     <code>Choice</code> between the given components.</li> <li><code>(comp1, comp2, comp3)</code> - This will automatically be converted into a     <code>Join</code> between the given components.</li> <li><code>[comp1, comp2, comp3]</code> - This will automatically be converted into a     <code>Sequential</code> between the given components.</li> </ul> <p>For each of these components we will show examples using the <code>\"sklearn\"</code> builder</p> <p>The components are:</p>"},{"location":"reference/pipelines/pipeline/#amltk.pipeline.components--component","title":"Component","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Component</code> of the pipeline with a possible item and no children.</p> <p>This is the basic building block of most pipelines, it accepts as it's <code>item=</code> some function that will be called with <code>build_item()</code> to build that one part of the pipeline.</p> <p>When <code>build_item()</code> is called, The <code>.config</code> on this node will be passed to the function to build the item.</p> <p>A common pattern is to use a <code>Component</code> to wrap a constructor, specifying the <code>space=</code> and <code>config=</code> to be used when building the item.</p> <pre><code>from amltk.pipeline import Component\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = Component(\n    RandomForestClassifier,\n    config={\"max_depth\": 3},\n    space={\"n_estimators\": (10, 100)}\n)\n\nconfig = {\"n_estimators\": 50}  # Sample from some space or something\nconfigured_rf = rf.configure(config)\n\nestimator = configured_rf.build_item()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class RandomForestClassifier(...) \u2502\n\u2502 config {'max_depth': 3}                  \u2502\n\u2502 space  {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier<pre>RandomForestClassifier(max_depth=3, n_estimators=50)</pre> </p> <p>Whenever some other node sees a function/constructor, i.e. <code>RandomForestClassifier</code>, this will automatically be converted into a <code>Component</code>.</p> <pre><code>from amltk.pipeline import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(RandomForestClassifier, name=\"my_pipeline\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The default <code>.name</code> of a component is the name of the class/function that it will use. You can explicitly set the <code>name=</code> if you want to when constructing the component.</p> <p>Like all <code>Node</code>s, a <code>Component</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    item: Callable[..., Item],\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"reference/pipelines/pipeline/#amltk.pipeline.components--sequential","title":"Sequential","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Sequential</code> set of operations in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act one after another, feeding the output of one into the next.</p> <pre><code>from amltk.pipeline import Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Sequential(\n    PCA(n_components=3),\n    Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)}),\n    name=\"my_pipeline\"\n)\n\nspace = pipeline.search_space(\"configspace\")\n\nconfiguration = space.sample_configuration()\n\nconfigured_pipeline = pipeline.configure(configuration)\n\nsklearn_pipeline = pipeline.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_pipeline:RandomForestClassifier:n_estimators, Type: UniformInteger, \nRange: [10, 100], Default: 55\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'my_pipeline:RandomForestClassifier:n_estimators': 37,\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                 \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                 \u2502\n\u2502                      \u2193                       \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item   class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 config {'n_estimators': 37}              \u2502 \u2502\n\u2502 \u2502 space  {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier', RandomForestClassifier())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('PCA', PCA(n_components=3)),\n                ('RandomForestClassifier', RandomForestClassifier())])</pre>PCA<pre>PCA(n_components=3)</pre>RandomForestClassifier<pre>RandomForestClassifier()</pre> </p> <p>You may also just chain together nodes using an infix operator <code>&gt;&gt;</code> if you prefer:</p> <pre><code>from amltk.pipeline import Join, Component, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = (\n    Sequential(name=\"my_pipeline\")\n    &gt;&gt; PCA(n_components=3)\n    &gt;&gt; Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees a list, i.e. <code>[comp1, comp2, comp3]</code>, this will automatically be converted into a <code>Sequential</code>.</p> <pre><code>from amltk.pipeline import Choice\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\npipeline_choice = Choice(\n    [SimpleImputer(), RandomForestClassifier()],\n    [StandardScaler(), MLPClassifier()],\n    name=\"pipeline_choice\"\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(pipeline_choice) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Sequential(Seq-45oJkIK6) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(Seq-ajnm7LxN) \u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u256e          \u2502 \u2502 \u256d\u2500 Fixed(StandardScaler) \u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer()   \u2502          \u2502 \u2502 \u2502 item StandardScaler()   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f          \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2502                  \u2193                  \u2502 \u2502              \u2193              \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u256e \u2502 \u2502 \u256d\u2500 Fixed(MLPClassifier) \u2500\u256e  \u2502 \u2502\n\u2502 \u2502 \u2502 item RandomForestClassifier()   \u2502 \u2502 \u2502 \u2502 item MLPClassifier()   \u2502  \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Sequential</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n\n    # Perhaps we need to do a deeper check on this...\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise DuplicateNamesError(self)\n\n    if name is None:\n        name = f\"Seq-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"reference/pipelines/pipeline/#amltk.pipeline.components--choice","title":"Choice","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Choice</code> between different subcomponents.</p> <p>This indicates that a choice should be made between the different children in <code>.nodes</code>, usually done when you <code>configure()</code> with some <code>config</code> from a <code>search_space()</code>.</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = Choice(rf, mlp, name=\"estimator\")\n\nspace = estimator_choice.search_space(\"configspace\")\n\nconfig = space.sample_configuration()\n\nconfigured_choice = estimator_choice.configure(config)\n\nchosen_estimator = configured_choice.chosen()\n\nestimator = chosen_estimator.build_item()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    estimator:MLPClassifier:activation, Type: Categorical, Choices: {logistic, \nrelu, tanh}, Default: logistic\n    estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: \n[10, 100], Default: 55\n    estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, \nRandomForestClassifier}, Default: MLPClassifier\n  Conditions:\n    estimator:MLPClassifier:activation | estimator:__choice__ == 'MLPClassifier'\n    estimator:RandomForestClassifier:n_estimators | estimator:__choice__ == \n'RandomForestClassifier'\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'estimator:RandomForestClassifier:n_estimators': 70,\n  'estimator:__choice__': 'RandomForestClassifier',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {'__choice__': 'RandomForestClassifier'}                              \u2502\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item   class                       \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502        RandomForestClassifier(...) \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 config {'n_estimators': 70}        \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2502 space  {'n_estimators': (10, 100)} \u2502    \u2502\n\u2502 \u2502               'relu',          \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item   class RandomForestClassifier(...) \u2502\n\u2502 config {'n_estimators': 70}              \u2502\n\u2502 space  {'n_estimators': (10, 100)}       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>RandomForestClassifier(n_estimators=70)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier<pre>RandomForestClassifier(n_estimators=70)</pre> </p> <p>You may also just add nodes to a <code>Choice</code> using an infix operator <code>|</code> if you prefer:</p> <pre><code>from amltk.pipeline import Choice, Component\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\nestimator_choice = (\n    Choice(name=\"estimator\") | mlp | rf\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Choice(estimator) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifier)\u2500\u256e    \u2502\n\u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                        \u2502    \u2502\n\u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(...)  \u2502    \u2502\n\u2502 \u2502           'activation': [      \u2502 \u2502 space {'n_estimators': (10, 100)}  \u2502    \u2502\n\u2502 \u2502               'logistic',      \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2502 \u2502               'relu',          \u2502                                           \u2502\n\u2502 \u2502               'tanh'           \u2502                                           \u2502\n\u2502 \u2502           ]                    \u2502                                           \u2502\n\u2502 \u2502       }                        \u2502                                           \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees a set, i.e. <code>{comp1, comp2, comp3}</code>, this will automatically be converted into a <code>Choice</code>.</p> <pre><code>from amltk.pipeline import Choice, Component, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.impute import SimpleImputer\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\nmlp = Component(MLPClassifier, space={\"activation\": [\"logistic\", \"relu\", \"tanh\"]})\n\npipeline = Sequential(\n    SimpleImputer(fill_value=0),\n    {mlp, rf},\n    name=\"my_pipeline\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                                         \u2502\n\u2502 \u2502 item SimpleImputer(fill_value=0) \u2502                                         \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                         \u2502\n\u2502                                      \u2193                                       \u2502\n\u2502 \u256d\u2500 Choice(Choice-lCPyh81L) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(MLPClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(RandomForestClassifie\u2500\u256e  \u2502 \u2502\n\u2502 \u2502 \u2502 item  class MLPClassifier(...) \u2502 \u2502 item  class                      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502 space {                        \u2502 \u2502       RandomForestClassifier(..\u2026 \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502           'activation': [      \u2502 \u2502 space {                          \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'logistic',      \u2502 \u2502           'n_estimators': (      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'relu',          \u2502 \u2502               10,                \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502               'tanh'           \u2502 \u2502               100                \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502           ]                    \u2502 \u2502           )                      \u2502  \u2502 \u2502\n\u2502 \u2502 \u2502       }                        \u2502 \u2502       }                          \u2502  \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Choice</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> <p>Order of nodes</p> <p>The given nodes of a choice are always ordered according to their name, so indexing <code>choice.nodes</code> may not be reliable if modifying the choice dynamically.</p> <p>Please use <code>choice[\"name\"]</code> to access the nodes instead.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes: tuple[Node, ...] = tuple(\n        sorted((as_node(n) for n in nodes), key=lambda n: n.name),\n    )\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes as we can not generate a __choice__ for {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Choice-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"reference/pipelines/pipeline/#amltk.pipeline.components--split","title":"Split","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p>A <code>Split</code> of data in a pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in parallel but on different subsets of data.</p> <pre><code>from amltk.pipeline import Component, Split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector\n\ncategorical_pipeline = [\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(drop=\"first\"),\n]\nnumerical_pipeline = Component(SimpleImputer, space={\"strategy\": [\"mean\", \"median\"]})\n\npreprocessor = Split(\n    {\n        \"categories\": categorical_pipeline,\n        \"numerical\": numerical_pipeline,\n    },\n    config={\n        # This is how you would configure the split for the sklearn builder in particular\n        \"categories\": make_column_selector(dtype_include=\"category\"),\n        \"numerical\": make_column_selector(dtype_exclude=\"category\"),\n    },\n    name=\"my_split\"\n)\n\nspace = preprocessor.search_space(\"configspace\")\n\nconfiguration = space.sample_configuration()\n\nconfigured_preprocessor = preprocessor.configure(configuration)\n\nbuilt_preprocessor = configured_preprocessor.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f24784ae500&gt;,                                                      \u2502\n\u2502            'numerical':                                                      \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f24784ad540&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item  class SimpleImputer(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502 space {                        \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502           'strategy': [        \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502               'mean',          \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502               'median'         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502           ]                    \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502       }                        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_split:numerical:SimpleImputer:strategy, Type: Categorical, Choices: \n{mean, median}, Default: mean\n\n</code>\n</pre> <pre>\n<code>Configuration(values={\n  'my_split:numerical:SimpleImputer:strategy': 'median',\n})\n</code>\n</pre> <pre>\n<code>\u256d\u2500 Split(my_split) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 config {                                                                     \u2502\n\u2502            'categories':                                                     \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f24784ae500&gt;,                                                      \u2502\n\u2502            'numerical':                                                      \u2502\n\u2502        &lt;sklearn.compose._column_transformer.make_column_selector object at   \u2502\n\u2502        0x7f24784ad540&gt;                                                       \u2502\n\u2502        }                                                                     \u2502\n\u2502 \u256d\u2500 Sequential(categories) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Sequential(numerical) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u256d\u2500 Component(SimpleImputer) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item SimpleImputer(fill_valu\u2026 \u2502 \u2502 \u2502 \u2502 item   class                   \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502      strategy='constant')     \u2502 \u2502 \u2502 \u2502        SimpleImputer(...)      \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502 config {'strategy': 'median'}  \u2502 \u2502 \u2502\n\u2502 \u2502                 \u2193                 \u2502 \u2502 \u2502 space  {                       \u2502 \u2502 \u2502\n\u2502 \u2502 \u256d\u2500 Fixed(OneHotEncoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502 \u2502            'strategy': [       \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 item OneHotEncoder(drop='fir\u2026 \u2502 \u2502 \u2502 \u2502                'mean',         \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502 \u2502                'median'        \u2502 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502            ]                   \u2502 \u2502 \u2502\n\u2502                                       \u2502 \u2502        }                       \u2502 \u2502 \u2502\n\u2502                                       \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502                                       \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('my_split',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ae500&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(strategy='median'),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ad540&gt;)]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_split',\n                 ColumnTransformer(transformers=[('categories',\n                                                  Pipeline(steps=[('SimpleImputer',\n                                                                   SimpleImputer(fill_value='missing',\n                                                                                 strategy='constant')),\n                                                                  ('OneHotEncoder',\n                                                                   OneHotEncoder(drop='first'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ae500&gt;),\n                                                 ('SimpleImputer',\n                                                  SimpleImputer(strategy='median'),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ad540&gt;)]))])</pre>my_split: ColumnTransformer<pre>ColumnTransformer(transformers=[('categories',\n                                 Pipeline(steps=[('SimpleImputer',\n                                                  SimpleImputer(fill_value='missing',\n                                                                strategy='constant')),\n                                                 ('OneHotEncoder',\n                                                  OneHotEncoder(drop='first'))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ae500&gt;),\n                                ('SimpleImputer',\n                                 SimpleImputer(strategy='median'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ad540&gt;)])</pre>categories<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ae500&gt;</pre>SimpleImputer<pre>SimpleImputer(fill_value='missing', strategy='constant')</pre>OneHotEncoder<pre>OneHotEncoder(drop='first')</pre>SimpleImputer<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x7f24784ad540&gt;</pre>SimpleImputer<pre>SimpleImputer(strategy='median')</pre> </p> <p>The split is a slight oddity when compared to the other kinds of components in that it allows a <code>dict</code> as it's first argument, where the keys are the names of the different paths through which data will go and the values are the actual nodes that will receive the data.</p> <p>If nodes are passed in as they are for all other components, usually the name of the first node will be important for any builder trying to make sense of how to use the <code>Split</code></p> <p>Like all <code>Node</code>s, a <code>Split</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike | dict[str, Node | NodeLike],\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    if any(isinstance(n, dict) for n in nodes):\n        if len(nodes) &gt; 1:\n            raise ValueError(\n                \"Can't handle multiple nodes with a dictionary as a node.\\n\"\n                f\"{nodes=}\",\n            )\n        _node = nodes[0]\n        assert isinstance(_node, dict)\n\n        def _construct(key: str, value: Node | NodeLike) -&gt; Node:\n            match value:\n                case list():\n                    return Sequential(*value, name=key)\n                case set() | tuple():\n                    return as_node(value, name=key)\n                case _:\n                    return Sequential(value, name=key)\n\n        _nodes = tuple(_construct(key, value) for key, value in _node.items())\n    else:\n        _nodes = tuple(as_node(n) for n in nodes)\n\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes they do not all contain unique names, {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Split-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"reference/pipelines/pipeline/#amltk.pipeline.components--join","title":"Join","text":"<p>         Bases: <code>Node[Item, Space]</code></p> <p><code>Join</code> together different parts of the pipeline.</p> <p>This indicates the different children in <code>.nodes</code> should act in tandem with one another, for example, concatenating the outputs of the various members of the <code>Join</code>.</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\njoin = Join(pca, kbest, name=\"my_feature_union\")\n\nspace = join.search_space(\"configspace\")\n\npipeline = join.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Join(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>\n<code>Configuration space object:\n  Hyperparameters:\n    my_feature_union:PCA:n_components, Type: UniformInteger, Range: [1, 3], \nDefault: 2\n    my_feature_union:SelectKBest:k, Type: UniformInteger, Range: [1, 3], \nDefault: 2\n\n</code>\n</pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA()), ('SelectKBest', SelectKBest())])</pre>PCAPCA<pre>PCA()</pre>SelectKBestSelectKBest<pre>SelectKBest()</pre> </p> <p>You may also just join together nodes using an infix operator <code>&amp;</code> if you prefer:</p> <pre><code>from amltk.pipeline import Join, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\n# Can not parametrize or name the join\njoin = pca &amp; kbest\n\n# With a parametrized join\njoin = (\n    Join(name=\"my_feature_union\") &amp; pca &amp; kbest\n)\nitem = join.build(\"sklearn\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Join(Join-m6jB9hfW) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502\n\u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('my_feature_union',\n                 FeatureUnion(transformer_list=[('PCA', PCA()),\n                                                ('SelectKBest',\n                                                 SelectKBest())]))])</pre>my_feature_union: FeatureUnion<pre>FeatureUnion(transformer_list=[('PCA', PCA()), ('SelectKBest', SelectKBest())])</pre>PCAPCA<pre>PCA()</pre>SelectKBestSelectKBest<pre>SelectKBest()</pre> </p> <p>Whenever some other node sees a tuple, i.e. <code>(comp1, comp2, comp3)</code>, this will automatically be converted into a <code>Join</code>.</p> <pre><code>from amltk.pipeline import Sequential, Component\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\n\npca = Component(PCA, space={\"n_components\": (1, 3)})\nkbest = Component(SelectKBest, space={\"k\": (1, 3)})\n\n# Can not parametrize or name the join\njoin = Sequential(\n    (pca, kbest),\n    RandomForestClassifier(n_estimators=5),\n    name=\"my_feature_union\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_feature_union) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Join(Join-vkoO0iXU) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 \u256d\u2500 Component(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u256d\u2500 Component(SelectKBest) \u2500\u2500\u2500\u2500\u2500\u256e \u2502 \u2502\n\u2502 \u2502 \u2502 item  class PCA(...)           \u2502 \u2502 item  class SelectKBest(...) \u2502 \u2502 \u2502\n\u2502 \u2502 \u2502 space {'n_components': (1, 3)} \u2502 \u2502 space {'k': (1, 3)}          \u2502 \u2502 \u2502\n\u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                    \u2193                                    \u2502\n\u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                         \u2502\n\u2502 \u2502 item RandomForestClassifier(n_estimators=5) \u2502                         \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Like all <code>Node</code>s, a <code>Join</code> accepts an explicit <code>name=</code>, <code>item=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    *nodes: Node | NodeLike,\n    name: str | None = None,\n    item: Item | Callable[[Item], Item] | None = None,\n    config: Config | None = None,\n    space: Space | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    _nodes = tuple(as_node(n) for n in nodes)\n    if not all_unique(_nodes, key=lambda node: node.name):\n        raise ValueError(\n            f\"Can't handle nodes they do not all contain unique names, {nodes=}.\"\n            \"\\nAll nodes must have a unique name. Please provide a `name=` to them\",\n        )\n\n    if name is None:\n        name = f\"Join-{randuid(8)}\"\n\n    super().__init__(\n        *_nodes,\n        name=name,\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"reference/pipelines/pipeline/#amltk.pipeline.components--fixed","title":"Fixed","text":"<p>         Bases: <code>Node[Item, None]</code></p> <p>A <code>Fixed</code> part of the pipeline that represents something that can not be configured and used directly as is.</p> <p>It consists of an <code>.item</code> that is fixed, non-configurable and non-searchable. It also has no children.</p> <p>This is useful for representing parts of the pipeline that are fixed, for example if you have a pipeline that is a <code>Sequential</code> of nodes, but you want to fix the first component to be a <code>PCA</code> with <code>n_components=3</code>, you can use a <code>Fixed</code> to represent that.</p> <pre><code>from amltk.pipeline import Component, Fixed, Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\nrf = Component(RandomForestClassifier, space={\"n_estimators\": (10, 100)})\npca = Fixed(PCA(n_components=3))\n\npipeline = Sequential(pca, rf, name=\"my_pipeline\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                \u2502\n\u2502                      \u2193                      \u2502\n\u2502 \u256d\u2500 Component(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item  class RandomForestClassifier(...) \u2502 \u2502\n\u2502 \u2502 space {'n_estimators': (10, 100)}       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>Whenever some other node sees an instance of something, i.e. something that can't be called, this will automatically be converted into a <code>Fixed</code>.</p> <pre><code>from amltk.pipeline import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\n\npipeline = Sequential(\n    PCA(n_components=3),\n    RandomForestClassifier(n_estimators=50),\n    name=\"my_pipeline\",\n)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Sequential(my_pipeline) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500 Fixed(PCA) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                     \u2502\n\u2502 \u2502 item PCA(n_components=3) \u2502                     \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                     \u2502\n\u2502                        \u2193                         \u2502\n\u2502 \u256d\u2500 Fixed(RandomForestClassifier) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 item RandomForestClassifier(n_estimators=50) \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The default <code>.name</code> of a component is the class name of the item that it will use. You can explicitly set the <code>name=</code> if you want to when constructing the component.</p> <p>A <code>Fixed</code> accepts only an explicit <code>name=</code>, <code>item=</code>, <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    item: Item,\n    *,\n    name: str | None = None,\n    config: None = None,\n    space: None = None,\n    fidelities: None = None,\n    config_transform: None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    super().__init__(\n        name=name if name is not None else entity_name(item),\n        item=item,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"reference/pipelines/pipeline/#amltk.pipeline.components--searchable","title":"Searchable","text":"<p>         Bases: <code>Node[None, Space]</code></p> <p>A <code>Searchable</code> node of the pipeline which just represents a search space, no item attached.</p> <p>While not usually applicable to pipelines you want to build, this component is useful for creating a search space, especially if the real pipeline you want to optimize can not be built directly. For example, if you are optimize a script, you may wish to use a <code>Searchable</code> to represent the search space of that script.</p> <pre><code>from amltk.pipeline import Searchable\n\nscript_space = Searchable({\"mode\": [\"orange\", \"blue\", \"red\"], \"n\": (10, 100)})\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Searchable(Searchable-rjvHhh5b) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 space {'mode': ['orange', 'blue', 'red'], 'n': (10, 100)} \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A <code>Searchable</code> explicitly does not allow for <code>item=</code> to be set, nor can it have any children. A <code>Searchable</code> accepts an explicit <code>name=</code>, <code>config=</code>, <code>space=</code>, <code>fidelities=</code>, <code>config_transform=</code> and <code>meta=</code>.</p> See Also <ul> <li><code>Node</code></li> </ul> Source code in <code>src/amltk/pipeline/components.py</code> <pre><code>def __init__(\n    self,\n    space: Space | None = None,\n    *,\n    name: str | None = None,\n    config: Config | None = None,\n    fidelities: Mapping[str, Any] | None = None,\n    config_transform: Callable[[Config, Any], Config] | None = None,\n    meta: Mapping[str, Any] | None = None,\n):\n    \"\"\"See [`Node`][amltk.pipeline.node.Node] for details.\"\"\"\n    if name is None:\n        name = f\"Searchable-{randuid(8)}\"\n\n    super().__init__(\n        name=name,\n        config=config,\n        space=space,\n        fidelities=fidelities,\n        config_transform=config_transform,\n        meta=meta,\n    )\n</code></pre>"},{"location":"reference/pipelines/pipeline/#node","title":"Node","text":"<p>A pipeline consists of <code>Node</code>s, which hold the various attributes required to build a pipeline, such as the <code>.item</code>, its <code>.space</code>, its <code>.config</code> and so on.</p> <p>The <code>Node</code>s are connected to each in a parent-child relation ship where the children are simply the <code>.nodes</code> that the parent leads to.</p> <p>To give these attributes and relations meaning, there are various subclasses of <code>Node</code> which give different syntactic meanings when you want to construct something like a <code>search_space()</code> or <code>build()</code> some concrete object out of the pipeline.</p> <p>For example, a <code>Sequential</code> node gives the meaning that each of its children in <code>.nodes</code> should follow one another while something like a <code>Choice</code> gives the meaning that only one of its children should be chosen.</p> <p>You will likely never have to create a <code>Node</code> directly, but instead use the various components to create the pipeline.</p> Hashing <p>When hashing a node, i.e. to put it in a <code>set</code> or as a key in a <code>dict</code>, only the name of the node and the hash of its children is used. This means that two nodes with the same connectivity will be equalling hashed,</p> Equality <p>When considering equality, this will be done by comparing all the fields of the node. This include even the <code>parent</code> and <code>branches</code> fields. This means two nodes are considered equal if they look the same and they are connected in to nodes that also look the same.</p>"},{"location":"reference/pipelines/spaces/","title":"Spaces","text":""},{"location":"reference/pipelines/spaces/#spaces","title":"Spaces","text":"<p>A common requirement when performing optimization of some pipeline is to be able to parametrize it. To do so we often think about parametrize each component separately, with the structure of the pipeline adding additional constraints.</p> <p>To facilitate this, we allow the construction of piplines, where each part of the pipeline can contains a <code>.space</code>. When we wish to extract out the entire search space from the pipeline, we can call <code>search_space(parser=...)</code> on the root node of our pipeline, returning some sort of space object.</p> <p>Now there are unfortunately quite a few search space implementations out there. Some support concepts such as forbidden combinations, conditionals and functional constraints, while others are fully constrained just numerical parameters. Other reasons to choose a particular space representation is dependant upon some <code>Optimizer</code> you may wish to use, where typically they will only have one preferred search space representation.</p> <p>To generalize over this, AMLTK itself will not care what is in a <code>.space</code> of each part of the pipeline, i.e.</p> <pre><code>from amltk.pipeline import Component\n\nc = Component(object, space=\"hmmm, a str space?\")\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Component(object) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 item  class object(...)    \u2502\n\u2502 space 'hmmm, a str space?' \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>What follow's below is a list of supported parsers you could pass <code>parser=</code> to extract a search space representation.</p>"},{"location":"reference/pipelines/spaces/#configspace","title":"ConfigSpace","text":"<p>ConfigSpace is a library for representing and sampling configurations for hyperparameter optimization. It features a straightforward API for defining hyperparameters, their ranges and even conditional dependencies.</p> <p>It is generally flexible enough for more complex use cases, even handling the complex pipelines of AutoSklearn and AutoPyTorch, large scale hyperparameter spaces over which to optimize entire pipelines at a time.</p> <p>Requirements</p> <p>This requires <code>ConfigSpace</code> which can be installed with:</p> <pre><code>pip install \"amltk[configspace]\"\n\n# Or directly\npip install ConfigSpace\n</code></pre> <p>In general, you should have the ConfigSpace documentation ready to consult for a full understanding of how to construct hyperparameter spaces with AMLTK.</p>"},{"location":"reference/pipelines/spaces/#amltk.pipeline.parsers.configspace--basic-usage","title":"Basic Usage","text":"<p>You can directly us the <code>parser()</code> function and pass that into the <code>search_space()</code> method of a <code>Node</code>, however you can also simply provide <code>search_space(parser=\"configspace\", ...)</code> for simplicity.</p> <pre><code>from amltk.pipeline import Component, Choice, Sequential\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\nmy_pipeline = (\n    Sequential(name=\"Pipeline\")\n    &gt;&gt; Component(PCA, space={\"n_components\": (1, 3)})\n    &gt;&gt; Choice(\n        Component(\n            SVC,\n            space={\"C\": (0.1, 10.0)}\n        ),\n        Component(\n            RandomForestClassifier,\n            space={\"n_estimators\": (10, 100), \"criterion\": [\"gini\", \"log_loss\"]},\n        ),\n        Component(\n            MLPClassifier,\n            space={\n                \"activation\": [\"identity\", \"logistic\", \"relu\"],\n                \"alpha\": (0.0001, 0.1),\n                \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n            },\n        ),\n        name=\"estimator\"\n    )\n)\n\nspace = my_pipeline.search_space(\"configspace\")\nprint(space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Pipeline:PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    Pipeline:estimator:MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    Pipeline:estimator:MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    Pipeline:estimator:MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    Pipeline:estimator:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    Pipeline:estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:estimator:SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    Pipeline:estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n  Conditions:\n    Pipeline:estimator:MLPClassifier:activation | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:MLPClassifier:alpha | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:MLPClassifier:learning_rate | Pipeline:estimator:__choice__ == 'MLPClassifier'\n    Pipeline:estimator:RandomForestClassifier:criterion | Pipeline:estimator:__choice__ == 'RandomForestClassifier'\n    Pipeline:estimator:RandomForestClassifier:n_estimators | Pipeline:estimator:__choice__ == 'RandomForestClassifier'\n    Pipeline:estimator:SVC:C | Pipeline:estimator:__choice__ == 'SVC'\n</code></pre> <p>Here we have an example of a few different kinds of hyperparmeters,</p> <ul> <li><code>PCA:n_components</code> is a integer with a range of 1 to 3, uniform distribution, as specified     by it's integer bounds in a tuple.</li> <li><code>SVC:C</code> is a float with a range of 0.1 to 10.0, uniform distribution, as specified     by it's float bounds in a tuple.</li> <li><code>RandomForestClassifier:criterion</code> is a categorical hyperparameter, with two choices,     <code>\"gini\"</code> and <code>\"log_loss\"</code>.</li> </ul> <p>There is also a <code>Choice</code> node, which is a special node that indicates that we could choose from one of these estimators. This leads to the conditionals that you can see in the printed out space.</p> <p>You may wish to remove all conditionals if an <code>Optimizer</code> does not support them, or you may wish to remove them for other reasons. You can do this by passing <code>conditionals=False</code> to the <code>parser()</code> function.</p> <pre><code>print(my_pipeline.search_space(\"configspace\", conditionals=False))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Pipeline:PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    Pipeline:estimator:MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    Pipeline:estimator:MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    Pipeline:estimator:MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    Pipeline:estimator:RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    Pipeline:estimator:RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    Pipeline:estimator:SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    Pipeline:estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n</code></pre> <p>Likewise, you can also remove all heirarchy from the space which may make downstream tasks easier, by passing <code>flat=True</code> to the <code>parser()</code> function.</p> <pre><code>print(my_pipeline.search_space(\"configspace\", flat=True))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MLPClassifier:activation, Type: Categorical, Choices: {identity, logistic, relu}, Default: identity\n    MLPClassifier:alpha, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.05005\n    MLPClassifier:learning_rate, Type: Categorical, Choices: {constant, invscaling, adaptive}, Default: constant\n    PCA:n_components, Type: UniformInteger, Range: [1, 3], Default: 2\n    RandomForestClassifier:criterion, Type: Categorical, Choices: {gini, log_loss}, Default: gini\n    RandomForestClassifier:n_estimators, Type: UniformInteger, Range: [10, 100], Default: 55\n    SVC:C, Type: UniformFloat, Range: [0.1, 10.0], Default: 5.05\n    estimator:__choice__, Type: Categorical, Choices: {MLPClassifier, RandomForestClassifier, SVC}, Default: MLPClassifier\n  Conditions:\n    MLPClassifier:activation | estimator:__choice__ == 'MLPClassifier'\n    MLPClassifier:alpha | estimator:__choice__ == 'MLPClassifier'\n    MLPClassifier:learning_rate | estimator:__choice__ == 'MLPClassifier'\n    RandomForestClassifier:criterion | estimator:__choice__ == 'RandomForestClassifier'\n    RandomForestClassifier:n_estimators | estimator:__choice__ == 'RandomForestClassifier'\n    SVC:C | estimator:__choice__ == 'SVC'\n</code></pre>"},{"location":"reference/pipelines/spaces/#amltk.pipeline.parsers.configspace--more-specific-hyperparameters","title":"More Specific Hyperparameters","text":"<p>You'll often want to be a bit more specific with your hyperparameters, here we just show a few examples of how you'd couple your pipelines a bit more towards <code>ConfigSpace</code>.</p> <pre><code>from ConfigSpace import Float, Categorical, Normal\nfrom amltk.pipeline import Searchable\n\ns = Searchable(\n    space={\n        \"lr\": Float(\"lr\", bounds=(1e-5, 1.), log=True, default=0.3),\n        \"balance\": Float(\"balance\", bounds=(-1.0, 1.0), distribution=Normal(0.0, 0.5)),\n        \"color\": Categorical(\"color\", [\"red\", \"green\", \"blue\"], weights=[2, 1, 1], default=\"blue\"),\n    },\n    name=\"Something-To-Search\",\n)\nprint(s.search_space(\"configspace\"))\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    Something-To-Search:balance, Type: NormalFloat, Mu: 0.0 Sigma: 0.5, Range: [-1.0, 1.0], Default: 0.0\n    Something-To-Search:color, Type: Categorical, Choices: {red, green, blue}, Default: blue, Probabilities: (0.5, 0.25, 0.25)\n    Something-To-Search:lr, Type: UniformFloat, Range: [1e-05, 1.0], Default: 0.3, on log-scale\n</code></pre>"},{"location":"reference/pipelines/spaces/#amltk.pipeline.parsers.configspace--conditional-ands-advanced-usage","title":"Conditional ands Advanced Usage","text":"<p>We will refer you to the ConfigSpace documentation for the construction of these. However once you've constructed a <code>ConfigurationSpace</code> and added any forbiddens and conditionals, you may simply set that as the <code>.space</code> attribute.</p> <pre><code>from amltk.pipeline import Component, Choice, Sequential\nfrom ConfigSpace import ConfigurationSpace, EqualsCondition, InCondition\n\nmyspace = ConfigurationSpace({\"A\": [\"red\", \"green\", \"blue\"], \"B\": (1, 10), \"C\": (-100.0, 0.0)})\nmyspace.add_conditions([\n    EqualsCondition(myspace[\"B\"], myspace[\"A\"], \"red\"),  # B is active when A is red\n    InCondition(myspace[\"C\"], myspace[\"A\"], [\"green\", \"blue\"]), # C is active when A is green or blue\n])\n\ncomponent = Component(object, space=myspace, name=\"MyThing\")\n\nparsed_space = component.search_space(\"configspace\")\nprint(parsed_space)\n</code></pre> <pre><code>Configuration space object:\n  Hyperparameters:\n    MyThing:A, Type: Categorical, Choices: {red, green, blue}, Default: red\n    MyThing:B, Type: UniformInteger, Range: [1, 10], Default: 6\n    MyThing:C, Type: UniformFloat, Range: [-100.0, 0.0], Default: -50.0\n  Conditions:\n    MyThing:B | MyThing:A == 'red'\n    MyThing:C | MyThing:A in {'green', 'blue'}\n</code></pre>"},{"location":"reference/pipelines/spaces/#optuna","title":"Optuna","text":"<p>Optuna parser for parsing out a <code>search_space()</code>. from a pipeline.</p> <p>Requirements</p> <p>This requires <code>Optuna</code> which can be installed with:</p> <pre><code>pip install amltk[optuna]\n\n# Or directly\npip install optuna\n</code></pre> Limitations <p>Optuna feature a very dynamic search space (define-by-run), where people typically sample from some trial object and use traditional python control flow to define conditionality.</p> <p>This means we can not trivially represent this conditionality in a static search space. While band-aids are possible, it naturally does not sit well with the static output of a parser.</p> <p>As such, our parser does not support conditionals or choices!. Users may still use the define-by-run within their optimization function itself.</p> <p>If you have experience with Optuna and have any suggestions, please feel free to open an issue or PR on GitHub!</p>"},{"location":"reference/pipelines/spaces/#amltk.pipeline.parsers.optuna--usage","title":"Usage","text":"<p>The typical way to represent a search space for Optuna is just to use a dictionary, where the keys are the names of the hyperparameters and the values are either integer/float tuples indicating boundaries or some discrete set of values. It is possible to have the value directly be a <code>BaseDistribution</code>, an optuna type, when you need to customize the distribution more.</p> <pre><code>from amltk.pipeline import Component\nfrom optuna.distributions import FloatDistribution\n\nc = Component(\n    object,\n    space={\n        \"myint\": (1, 10),\n        \"myfloat\": (1.0, 10.0),\n        \"mycategorical\": [\"a\", \"b\", \"c\"],\n        \"log-scale-custom\": FloatDistribution(1e-10, 1e-2, log=True),\n    },\n    name=\"name\",\n)\n\nspace = c.search_space(parser=\"optuna\")\n</code></pre> <p> <pre>\n<code>{\n    'name:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'name:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, step=None),\n    'name:mycategorical': CategoricalDistribution(choices=('a', 'b', 'c')),\n    'name:log-scale-custom': FloatDistribution(high=0.01, log=True, low=1e-10, \nstep=None)\n}\n</code>\n</pre> </p> <p>You may also just pass the <code>parser=</code> function directly if preferred</p> <pre><code>from amltk.pipeline.parsers.optuna import parser as optuna_parser\n\nspace = c.search_space(parser=optuna_parser)\n</code></pre> <p> <pre>\n<code>{\n    'name:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'name:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, step=None),\n    'name:mycategorical': CategoricalDistribution(choices=('a', 'b', 'c')),\n    'name:log-scale-custom': FloatDistribution(high=0.01, log=True, low=1e-10, \nstep=None)\n}\n</code>\n</pre> </p> <p>When using <code>search_space()</code> on a some nested structures, you may want to flatten the names of the hyperparameters. For this you can use <code>flat=</code></p> <pre><code>from amltk.pipeline import Searchable, Sequential\n\nseq = Sequential(\n    Searchable({\"myint\": (1, 10)}, name=\"nested_1\"),\n    Searchable({\"myfloat\": (1.0, 10.0)}, name=\"nested_2\"),\n    name=\"seq\"\n)\n\nhierarchical_space = seq.search_space(parser=\"optuna\", flat=False)  # Default\n\nflat_space = seq.search_space(parser=\"optuna\", flat=False)  # Default\n</code></pre> <p> <pre>\n<code>{\n    'seq:nested_1:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'seq:nested_2:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, \nstep=None)\n}\n</code>\n</pre> <pre>\n<code>{\n    'seq:nested_1:myint': IntDistribution(high=10, log=False, low=1, step=1),\n    'seq:nested_2:myfloat': FloatDistribution(high=10.0, log=False, low=1.0, \nstep=None)\n}\n</code>\n</pre> </p>"},{"location":"reference/scheduling/events/","title":"Events","text":""},{"location":"reference/scheduling/events/#events","title":"Events","text":"<p>One of the primary ways to respond to <code>@events</code> emitted with by a <code>Task</code> the <code>Scheduler</code> is through use of a callback.</p> <p>The reason for this is to enable an easier time for API's to utilize multiprocessing and remote compute from the <code>Scheduler</code>, without having to burden users with knowing the details of how to use multiprocessing.</p> <p>A callback subscribes to some event using a decorator but can also be done in a functional style if preferred. The below example is based on the event <code>@scheduler.on_start</code> but the same applies to all events.</p> DecoratorsFunctional <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.run()\n</code></pre> <p>hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\nscheduler.on_start(print_hello)\nscheduler.run()\n</code></pre> <p>hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p>There are a number of ways to customize the behaviour of these callbacks, notably to control how often they get called and when they get called.</p> Callback customization <code>on('event', repeat=...)</code><code>on('event', max_calls=...)</code><code>on('event', when=...)</code><code>on('event', every=...)</code> <p>This will cause the callback to be called <code>repeat</code> times successively. This is most useful in combination with <code>@scheduler.on_start</code> to launch a number of tasks at the start of the scheduler.</p> <p><pre><code>from amltk import Scheduler\n\nN_WORKERS = 2\n\ndef f(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(N_WORKERS)\ntask = scheduler.task(f)\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef on_start():\n    task.submit(1)\n\nscheduler.run()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def on_start() (2)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 2\n\u2503   @on_future_done 2\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 2\n\u2517\u2501\u2501 \u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 2                                                          \u2502\n    \u2502 @on_done 2                                                               \u2502\n    \u2502 @on_result 2                                                             \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: u59L1LWe \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <p>Limit the number of times a callback can be called, after which, the callback will be ignored.</p> <p><pre><code>from asyncio import Future\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)\n\ndef expensive_function(x: int) -&gt; int:\n    return x ** 2\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\n@scheduler.on_future_result(max_calls=3)\ndef print_result(future, result) -&gt; None:\n    scheduler.submit(expensive_function, 2)\n\nscheduler.run()\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 4\n    @on_future_done 4\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 4\n    \u2514\u2500\u2500 def print_result(future, result) -&gt; 'None' (3)\n</code>\n</pre> </p> </p> <p>A callable which takes no arguments and returns a <code>bool</code>. The callback will only be called when the <code>when</code> callable returns <code>True</code>.</p> <p>Below is a rather contrived example, but it shows how we can use the <code>when</code> parameter to control when the callback is called.</p> <p><pre><code>import random\nfrom amltk.scheduling import Scheduler\n\nLOCALE = random.choice([\"English\", \"German\"])\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start(when=lambda: LOCALE == \"English\")\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n@scheduler.on_start(when=lambda: LOCALE == \"German\")\ndef print_guten_tag() -&gt; None:\n    print(\"guten tag\")\n\nscheduler.run()\n</code></pre> <p>guten tag  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u251c\u2500\u2500 def print_hello() -&gt; 'None'\n    \u2514\u2500\u2500 def print_guten_tag() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> <p>Only call the callback every <code>every</code> times the event is emitted. This includes the first time it's called.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\n# Print \"hello\" only every 2 times the scheduler starts.\n@scheduler.on_start(every=2)\ndef print_hello() -&gt; None:\n    print(\"hello\")\n\n# Run the scheduler 5 times\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\nscheduler.run()\n</code></pre> <p>hello hello  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 5\n    @on_start 5\n    \u2514\u2500\u2500 def print_hello() -&gt; 'None' (2)\n    @on_finishing 5\n    @on_finished 5\n    @on_stop\n    @on_timeout\n    @on_future_submitted\n    @on_future_done\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p>"},{"location":"reference/scheduling/events/#amltk.scheduling.events--emitter-subscribers-and-events","title":"Emitter, Subscribers and Events","text":"<p>This part of the documentation is not necessary to understand or use for AMLTK. People wishing to build tools upon AMLTK may still find this a useful component to add to their arsenal.</p> <p>The core of making this functionality work is the <code>Emitter</code>. Its purpose is to have <code>@events</code> that can be emitted and subscribed to. Classes like the <code>Scheduler</code> and <code>Task</code> carry around with them an <code>Emitter</code> to enable all of this functionality.</p> <p>Creating an <code>Emitter</code> is rather straight-forward, but we must also create <code>Events</code> that people can subscribe to.</p> <pre><code>from amltk.scheduling import Emitter, Event\nemitter = Emitter(\"my-emitter\")\n\nevent: Event[int] = Event(\"my-event\") # (1)!\n\n@emitter.on(event)\ndef my_callback(x: int) -&gt; None:\n    print(f\"Got {x}!\")\n\nemitter.emit(event, 42) # (2)!\n</code></pre> <ol> <li>The typing <code>Event[int]</code> is used to indicate that the event will be emitting     an integer. This is not necessary, but it is useful for type-checking and     documentation.</li> <li>The <code>emitter.emit(event, 42)</code> is used to emit the event. This will call     all the callbacks registered for the event, i.e. <code>my_callback()</code>.</li> </ol> <p>Independent Events</p> <p>Given a single <code>Emitter</code> and a single instance of an <code>Event</code>, there is no way to have different <code>@events</code> for callbacks. There are two options, both used extensively in AMLTK.</p> <p>The first is to have different <code>Events</code> quite naturally, i.e. you distinguish between different things that can happen. However, you often want to have different objects emit the same <code>Event</code> but have different callbacks for each object.</p> <p>This makes most sense in the context of a <code>Task</code> the <code>Event</code> instances are shared as class variables in the <code>Task</code> class, however a user likely want's to subscribe to the <code>Event</code> for a specific instance of the <code>Task</code>.</p> <p>This is where the second option comes in, in which each object carries around its own <code>Emitter</code> instance. This is how a user can subscribe to the same kind of <code>Event</code> but individually for each <code>Task</code>.</p> <p>However, to shield users from this and to create named access points for users to subscribe to, we can use the <code>Subscriber</code> class, conveniently created by the <code>Emitter.subscriber()</code> method.</p> <pre><code>from amltk.scheduling import Emitter, Event\nemitter = Emitter(\"my-emitter\")\n\nclass GPT:\n\n    event: Event[str] = Event(\"my-event\")\n\n    def __init__(self) -&gt; None:\n        self.on_answer: Subscriber[str] = emitter.subscriber(self.event)\n\n    def ask(self, question: str) -&gt; None:\n        emitter.emit(self.event, \"hello world!\")\n\ngpt = GPT()\n\n@gpt.on_answer\ndef print_answer(answer: str) -&gt; None:\n    print(answer)\n\ngpt.ask(\"What is the conical way for an AI to greet someone?\")\n</code></pre> <p>Typically these event based systems make little sense in a synchronous context, however with the <code>Scheduler</code> and <code>Task</code> classes, they are used to enable a simple way to use multiprocessing and remote compute.</p>"},{"location":"reference/scheduling/executors/","title":"Executors","text":""},{"location":"reference/scheduling/executors/#executors","title":"Executors","text":"<p>The <code>Scheduler</code> uses an <code>Executor</code>, a builtin python native to <code>submit(f, *args, **kwargs)</code> to be computed else where, whether it be locally or remotely.</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler(executor=...)\n</code></pre> <p>Some parallelism libraries natively support this interface while we can wrap others. You can also wrap you own custom backend by using the <code>Executor</code> interface, which is relatively simple to implement.</p> <p>If there's any executor background you wish to integrate, we would be happy to consider it and greatly appreciate a PR!</p>"},{"location":"reference/scheduling/executors/#python","title":"<code>Python</code>","text":"<p>Python supports the <code>Executor</code> interface natively with the <code>concurrent.futures</code> module for processes with the <code>ProcessPoolExecutor</code> and <code>ThreadPoolExecutor</code> for threads.</p> Usage Process Pool ExecutorThread Pool Executor <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(2)  # (1)!\n</code></pre> <ol> <li>Explicitly use the <code>with_processes</code> method to create a <code>Scheduler</code> with    a <code>ProcessPoolExecutor</code> with 2 workers.    <pre><code> from concurrent.futures import ProcessPoolExecutor\n from amltk.scheduling import Scheduler\n\n executor = ProcessPoolExecutor(max_workers=2)\n scheduler = Scheduler(executor=executor)\n</code></pre></li> </ol> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_threads(2)  # (1)!\n</code></pre> <ol> <li>Explicitly use the <code>with_threads</code> method to create a <code>Scheduler</code> with    a <code>ThreadPoolExecutor</code> with 2 workers.    <pre><code> from concurrent.futures import ThreadPoolExecutor\n from amltk.scheduling import Scheduler\n\n executor = ThreadPoolExecutor(max_workers=2)\n scheduler = Scheduler(executor=executor)\n</code></pre></li> </ol> <p>Why to not use threads</p> <p>Python also defines a <code>ThreadPoolExecutor</code> but there are some known drawbacks to offloading heavy compute to threads. Notably, there's no way in python to terminate a thread from the outside while it's running.</p>"},{"location":"reference/scheduling/executors/#dask","title":"<code>dask</code>","text":"<p>Dask and the supporting extension <code>dask.distributed</code> provide a robust and flexible framework for scheduling compute across workers.</p> <p>Example</p> <pre><code>from dask.distributed import Client\nfrom amltk.scheduling import Scheduler\n\nclient = Client(...)\nexecutor = client.get_executor()\nscheduler = Scheduler(executor=executor)\n</code></pre>"},{"location":"reference/scheduling/executors/#dask-jobqueue","title":"<code>dask-jobqueue</code>","text":"<p><code>dask-jobqueue</code> is a package for scheduling jobs across common clusters setups such as PBS, Slurm, MOAB, SGE, LSF, and HTCondor.</p> <p>Please see the <code>dask-jobqueue</code> documentation In particular, we only control the parameter <code>n_workers=</code> and use <code>adaptive=</code> to control where to use <code>adapt()</code> or <code>scale()</code> method, every other keyword is forwarded to the relative cluster implementation.</p> <p>In general, you should specify the requirements of each individual worker and tune your load with the <code>n_workers=</code> parameter.</p> <p>If you have any tips, tricks, working setups, gotchas, please feel free to leave a PR or simply an issue!</p> Usage SlurmOthers <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_slurm(\n    n_workers=10,  # (1)!\n    adaptive=True,\n    queue=...,\n    cores=4,\n    memory=\"6 GB\",\n    walltime=\"00:10:00\"\n)\n</code></pre> <ol> <li>The <code>n_workers</code> parameter is used to set the number of workers    to start with.    The <code>adapt()</code>    method will be called on the cluster to dynamically scale up to <code>n_workers=</code> based on    the load.    The <code>with_slurm</code> method will create a <code>SLURMCluster</code>    and pass it to the <code>Scheduler</code> constructor.    <pre><code>from dask_jobqueue import SLURMCluster\nfrom amltk.scheduling import Scheduler\n\ncluster = SLURMCluster(\n    queue=...,\n    cores=4,\n    memory=\"6 GB\",\n    walltime=\"00:10:00\"\n)\ncluster.adapt(max_workers=10)\nexecutor = cluster.get_client().get_executor()\nscheduler = Scheduler(executor=executor)\n</code></pre></li> </ol> <p>Running outside the login node</p> <p>If you're running the scheduler itself in a job, this may not work on some cluster setups. The scheduler itself is lightweight and can run on the login node without issue. However you should make sure to offload heavy computations to a worker.</p> <p>If you get it to work, for example in an interactive job, please let us know!</p> <p>Modifying the launch command</p> <p>On some cluster commands, you'll need to modify the launch command. You can use the following to do so:</p> <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_slurm(n_workers=..., submit_command=\"sbatch --extra\"\n</code></pre> <p>Please see the <code>dask-jobqueue</code> documentation and the following methods:</p> <ul> <li><code>Scheduler.with_pbs()</code></li> <li><code>Scheduler.with_lsf()</code></li> <li><code>Scheduler.with_moab()</code></li> <li><code>Scheduler.with_sge()</code></li> <li><code>Scheduler.with_htcondor()</code></li> </ul>"},{"location":"reference/scheduling/executors/#loky","title":"<code>loky</code>","text":"<p>Loky is the default backend executor behind <code>joblib</code>, the parallelism that powers scikit-learn.</p> Usage SimpleExplicit <pre><code>from amltk import Scheduler\n\n# Pass any arguments you would pass to `loky.get_reusable_executor`\nscheduler = Scheduler.with_loky(...)\n</code></pre> <pre><code>import loky\nfrom amltk import Scheduler\n\nscheduler = Scheduler(executor=loky.get_reusable_executor(...))\n</code></pre> BLAS numeric backend <p>The loky executor seems to pick up on a different BLAS library (from scipy) which is different than those used by jobs from something like a <code>ProcessPoolExecutor</code>.</p> <p>This is likely not to matter for a majority of use-cases.</p>"},{"location":"reference/scheduling/executors/#ray","title":"<code>ray</code>","text":"<p>Ray is an open-source unified compute framework that makes it easy to scale AI and Python workloads \u2014 from reinforcement learning to deep learning to tuning, and model serving.</p> <p>In progress</p> <p>Ray is currently in the works of supporting the Python <code>Executor</code> interface. See this PR for more info.</p>"},{"location":"reference/scheduling/executors/#airflow","title":"<code>airflow</code>","text":"<p>Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. Their list of integrations to platforms is endless but features compute platforms such as Kubernetes, AWS, Microsoft Azure and GCP.</p> <p>In progress</p> <p>We plan to support <code>airflow</code> in the future. If you'd like to help out, please reach out to us!</p>"},{"location":"reference/scheduling/executors/#debugging","title":"Debugging","text":"<p>Sometimes you'll need to debug what's going on and remove the noise of processes and parallelism. For this, we have implemented a very basic <code>SequentialExecutor</code> to run everything in a sequential manner!</p> EasyExplicit <pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_sequential()\n</code></pre> <pre><code>from amltk.scheduling import Scheduler, SequetialExecutor\n\nscheduler = Scheduler(executor=SequentialExecutor())\n</code></pre> <p>Recursion</p> <p>If you use The <code>SequentialExecutor</code>, be careful that the stack of function calls can get quite large, quite quick. If you are using this for debugging, keep the number of submitted tasks from callbacks small and focus in on debugging. If using this for sequential ordering of operations, prefer to use <code>with_processes(1)</code> as this will still maintain order but not have these stack issues.</p>"},{"location":"reference/scheduling/plugins/","title":"Plugins","text":""},{"location":"reference/scheduling/plugins/#plugins","title":"Plugins","text":"<p>Plugins are a way to modify a <code>Task</code>, to add new functionality or change the behaviour of what goes on in the function that is dispatched to the <code>Scheduler</code>.</p> <p>Some plugins will also add new <code>@event</code>s to a task, which can be used to respond accordingly to something that may have occured with your task.</p> <p>You can add a plugin to a <code>Task</code> as so:</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef some_function(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(1)\n\n# When creating a task with the scheduler\ntask = scheduler.task(some_function, plugins=[Limiter(max_calls=10)])\n\n\n# or directly to a Task\ntask = Task(some_function, scheduler=scheduler, plugins=[Limiter(max_calls=10)])\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task some_function(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/10                                                               \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: aMJxFrXE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"reference/scheduling/plugins/#limiter","title":"Limiter","text":"<p>The <code>Limiter</code> can limit the number of times a function is called, how many concurrent instances of it can be running, or whether it can run while another task is running.</p> <p>The functionality of the <code>Limiter</code> could also be implemented without a plugin but it gives some nice utility.</p> Usage <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: prZZdvqu \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <code>@events</code> <code>@call-limit-reached</code><code>@concurrent-limit-reached</code><code>@disabled-due-to-running-task</code> <p>The event emitted when the task has reached its call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_calls=2)])\n\n@task.on(\"call-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Calls 0/2                                                                \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 7Y5guxau \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The event emitted when the task has reached its concurrent call limit.</p> <p>Will call any subscribers with the task as the first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\ntask = scheduler.task(fn, plugins=[Limiter(max_concurrent=2)])\n\n@task.on(\"concurrent-limit-reached\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Concurrent 0/2                                                           \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: ZnBZdYUT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The event emitter when the task was not submitted due to some other running task.</p> <p>Will call any subscribers with the task as first argument, followed by the arguments and keyword arguments that were passed to the task.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Limiter\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(2)\n\nother_task = scheduler.task(fn)\ntask = scheduler.task(fn, plugins=[Limiter(not_while_running=other_task)])\n\n@task.on(\"disabled-due-to-running-task\")\ndef callback(task: Task, *args, **kwargs):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin limiter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Not While def fn(...) Ref: KQU5C3aA                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @call-limit-reached                                                          \u2502\n\u2502 @concurrent-limit-reached                                                    \u2502\n\u2502 @disabled-due-to-running-task                                                \u2502\n\u2502 \u2514\u2500\u2500 def callback(task: 'Task', *args, **kwargs)                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: NVJe3uIT \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p>"},{"location":"reference/scheduling/plugins/#pynisher","title":"Pynisher","text":"<p>The <code>PynisherPlugin</code> uses pynisher to place memory, walltime and cputime constraints on processes, crashing them if these limits are reached. These default units are <code>bytes (\"B\")</code> and <code>seconds (\"s\")</code> but you can also use other units, please see the relevant API doc.</p> <p>It's best use is when used with <code>Scheduler.with_processes()</code> to have work performed in processes.</p> <p>Requirements</p> <p>This required <code>pynisher</code> which can be installed with:</p> <pre><code>pip install amltk[pynisher]\n\n# Or directly\npip install pynisher\n</code></pre> Usage <p><pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 3OWoECH8 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <code>@events</code> <code>@pynisher-timeout</code><code>@pynisher-memory-limit</code><code>@pynisher-cputime-limit</code><code>@pynisher-walltime-limit</code> <p>A Task timed out, either due to the wall time or cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-timeout\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 2RVF8GFC \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task was submitted but reached it's memory limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport numpy as np\n\ndef f(x: int) -&gt; int:\n    x = np.arange(100000000)\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(memory_limit=(1, \"KB\")))\n\n@task.on(\"pynisher-memory-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory     Wall Time  CPU Time                                          \u2502 \u2502\n\u2502 \u2502  (1, 'KB')  None       None                                              \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: s2sIhvVr \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task was submitted but reached it's cpu time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    i = 0\n    while True:\n        # Keep busying computing the answer to everything\n        i += 1\n\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(cputime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-cputime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    None       (1, 's')                                             \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: M7urRgQ7 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task was submitted but reached it's wall time limit.</p> <p>Will call any subscribers with the exception as the argument.</p> <pre><code>from amltk.scheduling import Task, Scheduler\nfrom amltk.scheduling.plugins.pynisher import PynisherPlugin\nimport time\n\ndef f(x: int) -&gt; int:\n    time.sleep(x)\n    return 42\n\nscheduler = Scheduler.with_processes()\ntask = scheduler.task(f, plugins=PynisherPlugin(walltime_limit=(1, \"s\")))\n\n@task.on(\"pynisher-walltime-limit\")\ndef callback(exception):\n    pass\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin pynisher-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Memory  Wall Time  CPU Time                                             \u2502 \u2502\n\u2502 \u2502  None    (1, 's')   None                                                 \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @pynisher-timeout                                                            \u2502\n\u2502 @pynisher-memory-limit                                                       \u2502\n\u2502 @pynisher-cputime-limit                                                      \u2502\n\u2502 @pynisher-walltime-limit                                                     \u2502\n\u2502 \u2514\u2500\u2500 def callback(exception)                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: lZNA7RGa \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> Scheduler Executor <p>This will place process limits on the task as soon as it starts running, whever it may be running. If you are using <code>Scheduler.with_sequential()</code> then this will place limits on the main process, likely not what you want. This also does not work with a <code>ThreadPoolExecutor</code>.</p> <p>If using this with something like [<code>dask-jobqueue</code>], then this will place limits on the workers it spawns. It would be better to place limits directly through dask job-queue then.</p> Platform Limitations (Mac, Windows) <p>Pynisher has some limitations with memory on Mac and Windows: automl/pynisher#features</p> <p>You can check this with <code>PynisherPlugin.supports(\"memory\")</code>, <code>PynisherPlugin.supports(\"cpu_time\")</code> and <code>PynisherPlugin.supports(\"wall_time\")</code>. See <code>PynisherPlugin.supports()</code></p>"},{"location":"reference/scheduling/plugins/#comm","title":"Comm","text":"<p>The <code>Comm.Plugin</code> enables two way-communication with running <code>Task</code>.</p> <p>The <code>Comm</code> provides an easy interface to communicate while the <code>Comm.Msg</code> encapsulates messages between the main process and the <code>Task</code>.</p> Usage <p>To setup a <code>Task</code> to work with a <code>Comm</code>, the <code>Task</code> must accept a <code>comm</code> as it's first argument.</p> <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef powers_of_two(comm: Comm, start: int, n: int) -&gt; None:\n    with comm.open():\n        for i in range(n):\n            comm.send(start ** (i+1))\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(powers_of_two, plugins=Comm.Plugin())\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(2, 5)\n\n@task.on(\"comm-open\")\ndef on_open(msg: Comm.Msg):\n    print(f\"Task has opened | {msg}\")\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>Task has opened | Comm.Msg(kind=&lt;Kind.OPEN: 'open'&gt;, data=None)\n[2, 4, 8, 16, 32]\n</code></pre> </p> <p>You can also block a worker, waiting for a response from the main process, allowing for the worker to <code>request()</code> data from the main process.</p> <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef my_worker(comm: Comm, n_tasks: int) -&gt; None:\n    with comm.open():\n        for task_number in range(n_tasks):\n            task = comm.request(task_number)\n            comm.send(f\"Task recieved {task} for {task_number}\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(my_worker, plugins=Comm.Plugin())\n\nitems = [\"A\", \"B\", \"C\"]\nresults = []\n\n@scheduler.on_start\ndef on_start():\n    task.submit(n_tasks=3)\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    task_number = msg.data\n    msg.respond(items[task_number])\n\n@task.on(\"comm-message\")\ndef on_message(msg: Comm.Msg):\n    results.append(msg.data)\n\nscheduler.run()\nprint(results)\n</code></pre> <pre><code>['Task recieved A for 0', 'Task recieved B for 1', 'Task recieved C for 2']\n</code></pre> </p> <code>@events</code> <code>@comm-message</code><code>@comm-request</code><code>@comm-open</code><code>@comm-close</code> <p>A Task has sent a message to the main process.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm, x: int) -&gt; int:\n    with comm.open():\n        comm.send(x + 1)\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@task.on(\"comm-message\")\ndef callback(msg: Comm.Msg):\n    print(msg.data)\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task fn(comm: 'Comm', x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message                                                                \u2502\n\u2502 \u2514\u2500\u2500 def callback(msg: 'Comm.Msg')                                            \u2502\n\u2502 @comm-request                                                                \u2502\n\u2502 @comm-open                                                                   \u2502\n\u2502 @comm-close                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: MzZGKHww \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>A Task has sent a request.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef greeter(comm: Comm, greeting: str) -&gt; None:\n    with comm.open():\n        name = comm.request()\n        comm.send(f\"{greeting} {name}!\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(greeter, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit(\"Hello\")\n\n@task.on(\"comm-request\")\ndef on_request(msg: Comm.Msg):\n    msg.respond(\"Alice\")\n\n@task.on(\"comm-message\")\ndef on_msg(msg: Comm.Msg):\n    print(msg.data)\n\nscheduler.run()\n</code></pre> <p>Hello Alice!  <pre>\n<code>\u256d\u2500 Task greeter(comm: 'Comm', greeting: 'str') -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message 1                                                              \u2502\n\u2502 \u2514\u2500\u2500 def on_msg(msg: 'Comm.Msg') (1)                                          \u2502\n\u2502 @comm-request 1                                                              \u2502\n\u2502 \u2514\u2500\u2500 def on_request(msg: 'Comm.Msg') (1)                                      \u2502\n\u2502 @comm-open 1                                                                 \u2502\n\u2502 @comm-close 1                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: NXQ81n27 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> <p>The task has signalled it's open.</p> <p>```python exec=\"true\" source=\"material-block\" html=\"true\" hl_lines=\"5 15-17\"  from amltk.scheduling import Scheduler  from amltk.scheduling.plugins import Comm</p> <p>def fn(comm: Comm) -&gt; None:      with comm.open():          pass from amltk._doc import make_picklable; make_picklable(fn)  # markdown-exec: hide</p> <p>scheduler = Scheduler.with_processes(1)  task = scheduler.task(fn, plugins=Comm.Plugin())</p> <p>@scheduler.on_start  def on_start():      task.submit()</p> <p>@task.on(\"comm-open\")  def callback(msg: Comm.Msg):      print(\"Comm has just used comm.open()\")</p> <p>scheduler.run()  from amltk._doc import doc_print; doc_print(print, task)  # markdown-exec: hide  ```</p> <p>The task has signalled it's close.</p> <pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Comm\n\ndef fn(comm: Comm) -&gt; None:\n    with comm.open():\n        pass\n        # Will send a close signal to the main process as it exists this block\n\n    print(\"Done\")\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=Comm.Plugin())\n\n@scheduler.on_start\ndef on_start():\n    task.submit()\n\n@task.on(\"comm-close\")\ndef on_close(msg: Comm.msg):\n    print(f\"Worker close with {msg}\")\n\nscheduler.run()\n</code></pre> <p>Worker close with Comm.Msg(kind=, data=None)  <pre>\n<code>\u256d\u2500 Task fn(comm: 'Comm') -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin comm-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Open Connections: 0                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @comm-message                                                                \u2502\n\u2502 @comm-request                                                                \u2502\n\u2502 @comm-open 1                                                                 \u2502\n\u2502 @comm-close 1                                                                \u2502\n\u2502 \u2514\u2500\u2500 def on_close(msg: 'Comm.msg') (1)                                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: Vn4vUSn5 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> Supported Backends <p>The current implementation relies on <code>Pipe</code> which only works between processes on the same system/cluster. There is also limited support with <code>dask</code> backends.</p> <p>This could be extended to allow for web sockets or other forms of connections but requires time. Please let us know in the Github issues if this is something you are interested in!</p>"},{"location":"reference/scheduling/plugins/#threadpoolctl","title":"ThreadPoolCTL","text":"<p>The <code>ThreadPoolCTLPlugin</code> if useful for parallel training of models. Without limiting with threadpoolctl, the number of threads used by a given model may oversubscribe to resources and cause significant slowdowns.</p> <p>This is the mechanism employed by scikit-learn to limit the number of threads used by a given model.</p> <p>See threadpoolctl documentation.</p> <p>Requirements</p> <p>This requires <code>threadpoolctl</code> which can be installed with:</p> <pre><code>pip install amltk[threadpoolctl]\n\n# Or directly\npip install threadpoolctl\n</code></pre> Usage <p><pre><code>from amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins.threadpoolctl import ThreadPoolCTLPlugin\n\nscheduler = Scheduler.with_processes(1)\n\ndef f() -&gt; None:\n    # ... some task that respects the limits set by threadpoolctl\n    pass\n\ntask = scheduler.task(f, plugins=ThreadPoolCTLPlugin(max_threads=1))\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f() -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin threadpoolctl-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Max Threads  User-API                                                   \u2502 \u2502\n\u2502 \u2502  1            None                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 2LBGRARC \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p>"},{"location":"reference/scheduling/plugins/#warning-filter","title":"Warning Filter","text":"<p>The <code>WarningFilter</code> if used to automatically filter out warnings from a <code>Task</code> as it runs.</p> <p>This wraps your function in context manager <code>warnings.catch_warnings()</code> and applies your arguments to <code>warnings.filterwarnings()</code>, as you would normally filter warnings in Python.</p> Usage <p><pre><code>import warnings\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import WarningFilter\n\ndef f() -&gt; None:\n    warnings.warn(\"This is a warning\")\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(f, plugins=WarningFilter(\"ignore\"))\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Task f() -&gt; 'None' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin warning-filter \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502  Args         Kwargs                                                     \u2502 \u2502\n\u2502 \u2502  ('ignore',)  {}                                                         \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted                                                                \u2502\n\u2502 @on_done                                                                     \u2502\n\u2502 @on_result                                                                   \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: 1UAO16Yr \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p>"},{"location":"reference/scheduling/plugins/#creating-your-own-plugin","title":"Creating Your Own Plugin","text":"<p>A plugin that can be attached to a Task.</p> <p>By inheriting from a <code>Plugin</code>, you can hook into a <code>Task</code>. A plugin can affect, modify and extend its behaviours. Please see the documentation of the methods for more information. Creating a plugin is only necesary if you need to modify actual behaviour of the task. For siply hooking into the lifecycle of a task, you can use the <code>@events</code> that a <code>Task</code> emits.</p> Creating a Plugin <p>For a full example of a simple plugin, see the <code>Limiter</code> plugin which prevents the task being submitted if for example, it has already been submitted too many times.</p> <p>The below example shows how to create a plugin that prints the task name before submitting it. It also emits an event when the task is submitted.</p> <p><pre><code>from __future__ import annotations\nfrom typing import Callable\n\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.plugins import Plugin\nfrom amltk.scheduling.events import Event\n\n# A simple plugin that prints the task name before submitting\nclass Printer(Plugin):\n    name = \"my-plugin\"\n\n    # Define an event the plugin will emit\n    # Event[Task] indicates the callback for the event will be called with the task\n    PRINTED: Event[str] = Event(\"printer-msg\")\n\n    def __init__(self, greeting: str):\n        self.greeting = greeting\n        self.n_greetings = 0\n\n    def attach_task(self, task) -&gt; None:\n        self.task = task\n        # Register an event with the task, this lets the task know valid events\n        # people can subscribe to and helps it show up in visuals\n        task.emitter.add_event(self.PRINTED)\n        task.on_submitted(self._print_submitted, hidden=True)  # You can hide this callback from visuals\n\n    def pre_submit(self, fn, *args, **kwargs) -&gt; tuple[Callable, tuple, dict]:\n        print(f\"{self.greeting} for {self.task} {args} {kwargs}\")\n        self.n_greetings += 1\n        return fn, args, kwargs\n\n    def _print_submitted(self, future, *args, **kwargs) -&gt; None:\n        msg = f\"Task was submitted {self.task} {args} {kwargs}\"\n        self.task.emitter.emit(self.PRINTED, msg)  # Emit the event with a msg\n\n    def copy(self) -&gt; Printer:\n        # Plugins need to be able to copy themselves as if fresh\n        return self.__class__(self.greeting)\n\n    def __rich__(self):\n        # Custome how the plugin is displayed in rich (Optional)\n        # rich is an optional dependancy of amltk so we move the imports into here\n        from rich.panel import Panel\n\n        return Panel(\n            f\"Greeting: {self.greeting} ({self.n_greetings})\",\n            title=f\"Plugin {self.name}\"\n        )\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\ntask = scheduler.task(fn, plugins=[Printer(\"Hello\")])\n\n@scheduler.on_start\ndef on_start():\n    task.submit(15)\n\n@task.on(\"printer-msg\")\ndef callback(msg: str):\n    print(\"\\nmsg\")\n\nscheduler.run()\n</code></pre> <p>Hello for Task(unique_ref=I50G2OoS) (15,) {}  msg  <pre>\n<code>\u256d\u2500 Task fn(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Plugin my-plugin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502 Greeting: Hello (1)                                                      \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502 @on_submitted 1                                                              \u2502\n\u2502 @on_done 1                                                                   \u2502\n\u2502 @on_result 1                                                                 \u2502\n\u2502 @on_exception                                                                \u2502\n\u2502 @on_cancelled                                                                \u2502\n\u2502 @printer-msg 1                                                               \u2502\n\u2502 \u2514\u2500\u2500 def callback(msg: 'str') (1)                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: I50G2OoS \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> </p> </p> <p>All methods are optional, and you can choose to implement only the ones you need. Most plugins will likely need to implement the <code>attach_task()</code> method, which is called when the plugin is attached to a task. In this method, you can for example subscribe to events on the task, create new subscribers for people to use or even store a reference to the task for later use.</p> <p>Plugins are also encouraged to utilize the events of a <code>Task</code> to further hook into the lifecycle of the task. For exampe, by saving a reference to the task in the <code>attach_task()</code> method, you can use the <code>emit()</code> method of the task to emit your own specialized events.</p>"},{"location":"reference/scheduling/queue_monitor/","title":"Queue monitor","text":""},{"location":"reference/scheduling/queue_monitor/#queue-monitor","title":"Queue Monitor","text":"<p>A <code>QueueMonitor</code> is a monitor for the scheduler queue.</p> <p>This module contains a monitor for the scheduler queue. The monitor tracks the queue state at every event emitted by the scheduler. The data can be converted to a pandas DataFrame or plotted as a stacked barchart.</p> <p>Monitoring Frequency</p> <p>To prevent repeated polling, we sample the scheduler queue at every scheduler event. This is because the queue is only modified upon one of these events. This means we don't need to poll the queue at a fixed interval. However, if you need more fine grained updates, you can add extra events/timings at which the monitor should <code>update()</code>.</p> <p>Performance impact</p> <p>If your tasks and callbacks are very fast (~sub 10ms), then the monitor has a non-nelgible impact however for most use cases, this should not be a problem. As anything, you should profile how much work the scheduler can get done, with and without the monitor, to see if it is a problem for your use case.</p> <p>In the below example, we have a very fast running function that runs on repeat, sometimes too fast for the scheduler to keep up, letting some futures buildup needing to be processed.</p> <pre><code>import time\nimport matplotlib.pyplot as plt\nfrom amltk.scheduling import Scheduler\nfrom amltk.scheduling.queue_monitor import QueueMonitor\n\ndef fast_function(x: int) -&gt; int:\n    return x + 1\n\nN_WORKERS = 2\nscheduler = Scheduler.with_processes(N_WORKERS)\nmonitor = QueueMonitor(scheduler)\ntask = scheduler.task(fast_function)\n\n@scheduler.on_start(repeat=N_WORKERS)\ndef start():\n    task.submit(1)\n\n@task.on_result\ndef result(_, x: int):\n    if scheduler.running():\n        task.submit(x)\n\nscheduler.run(timeout=1)\ndf = monitor.df()\nprint(df)\n</code></pre> <pre><code>                               queue_size  queued  finished  cancelled  idle\ntime                                                                        \n2023-12-12 03:41:13.169179902           0       0         0          0     2\n2023-12-12 03:41:13.186093388           1       1         0          0     1\n2023-12-12 03:41:13.186466001           2       2         0          0     0\n2023-12-12 03:41:13.197877040           1       1         0          0     1\n2023-12-12 03:41:13.197985782           2       2         0          0     0\n...                                   ...     ...       ...        ...   ...\n2023-12-12 03:41:14.187208871           2       2         0          0     0\n2023-12-12 03:41:14.187420976           2       2         0          0     0\n2023-12-12 03:41:14.198585847           2       2         0          0     0\n2023-12-12 03:41:14.198851751           1       0         1          0     1\n2023-12-12 03:41:14.198894270           0       0         0          0     2\n\n[1897 rows x 5 columns]\n</code></pre> <p>We can also <code>plot()</code> the data as a stacked barchart with a set interval.</p> <pre><code>fig, ax = plt.subplots()\nmonitor.plot(interval=(50, \"ms\"))\n</code></pre> <p> 2023-12-12T03:41:14.348260 image/svg+xml Matplotlib v3.8.2, https://matplotlib.org/ </p>"},{"location":"reference/scheduling/scheduler/","title":"Scheduler","text":""},{"location":"reference/scheduling/scheduler/#scheduler","title":"Scheduler","text":"<p>The <code>Scheduler</code> uses an <code>Executor</code>, a builtin python native with a <code>submit(f, *args, **kwargs)</code> function to submit compute to be compute else where, whether it be locally or remotely.</p> <p>The <code>Scheduler</code> is primarily used to dispatch compute to an <code>Executor</code> and emit <code>@events</code>, which can trigger user callbacks.</p> <p>Typically you should not use the <code>Scheduler</code> directly for dispatching and responding to computed functions, but rather use a <code>Task</code></p> Running in a Jupyter Notebook/Colab <p>If you are using a Jupyter Notebook, you likley need to use the following at the top of your notebook:</p> <pre><code>import nest_asyncio  # Only necessary in Notebooks\nnest_asyncio.apply()\n\nscheduler.run(...)\n</code></pre> <p>This is due to the fact a notebook runs in an async context. If you do not wish to use the above snippet, you can instead use:</p> <pre><code>await scheduler.async_run(...)\n</code></pre> Basic Usage <p>In this example, we create a scheduler that uses local processes as workers. We then create a task that will run a function <code>fn</code> and submit it to the scheduler. Lastly, a callback is registered to <code>@on_future_result</code> to print the result when the compute is done.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\ndef fn(x: int) -&gt; int:\n    return x + 1\n\nscheduler = Scheduler.with_processes(1)\n\n@scheduler.on_start\ndef launch_the_compute():\n    scheduler.submit(fn, 1)\n\n@scheduler.on_future_result\ndef callback(future, result):\n    print(f\"Result: {result}\")\n\nscheduler.run()\n</code></pre> <p>Result: 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty 1\n    @on_start 1\n    \u2514\u2500\u2500 def launch_the_compute() (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout\n    @on_future_submitted 1\n    @on_future_done 1\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 1\n    \u2514\u2500\u2500 def callback(future, result) (1)\n</code>\n</pre> </p> </p> <p>The last line in the previous example called <code>scheduler.run()</code> is what starts the scheduler running, in which it will first emit the <code>@on_start</code> event. This triggered the callback <code>launch_the_compute()</code> which submitted the function <code>fn</code> with the arguments <code>1</code>.</p> <p>The scheduler then ran the compute and waited for it to complete, emitting the <code>@on_future_result</code> event when it was done successfully. This triggered the callback <code>callback()</code> which printed the result.</p> <p>At this point, there is no more compute happening and no more events to respond to so the scheduler will halt.</p> <code>@events</code> Scheduler Status EventsSubmitted Compute Events <p>When the scheduler enters some important state, it will emit an event to let you know.</p> <code>@on_start</code><code>@on_finishing</code><code>@on_finished</code><code>@on_stop</code><code>@on_timeout</code><code>@on_empty</code> <p>A <code>Subscriber</code> which is called when the scheduler starts. This is the first event emitted by the scheduler and one of the only ways to submit the initial compute to the scheduler.</p> <pre><code>@scheduler.on_start\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finishing up. This occurs right before the scheduler shuts down the executor.</p> <pre><code>@scheduler.on_finishing\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is finished, has shutdown the executor and possibly terminated any remaining compute.</p> <pre><code>@scheduler.on_finished\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler is has been stopped due to the <code>stop()</code> method being called.</p> <pre><code>@scheduler.on_stop\ndef my_callback(stop_msg: str, exception: BaseException | None):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the scheduler reaches the timeout.</p> <pre><code>@scheduler.on_timeout\ndef my_callback():\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when the queue is empty. This can be useful to re-fill the queue and prevent the scheduler from exiting.</p> <pre><code>@scheduler.on_empty\ndef my_callback():\n    ...\n</code></pre> <p>When any compute goes through the <code>Scheduler</code>, it will emit an event to let you know. You should however prefer to use a <code>Task</code> as it will emit specific events for the task at hand, and not all compute.</p> <code>@on_future_submitted</code><code>@on_future_result</code><code>@on_future_exception</code><code>@on_future_done</code><code>@on_future_cancelled</code> <p>A <code>Subscriber</code> which is called when some compute is submitted.</p> <pre><code>@scheduler.on_future_submitted\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future returned with a result, no exception raise.</p> <pre><code>@scheduler.on_future_result\ndef my_callback(future: Future, result: Any):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute raised an uncaught exception.</p> <pre><code>@scheduler.on_future_exception\ndef my_callback(future: Future, exception: BaseException):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when some compute is done, regardless of whether it was successful or not.</p> <pre><code>@scheduler.on_future_done\ndef my_callback(future: Future):\n    ...\n</code></pre> <p>A <code>Subscriber</code> which is called when a future is cancelled. This usually occurs due to the underlying Scheduler, and is not something we do directly, other than when shutting down the scheduler.</p> <pre><code>@scheduler.on_future_cancelled\ndef my_callback(future: Future):\n    ...\n</code></pre> Common usages of <code>run()</code> <p>There are various ways to <code>run()</code> the scheduler, notably how long it should run with <code>timeout=</code> and also how it should react to any exception that may have occurred within the <code>Scheduler</code> itself or your callbacks.</p> <p>Please see the <code>run()</code> API doc for more details and features, however we show two common use cases of using the <code>timeout=</code> parameter.</p> <p>You can render a live display using <code>run(display=...)</code>. This require <code>rich</code> to be installed. You can install this with <code>pip install rich</code> or <code>pip install amltk[rich]</code>.</p> <code>run(timeout=...)</code><code>run(timeout=..., wait=False)</code> <p>You can tell the <code>Scheduler</code> to stop after a certain amount of time with the <code>timeout=</code> argument to <code>run()</code>.</p> <p>This will also trigger the <code>@on_timeout</code> event as seen in the <code>Scheduler</code> output.</p> <p><pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; int:\n    time.sleep(0.1)\n    return 42\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\n# The will endlessly loop the scheduler\n@scheduler.on_future_done\ndef submit_again(future: Future) -&gt; None:\n    if scheduler.running():\n        scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1)  # End after 1 second\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 10\n    @on_future_done 10\n    \u2514\u2500\u2500 def submit_again(future: 'Future') -&gt; 'None' (10)\n    @on_future_cancelled\n    @on_future_exception\n    @on_future_result 10\n</code>\n</pre> </p> </p> <p>By specifying that the <code>Scheduler</code> should not wait for ongoing tasks to finish, the <code>Scheduler</code> will attempt to cancel and possibly terminate any running tasks.</p> <p><pre><code>import time\nfrom amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef expensive_function() -&gt; None:\n    time.sleep(10)\n\n\n@scheduler.on_start\ndef submit_calculations() -&gt; None:\n    scheduler.submit(expensive_function)\n\nscheduler.run(timeout=1, wait=False)  # End after 1 second\n</code></pre> <p> <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 1}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2517\u2501\u2501 @on_empty\n    @on_start 1\n    \u2514\u2500\u2500 def submit_calculations() -&gt; 'None' (1)\n    @on_finishing 1\n    @on_finished 1\n    @on_stop\n    @on_timeout 1\n    @on_future_submitted 1\n    @on_future_done\n    @on_future_cancelled 1\n    @on_future_exception\n    @on_future_result\n</code>\n</pre> </p> </p> Forcibly Terminating Workers <p>As an <code>Executor</code> does not provide an interface to forcibly terminate workers, we provide <code>Scheduler(terminate=...)</code> as a custom strategy for cleaning up a provided executor. It is not possible to terminate running thread based workers, for example using <code>ThreadPoolExecutor</code> and any Executor using threads to spawn tasks will have to wait until all running tasks are finish before python can close.</p> <p>It's likely <code>terminate</code> will trigger the <code>EXCEPTION</code> event for any tasks that are running during the shutdown, not* a cancelled event. This is because we use a <code>Future</code> under the hood and these can not be cancelled once running. However there is no guarantee of this and is up to how the <code>Executor</code> handles this.</p> Scheduling something to be run later <p>You can schedule some function to be run later using the <code>scheduler.call_later()</code> method.</p> <p>Note</p> <p>This does not run the function in the background, it just schedules some function to be called later, where you could perhaps then use submit to scheduler a <code>Task</code> to run the function in the background.</p> <p><pre><code>from amltk.scheduling import Scheduler\n\nscheduler = Scheduler.with_processes(1)\n\ndef fn() -&gt; int:\n    print(\"Ending now!\")\n    scheduler.stop()\n\n@scheduler.on_start\ndef schedule_fn() -&gt; None:\n    scheduler.call_later(1, fn)\n\nscheduler.run(end_on_empty=False)\n</code></pre> <pre><code>Ending now!\n</code></pre> </p>"},{"location":"reference/scheduling/task/","title":"Task","text":""},{"location":"reference/scheduling/task/#tasks","title":"Tasks","text":"<p>A <code>Task</code> is a unit of work that can be scheduled by the <code>Scheduler</code>.</p> <p>It is defined by its <code>function=</code> to call. Whenever a <code>Task</code> has its <code>submit()</code> method called, the function will be dispatched to run by a <code>Scheduler</code>.</p> <p>When a task has returned, either successfully, or with an exception, it will emit <code>@events</code> to indicate so. You can subscribe to these events with callbacks and act accordingly.</p> <code>@events</code> <p>Check out the <code>@events</code> reference for more on how to customize these callbacks. You can also take a look at the API of <code>on()</code> for more information.</p> <code>@on_result</code><code>@on_exception</code><code>@on_done</code><code>@on_submitted</code><code>@on_cancelled</code> <p>Called when a task has successfully returned a value. Comes with Future <pre><code>@task.on_result\ndef on_result(future: Future[R], result: R):\n    print(f\"Future {future} returned {result}\")\n</code></pre></p> <p>Called when a task failed to return anything but an exception. Comes with Future <pre><code>@task.on_exception\ndef on_exception(future: Future[R], error: BaseException):\n    print(f\"Future {future} exceptioned {error}\")\n</code></pre></p> <p>Called when a task is done running with a result or exception. <pre><code>@task.on_done\ndef on_done(future: Future[R]):\n    print(f\"Future {future} is done\")\n</code></pre></p> <p>An event that is emitted when a future is submitted to the scheduler. It will pass the future as the first argument with args and kwargs following.</p> <p>This is done before any callbacks are attached to the future. <pre><code>@task.on_submitted\ndef on_submitted(future: Future[R], *args, **kwargs):\n    print(f\"Future {future} was submitted with {args=} and {kwargs=}\")\n</code></pre></p> <p>Called when a task is cancelled. <pre><code>@task.on_cancelled\ndef on_cancelled(future: Future[R]):\n    print(f\"Future {future} was cancelled\")\n</code></pre></p> Usage <p>The usual way to create a task is with <code>Scheduler.task()</code>, where you provide the <code>function=</code> to call.</p> <p><pre><code>from amltk import Scheduler\n\ndef f(x: int) -&gt; int:\n    return x * 2\n\nscheduler = Scheduler.with_processes(2)\ntask = scheduler.task(f)\n\n@scheduler.on_start\ndef on_start():\n    task.submit(1)\n\n@task.on_result\ndef on_result(future: Future[int], result: int):\n    print(f\"Task {future} returned {result}\")\n\nscheduler.run()\n</code></pre> <p>Task  returned 2  <pre>\n<code>\u256d\u2500 Scheduler \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502  Executor                  Queue: (0)                                        \u2502\n\u2502  \u256d\u2500 ProcessPoolExecutor\u2500\u256e                                                    \u2502\n\u2502  \u2502 {'max_workers': 2}   \u2502                                                    \u2502\n\u2502  \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                                                    \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u2523\u2501\u2501 @on_empty 1\n\u2503   @on_start 1\n\u2503   \u2514\u2500\u2500 def on_start() (1)\n\u2503   @on_finishing 1\n\u2503   @on_finished 1\n\u2503   @on_stop\n\u2503   @on_timeout\n\u2503   @on_future_submitted 1\n\u2503   @on_future_done 1\n\u2503   @on_future_cancelled\n\u2503   @on_future_exception\n\u2503   @on_future_result 1\n\u2517\u2501\u2501 \u256d\u2500 Task f(x: 'int') -&gt; 'int' \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n    \u2502 @on_submitted 1                                                          \u2502\n    \u2502 @on_done 1                                                               \u2502\n    \u2502 @on_result 1                                                             \u2502\n    \u2502 \u2514\u2500\u2500 def on_result(future: 'Future[int]', result: 'int') (1)              \u2502\n    \u2502 @on_exception                                                            \u2502\n    \u2502 @on_cancelled                                                            \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Ref: aWSnDe7t \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code>\n</pre> <p>If you'd like to simply just call the original function, without submitting it to the scheduler, you can always just call the task directly, i.e. <code>task(1)</code>.</p> <p>You can also provide <code>Plugins</code> to the task, to modify tasks, add functionality and add new events.</p>"}]}